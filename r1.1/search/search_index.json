{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "installation.html", "title": "Install", "text": ""}, {"location": "installation.html#1_install_dependencies", "title": "1. Install Dependencies:", "text": "<ul> <li>Install TensorFlow here</li> <li> <p>Install PyTorch here (for GPU users, choose CUDA 10.1)</p> </li> <li> <p>Extra Dependencies:</p> <ul> <li> <p>Windows:</p> <ul> <li> <p>Install Visual C++ 2015 build tools here and install default option.</p> </li> <li> <p>Install latest Visual C++ redistributable here and choose x86 for 32 bit OS, x64 for 64 bit OS.</p> </li> </ul> </li> <li> <p>Linux:     <pre><code>$ apt-get install libglib2.0-0 libsm6 libxrender1 libxext6\n</code></pre></p> </li> <li> <p>Mac:     <pre><code>$ echo No extra dependency needed \":)\"\n</code></pre></p> </li> </ul> </li> </ul>"}, {"location": "installation.html#2_install_fastestimator", "title": "2. Install FastEstimator:", "text": "<ul> <li> <p>Stable (Linux/Mac):     <pre><code>$ pip install fastestimator\n</code></pre></p> </li> <li> <p>Stable (Windows):</p> <p>First download zip file from available releases <pre><code>$ pip install fastestimator-x.x.x.zip\n</code></pre></p> </li> <li> <p>Most Recent (Linux/Mac):     <pre><code>$ pip install fastestimator-nightly\n</code></pre></p> </li> <li> <p>Most Recent (Windows):</p> <p>First download zip file here <pre><code>$ pip install fastestimator-master.zip\n</code></pre></p> </li> </ul>"}, {"location": "installation.html#docker_hub", "title": "Docker Hub", "text": "<p>Docker containers create isolated virtual environments that share resources with a host machine. Docker provides an easy way to set up a FastEstimator environment. You can simply pull our image from Docker Hub and get started:</p> <ul> <li>GPU:     <pre><code>docker pull fastestimator/fastestimator:latest-gpu\n</code></pre></li> <li>CPU:     <pre><code>docker pull fastestimator/fastestimator:latest-cpu\n</code></pre></li> </ul>"}, {"location": "apphub/index.html", "title": "FastEstimator Application Hub", "text": "<p>Welcome to the FastEstimator Application Hub! Here we showcase different end-to-end AI examples implemented in FastEstimator. We will keep implementing new AI ideas and making state-of-the-art solutions accessible to everyone.</p>"}, {"location": "apphub/index.html#purpose_of_application_hub", "title": "Purpose of Application Hub", "text": "<ul> <li>Provide a place to learn implementation details of state-of-the-art solutions</li> <li>Showcase FastEstimator functionalities in an end-to-end fashion</li> <li>Offer ready-made AI solutions for people to use in their own projects/products</li> </ul>"}, {"location": "apphub/index.html#why_not_just_learn_from_official_implementations", "title": "Why not just learn from official implementations?", "text": "<p>If you have ever spent time reading AI research papers, you will often find yourself asking: did I just spent 3 hours reading a paper where the underlying idea can be expressed in 3 minutes?</p> <p>Similarly, people may use 5000 lines of code to implement an idea which could have been expressed in 500 lines using a different AI framework. In FastEstimator, we strive to make things simpler and more intuitive while preserving flexibility. As a result, many state-of-the-art AI implementations can be simplified greatly such that the code directly reflects the key ideas. As an example, the official implementation of PGGAN includes 5000+ lines of code whereas our implementation requires less than 500.</p> <p>To summarize, we spent time learning from the official implementation, so you can save time by learning from us!</p>"}, {"location": "apphub/index.html#whats_included_in_each_example", "title": "What's included in each example?", "text": "<p>Each example contains three files:</p> <ol> <li>A TensorFlow python file (.py): The FastEstimator source code needed to run the example with TensorFlow.</li> <li>A PyTorch python file (.py): The FastEstimator source code needed to run the example with PyTorch.</li> <li>A jupyter notebook (.ipynb): A notebook that provides step-by-step instructions and explanations about the implementation.</li> </ol>"}, {"location": "apphub/index.html#how_do_i_run_each_example", "title": "How do I run each example", "text": "<p>One can simply execute the python file of any example: <pre><code>$ python mnist_tf.py\n</code></pre></p> <p>Or use our Command-Line Interface (CLI):</p> <pre><code>$ fastestimator train mnist_torch.py\n</code></pre> <p>One benefit of the CLI is that it allows users to configure the input args of <code>get_estimator</code>:</p> <pre><code>$ fastestimator train lenet_mnist.py --batch_size 64 --epochs 4\n</code></pre>"}, {"location": "apphub/NLP/imdb/imdb.html", "title": "Sentiment Prediction in IMDB Reviews using an LSTM", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nimport fastestimator as fe\nfrom fastestimator.dataset.data import imdb_review\nfrom fastestimator.op.numpyop.univariate.reshape import Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.backend import load_model\n</pre> import tempfile import os import numpy as np import torch import torch.nn as nn import torch.nn.functional as fn import fastestimator as fe from fastestimator.dataset.data import imdb_review from fastestimator.op.numpyop.univariate.reshape import Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.backend import load_model In\u00a0[2]: parameters Copied! <pre>MAX_WORDS = 10000\nMAX_LEN = 500\nbatch_size = 64\nepochs = 10\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\n</pre> MAX_WORDS = 10000 MAX_LEN = 500 batch_size = 64 epochs = 10 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None Building components <p>We are loading the dataset from tf.keras.datasets.imdb which contains movie reviews and sentiment scores. All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the <code>Pipeline</code>.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=batch_size,\n                       ops=Reshape(1, inputs=\"y\", outputs=\"y\"))\n</pre> train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=batch_size,                        ops=Reshape(1, inputs=\"y\", outputs=\"y\")) <p>First, we have to define the neural network architecture, and then pass the definition, associated model name, and optimizer into fe.build:</p> In\u00a0[4]: Copied! <pre>class ReviewSentiment(nn.Module):\n    def __init__(self, embedding_size=64, hidden_units=64):\n        super().__init__()\n        self.embedding = nn.Embedding(MAX_WORDS, embedding_size)\n        self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n        self.maxpool1d = nn.MaxPool1d(kernel_size=4)\n        self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)\n        self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)\n        self.fc2 = nn.Linear(in_features=250, out_features=1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.permute((0, 2, 1))\n        x = self.conv1d(x)\n        x = fn.relu(x)\n        x = self.maxpool1d(x)\n        output, _ = self.lstm(x)\n        x = output[:, -1]  # sequence output of only last timestamp\n        x = fn.tanh(x)\n        x = self.fc1(x)\n        x = fn.relu(x)\n        x = self.fc2(x)\n        x = fn.sigmoid(x)\n        return x\n</pre> class ReviewSentiment(nn.Module):     def __init__(self, embedding_size=64, hidden_units=64):         super().__init__()         self.embedding = nn.Embedding(MAX_WORDS, embedding_size)         self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)         self.maxpool1d = nn.MaxPool1d(kernel_size=4)         self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)         self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)         self.fc2 = nn.Linear(in_features=250, out_features=1)      def forward(self, x):         x = self.embedding(x)         x = x.permute((0, 2, 1))         x = self.conv1d(x)         x = fn.relu(x)         x = self.maxpool1d(x)         output, _ = self.lstm(x)         x = output[:, -1]  # sequence output of only last timestamp         x = fn.tanh(x)         x = self.fc1(x)         x = fn.relu(x)         x = self.fc2(x)         x = fn.sigmoid(x)         return x <p><code>Network</code> is the object that defines the whole training graph, including models, loss functions, optimizers etc. A <code>Network</code> can have several different models and loss functions (ex. GANs). <code>fe.Network</code> takes a series of operators, in this case just the basic <code>ModelOp</code>, loss op, and <code>UpdateOp</code> will suffice. It should be noted that \"y_pred\" is the key in the data dictionary which will store the predictions.</p> In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p><code>Estimator</code> is the API that wraps the <code>Pipeline</code>, <code>Network</code> and other training metadata together. <code>Estimator</code> also contains <code>Traces</code>, which are similar to the callbacks of Keras.</p> <p>In the training loop, we want to measure the validation loss and save the model that has the minimum loss. <code>BestModelSaver</code> is a convenient <code>Trace</code> to achieve this. Let's also measure accuracy over time using another <code>Trace</code>:</p> In\u00a0[6]: Copied! <pre>model_dir = tempfile.mkdtemp()\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> model_dir = tempfile.mkdtemp() traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) Training In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \n</pre> <pre>/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n</pre> <pre>FastEstimator-Train: step: 1; loss: 0.6905144; \nFastEstimator-Train: step: 100; loss: 0.69094294; steps/sec: 46.99; \nFastEstimator-Train: step: 200; loss: 0.6621749; steps/sec: 48.85; \nFastEstimator-Train: step: 300; loss: 0.5835465; steps/sec: 54.12; \nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 7.74 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 391; epoch: 1; loss: 0.5250161; min_loss: 0.5250161; since_best: 0; accuracy: 0.7377667446412374; \nFastEstimator-Train: step: 400; loss: 0.51533854; steps/sec: 55.54; \nFastEstimator-Train: step: 500; loss: 0.6381638; steps/sec: 60.01; \nFastEstimator-Train: step: 600; loss: 0.4390931; steps/sec: 58.14; \nFastEstimator-Train: step: 700; loss: 0.32808638; steps/sec: 59.57; \nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 6.7 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 782; epoch: 2; loss: 0.41990075; min_loss: 0.41990075; since_best: 0; accuracy: 0.8072277653124552; \nFastEstimator-Train: step: 800; loss: 0.4495564; steps/sec: 56.01; \nFastEstimator-Train: step: 900; loss: 0.38001418; steps/sec: 60.14; \nFastEstimator-Train: step: 1000; loss: 0.28246647; steps/sec: 60.33; \nFastEstimator-Train: step: 1100; loss: 0.36126548; steps/sec: 60.51; \nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 6.63 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 1173; epoch: 3; loss: 0.39232534; min_loss: 0.39232534; since_best: 0; accuracy: 0.8241752995655702; \nFastEstimator-Train: step: 1200; loss: 0.32620478; steps/sec: 55.57; \nFastEstimator-Train: step: 1300; loss: 0.33430642; steps/sec: 60.1; \nFastEstimator-Train: step: 1400; loss: 0.21134894; steps/sec: 62.23; \nFastEstimator-Train: step: 1500; loss: 0.34480703; steps/sec: 62.4; \nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 6.47 sec; \nFastEstimator-Eval: step: 1564; epoch: 4; loss: 0.3997118; min_loss: 0.39232534; since_best: 1; accuracy: 0.8274693273499785; \nFastEstimator-Train: step: 1600; loss: 0.14769143; steps/sec: 57.72; \nFastEstimator-Train: step: 1700; loss: 0.17477548; steps/sec: 60.4; \nFastEstimator-Train: step: 1800; loss: 0.34234992; steps/sec: 60.82; \nFastEstimator-Train: step: 1900; loss: 0.34789586; steps/sec: 61.12; \nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 6.55 sec; \nFastEstimator-Eval: step: 1955; epoch: 5; loss: 0.39978975; min_loss: 0.39232534; since_best: 2; accuracy: 0.8300950016708837; \nFastEstimator-Train: step: 2000; loss: 0.21192178; steps/sec: 56.29; \nFastEstimator-Train: step: 2100; loss: 0.24565384; steps/sec: 60.85; \nFastEstimator-Train: step: 2200; loss: 0.21373041; steps/sec: 60.08; \nFastEstimator-Train: step: 2300; loss: 0.24357724; steps/sec: 61.3; \nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 6.57 sec; \nFastEstimator-Eval: step: 2346; epoch: 6; loss: 0.39285892; min_loss: 0.39232534; since_best: 3; accuracy: 0.8357282665775528; \nFastEstimator-Train: step: 2400; loss: 0.08471558; steps/sec: 56.66; \nFastEstimator-Train: step: 2500; loss: 0.20877948; steps/sec: 60.78; \nFastEstimator-Train: step: 2600; loss: 0.09914401; steps/sec: 61.27; \nFastEstimator-Train: step: 2700; loss: 0.15458922; steps/sec: 60.94; \nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 6.53 sec; \nFastEstimator-Eval: step: 2737; epoch: 7; loss: 0.43396476; min_loss: 0.39232534; since_best: 4; accuracy: 0.8326729364586815; \nFastEstimator-Train: step: 2800; loss: 0.16790089; steps/sec: 56.49; \nFastEstimator-Train: step: 2900; loss: 0.09017545; steps/sec: 60.99; \nFastEstimator-Train: step: 3000; loss: 0.06604583; steps/sec: 61.62; \nFastEstimator-Train: step: 3100; loss: 0.2692815; steps/sec: 61.39; \nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 6.52 sec; \nFastEstimator-Eval: step: 3128; epoch: 8; loss: 0.47958812; min_loss: 0.39232534; since_best: 5; accuracy: 0.8260371413567575; \nFastEstimator-Train: step: 3200; loss: 0.12287539; steps/sec: 56.58; \nFastEstimator-Train: step: 3300; loss: 0.16652104; steps/sec: 62.04; \nFastEstimator-Train: step: 3400; loss: 0.08797251; steps/sec: 59.71; \nFastEstimator-Train: step: 3500; loss: 0.08476864; steps/sec: 60.8; \nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 6.56 sec; \nFastEstimator-Eval: step: 3519; epoch: 9; loss: 0.51876205; min_loss: 0.39232534; since_best: 6; accuracy: 0.824270778631785; \nFastEstimator-Train: step: 3600; loss: 0.14876236; steps/sec: 56.08; \nFastEstimator-Train: step: 3700; loss: 0.11151114; steps/sec: 60.72; \nFastEstimator-Train: step: 3800; loss: 0.05140955; steps/sec: 60.18; \nFastEstimator-Train: step: 3900; loss: 0.062084932; steps/sec: 59.42; \nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 6.62 sec; \nFastEstimator-Eval: step: 3910; epoch: 10; loss: 0.5560739; min_loss: 0.39232534; since_best: 7; accuracy: 0.831049792333031; \nFastEstimator-Finish: step: 3910; total_time: 87.54 sec; model_lr: 0.001; \n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We previously saved model weights corresponding to our minimum loss, and now we will load the weights using <code>load_model()</code>:</p> In\u00a0[8]: Copied! <pre>model_name = 'model_best_loss.pt'\nmodel_path = os.path.join(model_dir, model_name)\nload_model(model, model_path)\n</pre> model_name = 'model_best_loss.pt' model_path = os.path.join(model_dir, model_name) load_model(model, model_path) <pre>Loaded model weights from /tmp/tmp69qyfzvm/model_best_loss.pt\n</pre> <p>Let's get some random sequence and compare the prediction with the ground truth:</p> In\u00a0[9]: Copied! <pre>selected_idx = np.random.randint(10000)\nprint(\"Ground truth is: \",eval_data[selected_idx]['y'])\n</pre> selected_idx = np.random.randint(10000) print(\"Ground truth is: \",eval_data[selected_idx]['y']) <pre>Ground truth is:  0\n</pre> <p>Create data dictionary for the inference. The <code>Transform()</code> function in Pipeline and Network applies all the operations on the given data:</p> In\u00a0[10]: Copied! <pre>infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Finally, print the inferencing results.</p> In\u00a0[11]: Copied! <pre>print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0])\n</pre> print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0]) <pre>Prediction for the input sequence:  0.30634004\n</pre>"}, {"location": "apphub/NLP/imdb/imdb.html#sentiment-prediction-in-imdb-reviews-using-an-lstm", "title": "Sentiment Prediction in IMDB Reviews using an LSTM\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html", "title": "Languge Modeling using LSTM on Penn Treebank", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nimport numpy as np\nimport tensorflow as tf\nfrom fastestimator.op.numpyop import NumpyOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.adapt import EarlyStopping, LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\n</pre> import tempfile  import fastestimator as fe import numpy as np import tensorflow as tf from fastestimator.op.numpyop import NumpyOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace import Trace from fastestimator.trace.adapt import EarlyStopping, LRScheduler from fastestimator.trace.io import BestModelSaver In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs=30\nbatch_size=128\nseq_length=20\nvocab_size=10000\ndata_dir=None\nmax_train_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # Parameters epochs=30 batch_size=128 seq_length=20 vocab_size=10000 data_dir=None max_train_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.penn_treebank import load_data\ntrain_data, eval_data, _, vocab = load_data(root_dir=data_dir, seq_length=seq_length + 1)\n</pre> from fastestimator.dataset.data.penn_treebank import load_data train_data, eval_data, _, vocab = load_data(root_dir=data_dir, seq_length=seq_length + 1) In\u00a0[4]: Copied! <pre>class CreateInputAndTarget(NumpyOp):\n    def forward(self, data, state):\n        return data[:-1], data[1:]\n</pre> class CreateInputAndTarget(NumpyOp):     def forward(self, data, state):         return data[:-1], data[1:] In\u00a0[5]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=batch_size,\n                       ops=CreateInputAndTarget(inputs=\"x\", outputs=(\"x\", \"y\")),\n                       drop_last=True)\n</pre> pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=batch_size,                        ops=CreateInputAndTarget(inputs=\"x\", outputs=(\"x\", \"y\")),                        drop_last=True) In\u00a0[6]: Copied! <pre>def build_model(vocab_size, embedding_dim, rnn_units, seq_length):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[None, seq_length]),\n        tf.keras.layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\n</pre> def build_model(vocab_size, embedding_dim, rnn_units, seq_length):     model = tf.keras.Sequential([         tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[None, seq_length]),         tf.keras.layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),         tf.keras.layers.Dropout(0.5),         tf.keras.layers.Dense(vocab_size)     ])     return model In\u00a0[7]: Copied! <pre>model = fe.build(model_fn=lambda: build_model(vocab_size, embedding_dim=300, rnn_units=600, seq_length=seq_length),\n                     optimizer_fn=lambda: tf.optimizers.SGD(1.0, momentum=0.9))\n</pre> model = fe.build(model_fn=lambda: build_model(vocab_size, embedding_dim=300, rnn_units=600, seq_length=seq_length),                      optimizer_fn=lambda: tf.optimizers.SGD(1.0, momentum=0.9)) <p>We now define the <code>Network</code> object:</p> In\u00a0[8]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(\n        inputs=(\"y_pred\", \"y\"), outputs=\"ce\", form=\"sparse\", from_logits=True),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(         inputs=(\"y_pred\", \"y\"), outputs=\"ce\", form=\"sparse\", from_logits=True),     UpdateOp(model=model, loss_name=\"ce\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>A custom trace to calculate Perplexity.</li> <li>LRScheduler to apply custom learning rate schedule.</li> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>EarlyStopping Trace for stopping early.</li> </ol> In\u00a0[9]: Copied! <pre>def lr_schedule(step, init_lr):\n    if step &lt;= 1725:\n        lr = init_lr + init_lr * (step - 1) / 1725\n    else:\n        lr = max(2 * init_lr * ((6900 - step + 1725) / 6900), 1.0)\n    return lr\n\n\nclass Perplexity(Trace):\n    def on_epoch_end(self, data):\n        ce = data[\"ce\"]\n        data.write_with_log(self.outputs[0], np.exp(ce))\n\n\ntraces = [\n    Perplexity(inputs=\"ce\", outputs=\"perplexity\", mode=\"eval\"),\n    LRScheduler(model=model, lr_fn=lambda step: lr_schedule(step, init_lr=1.0)),\n    BestModelSaver(model=model, save_dir=save_dir, metric='perplexity', save_best_mode='min', load_best_final=True),\n    EarlyStopping(monitor=\"perplexity\", patience=5)\n]\n</pre> def lr_schedule(step, init_lr):     if step &lt;= 1725:         lr = init_lr + init_lr * (step - 1) / 1725     else:         lr = max(2 * init_lr * ((6900 - step + 1725) / 6900), 1.0)     return lr   class Perplexity(Trace):     def on_epoch_end(self, data):         ce = data[\"ce\"]         data.write_with_log(self.outputs[0], np.exp(ce))   traces = [     Perplexity(inputs=\"ce\", outputs=\"perplexity\", mode=\"eval\"),     LRScheduler(model=model, lr_fn=lambda step: lr_schedule(step, init_lr=1.0)),     BestModelSaver(model=model, save_dir=save_dir, metric='perplexity', save_best_mode='min', load_best_final=True),     EarlyStopping(monitor=\"perplexity\", patience=5) ] In\u00a0[10]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch, \n                         log_steps=300)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                           log_steps=300) In\u00a0[11]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 300; \nFastEstimator-Train: step: 1; ce: 9.210202; model_lr: 1.0; \nFastEstimator-Train: step: 300; ce: 6.110634; steps/sec: 8.33; model_lr: 1.1733333; \nFastEstimator-Train: step: 345; epoch: 1; epoch_time: 43.55 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 345; epoch: 1; ce: 5.8996396; perplexity: 364.90594; since_best_perplexity: 0; min_perplexity: 364.90594; \nFastEstimator-Train: step: 600; ce: 5.7039967; steps/sec: 8.27; model_lr: 1.3472464; \nFastEstimator-Train: step: 690; epoch: 2; epoch_time: 41.65 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 690; epoch: 2; ce: 5.5457325; perplexity: 256.14215; since_best_perplexity: 0; min_perplexity: 256.14215; \nFastEstimator-Train: step: 900; ce: 5.636613; steps/sec: 8.27; model_lr: 1.5211594; \nFastEstimator-Train: step: 1035; epoch: 3; epoch_time: 41.89 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1035; epoch: 3; ce: 5.3436885; perplexity: 209.28323; since_best_perplexity: 0; min_perplexity: 209.28323; \nFastEstimator-Train: step: 1200; ce: 5.3110213; steps/sec: 8.23; model_lr: 1.6950724; \nFastEstimator-Train: step: 1380; epoch: 4; epoch_time: 41.82 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1380; epoch: 4; ce: 5.209713; perplexity: 183.04152; since_best_perplexity: 0; min_perplexity: 183.04152; \nFastEstimator-Train: step: 1500; ce: 5.1398573; steps/sec: 8.25; model_lr: 1.8689855; \nFastEstimator-Train: step: 1725; epoch: 5; epoch_time: 41.79 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1725; epoch: 5; ce: 5.107191; perplexity: 165.20566; since_best_perplexity: 0; min_perplexity: 165.20566; \nFastEstimator-Train: step: 1800; ce: 5.003286; steps/sec: 8.25; model_lr: 1.9782609; \nFastEstimator-Train: step: 2070; epoch: 6; epoch_time: 41.9 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2070; epoch: 6; ce: 5.018823; perplexity: 151.23322; since_best_perplexity: 0; min_perplexity: 151.23322; \nFastEstimator-Train: step: 2100; ce: 4.9688864; steps/sec: 8.23; model_lr: 1.8913044; \nFastEstimator-Train: step: 2400; ce: 4.8305387; steps/sec: 8.23; model_lr: 1.8043479; \nFastEstimator-Train: step: 2415; epoch: 7; epoch_time: 41.97 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2415; epoch: 7; ce: 4.9463377; perplexity: 140.65887; since_best_perplexity: 0; min_perplexity: 140.65887; \nFastEstimator-Train: step: 2700; ce: 4.5900016; steps/sec: 8.23; model_lr: 1.7173913; \nFastEstimator-Train: step: 2760; epoch: 8; epoch_time: 41.98 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2760; epoch: 8; ce: 4.900966; perplexity: 134.41959; since_best_perplexity: 0; min_perplexity: 134.41959; \nFastEstimator-Train: step: 3000; ce: 4.6566253; steps/sec: 8.21; model_lr: 1.6304348; \nFastEstimator-Train: step: 3105; epoch: 9; epoch_time: 41.64 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3105; epoch: 9; ce: 4.8612027; perplexity: 129.17947; since_best_perplexity: 0; min_perplexity: 129.17947; \nFastEstimator-Train: step: 3300; ce: 4.6201677; steps/sec: 8.34; model_lr: 1.5434783; \nFastEstimator-Train: step: 3450; epoch: 10; epoch_time: 41.66 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3450; epoch: 10; ce: 4.833195; perplexity: 125.61168; since_best_perplexity: 0; min_perplexity: 125.61168; \nFastEstimator-Train: step: 3600; ce: 4.672325; steps/sec: 8.27; model_lr: 1.4565217; \nFastEstimator-Train: step: 3795; epoch: 11; epoch_time: 41.92 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3795; epoch: 11; ce: 4.8110547; perplexity: 122.86113; since_best_perplexity: 0; min_perplexity: 122.86113; \nFastEstimator-Train: step: 3900; ce: 4.5373406; steps/sec: 8.21; model_lr: 1.3695652; \nFastEstimator-Train: step: 4140; epoch: 12; epoch_time: 41.83 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4140; epoch: 12; ce: 4.802991; perplexity: 121.87438; since_best_perplexity: 0; min_perplexity: 121.87438; \nFastEstimator-Train: step: 4200; ce: 4.412928; steps/sec: 8.26; model_lr: 1.2826087; \nFastEstimator-Train: step: 4485; epoch: 13; epoch_time: 41.72 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4485; epoch: 13; ce: 4.7911124; perplexity: 120.43527; since_best_perplexity: 0; min_perplexity: 120.43527; \nFastEstimator-Train: step: 4500; ce: 4.402304; steps/sec: 8.26; model_lr: 1.1956521; \nFastEstimator-Train: step: 4800; ce: 4.4627676; steps/sec: 8.31; model_lr: 1.1086956; \nFastEstimator-Train: step: 4830; epoch: 14; epoch_time: 41.58 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4830; epoch: 14; ce: 4.78295; perplexity: 119.45622; since_best_perplexity: 0; min_perplexity: 119.45622; \nFastEstimator-Train: step: 5100; ce: 4.2155848; steps/sec: 8.24; model_lr: 1.0217391; \nFastEstimator-Train: step: 5175; epoch: 15; epoch_time: 41.92 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 5175; epoch: 15; ce: 4.777499; perplexity: 118.80688; since_best_perplexity: 0; min_perplexity: 118.80688; \nFastEstimator-Train: step: 5400; ce: 4.2096825; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 5520; epoch: 16; epoch_time: 41.88 sec; \nFastEstimator-Eval: step: 5520; epoch: 16; ce: 4.786487; perplexity: 119.87951; since_best_perplexity: 1; min_perplexity: 118.80688; \nFastEstimator-Train: step: 5700; ce: 4.19374; steps/sec: 8.22; model_lr: 1.0; \nFastEstimator-Train: step: 5865; epoch: 17; epoch_time: 41.86 sec; \nFastEstimator-Eval: step: 5865; epoch: 17; ce: 4.791095; perplexity: 120.43314; since_best_perplexity: 2; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6000; ce: 4.040009; steps/sec: 8.27; model_lr: 1.0; \nFastEstimator-Train: step: 6210; epoch: 18; epoch_time: 41.74 sec; \nFastEstimator-Eval: step: 6210; epoch: 18; ce: 4.7963467; perplexity: 121.067314; since_best_perplexity: 3; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6300; ce: 4.0719166; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 6555; epoch: 19; epoch_time: 41.94 sec; \nFastEstimator-Eval: step: 6555; epoch: 19; ce: 4.799467; perplexity: 121.44568; since_best_perplexity: 4; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6600; ce: 4.110601; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 6900; ce: 3.9709384; steps/sec: 8.3; model_lr: 1.0; \nFastEstimator-Train: step: 6900; epoch: 20; epoch_time: 41.54 sec; \nFastEstimator-EarlyStopping: 'perplexity' triggered an early stop. Its best value was 118.80687713623047 at epoch 15\nFastEstimator-Eval: step: 6900; epoch: 20; ce: 4.8052564; perplexity: 122.150795; since_best_perplexity: 5; min_perplexity: 118.80688; \nFastEstimator-BestModelSaver: Restoring model from /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Finish: step: 6900; total_time: 864.19 sec; model_lr: 1.0; \n</pre> In\u00a0[12]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"), ])  In\u00a0[13]: Copied! <pre>def get_next_word(data, vocab):\n    output = network.transform(data, mode=\"infer\") \n    index = output[\"y_pred\"].numpy().squeeze()[-1].argmax()\n    if index == 44:    # Removing unkwown predicition\n        index = output[\"y_pred\"].numpy().squeeze()[-1].argsort()[-2]\n    return index\n\ndef generate_sequence(inp_seq, vocab, min_paragraph_len=50):\n    data = pipeline.transform({\"x\": inp_seq}, mode=\"infer\")\n    generated_seq = data[\"x\"]\n    counter=0\n    next_entry=0\n    # Stopping at &lt;eos&gt; tag or after min_paragraph_len+30 words\n    while (counter&lt;min_paragraph_len or next_entry != 43) and counter&lt;min_paragraph_len+30:  \n        next_entry = get_next_word(data, vocab)\n        generated_seq = np.concatenate([generated_seq.squeeze(), [next_entry]])\n        data = {\"x\": generated_seq[-20:].reshape((1, 20))}\n        counter+=1\n\n    return \" \".join([vocab[i] for i in generated_seq])\n</pre> def get_next_word(data, vocab):     output = network.transform(data, mode=\"infer\")      index = output[\"y_pred\"].numpy().squeeze()[-1].argmax()     if index == 44:    # Removing unkwown predicition         index = output[\"y_pred\"].numpy().squeeze()[-1].argsort()[-2]     return index  def generate_sequence(inp_seq, vocab, min_paragraph_len=50):     data = pipeline.transform({\"x\": inp_seq}, mode=\"infer\")     generated_seq = data[\"x\"]     counter=0     next_entry=0     # Stopping at  tag or after min_paragraph_len+30 words     while (counter <p>We will provide a text sequence from the validation dataset to the model and generate a paragraph with the input text sequence.</p> In\u00a0[14]: Copied! <pre>for _ in range(2):\n    idx = np.random.choice(len(eval_data))\n    inp_seq = eval_data[\"x\"][idx]\n    print(\"Input Sequence:\", \" \".join([vocab[i] for i in inp_seq[:20]]))\n    gen_seq = generate_sequence(inp_seq, vocab, 50)\n    print(\"\\nGenerated Sequence:\", gen_seq)\n    print(\"\\n\")\n</pre> for _ in range(2):     idx = np.random.choice(len(eval_data))     inp_seq = eval_data[\"x\"][idx]     print(\"Input Sequence:\", \" \".join([vocab[i] for i in inp_seq[:20]]))     gen_seq = generate_sequence(inp_seq, vocab, 50)     print(\"\\nGenerated Sequence:\", gen_seq)     print(\"\\n\") <pre>Input Sequence: the russians in iran the russians seem to have lost interest in the whole subject &lt;eos&gt; meanwhile congress is cutting\n\nGenerated Sequence: the russians in iran the russians seem to have lost interest in the whole subject &lt;eos&gt; meanwhile congress is cutting the capital-gains tax cut to the u.s. and the u.s. trade deficit &lt;eos&gt; the u.s. trade deficit has been the highest since august N &lt;eos&gt; the dollar was mixed &lt;eos&gt; the dollar was mixed &lt;eos&gt; the nasdaq composite index rose N to N &lt;eos&gt; the index gained N to N &lt;eos&gt;\n\n\nInput Sequence: the pictures &lt;eos&gt; the state &lt;unk&gt; noted that &lt;unk&gt; banking practices are grounds for removing an officer or director and\n\nGenerated Sequence: the pictures &lt;eos&gt; the state &lt;unk&gt; noted that &lt;unk&gt; banking practices are grounds for removing an officer or director and chief executive officer &lt;eos&gt; mr. guber and mr. peters have been working on the board &lt;eos&gt; the company said the company will be able to pay for the $ N million of the company 's common shares outstanding &lt;eos&gt; the company said the company 's net income rose N N to $ N million from $ N million &lt;eos&gt;\n\n\n</pre> <p>As you can see, the network is able to generate meaningful sentences.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#languge-modeling-using-lstm-on-penn-treebank", "title": "Languge Modeling using LSTM on Penn Treebank\u00b6", "text": "<p>Language Modeling is the development of models to predict the next word of the sequence given the words that precede it. In this notebook we will demonstrate how to predict next word of a sequence using an LSTM. We will be using Penn Treebank dataset which contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download the Penn Treebank dataset via our dataset API.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We will create a custom NumpyOp to generate input and target sequences.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model is a LSTM.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#training-and-testing", "title": "Training and Testing\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will use the model to generate some sequences of text.</p>"}, {"location": "apphub/NLP/named_entity_recognition/bert.html", "title": "Named Entity Recognition using BERT Fine-Tuning", "text": "<p>For downstream NLP tasks such as question answering, named entity recognition, and language inference, models built on pre-trained word representations tend to perform better. BERT, which fine tunes a deep bi-directional representation on a series of tasks, achieves state-of-the-art results. Unlike traditional transformers, BERT is trained on \"masked language modeling,\" which means that it is allowed to see the whole sentence and does not limit the context it can take into account.</p> <p>For this example, we are leveraging the transformers library to load a BERT model, along with some config files:</p> In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom transformers import BertTokenizer, TFBertModel\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import mitmovie_ner\nfrom fastestimator.op.numpyop.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId\nfrom fastestimator.op.tensorop import TensorOp, Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.backend import feed_forward\n</pre> import tempfile import os import numpy as np  import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.models import Model from transformers import BertTokenizer, TFBertModel  import fastestimator as fe from fastestimator.dataset.data import mitmovie_ner from fastestimator.op.numpyop.numpyop import NumpyOp from fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId from fastestimator.op.tensorop import TensorOp, Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver from fastestimator.backend import feed_forward In\u00a0[2]: parameters Copied! <pre>max_len = 20\nbatch_size = 64\nepochs = 10\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> max_len = 20 batch_size = 64 epochs = 10 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We will need a custom <code>NumpyOp</code> that constructs attention masks for input sequences:</p> In\u00a0[3]: Copied! <pre>class AttentionMask(NumpyOp):\n    def forward(self, data, state):\n        masks = [float(i &gt; 0) for i in data]\n        return np.array(masks)\n</pre> class AttentionMask(NumpyOp):     def forward(self, data, state):         masks = [float(i &gt; 0) for i in data]         return np.array(masks) <p>Our <code>char2idx</code> function creates a look-up table to match ids and labels:</p> In\u00a0[4]: Copied! <pre>def char2idx(data):\n    tag2idx = {t: i for i, t in enumerate(data)}\n    return tag2idx\n</pre> def char2idx(data):     tag2idx = {t: i for i, t in enumerate(data)}     return tag2idx Building components <p>We are loading train and eval sequences from the MIT Movie datasets that is semantically tagged along with data and label vocabulary. For this example other nouns are omitted for the simplicity.</p> In\u00a0[5]: Copied! <pre>train_data, eval_data, data_vocab, label_vocab = mitmovie_ner.load_data(root_dir=data_dir)\n</pre> train_data, eval_data, data_vocab, label_vocab = mitmovie_ner.load_data(root_dir=data_dir) <pre>/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  y_train = np.array(y_train)\n/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  y_eval = np.array(y_eval)\n</pre> <p>Define a pipeline to tokenize and pad the input sequences and construct attention masks. Attention masks are used to avoid performing attention operations on padded tokens. We are using the BERT tokenizer for input sequence tokenization, and limiting our sequences to a max length of 50 for this example.</p> In\u00a0[6]: Copied! <pre>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntag2idx = char2idx(label_vocab)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),\n        WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),\n        WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),\n        PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),\n        PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),\n        AttentionMask(inputs=\"x\", outputs=\"x_masks\")\n    ])\n</pre> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) tag2idx = char2idx(label_vocab) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size,     ops=[         Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),         WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),         WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),         PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),         PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),         AttentionMask(inputs=\"x\", outputs=\"x_masks\")     ]) <p>Our neural network architecture leverages pre-trained weights as initialization for downstream tasks. The whole network is then trained during the fine-tuning.</p> In\u00a0[7]: Copied! <pre>def ner_model():\n    token_inputs = Input((max_len), dtype=tf.int32, name='input_words')\n    mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')\n    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    seq_output, _ = bert_model(token_inputs, attention_mask=mask_inputs)\n    output = Dense(len(label_vocab) + 1, activation='softmax')(seq_output)\n    model = Model([token_inputs, mask_inputs], output)\n    return model\n</pre> def ner_model():     token_inputs = Input((max_len), dtype=tf.int32, name='input_words')     mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')     bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")     seq_output, _ = bert_model(token_inputs, attention_mask=mask_inputs)     output = Dense(len(label_vocab) + 1, activation='softmax')(seq_output)     model = Model([token_inputs, mask_inputs], output)     return model <p>After defining the model, it is then instantiated by calling fe.build which also associates the model with a specific optimizer:</p> In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <p><code>fe.Network</code> takes a series of operators. In this case we use a <code>ModelOp</code> to run forward passes through the neural network. The <code>ReshapeOp</code> is then used to transform the prediction and ground truth to a two dimensional vector or scalar respectively before feeding them to the loss calculation.</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),\n        Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),\n        Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, len(label_vocab) + 1)),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n        UpdateOp(model=model, loss_name=\"loss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),         Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),         Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, len(label_vocab) + 1)),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),         UpdateOp(model=model, loss_name=\"loss\")     ]) <p>The <code>Estimator</code> takes four important arguments: network, pipeline, epochs, and traces. During the training, we want to compute accuracy as well as to save the model with the minimum loss. This can be done using <code>Traces</code>.</p> In\u00a0[10]: Copied! <pre>traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)]\n</pre> traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)] In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) Training In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nFastEstimator-Train: step: 1; loss: 3.534539; \nFastEstimator-Train: step: 100; loss: 0.7910271; steps/sec: 2.04; \nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nFastEstimator-Train: step: 153; epoch: 1; epoch_time: 93.27 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 153; epoch: 1; loss: 0.4999621; accuracy: 0.8571633237822349; since_best_loss: 0; min_loss: 0.4999621; \nFastEstimator-Train: step: 200; loss: 0.4584582; steps/sec: 1.71; \nFastEstimator-Train: step: 300; loss: 0.34510213; steps/sec: 1.97; \nFastEstimator-Train: step: 306; epoch: 2; epoch_time: 77.24 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 306; epoch: 2; loss: 0.33649036; accuracy: 0.9036635284486287; since_best_loss: 0; min_loss: 0.33649036; \nFastEstimator-Train: step: 400; loss: 0.28147238; steps/sec: 1.89; \nFastEstimator-Train: step: 459; epoch: 3; epoch_time: 81.27 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 459; epoch: 3; loss: 0.27175826; accuracy: 0.920384772820303; since_best_loss: 0; min_loss: 0.27175826; \nFastEstimator-Train: step: 500; loss: 0.2756353; steps/sec: 1.92; \nFastEstimator-Train: step: 600; loss: 0.22491762; steps/sec: 2.0; \nFastEstimator-Train: step: 612; epoch: 4; epoch_time: 76.74 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 612; epoch: 4; loss: 0.23077382; accuracy: 0.9328694228407696; since_best_loss: 0; min_loss: 0.23077382; \nFastEstimator-Train: step: 700; loss: 0.21992946; steps/sec: 2.0; \nFastEstimator-Train: step: 765; epoch: 5; epoch_time: 76.65 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 765; epoch: 5; loss: 0.20521004; accuracy: 0.9419566107245191; since_best_loss: 0; min_loss: 0.20521004; \nFastEstimator-Train: step: 800; loss: 0.22478268; steps/sec: 2.0; \nFastEstimator-Train: step: 900; loss: 0.18137912; steps/sec: 1.99; \nFastEstimator-Train: step: 918; epoch: 6; epoch_time: 76.67 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 918; epoch: 6; loss: 0.19312857; accuracy: 0.947359803520262; since_best_loss: 0; min_loss: 0.19312857; \nFastEstimator-Train: step: 1000; loss: 0.16234711; steps/sec: 1.99; \nFastEstimator-Train: step: 1071; epoch: 7; epoch_time: 76.88 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1071; epoch: 7; loss: 0.17946187; accuracy: 0.951105198526402; since_best_loss: 0; min_loss: 0.17946187; \nFastEstimator-Train: step: 1100; loss: 0.1411412; steps/sec: 1.99; \nFastEstimator-Train: step: 1200; loss: 0.21398099; steps/sec: 2.0; \nFastEstimator-Train: step: 1224; epoch: 8; epoch_time: 76.62 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1224; epoch: 8; loss: 0.17292295; accuracy: 0.9533974621367172; since_best_loss: 0; min_loss: 0.17292295; \nFastEstimator-Train: step: 1300; loss: 0.104645446; steps/sec: 1.98; \nFastEstimator-Train: step: 1377; epoch: 9; epoch_time: 77.22 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1377; epoch: 9; loss: 0.17160487; accuracy: 0.95440032746623; since_best_loss: 0; min_loss: 0.17160487; \nFastEstimator-Train: step: 1400; loss: 0.114030465; steps/sec: 1.99; \nFastEstimator-Train: step: 1500; loss: 0.118343726; steps/sec: 2.0; \nFastEstimator-Train: step: 1530; epoch: 10; epoch_time: 76.59 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1530; epoch: 10; loss: 0.16745299; accuracy: 0.9565083913221449; since_best_loss: 0; min_loss: 0.16745299; \nFastEstimator-Finish: step: 1530; total_time: 897.96 sec; model_lr: 1e-05; \n</pre> Inferencing <p>Load model weights using fe.build</p> In\u00a0[13]: Copied! <pre>model_name = 'model_best_loss.h5'\nmodel_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model_name = 'model_best_loss.h5' model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <p>Let's take random phrase about a movie and predict it's named entities in BIO format.</p> In\u00a0[14]: Copied! <pre>test_input = 'have you seen The dark night trilogy'\ntest_ground_truth = ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE']\n</pre> test_input = 'have you seen The dark night trilogy' test_ground_truth = ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE'] <p>Create a data dictionary for the inference. The <code>transform()</code> function in <code>Pipeline</code> and <code>Network</code> applies all their operations on the given data:</p> In\u00a0[15]: Copied! <pre>infer_data = {\"x\":test_input, \"y\":test_ground_truth}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":test_input, \"y\":test_ground_truth} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Get the predictions using feed_forward</p> In\u00a0[16]: Copied! <pre>predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False)\npredictions = np.array(predictions).reshape(20, len(label_vocab) + 1)\npredictions = np.argmax(predictions, axis=-1)\n</pre> predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False) predictions = np.array(predictions).reshape(20, len(label_vocab) + 1) predictions = np.argmax(predictions, axis=-1) In\u00a0[17]: Copied! <pre>def get_key(val): \n    for key, value in tag2idx.items(): \n         if val == value: \n            return key\n</pre> def get_key(val):      for key, value in tag2idx.items():           if val == value:              return key  In\u00a0[18]: Copied! <pre>print(\"Predictions: \", [get_key(pred) for pred in predictions])\n</pre> print(\"Predictions: \", [get_key(pred) for pred in predictions]) <pre>Predictions:  ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', None, None, None, None, None, None, None, None, None, None, None, None, None]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "apphub/NLP/named_entity_recognition/bert.html#named-entity-recognition-using-bert-fine-tuning", "title": "Named Entity Recognition using BERT Fine-Tuning\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-2-create-model-and-fastestimator-network", "title": "Step 2: Create <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html", "title": "Adversarial Robustness with Error Correcting Codes", "text": "In\u00a0[1]: Copied! <pre>import math\nimport tempfile\n\nfrom tensorflow.python.keras import Sequential, layers\nfrom tensorflow.python.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\nfrom tensorflow.python.keras.models import Model\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.layers.tensorflow import HadamardCode\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.op.tensorop import Average\nfrom fastestimator.op.tensorop.gradient import FGSM, Watch\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import EarlyStopping\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import math import tempfile  from tensorflow.python.keras import Sequential, layers from tensorflow.python.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D from tensorflow.python.keras.models import Model  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifar10 from fastestimator.layers.tensorflow import HadamardCode from fastestimator.op.numpyop.univariate import Normalize from fastestimator.op.tensorop import Average from fastestimator.op.tensorop.gradient import FGSM, Watch from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import EarlyStopping from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=60\nbatch_size=50\nlog_steps=500\nmax_train_steps_per_epoch=None\nmax_eval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=60 batch_size=50 log_steps=500 max_train_steps_per_epoch=None max_eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))])\n</pre> train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))]) In\u00a0[4]: Copied! <pre>def get_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        UpdateOp(model=model, loss_name=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_ce\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             max_train_steps_per_epoch=max_train_steps_per_epoch,\n                             max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                             monitor_names=[\"adv_ce\"])\n    return estimator\n</pre> def get_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         UpdateOp(model=model, loss_name=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_ce\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              max_train_steps_per_epoch=max_train_steps_per_epoch,                              max_eval_steps_per_epoch=max_eval_steps_per_epoch,                              monitor_names=[\"adv_ce\"])     return estimator In\u00a0[5]: Copied! <pre>softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax')\n</pre> softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax') In\u00a0[6]: Copied! <pre>def EccLeNet(input_shape=(32, 32, 3), classes=10):\n    model = Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(HadamardCode(classes))  # Note that this is the only difference between this model and the FE LeNet implementation\n    return model\n</pre> def EccLeNet(input_shape=(32, 32, 3), classes=10):     model = Sequential()     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.Flatten())     model.add(layers.Dense(64, activation='relu'))     model.add(HadamardCode(classes))  # Note that this is the only difference between this model and the FE LeNet implementation     return model In\u00a0[7]: Copied! <pre>ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc')\n</pre> ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc') In\u00a0[8]: Copied! <pre>def HydraEccLeNet(input_shape=(32, 32, 3), classes=10):\n    inputs = Input(input_shape)\n    conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)\n    flat = Flatten()(conv3)\n    # Create multiple heads\n    n_heads = 4\n    heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]\n    outputs = HadamardCode(classes)(heads)\n    return Model(inputs=inputs, outputs=outputs)\n</pre> def HydraEccLeNet(input_shape=(32, 32, 3), classes=10):     inputs = Input(input_shape)     conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)     pool1 = MaxPooling2D((2, 2))(conv1)     conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)     pool2 = MaxPooling2D((2, 2))(conv2)     conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)     flat = Flatten()(conv3)     # Create multiple heads     n_heads = 4     heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]     outputs = HadamardCode(classes)(heads)     return Model(inputs=inputs, outputs=outputs) In\u00a0[9]: Copied! <pre>hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc')\n</pre> hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc') In\u00a0[10]: Copied! <pre>softmax_estimator = get_estimator(softmax_model)\necc_estimator = get_estimator(ecc_model)\nhydra_estimator = get_estimator(hydra_model)\n</pre> softmax_estimator = get_estimator(softmax_model) ecc_estimator = get_estimator(ecc_model) hydra_estimator = get_estimator(hydra_model) In\u00a0[11]: Copied! <pre>softmax_estimator.fit('Softmax')\nsoftmax_results = softmax_estimator.test()\n</pre> softmax_estimator.fit('Softmax') softmax_results = softmax_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_ce: 2.3754928; \nFastEstimator-Train: step: 500; base_ce: 1.2483782; steps/sec: 33.49; \nFastEstimator-Train: step: 1000; base_ce: 1.3786774; steps/sec: 24.76; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 36.88 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; base_ce: 1.1597806; adv_ce: 1.9426155; base accuracy: 0.5904; adversarial accuracy: 0.2798; since_best_base_ce: 0; min_base_ce: 1.1597806; \nFastEstimator-Train: step: 1500; base_ce: 1.051444; steps/sec: 28.7; \nFastEstimator-Train: step: 2000; base_ce: 1.1310353; steps/sec: 24.35; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 38.0 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; base_ce: 0.9508422; adv_ce: 2.08377; base accuracy: 0.6692; adversarial accuracy: 0.2832; since_best_base_ce: 0; min_base_ce: 0.9508422; \nFastEstimator-Train: step: 2500; base_ce: 0.80142707; steps/sec: 22.01; \nFastEstimator-Train: step: 3000; base_ce: 0.7681661; steps/sec: 20.49; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 47.08 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; base_ce: 0.8998893; adv_ce: 2.2634797; base accuracy: 0.691; adversarial accuracy: 0.2908; since_best_base_ce: 0; min_base_ce: 0.8998893; \nFastEstimator-Train: step: 3500; base_ce: 0.78990793; steps/sec: 19.68; \nFastEstimator-Train: step: 4000; base_ce: 1.1161602; steps/sec: 21.64; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 48.53 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; base_ce: 0.83236206; adv_ce: 2.4632723; base accuracy: 0.712; adversarial accuracy: 0.2818; since_best_base_ce: 0; min_base_ce: 0.83236206; \nFastEstimator-Train: step: 4500; base_ce: 0.80196106; steps/sec: 20.12; \nFastEstimator-Train: step: 5000; base_ce: 0.75015104; steps/sec: 21.42; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 48.18 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; base_ce: 0.8077117; adv_ce: 2.6284113; base accuracy: 0.7242; adversarial accuracy: 0.2666; since_best_base_ce: 0; min_base_ce: 0.8077117; \nFastEstimator-Train: step: 5500; base_ce: 0.82036537; steps/sec: 23.3; \nFastEstimator-Train: step: 6000; base_ce: 0.70514345; steps/sec: 25.92; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 40.74 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; base_ce: 0.79506516; adv_ce: 2.9957755; base accuracy: 0.7246; adversarial accuracy: 0.256; since_best_base_ce: 0; min_base_ce: 0.79506516; \nFastEstimator-Train: step: 6500; base_ce: 0.54250854; steps/sec: 26.72; \nFastEstimator-Train: step: 7000; base_ce: 0.68669426; steps/sec: 28.35; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 36.35 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; base_ce: 0.78500015; adv_ce: 3.0124106; base accuracy: 0.7364; adversarial accuracy: 0.2438; since_best_base_ce: 0; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 7500; base_ce: 0.7418024; steps/sec: 26.62; \nFastEstimator-Train: step: 8000; base_ce: 0.60131586; steps/sec: 28.69; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 36.2 sec; \nFastEstimator-Eval: step: 8000; epoch: 8; base_ce: 0.8056283; adv_ce: 3.3994384; base accuracy: 0.7374; adversarial accuracy: 0.2362; since_best_base_ce: 1; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 8500; base_ce: 0.48421046; steps/sec: 26.83; \nFastEstimator-Train: step: 9000; base_ce: 0.4724892; steps/sec: 28.65; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 36.1 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; base_ce: 0.8141311; adv_ce: 3.7335455; base accuracy: 0.7344; adversarial accuracy: 0.2088; since_best_base_ce: 2; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 9500; base_ce: 0.3733459; steps/sec: 29.25; \nFastEstimator-Train: step: 10000; base_ce: 0.41766632; steps/sec: 31.09; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 33.19 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; base_ce: 0.85542786; adv_ce: 4.0170755; base accuracy: 0.7308; adversarial accuracy: 0.2066; since_best_base_ce: 3; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 10500; base_ce: 0.41894433; steps/sec: 29.18; \nFastEstimator-Train: step: 11000; base_ce: 0.65334654; steps/sec: 30.64; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 33.44 sec; \nFastEstimator-Eval: step: 11000; epoch: 11; base_ce: 0.9005296; adv_ce: 4.5361137; base accuracy: 0.7234; adversarial accuracy: 0.1788; since_best_base_ce: 4; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 11500; base_ce: 0.63553435; steps/sec: 29.32; \nFastEstimator-Train: step: 12000; base_ce: 0.56910044; steps/sec: 31.16; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 33.12 sec; \nFastEstimator-Eval: step: 12000; epoch: 12; base_ce: 0.93609977; adv_ce: 4.8335547; base accuracy: 0.7278; adversarial accuracy: 0.1944; since_best_base_ce: 5; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 12500; base_ce: 0.38925758; steps/sec: 26.87; \nFastEstimator-Train: step: 13000; base_ce: 0.24855006; steps/sec: 28.67; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 36.04 sec; \nFastEstimator-Eval: step: 13000; epoch: 13; base_ce: 1.0094614; adv_ce: 5.402545; base accuracy: 0.7202; adversarial accuracy: 0.182; since_best_base_ce: 6; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 13500; base_ce: 0.5594125; steps/sec: 26.92; \nFastEstimator-Train: step: 14000; base_ce: 0.21032642; steps/sec: 28.8; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 35.93 sec; \nFastEstimator-Eval: step: 14000; epoch: 14; base_ce: 1.0515163; adv_ce: 5.6984034; base accuracy: 0.7124; adversarial accuracy: 0.1804; since_best_base_ce: 7; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 14500; base_ce: 0.37550446; steps/sec: 28.22; \nFastEstimator-Train: step: 15000; base_ce: 0.32693386; steps/sec: 29.72; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 34.56 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; base_ce: 1.0898054; adv_ce: 6.124301; base accuracy: 0.7168; adversarial accuracy: 0.1726; since_best_base_ce: 8; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 15500; base_ce: 0.20683852; steps/sec: 28.59; \nFastEstimator-Train: step: 16000; base_ce: 0.34564048; steps/sec: 29.86; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 34.22 sec; \nFastEstimator-Eval: step: 16000; epoch: 16; base_ce: 1.1524608; adv_ce: 6.8542333; base accuracy: 0.7144; adversarial accuracy: 0.1746; since_best_base_ce: 9; min_base_ce: 0.78500015; \nFastEstimator-Train: step: 16500; base_ce: 0.2094497; steps/sec: 31.04; \nFastEstimator-Train: step: 17000; base_ce: 0.15683812; steps/sec: 33.25; \nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 31.14 sec; \nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 0.7850001454353333 at epoch 7\nFastEstimator-Eval: step: 17000; epoch: 17; base_ce: 1.1774385; adv_ce: 7.219962; base accuracy: 0.7166; adversarial accuracy: 0.1618; since_best_base_ce: 10; min_base_ce: 0.78500015; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/softmax_best_base_ce.h5\nFastEstimator-Finish: step: 17000; total_time: 742.28 sec; softmax_lr: 0.001; \nFastEstimator-Test: step: 17000; epoch: 17; base accuracy: 0.7282; adversarial accuracy: 0.234; \n</pre> In\u00a0[12]: Copied! <pre>ecc_estimator.fit('ECC')\necc_results = ecc_estimator.test()\n</pre> ecc_estimator.fit('ECC') ecc_results = ecc_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_ce: 2.2948906; \nFastEstimator-Train: step: 500; base_ce: 1.8532416; steps/sec: 30.21; \nFastEstimator-Train: step: 1000; base_ce: 1.7478514; steps/sec: 31.69; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 33.4 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; base_ce: 1.7138911; adv_ce: 2.1511297; base accuracy: 0.496; adversarial accuracy: 0.3198; since_best_base_ce: 0; min_base_ce: 1.7138911; \nFastEstimator-Train: step: 1500; base_ce: 1.4022256; steps/sec: 31.97; \nFastEstimator-Train: step: 2000; base_ce: 1.4052359; steps/sec: 34.93; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 29.96 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; base_ce: 1.5651137; adv_ce: 2.1517153; base accuracy: 0.5458; adversarial accuracy: 0.3336; since_best_base_ce: 0; min_base_ce: 1.5651137; \nFastEstimator-Train: step: 2500; base_ce: 1.3534999; steps/sec: 32.84; \nFastEstimator-Train: step: 3000; base_ce: 1.5177455; steps/sec: 35.16; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 29.45 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; base_ce: 1.3509419; adv_ce: 2.1408393; base accuracy: 0.6252; adversarial accuracy: 0.3738; since_best_base_ce: 0; min_base_ce: 1.3509419; \nFastEstimator-Train: step: 3500; base_ce: 1.4140029; steps/sec: 36.55; \nFastEstimator-Train: step: 4000; base_ce: 1.259077; steps/sec: 38.45; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 26.68 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; base_ce: 1.2993072; adv_ce: 2.1872435; base accuracy: 0.649; adversarial accuracy: 0.3834; since_best_base_ce: 0; min_base_ce: 1.2993072; \nFastEstimator-Train: step: 4500; base_ce: 1.146649; steps/sec: 36.42; \nFastEstimator-Train: step: 5000; base_ce: 0.93379724; steps/sec: 38.36; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 26.77 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; base_ce: 1.2297044; adv_ce: 2.198256; base accuracy: 0.6682; adversarial accuracy: 0.3808; since_best_base_ce: 0; min_base_ce: 1.2297044; \nFastEstimator-Train: step: 5500; base_ce: 0.9904622; steps/sec: 36.08; \nFastEstimator-Train: step: 6000; base_ce: 1.0625097; steps/sec: 36.52; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 27.55 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; base_ce: 1.2074428; adv_ce: 2.3546593; base accuracy: 0.6862; adversarial accuracy: 0.395; since_best_base_ce: 0; min_base_ce: 1.2074428; \nFastEstimator-Train: step: 6500; base_ce: 0.7872818; steps/sec: 33.25; \nFastEstimator-Train: step: 7000; base_ce: 0.88184655; steps/sec: 34.98; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 29.33 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; base_ce: 1.1496447; adv_ce: 2.326219; base accuracy: 0.699; adversarial accuracy: 0.3872; since_best_base_ce: 0; min_base_ce: 1.1496447; \nFastEstimator-Train: step: 7500; base_ce: 0.95065016; steps/sec: 33.08; \nFastEstimator-Train: step: 8000; base_ce: 1.1240141; steps/sec: 37.13; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 28.59 sec; \nFastEstimator-Eval: step: 8000; epoch: 8; base_ce: 1.1630411; adv_ce: 2.3640723; base accuracy: 0.6922; adversarial accuracy: 0.3986; since_best_base_ce: 1; min_base_ce: 1.1496447; \nFastEstimator-Train: step: 8500; base_ce: 1.2048315; steps/sec: 36.49; \nFastEstimator-Train: step: 9000; base_ce: 1.0226548; steps/sec: 39.1; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 26.49 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 9000; epoch: 9; base_ce: 1.1178105; adv_ce: 2.3132443; base accuracy: 0.706; adversarial accuracy: 0.4152; since_best_base_ce: 0; min_base_ce: 1.1178105; \nFastEstimator-Train: step: 9500; base_ce: 1.1182468; steps/sec: 36.0; \nFastEstimator-Train: step: 10000; base_ce: 0.6692859; steps/sec: 37.89; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 27.08 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; base_ce: 1.1335105; adv_ce: 2.4508705; base accuracy: 0.7068; adversarial accuracy: 0.4008; since_best_base_ce: 1; min_base_ce: 1.1178105; \nFastEstimator-Train: step: 10500; base_ce: 0.700335; steps/sec: 35.91; \nFastEstimator-Train: step: 11000; base_ce: 0.74321246; steps/sec: 38.14; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 27.03 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 11000; epoch: 11; base_ce: 1.077382; adv_ce: 2.298885; base accuracy: 0.7224; adversarial accuracy: 0.4282; since_best_base_ce: 0; min_base_ce: 1.077382; \nFastEstimator-Train: step: 11500; base_ce: 0.7760241; steps/sec: 36.38; \nFastEstimator-Train: step: 12000; base_ce: 0.53444064; steps/sec: 38.03; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 26.89 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 12000; epoch: 12; base_ce: 1.0708356; adv_ce: 2.4382222; base accuracy: 0.7252; adversarial accuracy: 0.4212; since_best_base_ce: 0; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 12500; base_ce: 0.6889807; steps/sec: 36.63; \nFastEstimator-Train: step: 13000; base_ce: 0.9038168; steps/sec: 38.36; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 26.69 sec; \nFastEstimator-Eval: step: 13000; epoch: 13; base_ce: 1.109232; adv_ce: 2.4642434; base accuracy: 0.7188; adversarial accuracy: 0.4212; since_best_base_ce: 1; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 13500; base_ce: 1.019563; steps/sec: 39.41; \nFastEstimator-Train: step: 14000; base_ce: 0.62402534; steps/sec: 42.24; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 24.52 sec; \nFastEstimator-Eval: step: 14000; epoch: 14; base_ce: 1.0935918; adv_ce: 2.4698822; base accuracy: 0.7236; adversarial accuracy: 0.4312; since_best_base_ce: 2; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 14500; base_ce: 0.8987514; steps/sec: 39.77; \nFastEstimator-Train: step: 15000; base_ce: 0.7350322; steps/sec: 39.5; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 25.24 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; base_ce: 1.1243794; adv_ce: 2.4714823; base accuracy: 0.7138; adversarial accuracy: 0.4264; since_best_base_ce: 3; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 15500; base_ce: 0.6691685; steps/sec: 35.86; \nFastEstimator-Train: step: 16000; base_ce: 0.7693179; steps/sec: 38.54; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 26.92 sec; \nFastEstimator-Eval: step: 16000; epoch: 16; base_ce: 1.1038796; adv_ce: 2.5672605; base accuracy: 0.7206; adversarial accuracy: 0.414; since_best_base_ce: 4; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 16500; base_ce: 0.55752677; steps/sec: 36.4; \nFastEstimator-Train: step: 17000; base_ce: 0.95395017; steps/sec: 38.6; \nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 26.68 sec; \nFastEstimator-Eval: step: 17000; epoch: 17; base_ce: 1.0992229; adv_ce: 2.4245234; base accuracy: 0.726; adversarial accuracy: 0.4484; since_best_base_ce: 5; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 17500; base_ce: 0.97094285; steps/sec: 36.61; \nFastEstimator-Train: step: 18000; base_ce: 0.6378223; steps/sec: 39.16; \nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 26.43 sec; \nFastEstimator-Eval: step: 18000; epoch: 18; base_ce: 1.1157478; adv_ce: 2.496202; base accuracy: 0.719; adversarial accuracy: 0.4378; since_best_base_ce: 6; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 18500; base_ce: 0.6535682; steps/sec: 36.22; \nFastEstimator-Train: step: 19000; base_ce: 0.7969613; steps/sec: 38.31; \nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 26.86 sec; \nFastEstimator-Eval: step: 19000; epoch: 19; base_ce: 1.0798271; adv_ce: 2.382594; base accuracy: 0.7284; adversarial accuracy: 0.4556; since_best_base_ce: 7; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 19500; base_ce: 0.3818073; steps/sec: 36.99; \nFastEstimator-Train: step: 20000; base_ce: 0.6504534; steps/sec: 38.94; \nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 26.36 sec; \nFastEstimator-Eval: step: 20000; epoch: 20; base_ce: 1.0884305; adv_ce: 2.4666903; base accuracy: 0.7278; adversarial accuracy: 0.4482; since_best_base_ce: 8; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 20500; base_ce: 0.644705; steps/sec: 37.49; \nFastEstimator-Train: step: 21000; base_ce: 0.6375694; steps/sec: 41.72; \nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 25.32 sec; \nFastEstimator-Eval: step: 21000; epoch: 21; base_ce: 1.1050826; adv_ce: 2.4939957; base accuracy: 0.7278; adversarial accuracy: 0.4492; since_best_base_ce: 9; min_base_ce: 1.0708356; \nFastEstimator-Train: step: 21500; base_ce: 0.7648297; steps/sec: 39.6; \nFastEstimator-Train: step: 22000; base_ce: 0.64078635; steps/sec: 42.45; \nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 24.41 sec; \nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 1.0708355903625488 at epoch 12\nFastEstimator-Eval: step: 22000; epoch: 22; base_ce: 1.1157986; adv_ce: 2.3670042; base accuracy: 0.7294; adversarial accuracy: 0.4818; since_best_base_ce: 10; min_base_ce: 1.0708356; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/ecc_best_base_ce.h5\nFastEstimator-Finish: step: 22000; total_time: 694.57 sec; ecc_lr: 0.001; \nFastEstimator-Test: step: 22000; epoch: 22; base accuracy: 0.7298; adversarial accuracy: 0.4174; \n</pre> In\u00a0[13]: Copied! <pre>hydra_estimator.fit('Hydra')\nhydra_results = hydra_estimator.test()\n</pre> hydra_estimator.fit('Hydra') hydra_results = hydra_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_ce: 2.3076158; \nFastEstimator-Train: step: 500; base_ce: 1.7894692; steps/sec: 39.9; \nFastEstimator-Train: step: 1000; base_ce: 1.6115338; steps/sec: 41.67; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 25.65 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; base_ce: 1.772432; adv_ce: 2.2166047; base accuracy: 0.4562; adversarial accuracy: 0.3138; since_best_base_ce: 0; min_base_ce: 1.772432; \nFastEstimator-Train: step: 1500; base_ce: 1.6917462; steps/sec: 39.34; \nFastEstimator-Train: step: 2000; base_ce: 1.5339103; steps/sec: 41.41; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 24.79 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; base_ce: 1.53139; adv_ce: 2.1004038; base accuracy: 0.5708; adversarial accuracy: 0.3744; since_best_base_ce: 0; min_base_ce: 1.53139; \nFastEstimator-Train: step: 2500; base_ce: 1.65984; steps/sec: 39.34; \nFastEstimator-Train: step: 3000; base_ce: 1.5187409; steps/sec: 41.11; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 24.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; base_ce: 1.407918; adv_ce: 2.1317058; base accuracy: 0.6104; adversarial accuracy: 0.3704; since_best_base_ce: 0; min_base_ce: 1.407918; \nFastEstimator-Train: step: 3500; base_ce: 1.4999541; steps/sec: 36.05; \nFastEstimator-Train: step: 4000; base_ce: 1.3882403; steps/sec: 38.61; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 26.82 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; base_ce: 1.3866385; adv_ce: 2.1057322; base accuracy: 0.6182; adversarial accuracy: 0.3706; since_best_base_ce: 0; min_base_ce: 1.3866385; \nFastEstimator-Train: step: 4500; base_ce: 1.4213487; steps/sec: 35.65; \nFastEstimator-Train: step: 5000; base_ce: 1.1099585; steps/sec: 38.0; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 27.18 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; base_ce: 1.285622; adv_ce: 2.1949887; base accuracy: 0.6532; adversarial accuracy: 0.3748; since_best_base_ce: 0; min_base_ce: 1.285622; \nFastEstimator-Train: step: 5500; base_ce: 1.0962647; steps/sec: 37.13; \nFastEstimator-Train: step: 6000; base_ce: 1.19081; steps/sec: 41.86; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 25.41 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; base_ce: 1.2323712; adv_ce: 2.191548; base accuracy: 0.6758; adversarial accuracy: 0.3874; since_best_base_ce: 0; min_base_ce: 1.2323712; \nFastEstimator-Train: step: 6500; base_ce: 1.0899575; steps/sec: 39.26; \nFastEstimator-Train: step: 7000; base_ce: 1.0509219; steps/sec: 40.43; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 25.11 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; base_ce: 1.1828624; adv_ce: 2.2274768; base accuracy: 0.691; adversarial accuracy: 0.3988; since_best_base_ce: 0; min_base_ce: 1.1828624; \nFastEstimator-Train: step: 7500; base_ce: 1.1362395; steps/sec: 38.76; \nFastEstimator-Train: step: 8000; base_ce: 1.1766993; steps/sec: 40.36; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 25.28 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; base_ce: 1.179946; adv_ce: 2.2246737; base accuracy: 0.6894; adversarial accuracy: 0.4022; since_best_base_ce: 0; min_base_ce: 1.179946; \nFastEstimator-Train: step: 8500; base_ce: 1.06356; steps/sec: 39.28; \nFastEstimator-Train: step: 9000; base_ce: 1.0581198; steps/sec: 41.16; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 24.9 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 9000; epoch: 9; base_ce: 1.1292516; adv_ce: 2.193486; base accuracy: 0.7054; adversarial accuracy: 0.4132; since_best_base_ce: 0; min_base_ce: 1.1292516; \nFastEstimator-Train: step: 9500; base_ce: 1.0268096; steps/sec: 38.59; \nFastEstimator-Train: step: 10000; base_ce: 1.2578716; steps/sec: 40.34; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 25.34 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; base_ce: 1.1553321; adv_ce: 2.3133547; base accuracy: 0.6994; adversarial accuracy: 0.4076; since_best_base_ce: 1; min_base_ce: 1.1292516; \nFastEstimator-Train: step: 10500; base_ce: 0.9665863; steps/sec: 35.17; \nFastEstimator-Train: step: 11000; base_ce: 0.9702922; steps/sec: 37.54; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 27.54 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 11000; epoch: 11; base_ce: 1.0691696; adv_ce: 2.2379944; base accuracy: 0.7196; adversarial accuracy: 0.4186; since_best_base_ce: 0; min_base_ce: 1.0691696; \nFastEstimator-Train: step: 11500; base_ce: 1.0323644; steps/sec: 35.62; \nFastEstimator-Train: step: 12000; base_ce: 0.7866042; steps/sec: 40.58; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 26.35 sec; \nFastEstimator-Eval: step: 12000; epoch: 12; base_ce: 1.0878896; adv_ce: 2.3281896; base accuracy: 0.7218; adversarial accuracy: 0.4046; since_best_base_ce: 1; min_base_ce: 1.0691696; \nFastEstimator-Train: step: 12500; base_ce: 0.80621284; steps/sec: 37.97; \nFastEstimator-Train: step: 13000; base_ce: 0.8485186; steps/sec: 38.49; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 26.18 sec; \nFastEstimator-Eval: step: 13000; epoch: 13; base_ce: 1.1110873; adv_ce: 2.3123732; base accuracy: 0.7134; adversarial accuracy: 0.428; since_best_base_ce: 2; min_base_ce: 1.0691696; \nFastEstimator-Train: step: 13500; base_ce: 0.6478882; steps/sec: 39.09; \nFastEstimator-Train: step: 14000; base_ce: 0.9344132; steps/sec: 41.71; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 24.77 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 14000; epoch: 14; base_ce: 1.0669813; adv_ce: 2.3520224; base accuracy: 0.7282; adversarial accuracy: 0.4164; since_best_base_ce: 0; min_base_ce: 1.0669813; \nFastEstimator-Train: step: 14500; base_ce: 0.7195329; steps/sec: 39.72; \nFastEstimator-Train: step: 15000; base_ce: 1.0449741; steps/sec: 42.56; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 24.33 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; base_ce: 1.0872117; adv_ce: 2.3199148; base accuracy: 0.7226; adversarial accuracy: 0.4142; since_best_base_ce: 1; min_base_ce: 1.0669813; \nFastEstimator-Train: step: 15500; base_ce: 0.8833719; steps/sec: 40.01; \nFastEstimator-Train: step: 16000; base_ce: 1.0172313; steps/sec: 41.95; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 24.42 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 16000; epoch: 16; base_ce: 1.0428473; adv_ce: 2.3096952; base accuracy: 0.732; adversarial accuracy: 0.434; since_best_base_ce: 0; min_base_ce: 1.0428473; \nFastEstimator-Train: step: 16500; base_ce: 0.9528541; steps/sec: 38.97; \nFastEstimator-Train: step: 17000; base_ce: 0.6921418; steps/sec: 41.57; \nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 24.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 17000; epoch: 17; base_ce: 1.0407811; adv_ce: 2.3547754; base accuracy: 0.7322; adversarial accuracy: 0.4232; since_best_base_ce: 0; min_base_ce: 1.0407811; \nFastEstimator-Train: step: 17500; base_ce: 0.85563445; steps/sec: 39.17; \nFastEstimator-Train: step: 18000; base_ce: 0.84442306; steps/sec: 41.73; \nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 24.74 sec; \nFastEstimator-Eval: step: 18000; epoch: 18; base_ce: 1.067384; adv_ce: 2.3860226; base accuracy: 0.7322; adversarial accuracy: 0.4146; since_best_base_ce: 1; min_base_ce: 1.0407811; \nFastEstimator-Train: step: 18500; base_ce: 0.6891441; steps/sec: 39.71; \nFastEstimator-Train: step: 19000; base_ce: 0.4970177; steps/sec: 41.33; \nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 24.69 sec; \nFastEstimator-Eval: step: 19000; epoch: 19; base_ce: 1.0554608; adv_ce: 2.3364587; base accuracy: 0.7318; adversarial accuracy: 0.4246; since_best_base_ce: 2; min_base_ce: 1.0407811; \nFastEstimator-Train: step: 19500; base_ce: 0.845229; steps/sec: 36.31; \nFastEstimator-Train: step: 20000; base_ce: 0.7282557; steps/sec: 37.79; \nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 27.0 sec; \nFastEstimator-Eval: step: 20000; epoch: 20; base_ce: 1.0579226; adv_ce: 2.3450043; base accuracy: 0.73; adversarial accuracy: 0.4272; since_best_base_ce: 3; min_base_ce: 1.0407811; \nFastEstimator-Train: step: 20500; base_ce: 0.6578657; steps/sec: 35.22; \nFastEstimator-Train: step: 21000; base_ce: 1.133787; steps/sec: 40.44; \nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 26.58 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 21000; epoch: 21; base_ce: 1.0174779; adv_ce: 2.310723; base accuracy: 0.7404; adversarial accuracy: 0.4388; since_best_base_ce: 0; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 21500; base_ce: 0.8653169; steps/sec: 39.44; \nFastEstimator-Train: step: 22000; base_ce: 0.661098; steps/sec: 42.19; \nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 24.51 sec; \nFastEstimator-Eval: step: 22000; epoch: 22; base_ce: 1.0703703; adv_ce: 2.414565; base accuracy: 0.7312; adversarial accuracy: 0.4176; since_best_base_ce: 1; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 22500; base_ce: 0.89490587; steps/sec: 38.64; \nFastEstimator-Train: step: 23000; base_ce: 0.9243805; steps/sec: 40.36; \nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 25.34 sec; \nFastEstimator-Eval: step: 23000; epoch: 23; base_ce: 1.0419381; adv_ce: 2.3707998; base accuracy: 0.7382; adversarial accuracy: 0.4332; since_best_base_ce: 2; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 23500; base_ce: 0.37459114; steps/sec: 39.22; \nFastEstimator-Train: step: 24000; base_ce: 0.77817583; steps/sec: 39.84; \nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 25.29 sec; \nFastEstimator-Eval: step: 24000; epoch: 24; base_ce: 1.0574633; adv_ce: 2.3102021; base accuracy: 0.7332; adversarial accuracy: 0.4494; since_best_base_ce: 3; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 24500; base_ce: 0.7001125; steps/sec: 38.9; \nFastEstimator-Train: step: 25000; base_ce: 0.6399208; steps/sec: 37.16; \nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 26.3 sec; \nFastEstimator-Eval: step: 25000; epoch: 25; base_ce: 1.0435; adv_ce: 2.3095825; base accuracy: 0.7404; adversarial accuracy: 0.446; since_best_base_ce: 4; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 25500; base_ce: 0.6391239; steps/sec: 38.9; \nFastEstimator-Train: step: 26000; base_ce: 0.91452485; steps/sec: 41.62; \nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 24.87 sec; \nFastEstimator-Eval: step: 26000; epoch: 26; base_ce: 1.0903724; adv_ce: 2.3547668; base accuracy: 0.7284; adversarial accuracy: 0.4442; since_best_base_ce: 5; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 26500; base_ce: 0.97995234; steps/sec: 39.34; \nFastEstimator-Train: step: 27000; base_ce: 0.7880356; steps/sec: 39.81; \nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 25.28 sec; \nFastEstimator-Eval: step: 27000; epoch: 27; base_ce: 1.04056; adv_ce: 2.3332062; base accuracy: 0.747; adversarial accuracy: 0.448; since_best_base_ce: 6; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 27500; base_ce: 0.7402453; steps/sec: 38.51; \nFastEstimator-Train: step: 28000; base_ce: 0.5625182; steps/sec: 39.95; \nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 25.49 sec; \nFastEstimator-Eval: step: 28000; epoch: 28; base_ce: 1.0731995; adv_ce: 2.4006343; base accuracy: 0.7344; adversarial accuracy: 0.4334; since_best_base_ce: 7; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 28500; base_ce: 0.6383535; steps/sec: 39.32; \nFastEstimator-Train: step: 29000; base_ce: 0.842826; steps/sec: 40.93; \nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 24.94 sec; \nFastEstimator-Eval: step: 29000; epoch: 29; base_ce: 1.0614614; adv_ce: 2.3708925; base accuracy: 0.7382; adversarial accuracy: 0.4478; since_best_base_ce: 8; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 29500; base_ce: 0.62300307; steps/sec: 38.82; \nFastEstimator-Train: step: 30000; base_ce: 0.7383355; steps/sec: 41.19; \nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 25.03 sec; \nFastEstimator-Eval: step: 30000; epoch: 30; base_ce: 1.0614293; adv_ce: 2.2735631; base accuracy: 0.7356; adversarial accuracy: 0.4646; since_best_base_ce: 9; min_base_ce: 1.0174779; \nFastEstimator-Train: step: 30500; base_ce: 0.61352885; steps/sec: 38.38; \nFastEstimator-Train: step: 31000; base_ce: 0.41669288; steps/sec: 40.06; \nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 25.51 sec; \nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 1.0174778699874878 at epoch 21\nFastEstimator-Eval: step: 31000; epoch: 31; base_ce: 1.0607773; adv_ce: 2.2538073; base accuracy: 0.7358; adversarial accuracy: 0.476; since_best_base_ce: 10; min_base_ce: 1.0174779; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp__tav_q_/hydra_ecc_best_base_ce.h5\nFastEstimator-Finish: step: 31000; total_time: 914.06 sec; hydra_ecc_lr: 0.001; \nFastEstimator-Test: step: 31000; epoch: 31; base accuracy: 0.7326; adversarial accuracy: 0.4258; \n</pre> In\u00a0[14]: Copied! <pre>logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'})\n</pre> logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'}) <p>As you can see, the conventional network using softmax to convert logits to class probabilities actually gets more and more vulnerable to adversarial attacks as training progresses. It also quickly overfits to the data, reaching an optimal performance around epoch 7. By simply switching the softmax layer for an error-correcting-code, the network is able to train for around 16 epochs before starting to cap out, and even then continuing to train it results in better and better adversarial performance. Creating a multi-headed ecc output layer allows still more training and higher peak performances. If you were to run the experiment out to 160 epochs you would find that the adversarial accuracy can reach between 60-70% with only a slight accuracy degradation on clean samples (performance still above 70%). This is significantly better performance than networks trained specifically to combat this attack, shown in the FGSM notebook. Note also that their is virtually no additional cost to training using ECC as opposed to softmax in terms of steps/sec. This is a big benefit over FGSM, where the training time for each step is doubled. With these benefits in mind, you may want to consider never using softmax again.</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#adversarial-robustness-with-error-correcting-codes", "title": "Adversarial Robustness with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#you-might-never-use-softmax-again", "title": "(You might never use Softmax again)\u00b6", "text": "<p>In this example we will show how using error correcting codes to convert model logits to probabilities can drastically reduce model overfitting while simultaneously increasing model robustness against adversarial attacks. In other words, why you should never use a softmax layer again. This phenomena was first publicized by the US Army in a 2019 Neurips Paper. For background on adversarial attacks, and on the attack type we will be demonstrating here, check out our FGSM apphub example. Note that in this apphub we will not be training against adversarial samples, but only performing adversarial attacks during evaluation to see how different models fair against them.</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#imports", "title": "Imports\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#getting-the-data", "title": "Getting the Data\u00b6", "text": "<p>For these experiments we will be using the CIFAR-10 Dataset</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#defining-an-estimator", "title": "Defining an Estimator\u00b6", "text": "<p>In this apphub we will be comparing three very similar models, all using the same training and evaluation routines. Hence a function to generate the estimators:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#the-models", "title": "The Models\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#1-a-lenet-model-with-softmax", "title": "1 - A LeNet model with Softmax\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#2-a-lenet-model-with-error-correcting-codes", "title": "2 - A LeNet model with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#3-a-lenet-model-using-ecc-and-multiple-feature-heads", "title": "3 - A LeNet model using ECC and multiple feature heads\u00b6", "text": "<p>While it is common practice to follow the feature extraction layers of convolution networks with several fully connected layers in order to perform classification, this can lead to the final logits being interdependent which can actually reduce the robustness of the network. One way around this is to divide your classification layers into multiple smaller independent units:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#the-experiments", "title": "The Experiments\u00b6", "text": "<p>Let's get Estimators for each of these models and compare them:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#comparing-the-results", "title": "Comparing the Results\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html", "title": "Adversarial Robustness with Error Correcting Codes (and Hinge Loss)", "text": "In\u00a0[1]: Copied! <pre>import math\nimport tempfile\n\nfrom tensorflow.python.keras import Sequential, layers\nfrom tensorflow.python.keras.layers import Concatenate, Conv2D, Dense, Flatten, Input, MaxPooling2D\nfrom tensorflow.python.keras.models import Model\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.layers.tensorflow import HadamardCode\nfrom fastestimator.op.numpyop.univariate import Hadamard, Normalize\nfrom fastestimator.op.tensorop import UnHadamard\nfrom fastestimator.op.tensorop.gradient import FGSM, Watch\nfrom fastestimator.op.tensorop.loss import CrossEntropy, Hinge\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import EarlyStopping\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import math import tempfile  from tensorflow.python.keras import Sequential, layers from tensorflow.python.keras.layers import Concatenate, Conv2D, Dense, Flatten, Input, MaxPooling2D from tensorflow.python.keras.models import Model  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifar10 from fastestimator.layers.tensorflow import HadamardCode from fastestimator.op.numpyop.univariate import Hadamard, Normalize from fastestimator.op.tensorop import UnHadamard from fastestimator.op.tensorop.gradient import FGSM, Watch from fastestimator.op.tensorop.loss import CrossEntropy, Hinge from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import EarlyStopping from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=60\nbatch_size=50\nlog_steps=500\nmax_train_steps_per_epoch=None\nmax_eval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=60 batch_size=50 log_steps=500 max_train_steps_per_epoch=None max_eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        Hadamard(inputs=\"y\", outputs=\"y_code\", n_classes=10)\n        ])\n</pre> train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         Hadamard(inputs=\"y\", outputs=\"y_code\", n_classes=10)         ]) In\u00a0[4]: Copied! <pre>def get_baseline_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        UpdateOp(model=model, loss_name=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_ce\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             max_train_steps_per_epoch=max_train_steps_per_epoch,\n                             max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                             monitor_names=[\"adv_ce\"])\n    return estimator\n</pre> def get_baseline_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         UpdateOp(model=model, loss_name=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_ce\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              max_train_steps_per_epoch=max_train_steps_per_epoch,                              max_eval_steps_per_epoch=max_eval_steps_per_epoch,                              monitor_names=[\"adv_ce\"])     return estimator In\u00a0[5]: Copied! <pre>def get_hinge_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred_code\"),\n        Hinge(inputs=(\"y_pred_code\", \"y_code\"), outputs=\"base_hinge\"),\n        UpdateOp(model=model, loss_name=\"base_hinge\"),\n        UnHadamard(inputs=\"y_pred_code\", outputs=\"y_pred\", n_classes=10, mode=('eval', 'test')),\n        # The adversarial attack:\n        FGSM(data=\"x\", loss=\"base_hinge\", outputs=\"x_adverse_hinge\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse_hinge\", outputs=\"y_pred_adv_hinge_code\", mode=('eval', 'test')),\n        Hinge(inputs=(\"y_pred_adv_hinge_code\", \"y_code\"), outputs=\"adv_hinge\", mode=('eval', 'test')),\n        UnHadamard(inputs=\"y_pred_adv_hinge_code\", outputs=\"y_pred_adv_hinge\", n_classes=10, mode=('eval', 'test')),\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv_hinge\", output_name=\"adversarial_accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_hinge\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_hinge\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             max_train_steps_per_epoch=max_train_steps_per_epoch,\n                             max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                             monitor_names=[\"adv_hinge\"])\n    return estimator\n</pre> def get_hinge_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred_code\"),         Hinge(inputs=(\"y_pred_code\", \"y_code\"), outputs=\"base_hinge\"),         UpdateOp(model=model, loss_name=\"base_hinge\"),         UnHadamard(inputs=\"y_pred_code\", outputs=\"y_pred\", n_classes=10, mode=('eval', 'test')),         # The adversarial attack:         FGSM(data=\"x\", loss=\"base_hinge\", outputs=\"x_adverse_hinge\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse_hinge\", outputs=\"y_pred_adv_hinge_code\", mode=('eval', 'test')),         Hinge(inputs=(\"y_pred_adv_hinge_code\", \"y_code\"), outputs=\"adv_hinge\", mode=('eval', 'test')),         UnHadamard(inputs=\"y_pred_adv_hinge_code\", outputs=\"y_pred_adv_hinge\", n_classes=10, mode=('eval', 'test')),     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv_hinge\", output_name=\"adversarial_accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_hinge\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_hinge\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              max_train_steps_per_epoch=max_train_steps_per_epoch,                              max_eval_steps_per_epoch=max_eval_steps_per_epoch,                              monitor_names=[\"adv_hinge\"])     return estimator In\u00a0[6]: Copied! <pre>softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax')\n</pre> softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax') In\u00a0[7]: Copied! <pre>def EccLeNet(input_shape=(32, 32, 3), code_length=16):\n    model = Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(code_length, activation='tanh'))  # Note that this is the only difference between this model and the FE LeNet implementation\n    return model\n</pre> def EccLeNet(input_shape=(32, 32, 3), code_length=16):     model = Sequential()     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.Flatten())     model.add(layers.Dense(64, activation='relu'))     model.add(layers.Dense(code_length, activation='tanh'))  # Note that this is the only difference between this model and the FE LeNet implementation     return model In\u00a0[8]: Copied! <pre>ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc')\n</pre> ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc') In\u00a0[9]: Copied! <pre>def HydraEccLeNet(input_shape=(32, 32, 3), code_length=16):\n    inputs = Input(input_shape)\n    conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)\n    flat = Flatten()(conv3)\n    # Create multiple heads\n    n_heads = 4\n    heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]\n    heads2 = [Dense(code_length // n_heads, activation='tanh')(head) for head in heads]\n    outputs = Concatenate()(heads2)\n    return Model(inputs=inputs, outputs=outputs)\n</pre> def HydraEccLeNet(input_shape=(32, 32, 3), code_length=16):     inputs = Input(input_shape)     conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)     pool1 = MaxPooling2D((2, 2))(conv1)     conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)     pool2 = MaxPooling2D((2, 2))(conv2)     conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)     flat = Flatten()(conv3)     # Create multiple heads     n_heads = 4     heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]     heads2 = [Dense(code_length // n_heads, activation='tanh')(head) for head in heads]     outputs = Concatenate()(heads2)     return Model(inputs=inputs, outputs=outputs) In\u00a0[10]: Copied! <pre>hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc')\n</pre> hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc') In\u00a0[11]: Copied! <pre>softmax_estimator = get_baseline_estimator(softmax_model)\necc_estimator = get_hinge_estimator(ecc_model)\nhydra_estimator = get_hinge_estimator(hydra_model)\n</pre> softmax_estimator = get_baseline_estimator(softmax_model) ecc_estimator = get_hinge_estimator(ecc_model) hydra_estimator = get_hinge_estimator(hydra_model) In\u00a0[12]: Copied! <pre>softmax_estimator.fit('Softmax')\nsoftmax_results = softmax_estimator.test()\n</pre> softmax_estimator.fit('Softmax') softmax_results = softmax_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_ce: 2.2913797; \nFastEstimator-Train: step: 500; base_ce: 1.129351; steps/sec: 80.35; \nFastEstimator-Train: step: 1000; base_ce: 0.9674388; steps/sec: 66.16; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 14.46 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 2.0666945; base_ce: 1.1314628; base_accuracy: 0.5976; adversarial_accuracy: 0.2934; since_best_base_ce: 0; min_base_ce: 1.1314628; \nFastEstimator-Train: step: 1500; base_ce: 0.7620763; steps/sec: 88.76; \nFastEstimator-Train: step: 2000; base_ce: 1.2483466; steps/sec: 80.25; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 11.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 2.1358154; base_ce: 0.9702002; base_accuracy: 0.6556; adversarial_accuracy: 0.2818; since_best_base_ce: 0; min_base_ce: 0.9702002; \nFastEstimator-Train: step: 2500; base_ce: 0.6228955; steps/sec: 82.13; \nFastEstimator-Train: step: 3000; base_ce: 0.85292035; steps/sec: 69.24; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 13.31 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 2.2940953; base_ce: 0.8567269; base_accuracy: 0.7016; adversarial_accuracy: 0.2634; since_best_base_ce: 0; min_base_ce: 0.8567269; \nFastEstimator-Train: step: 3500; base_ce: 1.0757314; steps/sec: 67.03; \nFastEstimator-Train: step: 4000; base_ce: 0.8255354; steps/sec: 64.26; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 15.24 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 2.4554718; base_ce: 0.81987983; base_accuracy: 0.7228; adversarial_accuracy: 0.2714; since_best_base_ce: 0; min_base_ce: 0.81987983; \nFastEstimator-Train: step: 4500; base_ce: 0.5227963; steps/sec: 61.48; \nFastEstimator-Train: step: 5000; base_ce: 0.6290733; steps/sec: 62.42; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 16.14 sec; \nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 2.7408102; base_ce: 0.82819456; base_accuracy: 0.7202; adversarial_accuracy: 0.2584; since_best_base_ce: 1; min_base_ce: 0.81987983; \nFastEstimator-Train: step: 5500; base_ce: 0.46184003; steps/sec: 60.35; \nFastEstimator-Train: step: 6000; base_ce: 0.74710757; steps/sec: 63.8; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 16.13 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 3.0637393; base_ce: 0.8165515; base_accuracy: 0.7276; adversarial_accuracy: 0.2316; since_best_base_ce: 0; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 6500; base_ce: 0.6648202; steps/sec: 59.67; \nFastEstimator-Train: step: 7000; base_ce: 0.5298349; steps/sec: 63.3; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 16.28 sec; \nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 3.450221; base_ce: 0.8594794; base_accuracy: 0.7202; adversarial_accuracy: 0.2226; since_best_base_ce: 1; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 7500; base_ce: 0.55798185; steps/sec: 60.22; \nFastEstimator-Train: step: 8000; base_ce: 0.71712387; steps/sec: 61.41; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 16.45 sec; \nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 3.7345269; base_ce: 0.8529271; base_accuracy: 0.7304; adversarial_accuracy: 0.223; since_best_base_ce: 2; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 8500; base_ce: 0.52675784; steps/sec: 60.57; \nFastEstimator-Train: step: 9000; base_ce: 0.4638951; steps/sec: 60.67; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 16.49 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 3.92901; base_ce: 0.862138; base_accuracy: 0.7234; adversarial_accuracy: 0.2072; since_best_base_ce: 3; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 9500; base_ce: 0.3654891; steps/sec: 59.84; \nFastEstimator-Train: step: 10000; base_ce: 0.24989706; steps/sec: 61.97; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 16.43 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 4.546957; base_ce: 0.96095663; base_accuracy: 0.7118; adversarial_accuracy: 0.1848; since_best_base_ce: 4; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 10500; base_ce: 0.3248038; steps/sec: 58.92; \nFastEstimator-Train: step: 11000; base_ce: 0.41753396; steps/sec: 62.94; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 16.43 sec; \nFastEstimator-Eval: step: 11000; epoch: 11; adv_ce: 5.142259; base_ce: 0.99953157; base_accuracy: 0.7168; adversarial_accuracy: 0.1872; since_best_base_ce: 5; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 11500; base_ce: 0.41104344; steps/sec: 57.45; \nFastEstimator-Train: step: 12000; base_ce: 0.26091018; steps/sec: 64.64; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 16.44 sec; \nFastEstimator-Eval: step: 12000; epoch: 12; adv_ce: 5.3693953; base_ce: 1.0091112; base_accuracy: 0.7152; adversarial_accuracy: 0.1794; since_best_base_ce: 6; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 12500; base_ce: 0.24132206; steps/sec: 60.3; \nFastEstimator-Train: step: 13000; base_ce: 0.23491889; steps/sec: 60.01; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 16.62 sec; \nFastEstimator-Eval: step: 13000; epoch: 13; adv_ce: 6.1618037; base_ce: 1.1255364; base_accuracy: 0.7186; adversarial_accuracy: 0.179; since_best_base_ce: 7; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 13500; base_ce: 0.20184961; steps/sec: 64.37; \nFastEstimator-Train: step: 14000; base_ce: 0.29378676; steps/sec: 73.5; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 14.57 sec; \nFastEstimator-Eval: step: 14000; epoch: 14; adv_ce: 6.383223; base_ce: 1.1051223; base_accuracy: 0.719; adversarial_accuracy: 0.174; since_best_base_ce: 8; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 14500; base_ce: 0.21271034; steps/sec: 78.73; \nFastEstimator-Train: step: 15000; base_ce: 0.200208; steps/sec: 82.96; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 12.38 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; adv_ce: 6.888222; base_ce: 1.1569692; base_accuracy: 0.7176; adversarial_accuracy: 0.1572; since_best_base_ce: 9; min_base_ce: 0.8165515; \nFastEstimator-Train: step: 15500; base_ce: 0.2577537; steps/sec: 82.62; \nFastEstimator-Train: step: 16000; base_ce: 0.12891757; steps/sec: 84.56; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 11.97 sec; \nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 0.8165515065193176 at epoch 6\nFastEstimator-Eval: step: 16000; epoch: 16; adv_ce: 7.65746; base_ce: 1.2928303; base_accuracy: 0.7038; adversarial_accuracy: 0.1596; since_best_base_ce: 10; min_base_ce: 0.8165515; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/softmax_best_base_ce.h5\nFastEstimator-Finish: step: 16000; total_time: 277.1 sec; softmax_lr: 0.001; \nFastEstimator-Test: step: 16000; epoch: 16; base_accuracy: 0.7188; adversarial_accuracy: 0.2392; \n</pre> In\u00a0[13]: Copied! <pre>ecc_estimator.fit('ECC')\necc_results = ecc_estimator.test()\n</pre> ecc_estimator.fit('ECC') ecc_results = ecc_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_hinge: 0.9720483; \nFastEstimator-Train: step: 500; base_hinge: 0.65065217; steps/sec: 86.2; \nFastEstimator-Train: step: 1000; base_hinge: 0.48177832; steps/sec: 83.56; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 12.04 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_hinge: 0.724342; base_hinge: 0.5987196; base_accuracy: 0.4514; adversarial_accuracy: 0.2944; since_best_base_hinge: 0; min_base_hinge: 0.5987196; \nFastEstimator-Train: step: 1500; base_hinge: 0.5693387; steps/sec: 81.56; \nFastEstimator-Train: step: 2000; base_hinge: 0.541312; steps/sec: 80.84; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 12.32 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_hinge: 0.7137549; base_hinge: 0.54428804; base_accuracy: 0.537; adversarial_accuracy: 0.3044; since_best_base_hinge: 0; min_base_hinge: 0.54428804; \nFastEstimator-Train: step: 2500; base_hinge: 0.55899805; steps/sec: 78.96; \nFastEstimator-Train: step: 3000; base_hinge: 0.45505947; steps/sec: 77.87; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 12.76 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_hinge: 0.6705899; base_hinge: 0.45147446; base_accuracy: 0.6176; adversarial_accuracy: 0.3572; since_best_base_hinge: 0; min_base_hinge: 0.45147446; \nFastEstimator-Train: step: 3500; base_hinge: 0.43476373; steps/sec: 74.89; \nFastEstimator-Train: step: 4000; base_hinge: 0.38228527; steps/sec: 75.76; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 13.27 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_hinge: 0.68376756; base_hinge: 0.41109648; base_accuracy: 0.6318; adversarial_accuracy: 0.3414; since_best_base_hinge: 0; min_base_hinge: 0.41109648; \nFastEstimator-Train: step: 4500; base_hinge: 0.44442576; steps/sec: 79.9; \nFastEstimator-Train: step: 5000; base_hinge: 0.3891029; steps/sec: 82.32; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 12.34 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_hinge: 0.6567371; base_hinge: 0.36597657; base_accuracy: 0.6568; adversarial_accuracy: 0.363; since_best_base_hinge: 0; min_base_hinge: 0.36597657; \nFastEstimator-Train: step: 5500; base_hinge: 0.3704931; steps/sec: 77.45; \nFastEstimator-Train: step: 6000; base_hinge: 0.40984428; steps/sec: 73.35; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 13.27 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_hinge: 0.65801245; base_hinge: 0.34470206; base_accuracy: 0.678; adversarial_accuracy: 0.3656; since_best_base_hinge: 0; min_base_hinge: 0.34470206; \nFastEstimator-Train: step: 6500; base_hinge: 0.3440132; steps/sec: 80.15; \nFastEstimator-Train: step: 7000; base_hinge: 0.2628339; steps/sec: 83.06; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 12.25 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_hinge: 0.65256685; base_hinge: 0.32853946; base_accuracy: 0.6916; adversarial_accuracy: 0.371; since_best_base_hinge: 0; min_base_hinge: 0.32853946; \nFastEstimator-Train: step: 7500; base_hinge: 0.18051398; steps/sec: 81.6; \nFastEstimator-Train: step: 8000; base_hinge: 0.23352867; steps/sec: 81.08; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 12.3 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_hinge: 0.64836806; base_hinge: 0.32261345; base_accuracy: 0.6926; adversarial_accuracy: 0.37; since_best_base_hinge: 0; min_base_hinge: 0.32261345; \nFastEstimator-Train: step: 8500; base_hinge: 0.24980038; steps/sec: 75.69; \nFastEstimator-Train: step: 9000; base_hinge: 0.2729612; steps/sec: 74.84; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 13.29 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; adv_hinge: 0.6530391; base_hinge: 0.32772756; base_accuracy: 0.6906; adversarial_accuracy: 0.3626; since_best_base_hinge: 1; min_base_hinge: 0.32261345; \nFastEstimator-Train: step: 9500; base_hinge: 0.28272608; steps/sec: 79.62; \nFastEstimator-Train: step: 10000; base_hinge: 0.11323662; steps/sec: 82.91; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 12.3 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_hinge: 0.6115315; base_hinge: 0.3017887; base_accuracy: 0.7164; adversarial_accuracy: 0.4026; since_best_base_hinge: 0; min_base_hinge: 0.3017887; \nFastEstimator-Train: step: 10500; base_hinge: 0.1635123; steps/sec: 80.01; \nFastEstimator-Train: step: 11000; base_hinge: 0.28864193; steps/sec: 82.4; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 12.32 sec; \nFastEstimator-Eval: step: 11000; epoch: 11; adv_hinge: 0.6173514; base_hinge: 0.3180277; base_accuracy: 0.6968; adversarial_accuracy: 0.3988; since_best_base_hinge: 1; min_base_hinge: 0.3017887; \nFastEstimator-Train: step: 11500; base_hinge: 0.28502467; steps/sec: 79.03; \nFastEstimator-Train: step: 12000; base_hinge: 0.24555106; steps/sec: 79.15; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 12.65 sec; \nFastEstimator-Eval: step: 12000; epoch: 12; adv_hinge: 0.606781; base_hinge: 0.3200605; base_accuracy: 0.6948; adversarial_accuracy: 0.4114; since_best_base_hinge: 2; min_base_hinge: 0.3017887; \nFastEstimator-Train: step: 12500; base_hinge: 0.15662248; steps/sec: 76.64; \nFastEstimator-Train: step: 13000; base_hinge: 0.2020681; steps/sec: 80.51; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 12.73 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 13000; epoch: 13; adv_hinge: 0.5785308; base_hinge: 0.29301724; base_accuracy: 0.7168; adversarial_accuracy: 0.4378; since_best_base_hinge: 0; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 13500; base_hinge: 0.22861297; steps/sec: 77.51; \nFastEstimator-Train: step: 14000; base_hinge: 0.3236181; steps/sec: 81.78; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 12.57 sec; \nFastEstimator-Eval: step: 14000; epoch: 14; adv_hinge: 0.5728293; base_hinge: 0.31178313; base_accuracy: 0.7008; adversarial_accuracy: 0.4438; since_best_base_hinge: 1; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 14500; base_hinge: 0.2981937; steps/sec: 79.19; \nFastEstimator-Train: step: 15000; base_hinge: 0.22167464; steps/sec: 81.08; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 12.48 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; adv_hinge: 0.56800824; base_hinge: 0.3034843; base_accuracy: 0.711; adversarial_accuracy: 0.453; since_best_base_hinge: 2; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 15500; base_hinge: 0.13327494; steps/sec: 78.79; \nFastEstimator-Train: step: 16000; base_hinge: 0.13513847; steps/sec: 83.04; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 12.37 sec; \nFastEstimator-Eval: step: 16000; epoch: 16; adv_hinge: 0.53621924; base_hinge: 0.2944945; base_accuracy: 0.7176; adversarial_accuracy: 0.4784; since_best_base_hinge: 3; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 16500; base_hinge: 0.24307278; steps/sec: 79.35; \nFastEstimator-Train: step: 17000; base_hinge: 0.21438818; steps/sec: 80.14; \nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 12.55 sec; \nFastEstimator-Eval: step: 17000; epoch: 17; adv_hinge: 0.53243375; base_hinge: 0.2946877; base_accuracy: 0.7208; adversarial_accuracy: 0.4866; since_best_base_hinge: 4; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 17500; base_hinge: 0.2600304; steps/sec: 76.64; \nFastEstimator-Train: step: 18000; base_hinge: 0.2710699; steps/sec: 80.69; \nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 12.71 sec; \nFastEstimator-Eval: step: 18000; epoch: 18; adv_hinge: 0.52874297; base_hinge: 0.3025954; base_accuracy: 0.7128; adversarial_accuracy: 0.493; since_best_base_hinge: 5; min_base_hinge: 0.29301724; \nFastEstimator-Train: step: 18500; base_hinge: 0.1871714; steps/sec: 77.07; \nFastEstimator-Train: step: 19000; base_hinge: 0.24277395; steps/sec: 79.41; \nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 12.78 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 19000; epoch: 19; adv_hinge: 0.51166725; base_hinge: 0.29020032; base_accuracy: 0.7236; adversarial_accuracy: 0.5082; since_best_base_hinge: 0; min_base_hinge: 0.29020032; \nFastEstimator-Train: step: 19500; base_hinge: 0.13240014; steps/sec: 77.01; \nFastEstimator-Train: step: 20000; base_hinge: 0.2651819; steps/sec: 81.51; \nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 12.63 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 20000; epoch: 20; adv_hinge: 0.49882263; base_hinge: 0.28652295; base_accuracy: 0.7258; adversarial_accuracy: 0.5216; since_best_base_hinge: 0; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 20500; base_hinge: 0.14134462; steps/sec: 77.67; \nFastEstimator-Train: step: 21000; base_hinge: 0.21163704; steps/sec: 81.18; \nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 12.6 sec; \nFastEstimator-Eval: step: 21000; epoch: 21; adv_hinge: 0.50655615; base_hinge: 0.29529086; base_accuracy: 0.7192; adversarial_accuracy: 0.513; since_best_base_hinge: 1; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 21500; base_hinge: 0.21828929; steps/sec: 78.85; \nFastEstimator-Train: step: 22000; base_hinge: 0.32995892; steps/sec: 83.66; \nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 12.32 sec; \nFastEstimator-Eval: step: 22000; epoch: 22; adv_hinge: 0.5129851; base_hinge: 0.2939868; base_accuracy: 0.7178; adversarial_accuracy: 0.5082; since_best_base_hinge: 2; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 22500; base_hinge: 0.21359493; steps/sec: 79.11; \nFastEstimator-Train: step: 23000; base_hinge: 0.21484666; steps/sec: 80.08; \nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 12.56 sec; \nFastEstimator-Eval: step: 23000; epoch: 23; adv_hinge: 0.50230604; base_hinge: 0.2944177; base_accuracy: 0.721; adversarial_accuracy: 0.52; since_best_base_hinge: 3; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 23500; base_hinge: 0.2986081; steps/sec: 76.6; \nFastEstimator-Train: step: 24000; base_hinge: 0.11756951; steps/sec: 81.32; \nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 12.67 sec; \nFastEstimator-Eval: step: 24000; epoch: 24; adv_hinge: 0.49573535; base_hinge: 0.29334903; base_accuracy: 0.718; adversarial_accuracy: 0.526; since_best_base_hinge: 4; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 24500; base_hinge: 0.24381065; steps/sec: 76.59; \nFastEstimator-Train: step: 25000; base_hinge: 0.1649466; steps/sec: 79.48; \nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 12.82 sec; \nFastEstimator-Eval: step: 25000; epoch: 25; adv_hinge: 0.4862012; base_hinge: 0.30284664; base_accuracy: 0.7108; adversarial_accuracy: 0.5342; since_best_base_hinge: 5; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 25500; base_hinge: 0.16841447; steps/sec: 77.41; \nFastEstimator-Train: step: 26000; base_hinge: 0.16662271; steps/sec: 81.66; \nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 12.58 sec; \nFastEstimator-Eval: step: 26000; epoch: 26; adv_hinge: 0.47293594; base_hinge: 0.29134712; base_accuracy: 0.7232; adversarial_accuracy: 0.548; since_best_base_hinge: 6; min_base_hinge: 0.28652295; \nFastEstimator-Train: step: 26500; base_hinge: 0.21119633; steps/sec: 77.31; \nFastEstimator-Train: step: 27000; base_hinge: 0.17389616; steps/sec: 81.15; \nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 12.63 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 27000; epoch: 27; adv_hinge: 0.46093524; base_hinge: 0.28287578; base_accuracy: 0.7288; adversarial_accuracy: 0.5614; since_best_base_hinge: 0; min_base_hinge: 0.28287578; \nFastEstimator-Train: step: 27500; base_hinge: 0.05262907; steps/sec: 78.62; \nFastEstimator-Train: step: 28000; base_hinge: 0.14051457; steps/sec: 82.26; \nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 12.44 sec; \nFastEstimator-Eval: step: 28000; epoch: 28; adv_hinge: 0.4624555; base_hinge: 0.28717962; base_accuracy: 0.7276; adversarial_accuracy: 0.5606; since_best_base_hinge: 1; min_base_hinge: 0.28287578; \nFastEstimator-Train: step: 28500; base_hinge: 0.06206832; steps/sec: 80.6; \nFastEstimator-Train: step: 29000; base_hinge: 0.22145921; steps/sec: 82.87; \nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 12.24 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 29000; epoch: 29; adv_hinge: 0.456716; base_hinge: 0.28246412; base_accuracy: 0.7322; adversarial_accuracy: 0.564; since_best_base_hinge: 0; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 29500; base_hinge: 0.15014073; steps/sec: 77.42; \nFastEstimator-Train: step: 30000; base_hinge: 0.20060506; steps/sec: 80.7; \nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 12.65 sec; \nFastEstimator-Eval: step: 30000; epoch: 30; adv_hinge: 0.4513559; base_hinge: 0.2847252; base_accuracy: 0.7274; adversarial_accuracy: 0.57; since_best_base_hinge: 1; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 30500; base_hinge: 0.15612331; steps/sec: 76.59; \nFastEstimator-Train: step: 31000; base_hinge: 0.2618798; steps/sec: 81.39; \nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 12.67 sec; \nFastEstimator-Eval: step: 31000; epoch: 31; adv_hinge: 0.44176352; base_hinge: 0.28683442; base_accuracy: 0.7302; adversarial_accuracy: 0.5806; since_best_base_hinge: 2; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 31500; base_hinge: 0.13087274; steps/sec: 76.98; \nFastEstimator-Train: step: 32000; base_hinge: 0.22726895; steps/sec: 80.06; \nFastEstimator-Train: step: 32000; epoch: 32; epoch_time: 12.74 sec; \nFastEstimator-Eval: step: 32000; epoch: 32; adv_hinge: 0.4663296; base_hinge: 0.29836673; base_accuracy: 0.7204; adversarial_accuracy: 0.557; since_best_base_hinge: 3; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 32500; base_hinge: 0.17779167; steps/sec: 77.87; \nFastEstimator-Train: step: 33000; base_hinge: 0.14685188; steps/sec: 80.8; \nFastEstimator-Train: step: 33000; epoch: 33; epoch_time: 12.61 sec; \nFastEstimator-Eval: step: 33000; epoch: 33; adv_hinge: 0.44809714; base_hinge: 0.28558087; base_accuracy: 0.7282; adversarial_accuracy: 0.5716; since_best_base_hinge: 4; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 33500; base_hinge: 0.18333197; steps/sec: 77.47; \nFastEstimator-Train: step: 34000; base_hinge: 0.1574696; steps/sec: 80.22; \nFastEstimator-Train: step: 34000; epoch: 34; epoch_time: 12.69 sec; \nFastEstimator-Eval: step: 34000; epoch: 34; adv_hinge: 0.44056252; base_hinge: 0.28778878; base_accuracy: 0.7264; adversarial_accuracy: 0.5796; since_best_base_hinge: 5; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 34500; base_hinge: 0.21325463; steps/sec: 77.05; \nFastEstimator-Train: step: 35000; base_hinge: 0.20648474; steps/sec: 80.68; \nFastEstimator-Train: step: 35000; epoch: 35; epoch_time: 12.69 sec; \nFastEstimator-Eval: step: 35000; epoch: 35; adv_hinge: 0.42195377; base_hinge: 0.2834217; base_accuracy: 0.7322; adversarial_accuracy: 0.6006; since_best_base_hinge: 6; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 35500; base_hinge: 0.14986329; steps/sec: 76.76; \nFastEstimator-Train: step: 36000; base_hinge: 0.1910067; steps/sec: 80.55; \nFastEstimator-Train: step: 36000; epoch: 36; epoch_time: 12.72 sec; \nFastEstimator-Eval: step: 36000; epoch: 36; adv_hinge: 0.43846917; base_hinge: 0.28581482; base_accuracy: 0.7264; adversarial_accuracy: 0.5842; since_best_base_hinge: 7; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 36500; base_hinge: 0.093400106; steps/sec: 76.1; \nFastEstimator-Train: step: 37000; base_hinge: 0.20568691; steps/sec: 79.59; \nFastEstimator-Train: step: 37000; epoch: 37; epoch_time: 12.85 sec; \nFastEstimator-Eval: step: 37000; epoch: 37; adv_hinge: 0.42621788; base_hinge: 0.28850666; base_accuracy: 0.7252; adversarial_accuracy: 0.5944; since_best_base_hinge: 8; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 37500; base_hinge: 0.14697212; steps/sec: 76.41; \nFastEstimator-Train: step: 38000; base_hinge: 0.17251854; steps/sec: 79.85; \nFastEstimator-Train: step: 38000; epoch: 38; epoch_time: 12.81 sec; \nFastEstimator-Eval: step: 38000; epoch: 38; adv_hinge: 0.4249133; base_hinge: 0.28864872; base_accuracy: 0.7222; adversarial_accuracy: 0.5958; since_best_base_hinge: 9; min_base_hinge: 0.28246412; \nFastEstimator-Train: step: 38500; base_hinge: 0.1266299; steps/sec: 78.27; \nFastEstimator-Train: step: 39000; base_hinge: 0.14579202; steps/sec: 81.51; \nFastEstimator-Train: step: 39000; epoch: 39; epoch_time: 12.52 sec; \nFastEstimator-EarlyStopping: 'base_hinge' triggered an early stop. Its best value was 0.2824641168117523 at epoch 29\nFastEstimator-Eval: step: 39000; epoch: 39; adv_hinge: 0.4217629; base_hinge: 0.28681317; base_accuracy: 0.7264; adversarial_accuracy: 0.5978; since_best_base_hinge: 10; min_base_hinge: 0.28246412; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/ecc_best_base_hinge.h5\nFastEstimator-Finish: step: 39000; total_time: 562.65 sec; ecc_lr: 0.001; \nFastEstimator-Test: step: 39000; epoch: 39; base_accuracy: 0.724; adversarial_accuracy: 0.5696; \n</pre> In\u00a0[14]: Copied! <pre>hydra_estimator.fit('Hydra')\nhydra_results = hydra_estimator.test()\n</pre> hydra_estimator.fit('Hydra') hydra_results = hydra_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; base_hinge: 1.007727; \nFastEstimator-Train: step: 500; base_hinge: 0.7029223; steps/sec: 79.37; \nFastEstimator-Train: step: 1000; base_hinge: 0.5938768; steps/sec: 80.64; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 12.92 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_hinge: 0.7107463; base_hinge: 0.58592; base_accuracy: 0.444; adversarial_accuracy: 0.2916; since_best_base_hinge: 0; min_base_hinge: 0.58592; \nFastEstimator-Train: step: 1500; base_hinge: 0.5222081; steps/sec: 77.67; \nFastEstimator-Train: step: 2000; base_hinge: 0.49780965; steps/sec: 78.4; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 12.82 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_hinge: 0.6803317; base_hinge: 0.5114545; base_accuracy: 0.5206; adversarial_accuracy: 0.3412; since_best_base_hinge: 0; min_base_hinge: 0.5114545; \nFastEstimator-Train: step: 2500; base_hinge: 0.5207081; steps/sec: 78.19; \nFastEstimator-Train: step: 3000; base_hinge: 0.46403983; steps/sec: 79.88; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 12.65 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_hinge: 0.6644043; base_hinge: 0.46950454; base_accuracy: 0.567; adversarial_accuracy: 0.3488; since_best_base_hinge: 0; min_base_hinge: 0.46950454; \nFastEstimator-Train: step: 3500; base_hinge: 0.40603283; steps/sec: 75.42; \nFastEstimator-Train: step: 4000; base_hinge: 0.3923445; steps/sec: 77.49; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 13.09 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_hinge: 0.64425445; base_hinge: 0.43538117; base_accuracy: 0.5906; adversarial_accuracy: 0.381; since_best_base_hinge: 0; min_base_hinge: 0.43538117; \nFastEstimator-Train: step: 4500; base_hinge: 0.39230484; steps/sec: 78.23; \nFastEstimator-Train: step: 5000; base_hinge: 0.3513677; steps/sec: 80.93; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 12.57 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_hinge: 0.63775545; base_hinge: 0.41264045; base_accuracy: 0.622; adversarial_accuracy: 0.3874; since_best_base_hinge: 0; min_base_hinge: 0.41264045; \nFastEstimator-Train: step: 5500; base_hinge: 0.333709; steps/sec: 77.66; \nFastEstimator-Train: step: 6000; base_hinge: 0.42386734; steps/sec: 79.09; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 12.75 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_hinge: 0.63049006; base_hinge: 0.38785324; base_accuracy: 0.651; adversarial_accuracy: 0.39; since_best_base_hinge: 0; min_base_hinge: 0.38785324; \nFastEstimator-Train: step: 6500; base_hinge: 0.3881634; steps/sec: 78.18; \nFastEstimator-Train: step: 7000; base_hinge: 0.4034718; steps/sec: 79.46; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 12.69 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_hinge: 0.641855; base_hinge: 0.38448378; base_accuracy: 0.647; adversarial_accuracy: 0.381; since_best_base_hinge: 0; min_base_hinge: 0.38448378; \nFastEstimator-Train: step: 7500; base_hinge: 0.34052792; steps/sec: 80.53; \nFastEstimator-Train: step: 8000; base_hinge: 0.43806535; steps/sec: 83.03; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 12.23 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_hinge: 0.62571025; base_hinge: 0.36942664; base_accuracy: 0.6638; adversarial_accuracy: 0.3916; since_best_base_hinge: 0; min_base_hinge: 0.36942664; \nFastEstimator-Train: step: 8500; base_hinge: 0.33974144; steps/sec: 78.38; \nFastEstimator-Train: step: 9000; base_hinge: 0.31150648; steps/sec: 71.43; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 13.38 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 9000; epoch: 9; adv_hinge: 0.6160934; base_hinge: 0.3464116; base_accuracy: 0.6832; adversarial_accuracy: 0.4014; since_best_base_hinge: 0; min_base_hinge: 0.3464116; \nFastEstimator-Train: step: 9500; base_hinge: 0.23986004; steps/sec: 77.28; \nFastEstimator-Train: step: 10000; base_hinge: 0.41196102; steps/sec: 81.07; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 12.66 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; adv_hinge: 0.6257987; base_hinge: 0.35541627; base_accuracy: 0.6738; adversarial_accuracy: 0.3974; since_best_base_hinge: 1; min_base_hinge: 0.3464116; \nFastEstimator-Train: step: 10500; base_hinge: 0.26076213; steps/sec: 75.51; \nFastEstimator-Train: step: 11000; base_hinge: 0.32601464; steps/sec: 75.36; \nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 13.23 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 11000; epoch: 11; adv_hinge: 0.62028766; base_hinge: 0.33794487; base_accuracy: 0.6956; adversarial_accuracy: 0.3998; since_best_base_hinge: 0; min_base_hinge: 0.33794487; \nFastEstimator-Train: step: 11500; base_hinge: 0.21475668; steps/sec: 72.14; \nFastEstimator-Train: step: 12000; base_hinge: 0.3604712; steps/sec: 70.87; \nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 13.99 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 12000; epoch: 12; adv_hinge: 0.6220209; base_hinge: 0.33451056; base_accuracy: 0.697; adversarial_accuracy: 0.3944; since_best_base_hinge: 0; min_base_hinge: 0.33451056; \nFastEstimator-Train: step: 12500; base_hinge: 0.30068347; steps/sec: 73.65; \nFastEstimator-Train: step: 13000; base_hinge: 0.30370423; steps/sec: 75.43; \nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 13.42 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 13000; epoch: 13; adv_hinge: 0.616508; base_hinge: 0.33238065; base_accuracy: 0.6948; adversarial_accuracy: 0.3986; since_best_base_hinge: 0; min_base_hinge: 0.33238065; \nFastEstimator-Train: step: 13500; base_hinge: 0.30079776; steps/sec: 74.79; \nFastEstimator-Train: step: 14000; base_hinge: 0.2283918; steps/sec: 78.21; \nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 13.07 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 14000; epoch: 14; adv_hinge: 0.6049673; base_hinge: 0.33148763; base_accuracy: 0.698; adversarial_accuracy: 0.409; since_best_base_hinge: 0; min_base_hinge: 0.33148763; \nFastEstimator-Train: step: 14500; base_hinge: 0.3066492; steps/sec: 74.76; \nFastEstimator-Train: step: 15000; base_hinge: 0.2918469; steps/sec: 77.99; \nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 13.1 sec; \nFastEstimator-Eval: step: 15000; epoch: 15; adv_hinge: 0.6270892; base_hinge: 0.33353606; base_accuracy: 0.7; adversarial_accuracy: 0.386; since_best_base_hinge: 1; min_base_hinge: 0.33148763; \nFastEstimator-Train: step: 15500; base_hinge: 0.24441415; steps/sec: 75.55; \nFastEstimator-Train: step: 16000; base_hinge: 0.22781277; steps/sec: 78.93; \nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 12.95 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 16000; epoch: 16; adv_hinge: 0.6047035; base_hinge: 0.32660407; base_accuracy: 0.7004; adversarial_accuracy: 0.4098; since_best_base_hinge: 0; min_base_hinge: 0.32660407; \nFastEstimator-Train: step: 16500; base_hinge: 0.2558497; steps/sec: 75.78; \nFastEstimator-Train: step: 17000; base_hinge: 0.2882976; steps/sec: 79.81; \nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 12.86 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 17000; epoch: 17; adv_hinge: 0.6056667; base_hinge: 0.3208551; base_accuracy: 0.7022; adversarial_accuracy: 0.4028; since_best_base_hinge: 0; min_base_hinge: 0.3208551; \nFastEstimator-Train: step: 17500; base_hinge: 0.20751992; steps/sec: 76.49; \nFastEstimator-Train: step: 18000; base_hinge: 0.16293184; steps/sec: 79.81; \nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 12.8 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 18000; epoch: 18; adv_hinge: 0.5919955; base_hinge: 0.31459105; base_accuracy: 0.7128; adversarial_accuracy: 0.42; since_best_base_hinge: 0; min_base_hinge: 0.31459105; \nFastEstimator-Train: step: 18500; base_hinge: 0.2297887; steps/sec: 75.47; \nFastEstimator-Train: step: 19000; base_hinge: 0.2184592; steps/sec: 77.62; \nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 13.07 sec; \nFastEstimator-Eval: step: 19000; epoch: 19; adv_hinge: 0.59401447; base_hinge: 0.31972674; base_accuracy: 0.7112; adversarial_accuracy: 0.414; since_best_base_hinge: 1; min_base_hinge: 0.31459105; \nFastEstimator-Train: step: 19500; base_hinge: 0.2566901; steps/sec: 75.37; \nFastEstimator-Train: step: 20000; base_hinge: 0.31192818; steps/sec: 79.2; \nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 12.95 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 20000; epoch: 20; adv_hinge: 0.58349407; base_hinge: 0.31102765; base_accuracy: 0.7166; adversarial_accuracy: 0.429; since_best_base_hinge: 0; min_base_hinge: 0.31102765; \nFastEstimator-Train: step: 20500; base_hinge: 0.33048096; steps/sec: 75.77; \nFastEstimator-Train: step: 21000; base_hinge: 0.22020546; steps/sec: 78.41; \nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 12.98 sec; \nFastEstimator-Eval: step: 21000; epoch: 21; adv_hinge: 0.57951987; base_hinge: 0.3130729; base_accuracy: 0.7154; adversarial_accuracy: 0.4266; since_best_base_hinge: 1; min_base_hinge: 0.31102765; \nFastEstimator-Train: step: 21500; base_hinge: 0.2436388; steps/sec: 75.94; \nFastEstimator-Train: step: 22000; base_hinge: 0.19141665; steps/sec: 78.86; \nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 12.92 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 22000; epoch: 22; adv_hinge: 0.5714458; base_hinge: 0.31004772; base_accuracy: 0.7156; adversarial_accuracy: 0.4366; since_best_base_hinge: 0; min_base_hinge: 0.31004772; \nFastEstimator-Train: step: 22500; base_hinge: 0.2346386; steps/sec: 75.87; \nFastEstimator-Train: step: 23000; base_hinge: 0.2227555; steps/sec: 78.81; \nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 12.94 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 23000; epoch: 23; adv_hinge: 0.5728474; base_hinge: 0.3098317; base_accuracy: 0.7176; adversarial_accuracy: 0.4386; since_best_base_hinge: 0; min_base_hinge: 0.3098317; \nFastEstimator-Train: step: 23500; base_hinge: 0.163346; steps/sec: 74.76; \nFastEstimator-Train: step: 24000; base_hinge: 0.2146762; steps/sec: 76.85; \nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 13.19 sec; \nFastEstimator-Eval: step: 24000; epoch: 24; adv_hinge: 0.5920725; base_hinge: 0.3194797; base_accuracy: 0.7064; adversarial_accuracy: 0.4198; since_best_base_hinge: 1; min_base_hinge: 0.3098317; \nFastEstimator-Train: step: 24500; base_hinge: 0.26413205; steps/sec: 74.6; \nFastEstimator-Train: step: 25000; base_hinge: 0.19393927; steps/sec: 76.53; \nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 13.24 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 25000; epoch: 25; adv_hinge: 0.55823886; base_hinge: 0.3062974; base_accuracy: 0.7196; adversarial_accuracy: 0.4534; since_best_base_hinge: 0; min_base_hinge: 0.3062974; \nFastEstimator-Train: step: 25500; base_hinge: 0.22403201; steps/sec: 73.96; \nFastEstimator-Train: step: 26000; base_hinge: 0.26596522; steps/sec: 77.8; \nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 13.19 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 26000; epoch: 26; adv_hinge: 0.55658454; base_hinge: 0.29793462; base_accuracy: 0.7272; adversarial_accuracy: 0.4524; since_best_base_hinge: 0; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 26500; base_hinge: 0.17813882; steps/sec: 74.35; \nFastEstimator-Train: step: 27000; base_hinge: 0.21485883; steps/sec: 78.4; \nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 13.1 sec; \nFastEstimator-Eval: step: 27000; epoch: 27; adv_hinge: 0.5723389; base_hinge: 0.31458738; base_accuracy: 0.707; adversarial_accuracy: 0.44; since_best_base_hinge: 1; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 27500; base_hinge: 0.15185395; steps/sec: 74.27; \nFastEstimator-Train: step: 28000; base_hinge: 0.27865165; steps/sec: 79.26; \nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 13.04 sec; \nFastEstimator-Eval: step: 28000; epoch: 28; adv_hinge: 0.5735497; base_hinge: 0.30879247; base_accuracy: 0.7202; adversarial_accuracy: 0.4356; since_best_base_hinge: 2; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 28500; base_hinge: 0.24824846; steps/sec: 76.32; \nFastEstimator-Train: step: 29000; base_hinge: 0.17269535; steps/sec: 77.05; \nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 13.04 sec; \nFastEstimator-Eval: step: 29000; epoch: 29; adv_hinge: 0.5488098; base_hinge: 0.30959663; base_accuracy: 0.7142; adversarial_accuracy: 0.463; since_best_base_hinge: 3; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 29500; base_hinge: 0.136942; steps/sec: 75.83; \nFastEstimator-Train: step: 30000; base_hinge: 0.23609023; steps/sec: 75.05; \nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 13.26 sec; \nFastEstimator-Eval: step: 30000; epoch: 30; adv_hinge: 0.5553243; base_hinge: 0.3113994; base_accuracy: 0.7178; adversarial_accuracy: 0.4566; since_best_base_hinge: 4; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 30500; base_hinge: 0.1271786; steps/sec: 76.69; \nFastEstimator-Train: step: 31000; base_hinge: 0.14635064; steps/sec: 78.81; \nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 12.86 sec; \nFastEstimator-Eval: step: 31000; epoch: 31; adv_hinge: 0.54818094; base_hinge: 0.3015276; base_accuracy: 0.7248; adversarial_accuracy: 0.4672; since_best_base_hinge: 5; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 31500; base_hinge: 0.3047498; steps/sec: 74.89; \nFastEstimator-Train: step: 32000; base_hinge: 0.17989486; steps/sec: 77.08; \nFastEstimator-Train: step: 32000; epoch: 32; epoch_time: 13.17 sec; \nFastEstimator-Eval: step: 32000; epoch: 32; adv_hinge: 0.5483753; base_hinge: 0.30732492; base_accuracy: 0.7166; adversarial_accuracy: 0.4668; since_best_base_hinge: 6; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 32500; base_hinge: 0.19096829; steps/sec: 75.16; \nFastEstimator-Train: step: 33000; base_hinge: 0.18576458; steps/sec: 77.72; \nFastEstimator-Train: step: 33000; epoch: 33; epoch_time: 13.08 sec; \nFastEstimator-Eval: step: 33000; epoch: 33; adv_hinge: 0.5341504; base_hinge: 0.30691355; base_accuracy: 0.7164; adversarial_accuracy: 0.4812; since_best_base_hinge: 7; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 33500; base_hinge: 0.124624774; steps/sec: 75.69; \nFastEstimator-Train: step: 34000; base_hinge: 0.24719924; steps/sec: 78.77; \nFastEstimator-Train: step: 34000; epoch: 34; epoch_time: 12.96 sec; \nFastEstimator-Eval: step: 34000; epoch: 34; adv_hinge: 0.53387624; base_hinge: 0.30544707; base_accuracy: 0.7218; adversarial_accuracy: 0.4796; since_best_base_hinge: 8; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 34500; base_hinge: 0.2165949; steps/sec: 75.61; \nFastEstimator-Train: step: 35000; base_hinge: 0.17847629; steps/sec: 80.94; \nFastEstimator-Train: step: 35000; epoch: 35; epoch_time: 12.79 sec; \nFastEstimator-Eval: step: 35000; epoch: 35; adv_hinge: 0.54723406; base_hinge: 0.30917642; base_accuracy: 0.7202; adversarial_accuracy: 0.4658; since_best_base_hinge: 9; min_base_hinge: 0.29793462; \nFastEstimator-Train: step: 35500; base_hinge: 0.23268871; steps/sec: 75.68; \nFastEstimator-Train: step: 36000; base_hinge: 0.17250317; steps/sec: 79.06; \nFastEstimator-Train: step: 36000; epoch: 36; epoch_time: 12.93 sec; \nFastEstimator-EarlyStopping: 'base_hinge' triggered an early stop. Its best value was 0.2979346215724945 at epoch 26\nFastEstimator-Eval: step: 36000; epoch: 36; adv_hinge: 0.5309303; base_hinge: 0.30545956; base_accuracy: 0.7232; adversarial_accuracy: 0.487; since_best_base_hinge: 10; min_base_hinge: 0.29793462; \nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpbikpoisy/hydra_ecc_best_base_hinge.h5\nFastEstimator-Finish: step: 36000; total_time: 536.31 sec; hydra_ecc_lr: 0.001; \nFastEstimator-Test: step: 36000; epoch: 36; base_accuracy: 0.7302; adversarial_accuracy: 0.461; \n</pre> In\u00a0[15]: Copied! <pre>logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'})\n</pre> logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'}) <p>As you can see, the conventional network using softmax to convert logits to class probabilities actually gets more and more vulnerable to adversarial attacks as training progresses. It also quickly overfits to the data, reaching an optimal performance around epoch 7. By switching the output layer of the model to generate an error correcting code and training with hinge loss, the model is able to train almost 6 times longer before reaching peak conventional accuracy. Moreover, the adversarial performance of the network continues to improve even after the main training runs out. This is significantly better performance than networks trained specifically to combat this attack, shown in the FGSM notebook. It can also be seen that there is no additional cost to training using ECC as opposed to softmax in terms of steps/sec. This is a big benefit over FGSM, where the training time for each step is doubled. With these benefits in mind, you may want to consider never using softmax again.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#adversarial-robustness-with-error-correcting-codes-and-hinge-loss", "title": "Adversarial Robustness with Error Correcting Codes (and Hinge Loss)\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#never-use-softmax-again", "title": "(Never use Softmax again)\u00b6", "text": "<p>In this example we will show how using error correcting codes as a model output can drastically reduce model overfitting while simultaneously increasing model robustness against adversarial attacks. In other words, why you should never use a softmax layer again. This is slightly more complicated than the our other ECC apphub example, but it allows for more accurate final probability estimates (the FE Hadamard network layer results in probability smoothing which prevents the network from ever being 100% confident in a class choice). The usefulness of error correcting codes was first publicized by the US Army in a 2019 Neurips Paper. For background on adversarial attacks, and on the attack type we will be demonstrating here, check out our FGSM apphub example. Note that in this apphub we will not be training against adversarial samples, but only performing adversarial attacks during evaluation to see how different models fair against them.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#imports", "title": "Imports\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#getting-the-data", "title": "Getting the Data\u00b6", "text": "<p>For these experiments we will be using the CIFAR-10 Dataset</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#defining-estimators", "title": "Defining Estimators\u00b6", "text": "<p>In this apphub we will be comparing a baseline model against two models using hinge loss to enable training with error correcting codes. The setting up the hinge loss models requires a few extra Ops along the way.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#the-models", "title": "The Models\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#1-a-lenet-model-with-softmax", "title": "1 - A LeNet model with Softmax\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#2-a-lenet-model-with-error-correcting-codes", "title": "2 - A LeNet model with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#3-a-lenet-model-using-ecc-and-multiple-feature-heads", "title": "3 - A LeNet model using ECC and multiple feature heads\u00b6", "text": "<p>While it is common practice to follow the feature extraction layers of convolution networks with several fully connected layers in order to perform classification, this can lead to the final logits being interdependent which can actually reduce the robustness of the network. One way around this is to divide your classification layers into multiple smaller independent units:</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#the-experiments", "title": "The Experiments\u00b6", "text": "<p>Let's get Estimators for each of these models and compare them:</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#comparing-the-results", "title": "Comparing the Results\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\n\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import to_tensor, argmax\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.tensorop import Average\nfrom fastestimator.op.tensorop.gradient import Watch, FGSM\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import ImgData, to_number\n</pre> import tempfile import os  import numpy as np  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import to_tensor, argmax from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot from fastestimator.op.tensorop import Average from fastestimator.op.tensorop.gradient import Watch, FGSM from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import ImgData, to_number In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=10\nbatch_size=50\nmax_train_steps_per_epoch=None\nmax_eval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=10 batch_size=50 max_train_steps_per_epoch=None max_eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifar10  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        test_data=test_data,\n        batch_size=batch_size,\n        ops=[\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n        ])\n</pre> pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         test_data=test_data,         batch_size=batch_size,         ops=[             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))         ]) In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\")\n</pre> model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\") In\u00a0[6]: Copied! <pre>network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),\n        Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),\n        UpdateOp(model=model, loss_name=\"avg_ce\")\n    ])\n</pre> network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),         Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),         UpdateOp(model=model, loss_name=\"avg_ce\")     ]) In\u00a0[7]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         monitor_names=[\"base_ce\", \"adv_ce\"],\n                         log_steps=1000)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          monitor_names=[\"base_ce\", \"adv_ce\"],                          log_steps=1000) In\u00a0[8]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; avg_ce: 2.3945074; adv_ce: 2.4872663; base_ce: 2.3017485; \nFastEstimator-Train: step: 1000; avg_ce: 1.3094263; adv_ce: 1.4686574; base_ce: 1.1501954; steps/sec: 25.77; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 42.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; avg_ce: 1.4899004; adv_ce: 1.6584656; base_ce: 1.3213345; clean_accuracy: 0.5408; adversarial_accuracy: 0.3734; since_best_base_ce: 0; min_base_ce: 1.3213345; \nFastEstimator-Train: step: 2000; avg_ce: 1.143224; adv_ce: 1.3376933; base_ce: 0.9487548; steps/sec: 34.22; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 29.22 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; avg_ce: 1.3468871; adv_ce: 1.5618094; base_ce: 1.1319652; clean_accuracy: 0.5928; adversarial_accuracy: 0.412; since_best_base_ce: 0; min_base_ce: 1.1319652; \nFastEstimator-Train: step: 3000; avg_ce: 1.379614; adv_ce: 1.6073439; base_ce: 1.1518841; steps/sec: 36.24; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 27.6 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; avg_ce: 1.3020732; adv_ce: 1.5298728; base_ce: 1.074274; clean_accuracy: 0.622; adversarial_accuracy: 0.4274; since_best_base_ce: 0; min_base_ce: 1.074274; \nFastEstimator-Train: step: 4000; avg_ce: 1.2436087; adv_ce: 1.4758196; base_ce: 1.0113977; steps/sec: 33.11; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 30.2 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; avg_ce: 1.2154673; adv_ce: 1.4576334; base_ce: 0.9733012; clean_accuracy: 0.665; adversarial_accuracy: 0.4618; since_best_base_ce: 0; min_base_ce: 0.9733012; \nFastEstimator-Train: step: 5000; avg_ce: 1.154286; adv_ce: 1.3962423; base_ce: 0.9123298; steps/sec: 32.78; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 30.51 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; avg_ce: 1.2138638; adv_ce: 1.4704447; base_ce: 0.9572834; clean_accuracy: 0.6696; adversarial_accuracy: 0.4552; since_best_base_ce: 0; min_base_ce: 0.9572834; \nFastEstimator-Train: step: 6000; avg_ce: 1.1946353; adv_ce: 1.4845756; base_ce: 0.904695; steps/sec: 33.34; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 29.99 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; avg_ce: 1.1873512; adv_ce: 1.4471037; base_ce: 0.9275986; clean_accuracy: 0.6784; adversarial_accuracy: 0.4648; since_best_base_ce: 0; min_base_ce: 0.9275986; \nFastEstimator-Train: step: 7000; avg_ce: 1.3036005; adv_ce: 1.5895638; base_ce: 1.0176373; steps/sec: 32.39; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 30.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; avg_ce: 1.1750631; adv_ce: 1.4450079; base_ce: 0.9051186; clean_accuracy: 0.6934; adversarial_accuracy: 0.4764; since_best_base_ce: 0; min_base_ce: 0.9051186; \nFastEstimator-Train: step: 8000; avg_ce: 1.1723398; adv_ce: 1.4465908; base_ce: 0.89808875; steps/sec: 32.64; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 30.64 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; avg_ce: 1.1422093; adv_ce: 1.4145594; base_ce: 0.869859; clean_accuracy: 0.702; adversarial_accuracy: 0.477; since_best_base_ce: 0; min_base_ce: 0.869859; \nFastEstimator-Train: step: 9000; avg_ce: 1.0794711; adv_ce: 1.3604188; base_ce: 0.7985235; steps/sec: 32.8; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 30.49 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; avg_ce: 1.1585152; adv_ce: 1.4406301; base_ce: 0.8764003; clean_accuracy: 0.7014; adversarial_accuracy: 0.4746; since_best_base_ce: 1; min_base_ce: 0.869859; \nFastEstimator-Train: step: 10000; avg_ce: 1.2206926; adv_ce: 1.5394913; base_ce: 0.901894; steps/sec: 33.19; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 30.13 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 10000; epoch: 10; avg_ce: 1.1538234; adv_ce: 1.4526846; base_ce: 0.85496193; clean_accuracy: 0.7084; adversarial_accuracy: 0.4792; since_best_base_ce: 0; min_base_ce: 0.85496193; \nFastEstimator-Finish: step: 10000; total_time: 330.22 sec; adv_model_lr: 0.001; \n</pre> In\u00a0[9]: Copied! <pre>model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\"))\n</pre> model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\")) In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.6962; adversarial_accuracy: 0.4758; \n</pre> <p>In spite of our training the network using adversarially crafted images, the adversarial attack is still effective at reducing the accuracy of the network. This does not, however, mean that the efforts were wasted.</p> In\u00a0[11]: Copied! <pre>clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\")\nclean_network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),\n        ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),\n        UpdateOp(model=clean_model, loss_name=\"base_ce\")\n    ])\nclean_traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nclean_estimator = fe.Estimator(pipeline=pipeline,\n                         network=clean_network,\n                         epochs=epochs,\n                         traces=clean_traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         log_steps=1000)\nclean_estimator.fit()\n</pre> clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\") clean_network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),         ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),         UpdateOp(model=clean_model, loss_name=\"base_ce\")     ]) clean_traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] clean_estimator = fe.Estimator(pipeline=pipeline,                          network=clean_network,                          epochs=epochs,                          traces=clean_traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          log_steps=1000) clean_estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; base_ce: 2.3599913; \nFastEstimator-Train: step: 1000; base_ce: 1.2336738; steps/sec: 81.68; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 12.53 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; base_ce: 1.1847152; clean_accuracy: 0.5684; adversarial_accuracy: 0.2694; since_best_base_ce: 0; min_base_ce: 1.1847152; \nFastEstimator-Train: step: 2000; base_ce: 0.81964266; steps/sec: 78.27; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 12.78 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; base_ce: 0.95957977; clean_accuracy: 0.6652; adversarial_accuracy: 0.2778; since_best_base_ce: 0; min_base_ce: 0.95957977; \nFastEstimator-Train: step: 3000; base_ce: 0.9629886; steps/sec: 78.25; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 12.78 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; base_ce: 0.8996327; clean_accuracy: 0.6946; adversarial_accuracy: 0.263; since_best_base_ce: 0; min_base_ce: 0.8996327; \nFastEstimator-Train: step: 4000; base_ce: 0.7768238; steps/sec: 77.71; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 12.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; base_ce: 0.86543256; clean_accuracy: 0.702; adversarial_accuracy: 0.2576; since_best_base_ce: 0; min_base_ce: 0.86543256; \nFastEstimator-Train: step: 5000; base_ce: 0.7760873; steps/sec: 77.93; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 12.83 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; base_ce: 0.7983937; clean_accuracy: 0.727; adversarial_accuracy: 0.2664; since_best_base_ce: 0; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 6000; base_ce: 0.56715065; steps/sec: 78.43; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 12.75 sec; \nFastEstimator-Eval: step: 6000; epoch: 6; base_ce: 0.79985946; clean_accuracy: 0.7318; adversarial_accuracy: 0.2704; since_best_base_ce: 1; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 7000; base_ce: 0.7633059; steps/sec: 72.81; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 13.74 sec; \nFastEstimator-Eval: step: 7000; epoch: 7; base_ce: 0.80506575; clean_accuracy: 0.73; adversarial_accuracy: 0.2464; since_best_base_ce: 2; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 8000; base_ce: 0.57881784; steps/sec: 76.72; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 13.03 sec; \nFastEstimator-Eval: step: 8000; epoch: 8; base_ce: 0.8497379; clean_accuracy: 0.733; adversarial_accuracy: 0.214; since_best_base_ce: 3; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 9000; base_ce: 0.61386603; steps/sec: 78.1; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 12.81 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; base_ce: 0.84185517; clean_accuracy: 0.731; adversarial_accuracy: 0.2206; since_best_base_ce: 4; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 10000; base_ce: 0.6447299; steps/sec: 77.0; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 12.99 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; base_ce: 0.8941338; clean_accuracy: 0.7278; adversarial_accuracy: 0.1848; since_best_base_ce: 5; min_base_ce: 0.7983937; \nFastEstimator-Finish: step: 10000; total_time: 148.17 sec; clean_model_lr: 0.001; \n</pre> <p>As before, we will reload the best weights and the test the model</p> In\u00a0[12]: Copied! <pre>clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\"))\n</pre> clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\")) In\u00a0[13]: Copied! <pre>print(\"Normal Network:\")\nnormal_results = clean_estimator.test(\"normal\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\nprint(\"Adversarially Trained Network:\")\nadversarial_results = estimator.test(\"adversarial\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\n</pre> print(\"Normal Network:\") normal_results = clean_estimator.test(\"normal\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") print(\"Adversarially Trained Network:\") adversarial_results = estimator.test(\"adversarial\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") <pre>Normal Network:\nFastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.7178; adversarial_accuracy: 0.2674; \nThe whitebox FGSM attack reduced accuracy by 0.45\n-----------\nAdversarially Trained Network:\nFastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.6962; adversarial_accuracy: 0.4758; \nThe whitebox FGSM attack reduced accuracy by 0.22\n-----------\n</pre> <p>As we can see, the normal network is significantly less robust against adversarial attacks than the one which was trained to resist them. The downside is that the adversarial network requires more epochs of training to converge, and the training steps take about twice as long since they require two forward pass operations. It is also interesting to note that as the regular model was training, it actually saw progressively worse adversarial accuracy. This may be an indication that the network is developing very brittle decision boundaries.</p> In\u00a0[14]: Copied! <pre>class_dictionary = {\n    0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n}\nbatch = pipeline.get_results(mode=\"test\")\n</pre> class_dictionary = {     0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\" } batch = pipeline.get_results(mode=\"test\") <p>Now let's run our sample data through the network and then visualize the results</p> In\u00a0[15]: Copied! <pre>batch = clean_network.transform(batch, mode=\"test\")\n</pre> batch = clean_network.transform(batch, mode=\"test\") In\u00a0[16]: Copied! <pre>n_samples = 10\ny = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])])\ny_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])])\ny_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])])\nimg = ImgData(x=batch[\"x\"][0:n_samples], x_adverse=batch[\"x_adverse\"][0:n_samples], y=y, y_pred=y_pred, y_adv=y_adv)\nfig = img.paint_figure()\n</pre> n_samples = 10 y = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])]) y_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])]) y_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])]) img = ImgData(x=batch[\"x\"][0:n_samples], x_adverse=batch[\"x_adverse\"][0:n_samples], y=y, y_pred=y_pred, y_adv=y_adv) fig = img.paint_figure() <p>As you can see, the adversarial images appear very similar to the unmodified images, and yet they are often able to modify the class predictions of the network. Note that if a network's prediction is already wrong, the attack is unlikely to change the incorrect prediction, but rather to increase the model's confidence in its incorrect prediction.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#adversarial-training-using-the-fast-gradient-sign-method-fgsm", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)\u00b6", "text": "<p>In this example we will demonstrate how to train a model to resist adversarial attacks constructed using the Fast Gradient Sign Method. For more background on adversarial attacks, visit: https://arxiv.org/abs/1412.6572</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load CIFAR10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the CIFAR10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#prepare-the-pipeline", "title": "Prepare the <code>Pipeline</code>\u00b6", "text": "<p>We will use a simple pipeline that just normalizes the images</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-construction", "title": "Model Construction\u00b6", "text": "<p>Here we will leverage the LeNet implementation built in to FastEstimator</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#network-defintion", "title": "<code>Network</code> defintion\u00b6", "text": "<p>This is where the adversarial attack will be implemented. To perform an FGSM attack, we first need to monitor gradients with respect to the input image. This can be accomplished in FastEstimator using the <code>Watch</code> TensorOp. We then will run the model forward once, compute the loss, and then pass the loss value into the <code>FGSM</code> TensorOp in order to create an adversarial image. We will then run the adversarial image through the model, compute the loss again, and average the two results together in order to update the model.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to connect the <code>Network</code> with the <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>) and save the best model (<code>BestModelSaver</code>) along the way. We will compute accuracy both with respect to the clean input images ('clean accuracy') as well as with respect to the adversarial input images ('adversarial accuracy'). At the end, we use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Let's start by re-loading the weights from the best model, since the model may have overfit during training</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#comparison-vs-network-without-adversarial-training", "title": "Comparison vs Network without Adversarial Training\u00b6", "text": "<p>To see whether training using adversarial hardening was actually useful, we will compare it to a network which is trained without considering any adversarial images. The setup will be similar to before, but we will only use the adversarial images for evaluation purposes and so the second <code>CrossEntropy</code> Op as well as the <code>Average</code> Op can be omitted.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#visualizing-adversarial-samples", "title": "Visualizing Adversarial Samples\u00b6", "text": "<p>Lets visualize some images generated by these adversarial attacks to make sure that everything is working as we would expect. The first step is to get some sample data from the pipeline:</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html", "title": "Anomaly Detection with Fastestimator", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nimport numpy as np\nimport tensorflow as tf\nfrom fastestimator.backend import binary_crossentropy\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Normalize\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.util import to_number\nfrom sklearn.metrics import auc, f1_score, roc_curve\nfrom tensorflow.python.keras import layers\n</pre> import tempfile  import fastestimator as fe import numpy as np import tensorflow as tf from fastestimator.backend import binary_crossentropy from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.univariate import ExpandDims, Normalize from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace import Trace from fastestimator.trace.io import BestModelSaver from fastestimator.util import to_number from sklearn.metrics import auc, f1_score, roc_curve from tensorflow.python.keras import layers In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs=20\nbatch_size=128\nmax_train_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # Parameters epochs=20 batch_size=128 max_train_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n\n# Create Training Dataset\nx_train, y_train = x_train[np.where((y_train == 1))], np.zeros(y_train[np.where((y_train == 1))].shape)\ntrain_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\n\n# Create Validation Dataset\nx_eval0, y_eval0 = x_eval[np.where((y_eval == 1))], np.ones(y_eval[np.where((y_eval == 1))].shape)\nx_eval1, y_eval1 = x_eval[np.where((y_eval != 1))], y_eval[np.where((y_eval != 1))]\n\n# Ensuring outliers comprise 50% of the dataset\nindex = np.random.choice(x_eval1.shape[0], int(x_eval0.shape[0]), replace=False)\nx_eval1, y_eval1 = x_eval1[index], np.zeros(y_eval1[index].shape)\n\nx_eval, y_eval = np.concatenate([x_eval0, x_eval1]), np.concatenate([y_eval0, y_eval1])\neval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n</pre> (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()  # Create Training Dataset x_train, y_train = x_train[np.where((y_train == 1))], np.zeros(y_train[np.where((y_train == 1))].shape) train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})  # Create Validation Dataset x_eval0, y_eval0 = x_eval[np.where((y_eval == 1))], np.ones(y_eval[np.where((y_eval == 1))].shape) x_eval1, y_eval1 = x_eval[np.where((y_eval != 1))], y_eval[np.where((y_eval != 1))]  # Ensuring outliers comprise 50% of the dataset index = np.random.choice(x_eval1.shape[0], int(x_eval0.shape[0]), replace=False) x_eval1, y_eval1 = x_eval1[index], np.zeros(y_eval1[index].shape)  x_eval, y_eval = np.concatenate([x_eval0, x_eval1]), np.concatenate([y_eval0, y_eval1]) eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval}) In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x\"),\n        Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        LambdaOp(fn=lambda x: x + np.random.normal(loc=0.0, scale=0.155, size=(28, 28, 1)),\n                 inputs=\"x\",\n                 outputs=\"x_w_noise\",\n                 mode=\"train\")\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x\"),         Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),         LambdaOp(fn=lambda x: x + np.random.normal(loc=0.0, scale=0.155, size=(28, 28, 1)),                  inputs=\"x\",                  outputs=\"x_w_noise\",                  mode=\"train\")     ]) <p>We can visualize sample images from our <code>Pipeline</code> using the 'get_results' method.</p> In\u00a0[5]: Copied! <pre>sample_batch = pipeline.get_results()\n\nimg = fe.util.ImgData(Image=sample_batch[\"x\"][0].numpy().reshape(1, 28, 28, 1), \n                      Noisy_Image=sample_batch[\"x_w_noise\"][0].numpy().reshape(1, 28, 28, 1))\nfig = img.paint_figure()\n</pre> sample_batch = pipeline.get_results()  img = fe.util.ImgData(Image=sample_batch[\"x\"][0].numpy().reshape(1, 28, 28, 1),                        Noisy_Image=sample_batch[\"x_w_noise\"][0].numpy().reshape(1, 28, 28, 1)) fig = img.paint_figure() In\u00a0[6]: Copied! <pre>def reconstructor(input_shape=(28, 28, 1)):\n    model = tf.keras.Sequential()\n    # Encoder Block\n    model.add(\n        layers.Conv2D(32, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      input_shape=input_shape))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(64, (5, 5),\n                      strides=(2, 2),\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(128, (5, 5),\n                      strides=(2, 2),\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n\n    # Decoder Block\n    model.add(\n        layers.Conv2DTranspose(32, (5, 5),\n                               strides=(2, 2),\n                               output_padding=(0, 0),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(\n        layers.Conv2DTranspose(16, (5, 5),\n                               strides=(2, 2),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(\n        layers.Conv2DTranspose(1, (5, 5),\n                               strides=(2, 2),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n                               activation='tanh'))\n    return model\n\n\ndef discriminator(input_shape=(28, 28, 1)):\n    model = tf.keras.Sequential()\n    model.add(\n        layers.Conv2D(16, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      input_shape=input_shape))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(32, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(64, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(128, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.LeakyReLU(0.2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation=\"sigmoid\"))\n    return model\n</pre> def reconstructor(input_shape=(28, 28, 1)):     model = tf.keras.Sequential()     # Encoder Block     model.add(         layers.Conv2D(32, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       input_shape=input_shape))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(64, (5, 5),                       strides=(2, 2),                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       padding='same'))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(128, (5, 5),                       strides=(2, 2),                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       padding='same'))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))      # Decoder Block     model.add(         layers.Conv2DTranspose(32, (5, 5),                                strides=(2, 2),                                output_padding=(0, 0),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.ReLU())     model.add(         layers.Conv2DTranspose(16, (5, 5),                                strides=(2, 2),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.ReLU())     model.add(         layers.Conv2DTranspose(1, (5, 5),                                strides=(2, 2),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02),                                activation='tanh'))     return model   def discriminator(input_shape=(28, 28, 1)):     model = tf.keras.Sequential()     model.add(         layers.Conv2D(16, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       input_shape=input_shape))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(32, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(64, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(128, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.LeakyReLU(0.2))     model.add(layers.Flatten())     model.add(layers.Dense(1, activation=\"sigmoid\"))     return model In\u00a0[7]: Copied! <pre>recon_model = fe.build(model_fn=reconstructor, optimizer_fn=lambda: tf.optimizers.RMSprop(2e-4), model_name=\"reconstructor\")\ndisc_model = fe.build(model_fn=discriminator,\n                      optimizer_fn=lambda: tf.optimizers.RMSprop(1e-4),\n                      model_name=\"discriminator\")\n</pre> recon_model = fe.build(model_fn=reconstructor, optimizer_fn=lambda: tf.optimizers.RMSprop(2e-4), model_name=\"reconstructor\") disc_model = fe.build(model_fn=discriminator,                       optimizer_fn=lambda: tf.optimizers.RMSprop(1e-4),                       model_name=\"discriminator\") In\u00a0[8]: Copied! <pre>class RLoss(TensorOp):\n    def __init__(self, alpha=0.2, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.alpha = alpha\n\n    def forward(self, data, state):\n        fake_score, x_fake, x = data\n        recon_loss = binary_crossentropy(y_true=x, y_pred=x_fake, from_logits=True)\n        adv_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.ones_like(fake_score), from_logits=True)\n        return adv_loss + self.alpha * recon_loss\n\n\nclass DLoss(TensorOp):\n    def forward(self, data, state):\n        true_score, fake_score = data\n        real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)\n        fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)\n        total_loss = real_loss + fake_loss\n        return total_loss\n</pre> class RLoss(TensorOp):     def __init__(self, alpha=0.2, inputs=None, outputs=None, mode=None):         super().__init__(inputs, outputs, mode)         self.alpha = alpha      def forward(self, data, state):         fake_score, x_fake, x = data         recon_loss = binary_crossentropy(y_true=x, y_pred=x_fake, from_logits=True)         adv_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.ones_like(fake_score), from_logits=True)         return adv_loss + self.alpha * recon_loss   class DLoss(TensorOp):     def forward(self, data, state):         true_score, fake_score = data         real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)         fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)         total_loss = real_loss + fake_loss         return total_loss <p>We now define the <code>Network</code> object:</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=recon_model, inputs=\"x_w_noise\", outputs=\"x_fake\", mode=\"train\"),\n    ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\", mode=\"eval\"),\n    ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),\n    ModelOp(model=disc_model, inputs=\"x\", outputs=\"true_score\"),\n    RLoss(inputs=(\"fake_score\", \"x_fake\", \"x\"), outputs=\"rloss\"),\n    UpdateOp(model=recon_model, loss_name=\"rloss\"),\n    DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),\n    UpdateOp(model=disc_model, loss_name=\"dloss\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=recon_model, inputs=\"x_w_noise\", outputs=\"x_fake\", mode=\"train\"),     ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\", mode=\"eval\"),     ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),     ModelOp(model=disc_model, inputs=\"x\", outputs=\"true_score\"),     RLoss(inputs=(\"fake_score\", \"x_fake\", \"x\"), outputs=\"rloss\"),     UpdateOp(model=recon_model, loss_name=\"rloss\"),     DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),     UpdateOp(model=disc_model, loss_name=\"dloss\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>A custom trace to calculate Area Under the Curve and F1-Score.</li> </ol> In\u00a0[10]: Copied! <pre>class F1AUCScores(Trace):\n\"\"\"Computes F1-Score and AUC Score for a classification task and reports it back to the logger.\n    \"\"\"\n    def __init__(self, true_key, pred_key, mode=(\"eval\", \"test\"), output_name=[\"auc_score\", \"f1_score\"]):\n        super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\n        self.y_true = []\n        self.y_pred = []\n\n    @property\n    def true_key(self):\n        return self.inputs[0]\n\n    @property\n    def pred_key(self):\n        return self.inputs[1]\n\n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n\n    def on_batch_end(self, data):\n        y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\n        assert y_pred.size == y_true.size\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n\n    def on_epoch_end(self, data):\n        fpr, tpr, thresholds = roc_curve(self.y_true, self.y_pred, pos_label=1)\n        roc_auc = auc(fpr, tpr)\n        eer_threshold = thresholds[np.nanargmin(np.absolute((1 - tpr - fpr)))]\n        y_pred_class = np.copy(self.y_pred)\n        y_pred_class[y_pred_class &gt;= eer_threshold] = 1\n        y_pred_class[y_pred_class &lt; eer_threshold] = 0\n        f_score = f1_score(self.y_true, y_pred_class)\n\n        data.write_with_log(self.outputs[0], roc_auc)\n        data.write_with_log(self.outputs[1], f_score)\n        \n\ntraces = [\n    F1AUCScores(true_key=\"y\", pred_key=\"fake_score\", mode=\"eval\", output_name=[\"auc_score\", \"f1_score\"]),\n    BestModelSaver(model=recon_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True),\n    BestModelSaver(model=disc_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True)\n]\n</pre> class F1AUCScores(Trace):     \"\"\"Computes F1-Score and AUC Score for a classification task and reports it back to the logger.     \"\"\"     def __init__(self, true_key, pred_key, mode=(\"eval\", \"test\"), output_name=[\"auc_score\", \"f1_score\"]):         super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)         self.y_true = []         self.y_pred = []      @property     def true_key(self):         return self.inputs[0]      @property     def pred_key(self):         return self.inputs[1]      def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []      def on_batch_end(self, data):         y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])         assert y_pred.size == y_true.size         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())      def on_epoch_end(self, data):         fpr, tpr, thresholds = roc_curve(self.y_true, self.y_pred, pos_label=1)         roc_auc = auc(fpr, tpr)         eer_threshold = thresholds[np.nanargmin(np.absolute((1 - tpr - fpr)))]         y_pred_class = np.copy(self.y_pred)         y_pred_class[y_pred_class &gt;= eer_threshold] = 1         y_pred_class[y_pred_class &lt; eer_threshold] = 0         f_score = f1_score(self.y_true, y_pred_class)          data.write_with_log(self.outputs[0], roc_auc)         data.write_with_log(self.outputs[1], f_score)           traces = [     F1AUCScores(true_key=\"y\", pred_key=\"fake_score\", mode=\"eval\", output_name=[\"auc_score\", \"f1_score\"]),     BestModelSaver(model=recon_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True),     BestModelSaver(model=disc_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True) ] In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; dloss: 1.4547124; rloss: 0.6044176; \nFastEstimator-Train: step: 53; epoch: 1; epoch_time: 6.41 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 53; epoch: 1; dloss: 1.4323395; rloss: 0.6304608; auc_score: 0.758243707426886; f1_score: 0.6554770318021201; since_best_f1_score: 0; max_f1_score: 0.6554770318021201; \nFastEstimator-Train: step: 100; dloss: 1.0820444; rloss: 0.72240007; steps/sec: 17.02; \nFastEstimator-Train: step: 106; epoch: 2; epoch_time: 2.1 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 106; epoch: 2; dloss: 1.418492; rloss: 0.66902; auc_score: 0.8146631993634652; f1_score: 0.7268722466960352; since_best_f1_score: 0; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 159; epoch: 3; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 159; epoch: 3; dloss: 1.4147544; rloss: 0.7020666; auc_score: 0.15153680451784432; f1_score: 0.2431718061674009; since_best_f1_score: 1; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 200; dloss: 1.0067211; rloss: 0.660446; steps/sec: 25.29; \nFastEstimator-Train: step: 212; epoch: 4; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 212; epoch: 4; dloss: 1.3572774; rloss: 0.7195197; auc_score: 0.019186089386559024; f1_score: 0.06696035242290749; since_best_f1_score: 2; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 265; epoch: 5; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 265; epoch: 5; dloss: 1.1044953; rloss: 0.6936346; auc_score: 0.013531409497564477; f1_score: 0.0599647266313933; since_best_f1_score: 3; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 300; dloss: 1.0064102; rloss: 0.59952056; steps/sec: 25.26; \nFastEstimator-Train: step: 318; epoch: 6; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 318; epoch: 6; dloss: 1.0178115; rloss: 0.64911795; auc_score: 0.4008981350307594; f1_score: 0.4140969162995595; since_best_f1_score: 4; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 371; epoch: 7; epoch_time: 2.09 sec; \nFastEstimator-Eval: step: 371; epoch: 7; dloss: 1.0237744; rloss: 0.6199698; auc_score: 0.37563857245434606; f1_score: 0.3857331571994716; since_best_f1_score: 5; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 400; dloss: 1.0064098; rloss: 0.5864141; steps/sec: 25.25; \nFastEstimator-Train: step: 424; epoch: 8; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 424; epoch: 8; dloss: 1.010116; rloss: 0.6061601; auc_score: 0.7393793786023405; f1_score: 0.6822183098591549; since_best_f1_score: 6; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 477; epoch: 9; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 477; epoch: 9; dloss: 1.0113664; rloss: 0.6000464; auc_score: 0.790935589667954; f1_score: 0.7227112676056338; since_best_f1_score: 7; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 500; dloss: 1.0064089; rloss: 0.58218443; steps/sec: 25.19; \nFastEstimator-Train: step: 530; epoch: 10; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 530; epoch: 10; dloss: 1.0653309; rloss: 0.59754586; auc_score: 0.37388422053600884; f1_score: 0.39136183340678715; since_best_f1_score: 8; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 583; epoch: 11; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 583; epoch: 11; dloss: 1.0164112; rloss: 0.5950321; auc_score: 0.5598245648081662; f1_score: 0.5395842547545334; since_best_f1_score: 9; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 600; dloss: 1.006409; rloss: 0.5809828; steps/sec: 25.18; \nFastEstimator-Train: step: 636; epoch: 12; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 636; epoch: 12; dloss: 1.0633637; rloss: 0.5527591; auc_score: 0.7476892623571193; f1_score: 0.678996036988111; since_best_f1_score: 10; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 689; epoch: 13; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 689; epoch: 13; dloss: 1.2391297; rloss: 0.6055295; auc_score: 0.2452661608026548; f1_score: 0.2874779541446208; since_best_f1_score: 11; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 700; dloss: 1.0498809; rloss: 0.57184714; steps/sec: 25.21; \nFastEstimator-Train: step: 742; epoch: 14; epoch_time: 2.1 sec; \nFastEstimator-Eval: step: 742; epoch: 14; dloss: 1.1737943; rloss: 0.60413545; auc_score: 0.4565204059849794; f1_score: 0.4269960299955889; since_best_f1_score: 12; max_f1_score: 0.7268722466960352; \nFastEstimator-Train: step: 795; epoch: 15; epoch_time: 2.11 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 795; epoch: 15; dloss: 1.0669132; rloss: 0.60187024; auc_score: 0.9049614780026781; f1_score: 0.8089788732394366; since_best_f1_score: 0; max_f1_score: 0.8089788732394366; \nFastEstimator-Train: step: 800; dloss: 1.08307; rloss: 0.5877172; steps/sec: 25.13; \nFastEstimator-Train: step: 848; epoch: 16; epoch_time: 2.11 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 848; epoch: 16; dloss: 1.3530512; rloss: 0.6081449; auc_score: 0.941642958334142; f1_score: 0.8656979304271246; since_best_f1_score: 0; max_f1_score: 0.8656979304271246; \nFastEstimator-Train: step: 900; dloss: 1.3662006; rloss: 0.58985895; steps/sec: 25.06; \nFastEstimator-Train: step: 901; epoch: 17; epoch_time: 2.14 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 901; epoch: 17; dloss: 1.227813; rloss: 0.5782926; auc_score: 0.9882008189563158; f1_score: 0.948526176858777; since_best_f1_score: 0; max_f1_score: 0.948526176858777; \nFastEstimator-Train: step: 954; epoch: 18; epoch_time: 2.11 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 954; epoch: 18; dloss: 1.328288; rloss: 0.5912411; auc_score: 0.9875805856896116; f1_score: 0.9559471365638766; since_best_f1_score: 0; max_f1_score: 0.9559471365638766; \nFastEstimator-Train: step: 1000; dloss: 1.2895771; rloss: 0.53317624; steps/sec: 24.93; \nFastEstimator-Train: step: 1007; epoch: 19; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 1007; epoch: 19; dloss: 1.3832934; rloss: 0.6033389; auc_score: 0.9015963826194958; f1_score: 0.813215859030837; since_best_f1_score: 1; max_f1_score: 0.9559471365638766; \nFastEstimator-Train: step: 1060; epoch: 20; epoch_time: 2.11 sec; \nFastEstimator-Eval: step: 1060; epoch: 20; dloss: 1.3230872; rloss: 0.5129401; auc_score: 0.4634978361699238; f1_score: 0.48722466960352423; since_best_f1_score: 2; max_f1_score: 0.9559471365638766; \nFastEstimator-BestModelSaver: Restoring model from /tmp/tmpf8gmdf9j/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Restoring model from /tmp/tmpf8gmdf9j/discriminator_best_f1_score.h5\nFastEstimator-Finish: step: 1060; total_time: 62.06 sec; discriminator_lr: 1e-04; reconstructor_lr: 0.0002; \n</pre> In\u00a0[13]: Copied! <pre>idx0 = np.random.randint(len(x_eval0))\nidx1 = np.random.randint(len(x_eval1))\n\ndata = [{\"x\": x_eval0[idx0]}, {\"x\": x_eval1[idx1]}]\nresult = [pipeline.transform(data[i], mode=\"infer\") for i in range(len(data))]\n</pre> idx0 = np.random.randint(len(x_eval0)) idx1 = np.random.randint(len(x_eval1))  data = [{\"x\": x_eval0[idx0]}, {\"x\": x_eval1[idx1]}] result = [pipeline.transform(data[i], mode=\"infer\") for i in range(len(data))] In\u00a0[14]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\"),\n    ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\")\n])\n\noutput_imgs = [network.transform(result[i], mode=\"infer\") for i in range(len(result))]\n</pre> network = fe.Network(ops=[     ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\"),     ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\") ])  output_imgs = [network.transform(result[i], mode=\"infer\") for i in range(len(result))] In\u00a0[15]: Copied! <pre>base_image = output_imgs[0][\"x\"].numpy()\nanomaly_image = output_imgs[1][\"x\"].numpy()\n\nrecon_base_image = output_imgs[0][\"x_fake\"].numpy()\nrecon_anomaly_image = output_imgs[1][\"x_fake\"].numpy()\n\nimg1 = fe.util.ImgData(Input_Image=base_image, Reconstructed_Image=recon_base_image)\nfig1 = img1.paint_figure()\n\nimg2 = fe.util.ImgData(Input_Image=anomaly_image, Reconstructed_Image=recon_anomaly_image)\nfig2 = img2.paint_figure()\n</pre> base_image = output_imgs[0][\"x\"].numpy() anomaly_image = output_imgs[1][\"x\"].numpy()  recon_base_image = output_imgs[0][\"x_fake\"].numpy() recon_anomaly_image = output_imgs[1][\"x_fake\"].numpy()  img1 = fe.util.ImgData(Input_Image=base_image, Reconstructed_Image=recon_base_image) fig1 = img1.paint_figure()  img2 = fe.util.ImgData(Input_Image=anomaly_image, Reconstructed_Image=recon_anomaly_image) fig2 = img2.paint_figure() <p>Note that the network is trained on inliers, so it's able to properly reconstruct them but does a poor job at reconstructing the outliers, thereby making it easier for discriminator to detect the outliers.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#anomaly-detection-with-fastestimator", "title": "Anomaly Detection with Fastestimator\u00b6", "text": "<p>In this notebook we will demonstrate how to do anomaly detection using one class classifier as described in Adversarially Learned One-Class Classifier for Novelty Detection. In real world, outliers or novelty class is often absent from the training dataset. Such problems can be efficiently modeled using one class classifiers. In the algorihm demonstrated below, two networks are trained to compete with each other where one network acts as a novelty detector and other enhaces the inliers and distorts the outliers. We use images of digit \"1\" from MNIST dataset for training and images of other digits as outliers.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download training images using tensorflow API. We will use images of digit <code>1</code> for training and test images of <code>1</code> as inliers and images of other digits as outliers. Outliers comprise 50% of our validation dataset.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We will use the <code>LambdaOp</code> to add noise to the images during training.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model consists of an Autoencoder (ecoder-decoder) network and a Discriminator network. [Credit: https://arxiv.org/pdf/1802.09088.pdf]</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#defining-loss", "title": "Defining Loss\u00b6", "text": "<p>The losses of both the networks are smilar to a standarad GAN network with the exception of the autoencoder having and additional reconstruction loss term to enforce similarity between the input and the reconstructed image. We first define custom <code>TensorOp</code>s to calculate the losses of both the networks.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will apply the model to visualize the reconstructed image of the inliers and outliers.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n</pre> import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 24\nbatch_size = 512\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 24 batch_size = 512 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifar10  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x_out\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),\n        RandomCrop(32, 32, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")),\n        CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\", max_holes=1),\n        ChannelTranspose(inputs=\"x_out\", outputs=\"x_out\"),\n        Onehot(inputs=\"y\", outputs=\"y_out\", mode=\"train\", num_classes=10, label_smoothing=0.2)\n    ])\n</pre> from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x_out\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         PadIfNeeded(min_height=40, min_width=40, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),         RandomCrop(32, 32, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")),         CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\", max_holes=1),         ChannelTranspose(inputs=\"x_out\", outputs=\"x_out\"),         Onehot(inputs=\"y\", outputs=\"y_out\", mode=\"train\", num_classes=10, label_smoothing=0.2)     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\ndata_yin = data[\"y\"]\ndata_yout = data[\"y_out\"]\n\nprint(\"the pipeline input image size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output image size: {}\".format(data_xout.numpy().shape))\nprint(\"the pipeline input label size: {}\".format(data_yin.numpy().shape))\nprint(\"the pipeline output label size: {}\".format(data_yout.numpy().shape))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"] data_yin = data[\"y\"] data_yout = data[\"y_out\"]  print(\"the pipeline input image size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output image size: {}\".format(data_xout.numpy().shape)) print(\"the pipeline input label size: {}\".format(data_yin.numpy().shape)) print(\"the pipeline output label size: {}\".format(data_yout.numpy().shape)) <pre>the pipeline input image size: (512, 32, 32, 3)\nthe pipeline output image size: (512, 3, 32, 32)\nthe pipeline input label size: (512, 1)\nthe pipeline output label size: (512, 10)\n</pre> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 2, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input img\")\naxs[0,1].set_title(\"pipeline output img\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    # pipeline image visualization \n    img_in = data_xin.numpy()[j]\n    axs[i,0].imshow(img_in)\n    \n    img_out = data_xout.numpy()[j].transpose((1,2,0))\n    img_out[:,:,0] = img_out[:,:,0] * 0.2471 + 0.4914 \n    img_out[:,:,1] = img_out[:,:,1] * 0.2435 + 0.4822\n    img_out[:,:,2] = img_out[:,:,2] * 0.2616 + 0.4465\n    axs[i,1].imshow(img_out)\n    \n    # pipeline label print\n    label_in = data_yin.numpy()[j]\n    label_out = data_yout.numpy()[j]\n    print(\"label_in:{} -&gt; label_out:{}\".format(label_in, label_out))\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 2, figsize=(12,12))  axs[0,0].set_title(\"pipeline input img\") axs[0,1].set_title(\"pipeline output img\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     # pipeline image visualization      img_in = data_xin.numpy()[j]     axs[i,0].imshow(img_in)          img_out = data_xout.numpy()[j].transpose((1,2,0))     img_out[:,:,0] = img_out[:,:,0] * 0.2471 + 0.4914      img_out[:,:,1] = img_out[:,:,1] * 0.2435 + 0.4822     img_out[:,:,2] = img_out[:,:,2] * 0.2616 + 0.4465     axs[i,1].imshow(img_out)          # pipeline label print     label_in = data_yin.numpy()[j]     label_out = data_yout.numpy()[j]     print(\"label_in:{} -&gt; label_out:{}\".format(label_in, label_out)) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <pre>label_in:[0] -&gt; label_out:[0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[4] -&gt; label_out:[0.02 0.02 0.02 0.02 0.82 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[9] -&gt; label_out:[0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.82]\nlabel_in:[1] -&gt; label_out:[0.02 0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[0] -&gt; label_out:[0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\n</pre> In\u00a0[7]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\n\nclass FastCifar(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = nn.Conv2d(3, 64, 3, padding=(1, 1))\n        self.conv0_bn = nn.BatchNorm2d(64, momentum=0.8)\n        self.conv1 = nn.Conv2d(64, 128, 3, padding=(1, 1))\n        self.conv1_bn = nn.BatchNorm2d(128, momentum=0.8)\n        self.residual1 = Residual(128, 128)\n        self.conv2 = nn.Conv2d(128, 256, 3, padding=(1, 1))\n        self.conv2_bn = nn.BatchNorm2d(256, momentum=0.8)\n        self.residual2 = Residual(256, 256)\n        self.conv3 = nn.Conv2d(256, 512, 3, padding=(1, 1))\n        self.conv3_bn = nn.BatchNorm2d(512, momentum=0.8)\n        self.residual3 = Residual(512, 512)\n        self.fc1 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # prep layer\n        x = self.conv0(x)\n        x = self.conv0_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        # layer 1\n        x = self.conv1(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv1_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual1(x)\n        # layer 2\n        x = self.conv2(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv2_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual2(x)\n        # layer 3\n        x = self.conv3(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv3_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual3(x)\n        # layer 4\n        x = fn.max_pool2d(x, kernel_size=x.size()[2:])\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = fn.softmax(x, dim=-1)\n        return x\n\n\nclass Residual(nn.Module):\n    def __init__(self, channel_in, channel_out):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channel_in, channel_out, 3, padding=(1, 1))\n        self.conv1_bn = nn.BatchNorm2d(channel_out)\n        self.conv2 = nn.Conv2d(channel_out, channel_out, 3, padding=(1, 1))\n        self.conv2_bn = nn.BatchNorm2d(channel_out)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv1_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = self.conv2(x)\n        x = self.conv2_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        return x\n\nmodel = fe.build(model_fn=FastCifar, optimizer_fn=\"adam\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as fn  class FastCifar(nn.Module):     def __init__(self):         super().__init__()         self.conv0 = nn.Conv2d(3, 64, 3, padding=(1, 1))         self.conv0_bn = nn.BatchNorm2d(64, momentum=0.8)         self.conv1 = nn.Conv2d(64, 128, 3, padding=(1, 1))         self.conv1_bn = nn.BatchNorm2d(128, momentum=0.8)         self.residual1 = Residual(128, 128)         self.conv2 = nn.Conv2d(128, 256, 3, padding=(1, 1))         self.conv2_bn = nn.BatchNorm2d(256, momentum=0.8)         self.residual2 = Residual(256, 256)         self.conv3 = nn.Conv2d(256, 512, 3, padding=(1, 1))         self.conv3_bn = nn.BatchNorm2d(512, momentum=0.8)         self.residual3 = Residual(512, 512)         self.fc1 = nn.Linear(512, 10)      def forward(self, x):         # prep layer         x = self.conv0(x)         x = self.conv0_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         # layer 1         x = self.conv1(x)         x = fn.max_pool2d(x, 2)         x = self.conv1_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual1(x)         # layer 2         x = self.conv2(x)         x = fn.max_pool2d(x, 2)         x = self.conv2_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual2(x)         # layer 3         x = self.conv3(x)         x = fn.max_pool2d(x, 2)         x = self.conv3_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual3(x)         # layer 4         x = fn.max_pool2d(x, kernel_size=x.size()[2:])         x = torch.flatten(x, 1)         x = self.fc1(x)         x = fn.softmax(x, dim=-1)         return x   class Residual(nn.Module):     def __init__(self, channel_in, channel_out):         super().__init__()         self.conv1 = nn.Conv2d(channel_in, channel_out, 3, padding=(1, 1))         self.conv1_bn = nn.BatchNorm2d(channel_out)         self.conv2 = nn.Conv2d(channel_out, channel_out, 3, padding=(1, 1))         self.conv2_bn = nn.BatchNorm2d(channel_out)      def forward(self, x):         x = self.conv1(x)         x = self.conv1_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = self.conv2(x)         x = self.conv2_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         return x  model = fe.build(model_fn=FastCifar, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y_out\"), outputs=\"ce\", mode=\"train\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"test\"),\n        UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y_out\"), outputs=\"ce\", mode=\"train\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"test\"),         UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")     ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\ndef lr_schedule(step):\n    if step &lt;= 490:\n        lr = step / 490 * 0.4\n    else:\n        lr = (2352 - step) / 1862 * 0.4\n    return lr * 0.1\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lr_schedule)\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n\nestimator.fit() # start the training\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  def lr_schedule(step):     if step &lt;= 490:         lr = step / 490 * 0.4     else:         lr = (2352 - step) / 1862 * 0.4     return lr * 0.1  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lr_schedule) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch)  estimator.fit() # start the training  <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \nFastEstimator-Train: step: 1; ce: 3.219695; model_lr: 8.163265e-05; \nFastEstimator-Train: step: 98; epoch: 1; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 98; epoch: 1; accuracy: 0.5402; \nFastEstimator-Train: step: 100; ce: 1.5576406; steps/sec: 10.01; model_lr: 0.008163265; \nFastEstimator-Train: step: 196; epoch: 2; epoch_time: 9.85 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 196; epoch: 2; accuracy: 0.6428; \nFastEstimator-Train: step: 200; ce: 1.4646513; steps/sec: 9.99; model_lr: 0.01632653; \nFastEstimator-Train: step: 294; epoch: 3; epoch_time: 9.82 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 294; epoch: 3; accuracy: 0.7756; \nFastEstimator-Train: step: 300; ce: 1.3107454; steps/sec: 9.98; model_lr: 0.024489796; \nFastEstimator-Train: step: 392; epoch: 4; epoch_time: 9.86 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 392; epoch: 4; accuracy: 0.7882; \nFastEstimator-Train: step: 400; ce: 1.2392472; steps/sec: 9.95; model_lr: 0.03265306; \nFastEstimator-Train: step: 490; epoch: 5; epoch_time: 9.86 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 490; epoch: 5; accuracy: 0.8258; \nFastEstimator-Train: step: 500; ce: 1.1978259; steps/sec: 9.9; model_lr: 0.039785177; \nFastEstimator-Train: step: 588; epoch: 6; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 588; epoch: 6; accuracy: 0.8466; \nFastEstimator-Train: step: 600; ce: 1.1920979; steps/sec: 9.94; model_lr: 0.03763695; \nFastEstimator-Train: step: 686; epoch: 7; epoch_time: 9.88 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 686; epoch: 7; accuracy: 0.8686; \nFastEstimator-Train: step: 700; ce: 1.1124842; steps/sec: 9.92; model_lr: 0.03548872; \nFastEstimator-Train: step: 784; epoch: 8; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 784; epoch: 8; accuracy: 0.8698; \nFastEstimator-Train: step: 800; ce: 1.0877913; steps/sec: 9.91; model_lr: 0.033340495; \nFastEstimator-Train: step: 882; epoch: 9; epoch_time: 9.9 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 882; epoch: 9; accuracy: 0.885; \nFastEstimator-Train: step: 900; ce: 1.1029329; steps/sec: 9.9; model_lr: 0.031192265; \nFastEstimator-Train: step: 980; epoch: 10; epoch_time: 9.91 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 980; epoch: 10; accuracy: 0.8972; \nFastEstimator-Train: step: 1000; ce: 1.080683; steps/sec: 9.91; model_lr: 0.02904404; \nFastEstimator-Train: step: 1078; epoch: 11; epoch_time: 9.94 sec; \nFastEstimator-Eval: step: 1078; epoch: 11; accuracy: 0.895; \nFastEstimator-Train: step: 1100; ce: 1.0479429; steps/sec: 9.89; model_lr: 0.026895812; \nFastEstimator-Train: step: 1176; epoch: 12; epoch_time: 9.91 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1176; epoch: 12; accuracy: 0.899; \nFastEstimator-Train: step: 1200; ce: 1.0285817; steps/sec: 9.9; model_lr: 0.024747584; \nFastEstimator-Train: step: 1274; epoch: 13; epoch_time: 9.89 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1274; epoch: 13; accuracy: 0.901; \nFastEstimator-Train: step: 1300; ce: 0.99329174; steps/sec: 9.87; model_lr: 0.022599356; \nFastEstimator-Train: step: 1372; epoch: 14; epoch_time: 9.98 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1372; epoch: 14; accuracy: 0.9116; \nFastEstimator-Train: step: 1400; ce: 0.9965744; steps/sec: 9.85; model_lr: 0.020451128; \nFastEstimator-Train: step: 1470; epoch: 15; epoch_time: 9.97 sec; \nFastEstimator-Eval: step: 1470; epoch: 15; accuracy: 0.911; \nFastEstimator-Train: step: 1500; ce: 0.9886917; steps/sec: 9.86; model_lr: 0.0183029; \nFastEstimator-Train: step: 1568; epoch: 16; epoch_time: 9.96 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1568; epoch: 16; accuracy: 0.9168; \nFastEstimator-Train: step: 1600; ce: 0.98263824; steps/sec: 9.87; model_lr: 0.016154673; \nFastEstimator-Train: step: 1666; epoch: 17; epoch_time: 9.94 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1666; epoch: 17; accuracy: 0.924; \nFastEstimator-Train: step: 1700; ce: 0.95481974; steps/sec: 9.88; model_lr: 0.014006444; \nFastEstimator-Train: step: 1764; epoch: 18; epoch_time: 9.92 sec; \nFastEstimator-Eval: step: 1764; epoch: 18; accuracy: 0.9214; \nFastEstimator-Train: step: 1800; ce: 0.9707108; steps/sec: 9.88; model_lr: 0.011858217; \nFastEstimator-Train: step: 1862; epoch: 19; epoch_time: 9.95 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1862; epoch: 19; accuracy: 0.9288; \nFastEstimator-Train: step: 1900; ce: 0.9509058; steps/sec: 9.86; model_lr: 0.00970999; \nFastEstimator-Train: step: 1960; epoch: 20; epoch_time: 9.97 sec; \nFastEstimator-Eval: step: 1960; epoch: 20; accuracy: 0.924; \nFastEstimator-Train: step: 2000; ce: 0.9307832; steps/sec: 9.84; model_lr: 0.0075617614; \nFastEstimator-Train: step: 2058; epoch: 21; epoch_time: 9.93 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2058; epoch: 21; accuracy: 0.9334; \nFastEstimator-Train: step: 2100; ce: 0.91421276; steps/sec: 9.84; model_lr: 0.0054135337; \nFastEstimator-Train: step: 2156; epoch: 22; epoch_time: 10.0 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2156; epoch: 22; accuracy: 0.9374; \nFastEstimator-Train: step: 2200; ce: 0.9126489; steps/sec: 9.82; model_lr: 0.0032653061; \nFastEstimator-Train: step: 2254; epoch: 23; epoch_time: 9.98 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2254; epoch: 23; accuracy: 0.9412; \nFastEstimator-Train: step: 2300; ce: 0.909637; steps/sec: 9.87; model_lr: 0.0011170784; \nFastEstimator-Train: step: 2352; epoch: 24; epoch_time: 9.96 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2352; epoch: 24; accuracy: 0.9418; \nFastEstimator-Finish: step: 2352; total_time: 257.44 sec; model_lr: 0.0; \n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: epoch: 24; accuracy: 0.9362; \n</pre> In\u00a0[11]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 3, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\naxs[0,2].set_title(\"predict result\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    data = {\"x\": test_data[\"x\"][j]}\n    axs[i,0].imshow(data[\"x\"], cmap=\"gray\")\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    img = data[\"x_out\"].squeeze(axis=0).transpose((1,2,0))\n    img[:,:,0] = img[:,:,0] * 0.2471 + 0.4914 \n    img[:,:,1] = img[:,:,1] * 0.2435 + 0.4822\n    img[:,:,2] = img[:,:,2] * 0.2616 + 0.4465\n    axs[i,1].imshow(img)\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predict = data[\"y_pred\"].numpy().squeeze(axis=(0))\n    axs[i,2].text(0.2, 0.5, \"predicted class: {}\".format(np.argmax(predict)))\n    axs[i,2].get_xaxis().set_visible(False)\n    axs[i,2].get_yaxis().set_visible(False)\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 3, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\") axs[0,2].set_title(\"predict result\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     data = {\"x\": test_data[\"x\"][j]}     axs[i,0].imshow(data[\"x\"], cmap=\"gray\")          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      img = data[\"x_out\"].squeeze(axis=0).transpose((1,2,0))     img[:,:,0] = img[:,:,0] * 0.2471 + 0.4914      img[:,:,1] = img[:,:,1] * 0.2435 + 0.4822     img[:,:,2] = img[:,:,2] * 0.2616 + 0.4465     axs[i,1].imshow(img)          # run the network     data = network.transform(data, mode=\"infer\")     predict = data[\"y_pred\"].numpy().squeeze(axis=(0))     axs[i,2].text(0.2, 0.5, \"predicted class: {}\".format(np.argmax(predict)))     axs[i,2].get_xaxis().set_visible(False)     axs[i,2].get_yaxis().set_visible(False)"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#cifar-10-image-classification-using-resnet-pytorch-backend", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)\u00b6", "text": "<p>In this example we are going to demonstrate how to train a CIFAR-10 image classification model using a ResNet architecture on the PyTorch backend. All training details including model structure, data preprocessing, learning rate control, etc. come from https://github.com/davidcpage/cifar10-fast.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load CIFAR-10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the CIFAR-10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#set-up-a-pre-processing-pipeline", "title": "Set up a pre-processing <code>Pipeline</code>\u00b6", "text": "<p>Here we set up the data pipeline. This will involve a variety of data augmentation including: random cropping, horizontal flipping, image obscuration, and smoothed one-hot label encoding. Beside all of this, the image channels need to be transposed from HWC to CHW format due to PyTorch conventions. We set up these processing steps using <code>Ops</code> and also bundle the data sources and batch_size together into our <code>Pipeline</code>.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the <code>Pipeline</code> works as expected, let's visualize the output and check its size. <code>Pipeline.get_results</code> will return a batch data of pipeline output for this purpose:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the PyTorch way in this example.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-construction", "title": "Model construction\u00b6", "text": "<p>The model definitions are implemented in PyTorch and instantiated by calling <code>fe.build</code> which also associates the model with a specific optimizer.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p><code>Ops</code> are the basic components of a network that include models, loss calculation units, and post-processing units. In this step we are going to combine those pieces together into a <code>Network</code>:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>), save our best model (<code>BestModelSaver</code>), and change the learning rate (<code>LRScheduler</code>) of our optimizer over time. We will then use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> will trigger model testing using all of the test data defined in the <code>Pipeline</code>. This will allow us to check our accuracy on previously unseen data.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>In this step we run image inference directly using the model that we just trained. We randomly select 5 images from testing dataset and infer them image by image using <code>Pipeline.transform</code> and <code>Network.transform</code>. Please be aware that the <code>Pipeline</code> is no longer the same as it was during training, because we don't want to use data augmentation during inference. This detail was already defined in the <code>Pipeline</code> (mode = \"!infer\").</p>"}, {"location": "apphub/image_classification/mnist/mnist.html", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 2\nbatch_size = 32\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 2 batch_size = 32 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import mnist\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import mnist  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=batch_size,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"), \n                            Minmax(inputs=\"x_out\", outputs=\"x_out\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=batch_size,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"),                              Minmax(inputs=\"x_out\", outputs=\"x_out\")]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\n\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\nprint(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy())))\nprint(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy())))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"]  print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) print(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy()))) print(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy()))) <pre>the pipeline input data size: (32, 28, 28)\nthe pipeline output data size: (32, 28, 28, 1)\nthe maximum pixel value of output image: 1.0\nthe minimum pixel value of output image: 0.0\n</pre> In\u00a0[6]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\ninputs = tf.gather(data_xin.numpy(), indices)\noutputs = tf.gather(data_xout.numpy(), indices)\nimg = fe.util.ImgData(pipeline_input=inputs, pipeline_output=outputs)\nfig = img.paint_figure()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False) inputs = tf.gather(data_xin.numpy(), indices) outputs = tf.gather(data_xout.numpy(), indices) img = fe.util.ImgData(pipeline_input=inputs, pipeline_output=outputs) fig = img.paint_figure() In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ]) In\u00a0[9]: Copied! <pre>from fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy   traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3120923; model_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.3962714; steps/sec: 130.51; model_lr: 0.000998283; \nFastEstimator-Train: step: 200; ce: 0.1641247; steps/sec: 128.14; model_lr: 0.0009930746; \nFastEstimator-Train: step: 300; ce: 0.20016384; steps/sec: 123.16; model_lr: 0.0009844112; \nFastEstimator-Train: step: 400; ce: 0.139256; steps/sec: 118.37; model_lr: 0.00097235345; \nFastEstimator-Train: step: 500; ce: 0.20949002; steps/sec: 116.86; model_lr: 0.000956986; \nFastEstimator-Train: step: 600; ce: 0.08091536; steps/sec: 115.89; model_lr: 0.00093841663; \nFastEstimator-Train: step: 700; ce: 0.069529384; steps/sec: 112.6; model_lr: 0.0009167756; \nFastEstimator-Train: step: 800; ce: 0.02633699; steps/sec: 109.37; model_lr: 0.00089221465; \nFastEstimator-Train: step: 900; ce: 0.12905718; steps/sec: 104.44; model_lr: 0.0008649062; \nFastEstimator-Train: step: 1000; ce: 0.018508099; steps/sec: 108.77; model_lr: 0.0008350416; \nFastEstimator-Train: step: 1100; ce: 0.10962237; steps/sec: 106.61; model_lr: 0.00080283044; \nFastEstimator-Train: step: 1200; ce: 0.047606118; steps/sec: 101.79; model_lr: 0.0007684987; \nFastEstimator-Train: step: 1300; ce: 0.13268313; steps/sec: 97.41; model_lr: 0.0007322871; \nFastEstimator-Train: step: 1400; ce: 0.026097888; steps/sec: 94.08; model_lr: 0.00069444976; \nFastEstimator-Train: step: 1500; ce: 0.020507228; steps/sec: 92.03; model_lr: 0.0006552519; \nFastEstimator-Train: step: 1600; ce: 0.0048278654; steps/sec: 92.14; model_lr: 0.00061496865; \nFastEstimator-Train: step: 1700; ce: 0.01370596; steps/sec: 88.92; model_lr: 0.0005738824; \nFastEstimator-Train: step: 1800; ce: 0.15647383; steps/sec: 89.46; model_lr: 0.00053228147; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 19.9 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp_19zst_o/model_best_accuracy\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.048851434; accuracy: 0.985; since_best_accuracy: 0; max_accuracy: 0.985; \nFastEstimator-Train: step: 1900; ce: 0.0039410545; steps/sec: 87.28; model_lr: 0.00049045763; \nFastEstimator-Train: step: 2000; ce: 0.017347965; steps/sec: 86.28; model_lr: 0.00044870423; \nFastEstimator-Train: step: 2100; ce: 0.0798438; steps/sec: 89.34; model_lr: 0.0004073141; \nFastEstimator-Train: step: 2200; ce: 0.17821585; steps/sec: 86.08; model_lr: 0.00036657765; \nFastEstimator-Train: step: 2300; ce: 0.002756747; steps/sec: 86.16; model_lr: 0.00032678054; \nFastEstimator-Train: step: 2400; ce: 0.00071113696; steps/sec: 85.27; model_lr: 0.00028820196; \nFastEstimator-Train: step: 2500; ce: 0.0027370513; steps/sec: 80.7; model_lr: 0.00025111248; \nFastEstimator-Train: step: 2600; ce: 0.0123588; steps/sec: 84.42; model_lr: 0.00021577229; \nFastEstimator-Train: step: 2700; ce: 0.0069204723; steps/sec: 81.53; model_lr: 0.00018242926; \nFastEstimator-Train: step: 2800; ce: 0.00642678; steps/sec: 77.9; model_lr: 0.00015131726; \nFastEstimator-Train: step: 2900; ce: 0.008096467; steps/sec: 81.94; model_lr: 0.00012265453; \nFastEstimator-Train: step: 3000; ce: 0.0023379987; steps/sec: 79.61; model_lr: 9.664212e-05; \nFastEstimator-Train: step: 3100; ce: 0.104631886; steps/sec: 79.75; model_lr: 7.346248e-05; \nFastEstimator-Train: step: 3200; ce: 0.003903159; steps/sec: 78.32; model_lr: 5.3278196e-05; \nFastEstimator-Train: step: 3300; ce: 0.024989499; steps/sec: 79.65; model_lr: 3.6230853e-05; \nFastEstimator-Train: step: 3400; ce: 0.02124865; steps/sec: 81.16; model_lr: 2.2440026e-05; \nFastEstimator-Train: step: 3500; ce: 0.008722213; steps/sec: 77.56; model_lr: 1.2002448e-05; \nFastEstimator-Train: step: 3600; ce: 0.18177237; steps/sec: 77.0; model_lr: 4.9913274e-06; \nFastEstimator-Train: step: 3700; ce: 0.005134264; steps/sec: 79.63; model_lr: 1.4558448e-06; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 22.99 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp_19zst_o/model_best_accuracy\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.021919494; accuracy: 0.994; since_best_accuracy: 0; max_accuracy: 0.994; \nFastEstimator-Finish: step: 3750; total_time: 46.88 sec; model_lr: 1.0001753e-06; \n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.9908; \n</pre> In\u00a0[11]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\n\ninputs = []\noutputs = []\npredictions = []\n\nfor idx in indices:\n    inputs.append(test_data[\"x\"][idx])\n    data = {\"x\": inputs[-1]}\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))\n\nimg = fe.util.ImgData(pipeline_input=np.stack(inputs), pipeline_output=np.stack(outputs), predictions=np.stack(predictions))\nfig = img.paint_figure()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False)  inputs = [] outputs = [] predictions = []  for idx in indices:     inputs.append(test_data[\"x\"][idx])     data = {\"x\": inputs[-1]}          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))          # run the network     data = network.transform(data, mode=\"infer\")     predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))  img = fe.util.ImgData(pipeline_input=np.stack(inputs), pipeline_output=np.stack(outputs), predictions=np.stack(predictions)) fig = img.paint_figure()"}, {"location": "apphub/image_classification/mnist/mnist.html#mnist-image-classification-using-lenet-tensorflow-backend", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)\u00b6", "text": "<p>In this example, we are going to demonstrate how to train an MNIST image classification model using a LeNet model architecture and TensorFlow backend.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/mnist/mnist.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the MNIST dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#set-up-a-preprocessing-pipeline", "title": "Set up a preprocessing pipeline\u00b6", "text": "<p>In this example, the data preprocessing steps include adding a channel to the images (since they are grey-scale) and normalizing the image pixel values to the range [0, 1]. We set up these processing steps using <code>Ops</code>. The <code>Pipeline</code> also takes our data sources and batch size as inputs.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch  of pipeline output to enable this:</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Here we are going to import one of FastEstimator's pre-defined model architectures, which was written in TensorFlow. We create a model instance by compiling our model definition function along with a specific model optimizer.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect the model and <code>Ops</code> together into a <code>Network</code>. <code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update rules, or even models themselves.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which compute accuracy (<code>Accuracy</code>), save the best model (<code>BestModelSaver</code>), and change the model learning rate over time (<code>LRScheduler</code>).</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing using the test dataset that was specified in <code>Pipeline</code>. We can evaluate the model's accuracy on this previously unseen data.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Now let's run inferencing on several images directly using the model that we just trained. We randomly select 5 images from the testing dataset and infer them image by image by leveraging <code>Pipeline.transform</code> and <code>Network.transform</code>:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom typing import Any, Dict, Tuple\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import tempfile import matplotlib.pyplot as plt from typing import Any, Dict, Tuple In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 20\nbatch_size = 100\nmax_train_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 20 batch_size = 100 max_train_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\n\ntrain_data, test_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data  train_data, test_data = load_data() In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1) \n        Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1] \n        Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value\n    ])\n</pre> from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax  pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1)          Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1]          Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"] print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) <pre>the pipeline input data size: (100, 28, 28)\nthe pipeline output data size: (100, 28, 28, 1)\n</pre> <p>Let's randomly select 5 samples and visualize the differences between the <code>Pipeline</code> input and output.</p> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 2, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\n\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    img_in = data_xin.numpy()[j]\n    axs[i,0].imshow(img_in, cmap=\"gray\")\n    \n    img_out = data_xout.numpy()[j,:,:,0]\n    axs[i,1].imshow(img_out, cmap=\"gray\")\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 2, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\")   for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     img_in = data_xin.numpy()[j]     axs[i,0].imshow(img_in, cmap=\"gray\")          img_out = data_xout.numpy()[j,:,:,0]     axs[i,1].imshow(img_out, cmap=\"gray\")      In\u00a0[7]: Copied! <pre>LATENT_DIM = 50\n\ndef encoder_net():\n    infer_model = tf.keras.Sequential()\n    infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n    infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Flatten())\n    infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))\n    return infer_model\n\n\ndef decoder_net():\n    generative_model = tf.keras.Sequential()\n    generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))\n    generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))\n    generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))\n    return generative_model\n\nencode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\")\ndecode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\")\n</pre> LATENT_DIM = 50  def encoder_net():     infer_model = tf.keras.Sequential()     infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))     infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Flatten())     infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))     return infer_model   def decoder_net():     generative_model = tf.keras.Sequential()     generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))     generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))     generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))     return generative_model  encode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\") decode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass SplitOp(TensorOp):\n\"\"\"To split the infer net output into two \"\"\"\n    def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n        mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)\n        return mean, logvar\n</pre> from fastestimator.op.tensorop import TensorOp  class SplitOp(TensorOp):     \"\"\"To split the infer net output into two \"\"\"     def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:         mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)         return mean, logvar In\u00a0[9]: Copied! <pre>class ReparameterizeOp(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:\n        mean, logvar = data\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n</pre> class ReparameterizeOp(TensorOp):     def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:         mean, logvar = data         eps = tf.random.normal(shape=mean.shape)         return eps * tf.exp(logvar * .5) + mean In\u00a0[10]: Copied! <pre>import math\n\nclass CVAELoss(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:\n        cross_ent_mean, mean, logvar, z = data   \n        \n        cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches \n                                                         # make it total cross entropy over pixels \n        logpz = self._log_normal_pdf(z, 0., 0.)\n        logqz_x = self._log_normal_pdf(z, mean, logvar)\n        total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)\n\n        return total_loss\n    \n    @staticmethod\n    def _log_normal_pdf(sample, mean, logvar, raxis=1):\n        log2pi = tf.math.log(2. * tf.constant(math.pi))\n        return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n</pre> import math  class CVAELoss(TensorOp):     def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:         cross_ent_mean, mean, logvar, z = data                     cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches                                                           # make it total cross entropy over pixels          logpz = self._log_normal_pdf(z, 0., 0.)         logqz_x = self._log_normal_pdf(z, mean, logvar)         total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)          return total_loss          @staticmethod     def _log_normal_pdf(sample, mean, logvar, raxis=1):         log2pi = tf.math.log(2. * tf.constant(math.pi))         return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis) In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),\n    SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),\n    ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"), \n    ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),\n    CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"), \n    CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),\n    UpdateOp(model=encode_model, loss_name=\"loss\"),\n    UpdateOp(model=decode_model, loss_name=\"loss\"),\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),     SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),     ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"),      ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),     CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"),      CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),     UpdateOp(model=encode_model, loss_name=\"loss\"),     UpdateOp(model=decode_model, loss_name=\"loss\"), ]) In\u00a0[12]: Copied! <pre>from fastestimator.trace.io import ModelSaver\n\ntraces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs), \n          ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         log_steps=600)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.trace.io import ModelSaver  traces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs),            ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          log_steps=600)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; encoder_lr: 0.001; decoder_lr: 0.001; \nFastEstimator-Train: step: 1; loss: 543.8469; \nFastEstimator-Train: step: 600; loss: 107.70025; steps/sec: 226.96; \nFastEstimator-Train: step: 600; epoch: 1; epoch_time: 6.37 sec; \nFastEstimator-Train: step: 1200; loss: 93.69548; steps/sec: 192.28; \nFastEstimator-Train: step: 1200; epoch: 2; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 1800; loss: 86.93849; steps/sec: 193.82; \nFastEstimator-Train: step: 1800; epoch: 3; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 2400; loss: 87.41001; steps/sec: 195.48; \nFastEstimator-Train: step: 2400; epoch: 4; epoch_time: 3.06 sec; \nFastEstimator-Train: step: 3000; loss: 83.25523; steps/sec: 193.89; \nFastEstimator-Train: step: 3000; epoch: 5; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 3600; loss: 83.15831; steps/sec: 194.31; \nFastEstimator-Train: step: 3600; epoch: 6; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 4200; loss: 81.78505; steps/sec: 192.58; \nFastEstimator-Train: step: 4200; epoch: 7; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 4800; loss: 84.475464; steps/sec: 193.11; \nFastEstimator-Train: step: 4800; epoch: 8; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 5400; loss: 80.58606; steps/sec: 194.08; \nFastEstimator-Train: step: 5400; epoch: 9; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 6000; loss: 81.36517; steps/sec: 195.2; \nFastEstimator-Train: step: 6000; epoch: 10; epoch_time: 3.08 sec; \nFastEstimator-Train: step: 6600; loss: 81.0715; steps/sec: 194.17; \nFastEstimator-Train: step: 6600; epoch: 11; epoch_time: 3.08 sec; \nFastEstimator-Train: step: 7200; loss: 80.676956; steps/sec: 193.39; \nFastEstimator-Train: step: 7200; epoch: 12; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 7800; loss: 82.24495; steps/sec: 194.2; \nFastEstimator-Train: step: 7800; epoch: 13; epoch_time: 3.07 sec; \nFastEstimator-Train: step: 8400; loss: 78.083145; steps/sec: 192.18; \nFastEstimator-Train: step: 8400; epoch: 14; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 9000; loss: 78.90831; steps/sec: 193.96; \nFastEstimator-Train: step: 9000; epoch: 15; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 9600; loss: 82.3984; steps/sec: 193.15; \nFastEstimator-Train: step: 9600; epoch: 16; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 10200; loss: 75.02458; steps/sec: 192.84; \nFastEstimator-Train: step: 10200; epoch: 17; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 10800; loss: 76.140915; steps/sec: 193.44; \nFastEstimator-Train: step: 10800; epoch: 18; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 11400; loss: 76.65802; steps/sec: 190.19; \nFastEstimator-Train: step: 11400; epoch: 19; epoch_time: 3.15 sec; \nFastEstimator-Train: step: 12000; loss: 76.98837; steps/sec: 191.66; \nSaved model to /tmp/tmpbpgss6_a/encoder_epoch_20.h5\nSaved model to /tmp/tmpbpgss6_a/decoder_epoch_20.h5\nFastEstimator-Train: step: 12000; epoch: 20; epoch_time: 3.11 sec; \nFastEstimator-Finish: step: 12000; total_time: 65.35 sec; encoder_lr: 0.001; decoder_lr: 0.001; \n</pre> In\u00a0[13]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 3, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\naxs[0,2].set_title(\"decoder output\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    data = {\"x\": test_data[\"x\"][j]}\n    axs[i,0].imshow(data[\"x\"], cmap=\"gray\")\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    img = data[\"x_out\"].squeeze(axis=(0,3))\n    axs[i,1].imshow(img, cmap=\"gray\")\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    img = data[\"x_logit\"].numpy().squeeze(axis=(0,3))\n    axs[i,2].imshow(img, cmap=\"gray\")\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 3, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\") axs[0,2].set_title(\"decoder output\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     data = {\"x\": test_data[\"x\"][j]}     axs[i,0].imshow(data[\"x\"], cmap=\"gray\")          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      img = data[\"x_out\"].squeeze(axis=(0,3))     axs[i,1].imshow(img, cmap=\"gray\")          # run the network     data = network.transform(data, mode=\"infer\")     img = data[\"x_logit\"].numpy().squeeze(axis=(0,3))     axs[i,2].imshow(img, cmap=\"gray\")"}, {"location": "apphub/image_generation/cvae/cvae.html#convolutional-variational-autoencoder-using-the-mnist-dataset-tensorflow-backend", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#introduction-to-cvae", "title": "Introduction to CVAE\u00b6", "text": "<p>CVAEs are Convolutional Variational Autoencoders. They are composed of two models using convolutions: an encoder to cast the input into a latent dimension, and a decoder that will move data from the latent dimension back to the input space. The figure below illustrates the main idea behind CVAEs.</p> <p>In this example, we will use a CVAE to generate data similar to the MNIST dataset using the TensorFlow backend. All training details including model structure, data preprocessing, loss calculation, etc. come from the TensorFlow CVAE tutorial </p>"}, {"location": "apphub/image_generation/cvae/cvae.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation dataset and prepare FastEstimator's data <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>Let's use a FastEstimator API to load the MNIST dataset:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#set-up-the-preprocessing-pipline", "title": "Set up the preprocessing <code>Pipline</code>\u00b6", "text": "<p>In this example, the data preprocessing steps include expanding image dimension, normalizing the image pixel values to the range [0, 1], and binarizing pixel values. We set up these processing steps using <code>Ops</code>, while also defining the data source and batch size for the <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch of data for this purpose:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Both of our models' definitions are implemented in TensorFlow and instantiated by calling <code>fe.build</code> (which also associates the model with specific optimizers).</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops", "title": "Customize <code>Ops</code>\u00b6", "text": "<p><code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update units, or even the model itself. Some <code>Ops</code> such as cross entropy are pre-defined in FastEstimator, but for any logic that is not there yet, users need to define their own <code>Ops</code>. Please keep all custom <code>Ops</code> backend-consistent with your model backend. In this case all <code>Ops</code> need to be implemented in TensorFlow since our model is built from Tensorflow.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-splitop", "title": "Customize Ops - SplitOp\u00b6", "text": "<p>Because the encoder output contains both mean and log of variance, we need to split them into two outputs:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-reparameterizeop", "title": "Customize Ops - ReparameterizeOp\u00b6", "text": "<p>In this example case, the input to the decoder is a random sample from a normal distribution whose mean and variation are the output of the encoder. We are going to build an <code>Op</code> called \"ReparameterizeOp\" to accomplish this:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-cvaeloss", "title": "Customize Ops - CVAELoss\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect all models and <code>Ops</code> together into a <code>Network</code></p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to compile the <code>Network</code> and <code>Pipeline</code> and indicate in <code>traces</code> that we want to save the best models. We can then use <code>estimator.fit()</code> to start the training process:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the model is trained, we will try to run our models on some testing data. We randomly select 5 images from the testing dataset and infer them image by image:</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html", "title": "Horse to Zebra Unpaired Image Translation with CycleGAN in FastEstimator", "text": "<p>This notebook demonstrates how to perform an unpaired image to image translation using CycleGAN in FastEstimator. The details of the method is found in Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. We will specifically look at the problem of translating horse images to zebra images.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.init import normal_\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\n</pre> import tempfile  import numpy as np import torch import torch.nn as nn from torch.nn.init import normal_  import fastestimator as fe from fastestimator.backend import reduce_mean In\u00a0[2]: parameters Copied! <pre>#Parameters\nepochs = 200\nbatch_size = 1\nmax_train_steps_per_epoch = None\nsave_dir=tempfile.mkdtemp()\nweight = 10.0\ndata_dir=None\n</pre> #Parameters epochs = 200 batch_size = 1 max_train_steps_per_epoch = None save_dir=tempfile.mkdtemp() weight = 10.0 data_dir=None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.horse2zebra import load_data\ntrain_data, test_data = load_data(batch_size=batch_size, root_dir=data_dir)\n</pre> from fastestimator.dataset.data.horse2zebra import load_data train_data, test_data = load_data(batch_size=batch_size, root_dir=data_dir) <p>Let's create the pipeline. As, batch_size must be <code>None</code> when BatchDataset is being used, we will not provide the batch_size argument.</p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, RandomCrop, Resize\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    ops=[\n        ReadImage(inputs=[\"A\", \"B\"], outputs=[\"A\", \"B\"]),\n        Normalize(inputs=[\"A\", \"B\"], outputs=[\"real_A\", \"real_B\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n        Resize(height=286, width=286, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),\n        RandomCrop(height=256, width=256, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),\n        Resize(height=286, width=286, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),\n        RandomCrop(height=256, width=256, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"real_A\", image_out=\"real_A\", mode=\"train\")),\n        Sometimes(HorizontalFlip(image_in=\"real_B\", image_out=\"real_B\", mode=\"train\")),\n        ChannelTranspose(inputs=[\"real_A\", \"real_B\"], outputs=[\"real_A\", \"real_B\"])\n    ])\n</pre> from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, RandomCrop, Resize from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  pipeline = fe.Pipeline(     train_data=train_data,     ops=[         ReadImage(inputs=[\"A\", \"B\"], outputs=[\"A\", \"B\"]),         Normalize(inputs=[\"A\", \"B\"], outputs=[\"real_A\", \"real_B\"], mean=1.0, std=1.0, max_pixel_value=127.5),         Resize(height=286, width=286, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),         RandomCrop(height=256, width=256, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),         Resize(height=286, width=286, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),         RandomCrop(height=256, width=256, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"real_A\", image_out=\"real_A\", mode=\"train\")),         Sometimes(HorizontalFlip(image_in=\"real_B\", image_out=\"real_B\", mode=\"train\")),         ChannelTranspose(inputs=[\"real_A\", \"real_B\"], outputs=[\"real_A\", \"real_B\"])     ]) <p>We can visualize sample images from the <code>pipeline</code> using <code>get_results</code> method.</p> In\u00a0[5]: Copied! <pre>def Minmax(img):\n    img_max = np.max(img)\n    img_min = np.min(img)\n    img = (img - img_min)/max((img_max - img_min), 1e-7)\n    img = (img*255).astype(np.uint8)\n    return img\n</pre> def Minmax(img):     img_max = np.max(img)     img_min = np.min(img)     img = (img - img_min)/max((img_max - img_min), 1e-7)     img = (img*255).astype(np.uint8)     return img In\u00a0[6]: Copied! <pre>sample_batch = pipeline.get_results()\nhorse_img = sample_batch[\"real_A\"][0]\nhorse_img = np.transpose(horse_img.numpy(), (1, 2, 0))\nhorse_img = np.expand_dims(Minmax(horse_img), 0)\n\nzebra_img = sample_batch[\"real_B\"][0]\nzebra_img = np.transpose(zebra_img.numpy(), (1, 2, 0))\nzebra_img = np.expand_dims(Minmax(zebra_img), 0)\n\nimg = fe.util.ImgData(Sample_Horse_Image=horse_img, Sample_Zebra_Image=zebra_img)\nfig = img.paint_figure()\n</pre> sample_batch = pipeline.get_results() horse_img = sample_batch[\"real_A\"][0] horse_img = np.transpose(horse_img.numpy(), (1, 2, 0)) horse_img = np.expand_dims(Minmax(horse_img), 0)  zebra_img = sample_batch[\"real_B\"][0] zebra_img = np.transpose(zebra_img.numpy(), (1, 2, 0)) zebra_img = np.expand_dims(Minmax(zebra_img), 0)  img = fe.util.ImgData(Sample_Horse_Image=horse_img, Sample_Zebra_Image=zebra_img) fig = img.paint_figure() <p>In CycleGAN, there are 2 generators and 2 discriminators being trained.</p> <ul> <li>Generator <code>g_AtoB</code> learns to map horse images to zebra images</li> <li>Generator <code>g_BtoA</code> learns to map zebra images to horse images</li> <li>Discriminator <code>d_A</code> learns to differentiate between real hores images and fake horse images produced by <code>g_BtoA</code></li> <li>Discriminator <code>d_B</code> learns to differentiate between image zebra and fake zebra images produced by <code>g_AtoB</code></li> </ul> <p>The architecture of generator is a modified resnet, and the architecture of discriminator is a PatchGAN.</p> In\u00a0[7]: Copied! <pre>class ResidualBlock(nn.Module):\n\"\"\"Residual block architecture\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super().__init__()\n        self.layers = nn.Sequential(nn.ReflectionPad2d(1),\n                                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size),\n                                    nn.InstanceNorm2d(out_channels),\n                                    nn.ReLU(inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size),\n                                    nn.InstanceNorm2d(out_channels))\n\n        for layer in self.layers:\n            if isinstance(layer, nn.Conv2d):\n                normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x_out = self.layers(x)\n        x_out = x_out + x\n        return x_out\n\n\nclass Discriminator(nn.Module):\n\"\"\"Discriminator network architecture\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n                                    nn.InstanceNorm2d(128),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n                                    nn.InstanceNorm2d(256),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(256, 512, kernel_size=4, stride=1),\n                                    nn.InstanceNorm2d(512),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(512, 1, kernel_size=4, stride=1))\n\n        for layer in self.layers:\n            if isinstance(layer, nn.Conv2d):\n                normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\n\nclass Generator(nn.Module):\n\"\"\"Generator network architecture\"\"\"\n    def __init__(self, num_blocks=9):\n        super().__init__()\n        self.layers1 = nn.Sequential(nn.ReflectionPad2d(3),\n                                     nn.Conv2d(3, 64, kernel_size=7, stride=1),\n                                     nn.InstanceNorm2d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n                                     nn.InstanceNorm2d(128),\n                                     nn.ReLU(inplace=True),\n                                     nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n                                     nn.InstanceNorm2d(256),\n                                     nn.ReLU(inplace=True))\n        self.resblocks = nn.Sequential(*[ResidualBlock(256, 256) for i in range(num_blocks)])\n        self.layers2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n                                     nn.InstanceNorm2d(128),\n                                     nn.ReLU(inplace=True),\n                                     nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n                                     nn.InstanceNorm2d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.ReflectionPad2d(3),\n                                     nn.Conv2d(64, 3, kernel_size=7, stride=1))\n\n        for block in [self.layers1, self.layers2]:\n            for layer in block:\n                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n                    normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x = self.layers1(x)\n        x = self.resblocks(x)\n        x = self.layers2(x)\n        x = torch.tanh(x)\n        return x\n</pre> class ResidualBlock(nn.Module):     \"\"\"Residual block architecture\"\"\"     def __init__(self, in_channels, out_channels, kernel_size=3):         super().__init__()         self.layers = nn.Sequential(nn.ReflectionPad2d(1),                                     nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size),                                     nn.InstanceNorm2d(out_channels),                                     nn.ReLU(inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size),                                     nn.InstanceNorm2d(out_channels))          for layer in self.layers:             if isinstance(layer, nn.Conv2d):                 normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x_out = self.layers(x)         x_out = x_out + x         return x_out   class Discriminator(nn.Module):     \"\"\"Discriminator network architecture\"\"\"     def __init__(self):         super().__init__()         self.layers = nn.Sequential(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),                                     nn.InstanceNorm2d(128),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),                                     nn.InstanceNorm2d(256),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(256, 512, kernel_size=4, stride=1),                                     nn.InstanceNorm2d(512),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(512, 1, kernel_size=4, stride=1))          for layer in self.layers:             if isinstance(layer, nn.Conv2d):                 normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x = self.layers(x)         return x   class Generator(nn.Module):     \"\"\"Generator network architecture\"\"\"     def __init__(self, num_blocks=9):         super().__init__()         self.layers1 = nn.Sequential(nn.ReflectionPad2d(3),                                      nn.Conv2d(3, 64, kernel_size=7, stride=1),                                      nn.InstanceNorm2d(64),                                      nn.ReLU(inplace=True),                                      nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),                                      nn.InstanceNorm2d(128),                                      nn.ReLU(inplace=True),                                      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),                                      nn.InstanceNorm2d(256),                                      nn.ReLU(inplace=True))         self.resblocks = nn.Sequential(*[ResidualBlock(256, 256) for i in range(num_blocks)])         self.layers2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),                                      nn.InstanceNorm2d(128),                                      nn.ReLU(inplace=True),                                      nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),                                      nn.InstanceNorm2d(64),                                      nn.ReLU(inplace=True),                                      nn.ReflectionPad2d(3),                                      nn.Conv2d(64, 3, kernel_size=7, stride=1))          for block in [self.layers1, self.layers2]:             for layer in block:                 if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):                     normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x = self.layers1(x)         x = self.resblocks(x)         x = self.layers2(x)         x = torch.tanh(x)         return x In\u00a0[8]: Copied! <pre>g_AtoB = fe.build(model_fn=Generator,\n                  model_name=\"g_AtoB\",\n                  optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\ng_BtoA = fe.build(model_fn=Generator,\n                  model_name=\"g_BtoA\",\n                  optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\nd_A = fe.build(model_fn=Discriminator,\n               model_name=\"d_A\",\n               optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\nd_B = fe.build(model_fn=Discriminator,\n               model_name=\"d_B\",\n               optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\n</pre> g_AtoB = fe.build(model_fn=Generator,                   model_name=\"g_AtoB\",                   optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) g_BtoA = fe.build(model_fn=Generator,                   model_name=\"g_BtoA\",                   optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) d_A = fe.build(model_fn=Discriminator,                model_name=\"d_A\",                optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) d_B = fe.build(model_fn=Discriminator,                model_name=\"d_B\",                optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) <p>Because horse images and zebra images are unpaired, the loss of generator is quite complex. The generator's loss is composed of three terms: * adversarial * cycle-consistency * identity. The cycle-consistency term and identity term are weighted by a parameter <code>LAMBDA</code>. In the paper the authors used 10 for <code>LAMBDA</code>.</p> <p>Let's consider computing the loss for <code>g_AtoB</code> which translates horses to zebras.</p> <ol> <li>Adversarial term that is computed as binary cross entropy between ones and <code>d_A</code>'s prediction on the translated images</li> <li>Cycle consistency term is computed with mean absolute error between original horse images and the cycled horse images that are translated forward by <code>g_AtoB</code> and then backward by <code>g_BtoA</code>.</li> <li>Identity term that is computed with the mean absolute error between original zebra and the output of <code>g_AtoB</code> on these images.</li> </ol> <p>The discriminator's loss is the standard adversarial loss that is computed as binary cross entropy between:</p> <ul> <li>Ones and real images</li> <li>Zeros and fake images</li> </ul> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass GLoss(TensorOp):\n\"\"\"TensorOp to compute generator loss\"\"\"\n    def __init__(self, inputs, weight, device, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_fn = nn.MSELoss(reduction=\"none\")\n        self.LAMBDA = weight\n        self.device = device\n        self.average_loss = average_loss\n\n    def _adversarial_loss(self, fake_img):\n        return torch.mean(self.loss_fn(fake_img, torch.ones_like(fake_img, device=self.device)), dim=(2, 3))\n\n    def _identity_loss(self, real_img, same_img):\n        return 0.5 * self.LAMBDA * torch.mean(torch.abs(real_img - same_img), dim=(1, 2, 3))\n\n    def _cycle_loss(self, real_img, cycled_img):\n        return self.LAMBDA * torch.mean(torch.abs(real_img - cycled_img), dim=(1, 2, 3))\n\n    def forward(self, data, state):\n        real_img, fake_img, cycled_img, same_img = data\n        total_loss = self._adversarial_loss(fake_img) + self._identity_loss(real_img, same_img) + self._cycle_loss(\n            real_img, cycled_img)\n\n        if self.average_loss:\n            total_loss = reduce_mean(total_loss)\n\n        return total_loss\n\n\nclass DLoss(TensorOp):\n\"\"\"TensorOp to compute discriminator loss\"\"\"\n    def __init__(self, inputs, device, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_fn = nn.MSELoss(reduction=\"none\")\n        self.device = device\n        self.average_loss = average_loss\n\n    def forward(self, data, state):\n        real_img, fake_img = data\n        real_img_loss = torch.mean(self.loss_fn(real_img, torch.ones_like(real_img, device=self.device)), dim=(2, 3))\n        fake_img_loss = torch.mean(self.loss_fn(fake_img, torch.zeros_like(real_img, device=self.device)), dim=(2, 3))\n        total_loss = real_img_loss + fake_img_loss\n\n        if self.average_loss:\n            total_loss = reduce_mean(total_loss)\n\n        return 0.5 * total_loss\n</pre> from fastestimator.op.tensorop import TensorOp  class GLoss(TensorOp):     \"\"\"TensorOp to compute generator loss\"\"\"     def __init__(self, inputs, weight, device, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_fn = nn.MSELoss(reduction=\"none\")         self.LAMBDA = weight         self.device = device         self.average_loss = average_loss      def _adversarial_loss(self, fake_img):         return torch.mean(self.loss_fn(fake_img, torch.ones_like(fake_img, device=self.device)), dim=(2, 3))      def _identity_loss(self, real_img, same_img):         return 0.5 * self.LAMBDA * torch.mean(torch.abs(real_img - same_img), dim=(1, 2, 3))      def _cycle_loss(self, real_img, cycled_img):         return self.LAMBDA * torch.mean(torch.abs(real_img - cycled_img), dim=(1, 2, 3))      def forward(self, data, state):         real_img, fake_img, cycled_img, same_img = data         total_loss = self._adversarial_loss(fake_img) + self._identity_loss(real_img, same_img) + self._cycle_loss(             real_img, cycled_img)          if self.average_loss:             total_loss = reduce_mean(total_loss)          return total_loss   class DLoss(TensorOp):     \"\"\"TensorOp to compute discriminator loss\"\"\"     def __init__(self, inputs, device, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_fn = nn.MSELoss(reduction=\"none\")         self.device = device         self.average_loss = average_loss      def forward(self, data, state):         real_img, fake_img = data         real_img_loss = torch.mean(self.loss_fn(real_img, torch.ones_like(real_img, device=self.device)), dim=(2, 3))         fake_img_loss = torch.mean(self.loss_fn(fake_img, torch.zeros_like(real_img, device=self.device)), dim=(2, 3))         total_loss = real_img_loss + fake_img_loss          if self.average_loss:             total_loss = reduce_mean(total_loss)          return 0.5 * total_loss <p>We implement an image buffer as a <code>TensorOp</code> which stores the previous images produced by the generators to updated the discriminators as outlined in Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.</p> In\u00a0[10]: Copied! <pre>class Buffer(TensorOp):\n    def __init__(self, image_in=None, image_out=None, mode=None, buffer_size=50):\n        super().__init__(inputs=image_in, outputs=image_out, mode=mode)\n        self.buffer_size = buffer_size\n        self.num_imgs = 0\n        self.image_buffer = []\n\n    def forward(self, data, state):\n        output = []\n        for image in data:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs &lt; self.buffer_size:\n                self.image_buffer.append(image)\n                output.append(image)\n                self.num_imgs += 1\n            else:\n                if np.random.uniform() &gt; 0.5:\n                    idx = np.random.randint(self.buffer_size)\n                    temp = self.image_buffer[idx].clone()\n                    self.image_buffer[idx] = image\n                    output.append(temp)\n                else:\n                    output.append(image)\n\n        output = torch.cat(output, 0)\n        return output\n</pre> class Buffer(TensorOp):     def __init__(self, image_in=None, image_out=None, mode=None, buffer_size=50):         super().__init__(inputs=image_in, outputs=image_out, mode=mode)         self.buffer_size = buffer_size         self.num_imgs = 0         self.image_buffer = []      def forward(self, data, state):         output = []         for image in data:             image = torch.unsqueeze(image.data, 0)             if self.num_imgs &lt; self.buffer_size:                 self.image_buffer.append(image)                 output.append(image)                 self.num_imgs += 1             else:                 if np.random.uniform() &gt; 0.5:                     idx = np.random.randint(self.buffer_size)                     temp = self.image_buffer[idx].clone()                     self.image_buffer[idx] = image                     output.append(temp)                 else:                     output.append(image)          output = torch.cat(output, 0)         return output <p>Once associated losses are defined, we can now define the <code>Network</code> object.</p> In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),\n    ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),\n    Buffer(image_in=\"fake_A\", image_out=\"buffer_fake_A\"),\n    Buffer(image_in=\"fake_B\", image_out=\"buffer_fake_B\"),\n    ModelOp(inputs=\"real_A\", model=d_A, outputs=\"d_real_A\"),\n    ModelOp(inputs=\"fake_A\", model=d_A, outputs=\"d_fake_A\"),\n    ModelOp(inputs=\"buffer_fake_A\", model=d_A, outputs=\"buffer_d_fake_A\"),\n    ModelOp(inputs=\"real_B\", model=d_B, outputs=\"d_real_B\"),\n    ModelOp(inputs=\"fake_B\", model=d_B, outputs=\"d_fake_B\"),\n    ModelOp(inputs=\"buffer_fake_B\", model=d_B, outputs=\"buffer_d_fake_B\"),\n    ModelOp(inputs=\"real_A\", model=g_BtoA, outputs=\"same_A\"),\n    ModelOp(inputs=\"fake_B\", model=g_BtoA, outputs=\"cycled_A\"),\n    ModelOp(inputs=\"real_B\", model=g_AtoB, outputs=\"same_B\"),\n    ModelOp(inputs=\"fake_A\", model=g_AtoB, outputs=\"cycled_B\"),\n    GLoss(inputs=(\"real_A\", \"d_fake_B\", \"cycled_A\", \"same_A\"), weight=weight, device=device, outputs=\"g_AtoB_loss\"),\n    GLoss(inputs=(\"real_B\", \"d_fake_A\", \"cycled_B\", \"same_B\"), weight=weight, device=device, outputs=\"g_BtoA_loss\"),\n    DLoss(inputs=(\"d_real_A\", \"buffer_d_fake_A\"), outputs=\"d_A_loss\", device=device),\n    DLoss(inputs=(\"d_real_B\", \"buffer_d_fake_B\"), outputs=\"d_B_loss\", device=device),\n    UpdateOp(model=g_AtoB, loss_name=\"g_AtoB_loss\"),\n    UpdateOp(model=g_BtoA, loss_name=\"g_BtoA_loss\"),\n    UpdateOp(model=d_A, loss_name=\"d_A_loss\"),\n    UpdateOp(model=d_B, loss_name=\"d_B_loss\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  network = fe.Network(ops=[     ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),     ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),     Buffer(image_in=\"fake_A\", image_out=\"buffer_fake_A\"),     Buffer(image_in=\"fake_B\", image_out=\"buffer_fake_B\"),     ModelOp(inputs=\"real_A\", model=d_A, outputs=\"d_real_A\"),     ModelOp(inputs=\"fake_A\", model=d_A, outputs=\"d_fake_A\"),     ModelOp(inputs=\"buffer_fake_A\", model=d_A, outputs=\"buffer_d_fake_A\"),     ModelOp(inputs=\"real_B\", model=d_B, outputs=\"d_real_B\"),     ModelOp(inputs=\"fake_B\", model=d_B, outputs=\"d_fake_B\"),     ModelOp(inputs=\"buffer_fake_B\", model=d_B, outputs=\"buffer_d_fake_B\"),     ModelOp(inputs=\"real_A\", model=g_BtoA, outputs=\"same_A\"),     ModelOp(inputs=\"fake_B\", model=g_BtoA, outputs=\"cycled_A\"),     ModelOp(inputs=\"real_B\", model=g_AtoB, outputs=\"same_B\"),     ModelOp(inputs=\"fake_A\", model=g_AtoB, outputs=\"cycled_B\"),     GLoss(inputs=(\"real_A\", \"d_fake_B\", \"cycled_A\", \"same_A\"), weight=weight, device=device, outputs=\"g_AtoB_loss\"),     GLoss(inputs=(\"real_B\", \"d_fake_A\", \"cycled_B\", \"same_B\"), weight=weight, device=device, outputs=\"g_BtoA_loss\"),     DLoss(inputs=(\"d_real_A\", \"buffer_d_fake_A\"), outputs=\"d_A_loss\", device=device),     DLoss(inputs=(\"d_real_B\", \"buffer_d_fake_B\"), outputs=\"d_B_loss\", device=device),     UpdateOp(model=g_AtoB, loss_name=\"g_AtoB_loss\"),     UpdateOp(model=g_BtoA, loss_name=\"g_BtoA_loss\"),     UpdateOp(model=d_A, loss_name=\"d_A_loss\"),     UpdateOp(model=d_B, loss_name=\"d_B_loss\") ]) <p>Here, we use a linear learning rate decay for training.</p> In\u00a0[12]: Copied! <pre>def lr_schedule(epoch):\n    if epoch&lt;=100:\n        lr = 2e-4\n    else:\n        lr = 2e-4*(200 - epoch)/100\n    return lr\n</pre> def lr_schedule(epoch):     if epoch&lt;=100:         lr = 2e-4     else:         lr = 2e-4*(200 - epoch)/100     return lr <p>In this example we will use <code>ModelSaver</code> traces to save the two generators <code>g_AtoB</code> and <code>g_BtoA</code> throughout training and <code>LRScheduler</code> traces to update the learning rate.</p> In\u00a0[13]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import ModelSaver\n\ntraces = [\n    ModelSaver(model=g_AtoB, save_dir=save_dir, frequency=10),\n    ModelSaver(model=g_BtoA, save_dir=save_dir, frequency=10),\n    LRScheduler(model=g_AtoB, lr_fn=lr_schedule),\n    LRScheduler(model=g_BtoA, lr_fn=lr_schedule),\n    LRScheduler(model=d_A, lr_fn=lr_schedule),\n    LRScheduler(model=d_B, lr_fn=lr_schedule)\n]\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import ModelSaver  traces = [     ModelSaver(model=g_AtoB, save_dir=save_dir, frequency=10),     ModelSaver(model=g_BtoA, save_dir=save_dir, frequency=10),     LRScheduler(model=g_AtoB, lr_fn=lr_schedule),     LRScheduler(model=g_BtoA, lr_fn=lr_schedule),     LRScheduler(model=d_A, lr_fn=lr_schedule),     LRScheduler(model=d_B, lr_fn=lr_schedule) ] In\u00a0[14]: Copied! <pre>estimator = fe.Estimator(network=network, \n                         pipeline=pipeline, \n                         epochs=epochs, \n                         traces=traces,\n                         log_steps=1000, \n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                           pipeline=pipeline,                           epochs=epochs,                           traces=traces,                          log_steps=1000,                           max_train_steps_per_epoch=max_train_steps_per_epoch) In\u00a0[15]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() In\u00a0[16]: Copied! <pre>idx = np.random.randint(len(test_data))\ndata = test_data[idx][0]\nresult = pipeline.transform(data, mode=\"infer\")\n</pre> idx = np.random.randint(len(test_data)) data = test_data[idx][0] result = pipeline.transform(data, mode=\"infer\") In\u00a0[17]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),\n    ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),\n])\n\npredictions = network.transform(result, mode=\"infer\")\nhorse_img = np.transpose(predictions[\"real_A\"].numpy(), (0, 2, 3, 1))\nzebra_img = np.transpose(predictions[\"real_B\"].numpy(), (0, 2, 3, 1))\nfake_zebra = np.transpose(predictions[\"fake_B\"].numpy(), (0, 2, 3, 1))\nfake_horse = np.transpose(predictions[\"fake_A\"].numpy(), (0, 2, 3, 1))\n</pre> network = fe.Network(ops=[     ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),     ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"), ])  predictions = network.transform(result, mode=\"infer\") horse_img = np.transpose(predictions[\"real_A\"].numpy(), (0, 2, 3, 1)) zebra_img = np.transpose(predictions[\"real_B\"].numpy(), (0, 2, 3, 1)) fake_zebra = np.transpose(predictions[\"fake_B\"].numpy(), (0, 2, 3, 1)) fake_horse = np.transpose(predictions[\"fake_A\"].numpy(), (0, 2, 3, 1)) In\u00a0[18]: Copied! <pre>img1 = fe.util.ImgData(Real_Horse_Image=horse_img, Translated_Zebra_Image=fake_zebra)\nfig1 = img1.paint_figure()\n\nimg2 = fe.util.ImgData(Real_Zebra_Image=zebra_img, Translated_Horse_Image=fake_horse)\nfig2 = img2.paint_figure()\n</pre> img1 = fe.util.ImgData(Real_Horse_Image=horse_img, Translated_Zebra_Image=fake_zebra) fig1 = img1.paint_figure()  img2 = fe.util.ImgData(Real_Zebra_Image=zebra_img, Translated_Horse_Image=fake_horse) fig2 = img2.paint_figure() <p>Note the addition of zebra-like stripe texture on top of horses when translating from horses to zebras. When translating zebras to horses, we can observe that generator removes the stripe texture from zebras.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#horse-to-zebra-unpaired-image-translation-with-cyclegan-in-fastestimator", "title": "Horse to Zebra Unpaired Image Translation with CycleGAN in FastEstimator\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download the dataset of horses and zebras via our dataset API. The images will be first downloaded from here. As this task requires an unpaired datasets of horse and zebra images, horse2zebra dataset is implemented using <code>BatchDataset</code> in FastEstimator. Hence, we need to specify the batch size while loading the dataset.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-1-create-pipeline", "title": "Step 1: Create pipeline\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-2-create-network", "title": "Step 2: Create Network\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#defining-loss-functions", "title": "Defining Loss functions\u00b6", "text": "<p>For each network, we need to define associated losses.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-3-estimator", "title": "Step 3: Estimator\u00b6", "text": "<p>Finally, we are ready to define <code>Estimator</code> object and then call <code>fit</code> method to start the training. Just for the sake of demo purpose, we would only run 50 epochs.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Below are infering results of the two generators.</p>"}, {"location": "apphub/image_generation/dcgan/dcgan.html", "title": "DCGAN on the MNIST Dataset", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras import layers\nfrom matplotlib import pyplot as plt\nimport fastestimator as fe\nfrom fastestimator.backend import binary_crossentropy, feed_forward\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Normalize\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import ModelSaver\n</pre> import tempfile import os import numpy as np import tensorflow as tf from tensorflow.python.keras import layers from matplotlib import pyplot as plt import fastestimator as fe from fastestimator.backend import binary_crossentropy, feed_forward from fastestimator.dataset.data import mnist from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.univariate import ExpandDims, Normalize from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import ModelSaver In\u00a0[2]: parameters Copied! <pre>batch_size = 256\nepochs = 50\nmax_train_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\nmodel_name = 'model_epoch_50.h5'\n</pre> batch_size = 256 epochs = 50 max_train_steps_per_epoch = None save_dir = tempfile.mkdtemp() model_name = 'model_epoch_50.h5' Building components <p>We are loading data from tf.keras.datasets.mnist and defining a series of operations to perform on the data before the training:</p> In\u00a0[3]: Copied! <pre>train_data, _ = mnist.load_data()\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x\"),\n        Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        LambdaOp(fn=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")\n    ])\n</pre> train_data, _ = mnist.load_data() pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x\"),         Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),         LambdaOp(fn=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")     ]) <p>First, we have to define the network architecture for both our Generator and Discriminator. After defining the architecture, users are expected to feed the architecture definition, along with associated model names and optimizers, to fe.build.</p> In\u00a0[4]: Copied! <pre>def generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((7, 7, 256)))\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    return model\n</pre> def generator():     model = tf.keras.Sequential()     model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Reshape((7, 7, 256)))     model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))     return model In\u00a0[5]: Copied! <pre>def discriminator():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n</pre> def discriminator():     model = tf.keras.Sequential()     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Flatten())     model.add(layers.Dense(1))     return model In\u00a0[6]: Copied! <pre>gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\ndisc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) disc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <p>We define the generator and discriminator losses. These can have multiple inputs and outputs.</p> In\u00a0[7]: Copied! <pre>class GLoss(TensorOp):\n\"\"\"Compute generator loss.\"\"\"\n    def forward(self, data, state):\n        return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True)\n</pre> class GLoss(TensorOp):     \"\"\"Compute generator loss.\"\"\"     def forward(self, data, state):         return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True) In\u00a0[8]: Copied! <pre>class DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def forward(self, data, state):\n        true_score, fake_score = data\n        real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)\n        fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)\n        total_loss = real_loss + fake_loss\n        return total_loss\n</pre> class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def forward(self, data, state):         true_score, fake_score = data         real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)         fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)         total_loss = real_loss + fake_loss         return total_loss <p><code>fe.Network</code> takes series of operators. Here we pass our models wrapped into <code>ModelOps</code> along with our loss functions and some update rules:</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),\n        ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),\n        GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n        UpdateOp(model=gen_model, loss_name=\"gloss\"),\n        ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),\n        DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),\n        UpdateOp(model=disc_model, loss_name=\"dloss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),         ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),         GLoss(inputs=\"fake_score\", outputs=\"gloss\"),         UpdateOp(model=gen_model, loss_name=\"gloss\"),         ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),         DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),         UpdateOp(model=disc_model, loss_name=\"dloss\")     ]) <p>We will define an <code>Estimator</code> that has four notable arguments: network, pipeline, epochs and traces. Our <code>Network</code> and <code>Pipeline</code> objects are passed here as an argument along with the number of epochs and a <code>Trace</code>, in this case one designed to save our model every 5 epochs.</p> In\u00a0[10]: Copied! <pre>traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5)\n</pre> traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5) In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch) Training In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 1e-04; model1_lr: 1e-04; \nFastEstimator-Train: step: 1; gloss: 0.7122225; dloss: 1.3922014; \nFastEstimator-Train: step: 100; gloss: 0.906471; dloss: 0.8187004; steps/sec: 10.09; \nFastEstimator-Train: step: 200; gloss: 0.59155834; dloss: 1.5896755; steps/sec: 9.93; \nFastEstimator-Train: step: 235; epoch: 1; epoch_time: 28.53 sec; \nFastEstimator-Train: step: 300; gloss: 0.7163421; dloss: 1.3333399; steps/sec: 8.9; \nFastEstimator-Train: step: 400; gloss: 0.6816584; dloss: 1.6007018; steps/sec: 9.88; \nFastEstimator-Train: step: 470; epoch: 2; epoch_time: 23.95 sec; \nFastEstimator-Train: step: 500; gloss: 0.7051203; dloss: 1.4395489; steps/sec: 9.69; \nFastEstimator-Train: step: 600; gloss: 0.75529504; dloss: 1.2358603; steps/sec: 9.86; \nFastEstimator-Train: step: 700; gloss: 0.8082159; dloss: 1.2728964; steps/sec: 9.84; \nFastEstimator-Train: step: 705; epoch: 3; epoch_time: 24.03 sec; \nFastEstimator-Train: step: 800; gloss: 0.8434949; dloss: 1.3006642; steps/sec: 9.65; \nFastEstimator-Train: step: 900; gloss: 0.84470236; dloss: 1.2344811; steps/sec: 9.79; \nFastEstimator-Train: step: 940; epoch: 4; epoch_time: 24.16 sec; \nFastEstimator-Train: step: 1000; gloss: 0.9431131; dloss: 1.0444374; steps/sec: 9.66; \nFastEstimator-Train: step: 1100; gloss: 0.6982814; dloss: 1.5213135; steps/sec: 9.77; \nSaved model to /tmp/tmpspul4xo8/model_epoch_5.h5\nFastEstimator-Train: step: 1175; epoch: 5; epoch_time: 24.17 sec; \nFastEstimator-Train: step: 1200; gloss: 1.2540445; dloss: 0.8278078; steps/sec: 9.63; \nFastEstimator-Train: step: 1300; gloss: 0.70465124; dloss: 1.7595482; steps/sec: 9.76; \nFastEstimator-Train: step: 1400; gloss: 0.83103234; dloss: 1.168882; steps/sec: 9.77; \nFastEstimator-Train: step: 1410; epoch: 6; epoch_time: 24.22 sec; \nFastEstimator-Train: step: 1500; gloss: 0.86833733; dloss: 1.2078841; steps/sec: 9.57; \nFastEstimator-Train: step: 1600; gloss: 0.82795817; dloss: 1.2242851; steps/sec: 9.75; \nFastEstimator-Train: step: 1645; epoch: 7; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 1700; gloss: 0.9743507; dloss: 1.0731742; steps/sec: 9.59; \nFastEstimator-Train: step: 1800; gloss: 0.89325964; dloss: 1.1766281; steps/sec: 9.76; \nFastEstimator-Train: step: 1880; epoch: 8; epoch_time: 24.25 sec; \nFastEstimator-Train: step: 1900; gloss: 1.0287898; dloss: 0.9916363; steps/sec: 9.6; \nFastEstimator-Train: step: 2000; gloss: 0.8240694; dloss: 1.313368; steps/sec: 9.74; \nFastEstimator-Train: step: 2100; gloss: 0.9738071; dloss: 1.1259043; steps/sec: 9.73; \nFastEstimator-Train: step: 2115; epoch: 9; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 2200; gloss: 1.0899432; dloss: 1.0272337; steps/sec: 9.57; \nFastEstimator-Train: step: 2300; gloss: 0.868231; dloss: 1.2400149; steps/sec: 9.72; \nSaved model to /tmp/tmpspul4xo8/model_epoch_10.h5\nFastEstimator-Train: step: 2350; epoch: 10; epoch_time: 24.35 sec; \nFastEstimator-Train: step: 2400; gloss: 0.9001913; dloss: 1.1931081; steps/sec: 9.58; \nFastEstimator-Train: step: 2500; gloss: 1.0865673; dloss: 0.8990781; steps/sec: 9.71; \nFastEstimator-Train: step: 2585; epoch: 11; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 2600; gloss: 0.7485407; dloss: 1.672249; steps/sec: 9.58; \nFastEstimator-Train: step: 2700; gloss: 1.045316; dloss: 1.0383615; steps/sec: 9.73; \nFastEstimator-Train: step: 2800; gloss: 0.7666995; dloss: 1.4343789; steps/sec: 9.72; \nFastEstimator-Train: step: 2820; epoch: 12; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 2900; gloss: 1.1756387; dloss: 0.96622103; steps/sec: 9.6; \nFastEstimator-Train: step: 3000; gloss: 0.9090629; dloss: 1.1984154; steps/sec: 9.72; \nFastEstimator-Train: step: 3055; epoch: 13; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 3100; gloss: 0.9301505; dloss: 1.113826; steps/sec: 9.59; \nFastEstimator-Train: step: 3200; gloss: 0.99965835; dloss: 1.0707076; steps/sec: 9.74; \nFastEstimator-Train: step: 3290; epoch: 14; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 3300; gloss: 0.80838567; dloss: 1.6384692; steps/sec: 9.55; \nFastEstimator-Train: step: 3400; gloss: 0.8714433; dloss: 1.326818; steps/sec: 9.74; \nFastEstimator-Train: step: 3500; gloss: 0.9549879; dloss: 1.2086997; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_15.h5\nFastEstimator-Train: step: 3525; epoch: 15; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 3600; gloss: 1.0164418; dloss: 1.0690243; steps/sec: 9.57; \nFastEstimator-Train: step: 3700; gloss: 1.0357686; dloss: 1.0537144; steps/sec: 9.72; \nFastEstimator-Train: step: 3760; epoch: 16; epoch_time: 24.32 sec; \nFastEstimator-Train: step: 3800; gloss: 0.7402923; dloss: 1.4840925; steps/sec: 9.6; \nFastEstimator-Train: step: 3900; gloss: 0.91192436; dloss: 1.3617609; steps/sec: 9.72; \nFastEstimator-Train: step: 3995; epoch: 17; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 4000; gloss: 1.2626994; dloss: 0.9568275; steps/sec: 9.59; \nFastEstimator-Train: step: 4100; gloss: 0.97824305; dloss: 1.300906; steps/sec: 9.74; \nFastEstimator-Train: step: 4200; gloss: 0.93075603; dloss: 1.387594; steps/sec: 9.73; \nFastEstimator-Train: step: 4230; epoch: 18; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 4300; gloss: 1.0180345; dloss: 1.0898602; steps/sec: 9.59; \nFastEstimator-Train: step: 4400; gloss: 1.051662; dloss: 1.3392837; steps/sec: 9.71; \nFastEstimator-Train: step: 4465; epoch: 19; epoch_time: 24.33 sec; \nFastEstimator-Train: step: 4500; gloss: 1.0151768; dloss: 1.1482071; steps/sec: 9.56; \nFastEstimator-Train: step: 4600; gloss: 1.107022; dloss: 0.96815336; steps/sec: 9.71; \nFastEstimator-Train: step: 4700; gloss: 1.0924942; dloss: 1.1389236; steps/sec: 9.72; \nSaved model to /tmp/tmpspul4xo8/model_epoch_20.h5\nFastEstimator-Train: step: 4700; epoch: 20; epoch_time: 24.39 sec; \nFastEstimator-Train: step: 4800; gloss: 1.1345683; dloss: 1.2068424; steps/sec: 9.54; \nFastEstimator-Train: step: 4900; gloss: 1.142304; dloss: 0.9673606; steps/sec: 9.74; \nFastEstimator-Train: step: 4935; epoch: 21; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 5000; gloss: 0.9886; dloss: 1.0960109; steps/sec: 9.57; \nFastEstimator-Train: step: 5100; gloss: 0.8936993; dloss: 1.2922779; steps/sec: 9.74; \nFastEstimator-Train: step: 5170; epoch: 22; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 5200; gloss: 1.1095095; dloss: 1.1243165; steps/sec: 9.57; \nFastEstimator-Train: step: 5300; gloss: 1.2485275; dloss: 0.89292765; steps/sec: 9.74; \nFastEstimator-Train: step: 5400; gloss: 1.0476826; dloss: 1.2994311; steps/sec: 9.75; \nFastEstimator-Train: step: 5405; epoch: 23; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 5500; gloss: 1.3308836; dloss: 0.871735; steps/sec: 9.63; \nFastEstimator-Train: step: 5600; gloss: 1.115385; dloss: 1.2837725; steps/sec: 9.74; \nFastEstimator-Train: step: 5640; epoch: 24; epoch_time: 24.23 sec; \nFastEstimator-Train: step: 5700; gloss: 1.1920481; dloss: 1.0993654; steps/sec: 9.62; \nFastEstimator-Train: step: 5800; gloss: 1.3005233; dloss: 0.914361; steps/sec: 9.74; \nSaved model to /tmp/tmpspul4xo8/model_epoch_25.h5\nFastEstimator-Train: step: 5875; epoch: 25; epoch_time: 24.27 sec; \nFastEstimator-Train: step: 5900; gloss: 1.3146336; dloss: 0.8816396; steps/sec: 9.6; \nFastEstimator-Train: step: 6000; gloss: 0.9764897; dloss: 1.289681; steps/sec: 9.75; \nFastEstimator-Train: step: 6100; gloss: 1.1467731; dloss: 1.1918977; steps/sec: 9.75; \nFastEstimator-Train: step: 6110; epoch: 26; epoch_time: 24.26 sec; \nFastEstimator-Train: step: 6200; gloss: 1.6301311; dloss: 0.9541445; steps/sec: 9.6; \nFastEstimator-Train: step: 6300; gloss: 1.2840165; dloss: 0.9587291; steps/sec: 9.73; \nFastEstimator-Train: step: 6345; epoch: 27; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 6400; gloss: 1.1097628; dloss: 1.0090048; steps/sec: 9.59; \nFastEstimator-Train: step: 6500; gloss: 1.2495477; dloss: 0.89897555; steps/sec: 9.73; \nFastEstimator-Train: step: 6580; epoch: 28; epoch_time: 24.32 sec; \nFastEstimator-Train: step: 6600; gloss: 1.1773547; dloss: 1.1330662; steps/sec: 9.58; \nFastEstimator-Train: step: 6700; gloss: 1.246088; dloss: 0.8964198; steps/sec: 9.73; \nFastEstimator-Train: step: 6800; gloss: 1.2250234; dloss: 1.0358574; steps/sec: 9.73; \nFastEstimator-Train: step: 6815; epoch: 29; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 6900; gloss: 1.1256618; dloss: 1.196687; steps/sec: 9.59; \nFastEstimator-Train: step: 7000; gloss: 1.1131527; dloss: 1.0596428; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_30.h5\nFastEstimator-Train: step: 7050; epoch: 30; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 7100; gloss: 1.1662202; dloss: 1.0555116; steps/sec: 9.57; \nFastEstimator-Train: step: 7200; gloss: 1.0653521; dloss: 1.1444951; steps/sec: 9.71; \nFastEstimator-Train: step: 7285; epoch: 31; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 7300; gloss: 1.1732882; dloss: 1.1456137; steps/sec: 9.56; \nFastEstimator-Train: step: 7400; gloss: 1.0872216; dloss: 1.128233; steps/sec: 9.74; \nFastEstimator-Train: step: 7500; gloss: 1.2431256; dloss: 1.1538315; steps/sec: 9.73; \nFastEstimator-Train: step: 7520; epoch: 32; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 7600; gloss: 1.0806718; dloss: 1.2206206; steps/sec: 9.57; \nFastEstimator-Train: step: 7700; gloss: 1.1804712; dloss: 1.1420157; steps/sec: 9.71; \nFastEstimator-Train: step: 7755; epoch: 33; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 7800; gloss: 1.1762993; dloss: 1.0413929; steps/sec: 9.54; \nFastEstimator-Train: step: 7900; gloss: 1.2267275; dloss: 0.98290396; steps/sec: 9.71; \nFastEstimator-Train: step: 7990; epoch: 34; epoch_time: 24.37 sec; \nFastEstimator-Train: step: 8000; gloss: 1.1847881; dloss: 1.0905983; steps/sec: 9.55; \nFastEstimator-Train: step: 8100; gloss: 1.1490288; dloss: 1.1739209; steps/sec: 9.72; \nFastEstimator-Train: step: 8200; gloss: 1.0283768; dloss: 1.059457; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_35.h5\nFastEstimator-Train: step: 8225; epoch: 35; epoch_time: 24.33 sec; \nFastEstimator-Train: step: 8300; gloss: 1.2351133; dloss: 1.1085691; steps/sec: 9.59; \nFastEstimator-Train: step: 8400; gloss: 1.1488228; dloss: 1.1410246; steps/sec: 9.74; \nFastEstimator-Train: step: 8460; epoch: 36; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 8500; gloss: 1.152446; dloss: 1.1371456; steps/sec: 9.55; \nFastEstimator-Train: step: 8600; gloss: 1.2175394; dloss: 1.1543391; steps/sec: 9.74; \nFastEstimator-Train: step: 8695; epoch: 37; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 8700; gloss: 1.1803217; dloss: 1.1517241; steps/sec: 9.59; \nFastEstimator-Train: step: 8800; gloss: 0.9561673; dloss: 1.2418871; steps/sec: 9.77; \nFastEstimator-Train: step: 8900; gloss: 1.0239995; dloss: 1.243228; steps/sec: 9.74; \nFastEstimator-Train: step: 8930; epoch: 38; epoch_time: 24.26 sec; \nFastEstimator-Train: step: 9000; gloss: 0.98074543; dloss: 1.2558163; steps/sec: 9.59; \nFastEstimator-Train: step: 9100; gloss: 1.0084043; dloss: 1.2773612; steps/sec: 9.74; \nFastEstimator-Train: step: 9165; epoch: 39; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 9200; gloss: 1.0313301; dloss: 1.2312038; steps/sec: 9.58; \nFastEstimator-Train: step: 9300; gloss: 1.0100834; dloss: 1.2482088; steps/sec: 9.73; \nFastEstimator-Train: step: 9400; gloss: 0.9327201; dloss: 1.3188391; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_40.h5\nFastEstimator-Train: step: 9400; epoch: 40; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 9500; gloss: 1.1315899; dloss: 1.1447232; steps/sec: 9.55; \nFastEstimator-Train: step: 9600; gloss: 1.1352619; dloss: 1.0802212; steps/sec: 9.72; \nFastEstimator-Train: step: 9635; epoch: 41; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 9700; gloss: 1.01453; dloss: 1.0975459; steps/sec: 9.59; \nFastEstimator-Train: step: 9800; gloss: 0.90930146; dloss: 1.2942967; steps/sec: 9.76; \nFastEstimator-Train: step: 9870; epoch: 42; epoch_time: 24.27 sec; \nFastEstimator-Train: step: 9900; gloss: 1.0540565; dloss: 1.170531; steps/sec: 9.58; \nFastEstimator-Train: step: 10000; gloss: 1.061863; dloss: 1.2722391; steps/sec: 9.76; \nFastEstimator-Train: step: 10100; gloss: 0.9647354; dloss: 1.1689386; steps/sec: 9.73; \nFastEstimator-Train: step: 10105; epoch: 43; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 10200; gloss: 1.2080085; dloss: 1.075758; steps/sec: 9.57; \nFastEstimator-Train: step: 10300; gloss: 1.0741084; dloss: 1.1352613; steps/sec: 9.72; \nFastEstimator-Train: step: 10340; epoch: 44; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 10400; gloss: 1.1394867; dloss: 0.9788251; steps/sec: 9.58; \nFastEstimator-Train: step: 10500; gloss: 1.1983887; dloss: 1.0792823; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_45.h5\nFastEstimator-Train: step: 10575; epoch: 45; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 10600; gloss: 0.96989757; dloss: 1.2448618; steps/sec: 9.59; \nFastEstimator-Train: step: 10700; gloss: 0.9579427; dloss: 1.2773428; steps/sec: 9.7; \nFastEstimator-Train: step: 10800; gloss: 0.97453225; dloss: 1.2194138; steps/sec: 9.72; \nFastEstimator-Train: step: 10810; epoch: 46; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 10900; gloss: 1.0218571; dloss: 1.1765122; steps/sec: 9.57; \nFastEstimator-Train: step: 11000; gloss: 1.1221988; dloss: 1.1675267; steps/sec: 9.71; \nFastEstimator-Train: step: 11045; epoch: 47; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 11100; gloss: 0.900293; dloss: 1.1741953; steps/sec: 9.57; \nFastEstimator-Train: step: 11200; gloss: 1.1080045; dloss: 1.1090837; steps/sec: 9.73; \nFastEstimator-Train: step: 11280; epoch: 48; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 11300; gloss: 1.1028197; dloss: 1.1044464; steps/sec: 9.59; \nFastEstimator-Train: step: 11400; gloss: 1.0530615; dloss: 1.2150866; steps/sec: 9.74; \nFastEstimator-Train: step: 11500; gloss: 0.8997061; dloss: 1.3101699; steps/sec: 9.75; \nFastEstimator-Train: step: 11515; epoch: 49; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 11600; gloss: 1.0087067; dloss: 1.3297874; steps/sec: 9.58; \nFastEstimator-Train: step: 11700; gloss: 1.0053492; dloss: 1.2499433; steps/sec: 9.74; \nSaved model to /tmp/tmpspul4xo8/model_epoch_50.h5\nFastEstimator-Train: step: 11750; epoch: 50; epoch_time: 24.3 sec; \nFastEstimator-Finish: step: 11750; total_time: 1219.47 sec; model_lr: 1e-04; model1_lr: 1e-04; \n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We will load the trained generator weights using fe.build</p> In\u00a0[19]: Copied! <pre>model_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <pre>Loaded model weights from /tmp/tmpspul4xo8/model_epoch_50.h5\n</pre> <p>We will the generate some images from random noise:</p> In\u00a0[20]: Copied! <pre>images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False)\n</pre> images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False) <pre>WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n</pre> In\u00a0[21]: Copied! <pre>fig, axes = plt.subplots(4, 4)\naxes = np.ravel(axes)\nfor i in range(images.shape[0]):\n    axes[i].axis('off')\n    axes[i].imshow(np.squeeze(images[i, ...] * 127.5 + 127.5), cmap='gray')\n</pre> fig, axes = plt.subplots(4, 4) axes = np.ravel(axes) for i in range(images.shape[0]):     axes[i].axis('off')     axes[i].imshow(np.squeeze(images[i, ...] * 127.5 + 127.5), cmap='gray')"}, {"location": "apphub/image_generation/dcgan/dcgan.html#dcgan-on-the-mnist-dataset", "title": "DCGAN on the MNIST Dataset\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-1-prepare-training-and-define-a-pipeline", "title": "Step 1: Prepare training and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html", "title": "Progressive Growing GAN (PGGAN)", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nimport fastestimator as fe\nfrom fastestimator.schedule import EpochScheduler\nfrom fastestimator.util import get_num_devices\n</pre> import os import tempfile  import cv2 import numpy as np import torch import matplotlib.pyplot as plt  import fastestimator as fe from fastestimator.schedule import EpochScheduler from fastestimator.util import get_num_devices In\u00a0[2]: parameters Copied! <pre>target_size=128\nepochs=55\nsave_dir=tempfile.mkdtemp()\nmax_train_steps_per_epoch=None\ndata_dir=None\n</pre> target_size=128 epochs=55 save_dir=tempfile.mkdtemp() max_train_steps_per_epoch=None data_dir=None In\u00a0[3]: Copied! <pre>num_grow = np.log2(target_size) - 2\nassert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\"\nnum_phases = int(2 * num_grow + 1)\nassert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size)\nnum_grow, phase_length = int(num_grow), int(epochs / num_phases)\nevent_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)]\nevent_size = [4] + [2**(i + 3) for i in range(num_grow)]\n</pre> num_grow = np.log2(target_size) - 2 assert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\" num_phases = int(2 * num_grow + 1) assert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size) num_grow, phase_length = int(num_grow), int(epochs / num_phases) event_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)] event_size = [4] + [2**(i + 3) for i in range(num_grow)] In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import nih_chestxray\n\ndataset = nih_chestxray.load_data(root_dir=data_dir)\n</pre> from fastestimator.dataset.data import nih_chestxray  dataset = nih_chestxray.load_data(root_dir=data_dir) In\u00a0[5]: Copied! <pre>from fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\nresize_map = {\n    epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map1 = {\n    epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map2 = {\n    epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_size_map = {\n    epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_scheduler = EpochScheduler(epoch_dict=batch_size_map)\npipeline = fe.Pipeline(\n    batch_size=batch_scheduler,\n    train_data=dataset,\n    drop_last=True,\n    ops=[\n        ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),\n        EpochScheduler(epoch_dict=resize_map),\n        EpochScheduler(epoch_dict=resize_low_res_map1),\n        EpochScheduler(epoch_dict=resize_low_res_map2),\n        Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n        ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),\n        LambdaOp(fn=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\")\n    ])\n</pre> from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  resize_map = {     epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map1 = {     epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map2 = {     epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } batch_size_map = {     epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()     for (epoch, size) in zip(event_epoch, event_size) } batch_scheduler = EpochScheduler(epoch_dict=batch_size_map) pipeline = fe.Pipeline(     batch_size=batch_scheduler,     train_data=dataset,     drop_last=True,     ops=[         ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),         EpochScheduler(epoch_dict=resize_map),         EpochScheduler(epoch_dict=resize_low_res_map1),         EpochScheduler(epoch_dict=resize_low_res_map2),         Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),         ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),         LambdaOp(fn=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\")     ]) <p>Let's visualize how our <code>Pipeline</code> changes image resolution at the different epochs we specified using <code>Schedulers</code>. FastEstimator as a <code>get_results</code> method to aid in this. In order to correctly visualize the output of the <code>Pipeline</code>, we need to provide epoch numbers to the <code>get_results</code> method:</p> In\u00a0[6]: Copied! <pre>plt.figure(figsize=(50,50))\nfor i, epoch in enumerate(event_epoch):\n    batch_data = pipeline.get_results(epoch=epoch)\n    img = np.squeeze(batch_data[\"x\"][0] + 0.5)\n    plt.subplot(1, 9, i+1)\n    plt.imshow(img, cmap='gray')\n</pre> plt.figure(figsize=(50,50)) for i, epoch in enumerate(event_epoch):     batch_data = pipeline.get_results(epoch=epoch)     img = np.squeeze(batch_data[\"x\"][0] + 0.5)     plt.subplot(1, 9, i+1)     plt.imshow(img, cmap='gray')  In\u00a0[7]: Copied! <pre>from torch.optim import Adam\n\ndef _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):\n    return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)\n\n\nclass EqualizedLRDense(torch.nn.Linear):\n    def __init__(self, in_features, out_features, gain=np.sqrt(2)):\n        super().__init__(in_features, out_features, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        self.wscale = np.float32(gain / np.sqrt(in_features))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\nclass ApplyBias(torch.nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.bias = torch.nn.Parameter(torch.Tensor(in_features))\n        torch.nn.init.constant_(self.bias.data, val=0.0)\n\n    def forward(self, x):\n        if len(x.shape) == 4:\n            x = x + self.bias.view(1, -1, 1, 1).expand_as(x)\n        else:\n            x = x + self.bias\n        return x\n\n\nclass EqualizedLRConv2D(torch.nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):\n        super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        fan_in = np.float32(np.prod(self.weight.data.shape[1:]))\n        self.wscale = np.float32(gain / np.sqrt(fan_in))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\ndef pixel_normalization(x, eps=1e-8):\n    return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)\n\n\ndef mini_batch_std(x, group_size=4, eps=1e-8):\n    b, c, h, w = x.shape\n    group_size = min(group_size, b)\n    y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]\n    y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]\n    y = torch.mean(y**2, axis=0)  # [M, C, H, W]\n    y = torch.sqrt(y + eps)  # [M, C, H, W]\n    y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]\n    y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]\n    return torch.cat((x, y), 1)\n\n\ndef fade_in(x, y, alpha):\n    return (1.0 - alpha) * x + alpha * y\n\n\nclass ToRGB(torch.nn.Module):\n    def __init__(self, in_channels, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)\n        self.bias = ApplyBias(in_features=num_channels)\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        return x\n\n\nclass FromRGB(torch.nn.Module):\n    def __init__(self, res, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)\n        self.bias = ApplyBias(in_features=_nf(res - 1))\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        return x\n\n\nclass BlockG1D(torch.nn.Module):\n    def __init__(self, res=2, latent_dim=512):\n        super().__init__()\n        self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512]\n        x = pixel_normalization(x)  # [batch, 512]\n        x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]\n        x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)\n        return x\n\n\nclass BlockG2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]\n        x = self.upsample(x)\n        x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        return x\n\n\ndef _block_G(res, latent_dim=512, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockG1D(res=res, latent_dim=latent_dim)\n    else:\n        model = BlockG2D(res=res)\n    return model\n\n\nclass Gen(torch.nn.Module):\n    def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.g_blocks = torch.nn.ModuleList(g_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        for g in self.g_blocks[:-1]:\n            x = g(x)\n        previous_img = self.rgb_blocks[0](x)\n        previous_img = self.upsample(previous_img)\n        x = self.g_blocks[-1](x)\n        new_img = self.rgb_blocks[1](x)\n        return fade_in(previous_img, new_img, self.fade_in_alpha)\n\n\ndef build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):\n    g_blocks = [\n        _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)\n    ]\n    rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]\n    for idx in range(2, len(g_blocks) + 1):\n        generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    final_model_list = g_blocks + [rgb_blocks[-1]]\n    generators.append(torch.nn.Sequential(*final_model_list))\n    return generators\n\n\nclass BlockD1D(torch.nn.Module):\n    def __init__(self, res=2):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)\n        self.bias3 = ApplyBias(in_features=1)\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512, 4, 4]\n        x = mini_batch_std(x)  # [batch, 513, 4, 4]\n        x = self.elr_conv2d(x)  # [batch, 512, 4, 4]\n        x = self.bias1(x)  # [batch, 512, 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]\n        x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]\n        x = self.elr_dense1(x)  # [batch, 512]\n        x = self.bias2(x)  # [batch, 512]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]\n        x = self.elr_dense2(x)  # [batch, 1]\n        x = self.bias3(x)  # [batch, 1]\n        return x\n\n\nclass BlockD2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.elr_conv2d1(x)\n        x = self.bias1(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.elr_conv2d2(x)\n        x = self.bias2(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.pool(x)\n        return x\n\n\ndef _block_D(res, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockD1D(res)\n    else:\n        model = BlockD2D(res)\n    return model\n\n\nclass Disc(torch.nn.Module):\n    def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.d_blocks = torch.nn.ModuleList(d_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        new_x = self.rgb_blocks[1](x)\n        new_x = self.d_blocks[-1](new_x)\n        downscale_x = self.pool(x)\n        downscale_x = self.rgb_blocks[0](downscale_x)\n        x = fade_in(downscale_x, new_x, self.fade_in_alpha)\n        for d in self.d_blocks[:-1][::-1]:\n            x = d(x)\n        return x\n\n\ndef build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):\n    d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]\n    rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]\n    for idx in range(2, len(d_blocks) + 1):\n        discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    return discriminators\n\n\n\nfade_in_alpha = torch.tensor(1.0)\nd_models = fe.build(\n    model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),\n    model_name=[\"d_{}\".format(size) for size in event_size])\n\ng_models = fe.build(\n    model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],\n    model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"])\n</pre> from torch.optim import Adam  def _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):     return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)   class EqualizedLRDense(torch.nn.Linear):     def __init__(self, in_features, out_features, gain=np.sqrt(2)):         super().__init__(in_features, out_features, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         self.wscale = np.float32(gain / np.sqrt(in_features))      def forward(self, x):         return super().forward(x) * self.wscale   class ApplyBias(torch.nn.Module):     def __init__(self, in_features):         super().__init__()         self.in_features = in_features         self.bias = torch.nn.Parameter(torch.Tensor(in_features))         torch.nn.init.constant_(self.bias.data, val=0.0)      def forward(self, x):         if len(x.shape) == 4:             x = x + self.bias.view(1, -1, 1, 1).expand_as(x)         else:             x = x + self.bias         return x   class EqualizedLRConv2D(torch.nn.Conv2d):     def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):         super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         fan_in = np.float32(np.prod(self.weight.data.shape[1:]))         self.wscale = np.float32(gain / np.sqrt(fan_in))      def forward(self, x):         return super().forward(x) * self.wscale   def pixel_normalization(x, eps=1e-8):     return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)   def mini_batch_std(x, group_size=4, eps=1e-8):     b, c, h, w = x.shape     group_size = min(group_size, b)     y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]     y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]     y = torch.mean(y**2, axis=0)  # [M, C, H, W]     y = torch.sqrt(y + eps)  # [M, C, H, W]     y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]     y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]     return torch.cat((x, y), 1)   def fade_in(x, y, alpha):     return (1.0 - alpha) * x + alpha * y   class ToRGB(torch.nn.Module):     def __init__(self, in_channels, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)         self.bias = ApplyBias(in_features=num_channels)      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         return x   class FromRGB(torch.nn.Module):     def __init__(self, res, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)         self.bias = ApplyBias(in_features=_nf(res - 1))      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         return x   class BlockG1D(torch.nn.Module):     def __init__(self, res=2, latent_dim=512):         super().__init__()         self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.res = res      def forward(self, x):         # x: [batch, 512]         x = pixel_normalization(x)  # [batch, 512]         x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]         x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]         x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]         x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]         x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)         return x   class BlockG2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]         x = self.upsample(x)         x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         return x   def _block_G(res, latent_dim=512, initial_resolution=2):     if res == initial_resolution:         model = BlockG1D(res=res, latent_dim=latent_dim)     else:         model = BlockG2D(res=res)     return model   class Gen(torch.nn.Module):     def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.g_blocks = torch.nn.ModuleList(g_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         for g in self.g_blocks[:-1]:             x = g(x)         previous_img = self.rgb_blocks[0](x)         previous_img = self.upsample(previous_img)         x = self.g_blocks[-1](x)         new_img = self.rgb_blocks[1](x)         return fade_in(previous_img, new_img, self.fade_in_alpha)   def build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):     g_blocks = [         _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)     ]     rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]     generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]     for idx in range(2, len(g_blocks) + 1):         generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     final_model_list = g_blocks + [rgb_blocks[-1]]     generators.append(torch.nn.Sequential(*final_model_list))     return generators   class BlockD1D(torch.nn.Module):     def __init__(self, res=2):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)         self.bias3 = ApplyBias(in_features=1)         self.res = res      def forward(self, x):         # x: [batch, 512, 4, 4]         x = mini_batch_std(x)  # [batch, 513, 4, 4]         x = self.elr_conv2d(x)  # [batch, 512, 4, 4]         x = self.bias1(x)  # [batch, 512, 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]         x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]         x = self.elr_dense1(x)  # [batch, 512]         x = self.bias2(x)  # [batch, 512]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]         x = self.elr_dense2(x)  # [batch, 1]         x = self.bias3(x)  # [batch, 1]         return x   class BlockD2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         x = self.elr_conv2d1(x)         x = self.bias1(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.elr_conv2d2(x)         x = self.bias2(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.pool(x)         return x   def _block_D(res, initial_resolution=2):     if res == initial_resolution:         model = BlockD1D(res)     else:         model = BlockD2D(res)     return model   class Disc(torch.nn.Module):     def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.d_blocks = torch.nn.ModuleList(d_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         new_x = self.rgb_blocks[1](x)         new_x = self.d_blocks[-1](new_x)         downscale_x = self.pool(x)         downscale_x = self.rgb_blocks[0](downscale_x)         x = fade_in(downscale_x, new_x, self.fade_in_alpha)         for d in self.d_blocks[:-1][::-1]:             x = d(x)         return x   def build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):     d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]     rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]     discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]     for idx in range(2, len(d_blocks) + 1):         discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     return discriminators    fade_in_alpha = torch.tensor(1.0) d_models = fe.build(     model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),     model_name=[\"d_{}\".format(size) for size in event_size])  g_models = fe.build(     model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],     model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"]) In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.backend import feed_forward, get_gradient\n\nclass ImageBlender(TensorOp):\n    def __init__(self, alpha, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.alpha = alpha\n\n    def forward(self, data, state):\n        image, image_lowres = data\n        new_img = self.alpha * image + (1 - self.alpha) * image_lowres\n        return new_img\n\n\nclass Interpolate(TensorOp):\n    def forward(self, data, state):\n        fake, real = data\n        batch_size = real.shape[0]\n        coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)\n        return real + (fake - real) * coeff\n\n\nclass GradientPenalty(TensorOp):\n    def __init__(self, inputs, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n    def forward(self, data, state):\n        x_interp, interp_score = data\n        gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)\n        grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))\n        gp = (grad_l2 - 1.0)**2\n        return gp\n\n\nclass GLoss(TensorOp):\n    def forward(self, data, state):\n        return -torch.mean(data)\n\n\nclass DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.wgan_lambda = wgan_lambda\n        self.wgan_epsilon = wgan_epsilon\n\n    def forward(self, data, state):\n        real_score, fake_score, gp = data\n        loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon\n        return torch.mean(loss)\n\n\nfake_img_map = {\n    epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nfake_score_map = {\n    epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\nreal_score_map = {\n    epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ninterp_score_map = {\n    epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ng_update_map = {\n    epoch: UpdateOp(loss_name=\"gloss\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nd_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)}\nnetwork = fe.Network(ops=[\n    EpochScheduler(fake_img_map),\n    EpochScheduler(fake_score_map),\n    ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),\n    EpochScheduler(real_score_map),\n    Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),\n    EpochScheduler(interp_score_map),\n    GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),\n    GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n    DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),\n    EpochScheduler(g_update_map),\n    EpochScheduler(d_update_map)\n])\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.backend import feed_forward, get_gradient  class ImageBlender(TensorOp):     def __init__(self, alpha, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.alpha = alpha      def forward(self, data, state):         image, image_lowres = data         new_img = self.alpha * image + (1 - self.alpha) * image_lowres         return new_img   class Interpolate(TensorOp):     def forward(self, data, state):         fake, real = data         batch_size = real.shape[0]         coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)         return real + (fake - real) * coeff   class GradientPenalty(TensorOp):     def __init__(self, inputs, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)      def forward(self, data, state):         x_interp, interp_score = data         gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)         grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))         gp = (grad_l2 - 1.0)**2         return gp   class GLoss(TensorOp):     def forward(self, data, state):         return -torch.mean(data)   class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.wgan_lambda = wgan_lambda         self.wgan_epsilon = wgan_epsilon      def forward(self, data, state):         real_score, fake_score, gp = data         loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon         return torch.mean(loss)   fake_img_map = {     epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } fake_score_map = {     epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } real_score_map = {     epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } interp_score_map = {     epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } g_update_map = {     epoch: UpdateOp(loss_name=\"gloss\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } d_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)} network = fe.Network(ops=[     EpochScheduler(fake_img_map),     EpochScheduler(fake_score_map),     ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),     EpochScheduler(real_score_map),     Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),     EpochScheduler(interp_score_map),     GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),     GLoss(inputs=\"fake_score\", outputs=\"gloss\"),     DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),     EpochScheduler(g_update_map),     EpochScheduler(d_update_map) ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace import Trace\nfrom fastestimator.trace.io import ModelSaver\n\nclass AlphaController(Trace):\n    def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):\n        super().__init__(inputs=None, outputs=None, mode=\"train\")\n        self.alpha = alpha\n        self.fade_start_epochs = fade_start_epochs\n        self.duration = duration\n        self.batch_scheduler = batch_scheduler\n        self.num_examples = num_examples\n        self.change_alpha = False\n        self.nimg_total = self.duration * self.num_examples\n        self._idx = 0\n        self.nimg_so_far = 0\n        self.current_batch_size = None\n\n    def on_epoch_begin(self, state):\n        # check whetehr the current epoch is in smooth transition of resolutions\n        fade_epoch = self.fade_start_epochs[self._idx]\n        if self.system.epoch_idx == fade_epoch:\n            self.change_alpha = True\n            self.nimg_so_far = 0\n            self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)\n            print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))\n        elif self.system.epoch_idx == fade_epoch + self.duration:\n            print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))\n            self.change_alpha = False\n            if self._idx + 1 &lt; len(self.fade_start_epochs):\n                self._idx += 1\n            self.alpha.data = torch.tensor(1.0)\n\n    def on_batch_begin(self, state):\n        # if in resolution transition, smoothly change the alpha from 0 to 1\n        if self.change_alpha:\n            self.nimg_so_far += self.current_batch_size\n            self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)\n            \ntraces = [\n    AlphaController(alpha=fade_in_alpha,\n                    fade_start_epochs=event_epoch[1:],\n                    duration=phase_length,\n                    batch_scheduler=batch_scheduler,\n                    num_examples=len(dataset)),\n    ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]\n            \n            \nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> from fastestimator.trace import Trace from fastestimator.trace.io import ModelSaver  class AlphaController(Trace):     def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):         super().__init__(inputs=None, outputs=None, mode=\"train\")         self.alpha = alpha         self.fade_start_epochs = fade_start_epochs         self.duration = duration         self.batch_scheduler = batch_scheduler         self.num_examples = num_examples         self.change_alpha = False         self.nimg_total = self.duration * self.num_examples         self._idx = 0         self.nimg_so_far = 0         self.current_batch_size = None      def on_epoch_begin(self, state):         # check whetehr the current epoch is in smooth transition of resolutions         fade_epoch = self.fade_start_epochs[self._idx]         if self.system.epoch_idx == fade_epoch:             self.change_alpha = True             self.nimg_so_far = 0             self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)             print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))         elif self.system.epoch_idx == fade_epoch + self.duration:             print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))             self.change_alpha = False             if self._idx + 1 &lt; len(self.fade_start_epochs):                 self._idx += 1             self.alpha.data = torch.tensor(1.0)      def on_batch_begin(self, state):         # if in resolution transition, smoothly change the alpha from 0 to 1         if self.change_alpha:             self.nimg_so_far += self.current_batch_size             self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)              traces = [     AlphaController(alpha=fade_in_alpha,                     fade_start_epochs=event_epoch[1:],                     duration=phase_length,                     batch_scheduler=batch_scheduler,                     num_examples=len(dataset)),     ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]                           estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit()"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-gan-pggan", "title": "Progressive Growing GAN (PGGAN)\u00b6", "text": "<p>In this notebook, we will demonstrate the functionality of <code>Scheduler</code> which enables advanced training schemes such as the progressive training method described in Karras et al.. We will train a PGGAN to produce synthetic frontal chest X-ray images where both the generator and the discriminator grow from $4\\times4$ to $128\\times128$.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-strategy", "title": "Progressive Growing Strategy\u00b6", "text": "<p>Karras et al. propose a training scheme in which both the generator and the discriminator progressively grow from a low resolution to a high resolution. Both networks begin their training based on $4\\times4$ images as illustrated below.  Then, both networks progress from $4\\times4$ to $8\\times8$ by an adding an additional block that contains a couple of convolutional layers.  Both the generator and the discriminator progressively grow until reaching the desired resolution of $1024\\times 1024$.  Image Credit: Presentation slide</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#smooth-transition-between-resolutions", "title": "Smooth Transition between Resolutions\u00b6", "text": "<p>However, when growing the networks, the new blocks must be slowly faded into the networks in order to smoothly transition between different resolutions. For example, when growing the generator from $16\\times16$ to $32\\times32$, the newly added block of $32\\times32$ is slowly faded into the already well trained $16\\times16$ network by linearly increasing a fade-factor $\\alpha$ from $0$ to $1$. Once the network is fully transitioned to $32\\times32$, the network is trained a bit further to stabilize before growing to $64\\times64$.  Image Credit: PGGAN Paper</p> <p>With this progressive training strategy, PGGAN has achieved the state-of-the-art results in producing high fidelity synthetic images.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#problem-setting", "title": "Problem Setting\u00b6", "text": "<p>In this PGGAN example, we decided the following:</p> <ul> <li>560K images will be used when transitioning from a lower resolution to a higher resolution.</li> <li>560K images will be used when stabilizing the fully transitioned network.</li> <li>Initial resolution will be $4\\times4$.</li> <li>Final resolution will be $128\\times128$.</li> </ul> <p>The number of images for both transitioning and stabilizing is equivalent to 5 epochs; the networks would smoothly grow over 5 epochs and would stabilize for 5 epochs. This yields the following schedule for growing both networks:</p> <ul> <li>From $1^{st}$ epoch to $5^{th}$ epoch: train $4\\times4$ resolution</li> <li>From $6^{th}$ epoch to $10^{th}$ epoch: transition from $4\\times4$ to $8\\times8$</li> <li>From $11^{th}$ epoch to $15^{th}$ epoch: stabilize $8\\times8$</li> <li>From $16^{th}$ epoch to $20^{th}$ epoch: transition from $8\\times8$ to $16\\times16$</li> <li>From $21^{st}$ epoch to $25^{th}$ epoch: stabilize $16\\times16$</li> <li>From $26^{th}$ epoch to $30^{th}$ epoch: transition from $16\\times16$ to $32\\times32$</li> <li>From $31^{st}$ epoch to $35^{th}$ epoch: stabilize $32\\times32$</li> </ul> <p>$\\cdots$</p> <ul> <li>From $51^{th}$ epoch to $55^{th}$ epoch: stabilize $128\\times128$</li> </ul>"}, {"location": "apphub/image_generation/pggan/pggan.html#configure-growing-parameters", "title": "Configure growing parameters\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-input-pipeline", "title": "Defining Input <code>Pipeline</code>\u00b6", "text": "<p>First, we need to download the chest frontal X-ray dataset from the National Institute of Health (NIH); the dataset has over 112,000 images with resolution $1024\\times1024$. We use the pre-built <code>fastestimator.dataset.nih_chestxray</code> API to download these images. A detailed description of the dataset is available here.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#note-please-make-sure-to-have-a-stable-internet-connection-when-downloading-the-dataset-for-the-first-time-since-the-size-of-the-dataset-is-over-40gb", "title": "Note: Please make sure to have a stable internet connection when downloading the dataset for the first time since the size of the dataset is over 40GB.\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#given-the-images-we-need-the-following-preprocessing-operations-to-execute-dynamically-for-every-batch", "title": "Given the images, we need the following preprocessing operations to execute dynamically for every batch:\u00b6", "text": "<ol> <li>Read the image.</li> <li>Resize the image to the correct size based on the current epoch.</li> <li>Create a lower resolution of the image, which is accomplished by downsampling by a factor of 2 then upsampling by a factor of 2.</li> <li>Rescale the pixels of both the original image and lower resolution image to the range [-1, 1]</li> <li>Convert both the original image and lower resolution image from channel last to channel first</li> <li>Create the latent vector used by the generator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-network", "title": "Defining <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-the-generator-and-the-discriminator", "title": "Defining the generator and the discriminator\u00b6", "text": "<p>To express the progressive growing of networks, we return a list of models that progressively grow from $4 \\times 4$ to $1024 \\times 1024$ such that $i^{th}$ model in the list is a superset of the previous models. We define a <code>fade_in_alpha</code> to control the smoothness of growth. <code>fe.build</code> then bundles each model, optimizer, and model name together for use.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#the-following-operations-will-happen-in-our-network", "title": "The Following operations will happen in our <code>Network</code>:\u00b6", "text": "<ol> <li>random vector -&gt; generator -&gt; fake images</li> <li>fake images -&gt; discriminator -&gt; fake scores</li> <li>real image, low resolution real image -&gt; blender -&gt; blended real images</li> <li>blended real images -&gt; discriminator -&gt; real scores</li> <li>fake images, real images -&gt; interpolater -&gt; interpolated images</li> <li>interpolated images -&gt; discriminator -&gt; interpolated scores</li> <li>interpolated scores, interpolated image -&gt; get_gradient -&gt; gradient penalty</li> <li>fake_score -&gt; GLoss -&gt; generator loss</li> <li>real score, fake score, gradient penalty -&gt; DLoss -&gt; discriminator loss</li> <li>update generator</li> <li>update discriminator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-estimator", "title": "Defining Estimator\u00b6", "text": "<p>Given that <code>Pipeline</code> and <code>Network</code> are properly defined, we need to define an <code>AlphaController</code> <code>Trace</code> to help both the generator and the discriminator smoothly grow by controlling the value of the <code>fade_in_alpha</code> tensor created previously.  We will also use <code>ModelSaver</code> to save our model during every training phase.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#note-for-128x128-resolution-it-takes-about-24-hours-on-single-v100-gpu-for-1024x1024-resolution-it-takes-25-days-on-4-v100-gpus", "title": "Note: for 128x128 resolution, it takes about 24 hours on single V100 GPU.    for 1024x1024 resolution, it takes ~ 2.5 days on 4 V100 GPUs.\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html", "title": "Instance Detection with RetinaNet", "text": "<p>We are going to implement RetinaNet by Lin et al., 2017 for COCO dataset instance detection.</p> In\u00a0[\u00a0]: Copied! <pre>import tempfile\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom matplotlib import pyplot as plt\n\nimport fastestimator as fe\n</pre> import tempfile  import cv2 import numpy as np import torch import torch.nn as nn import torchvision from matplotlib import pyplot as plt  import fastestimator as fe In\u00a0[2]: parameters Copied! <pre>#training parameters\ndata_dir=None\nbatch_size = 16\nepochs = 13\nimage_size=512\nnum_classes=90\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nmodel_dir=tempfile.mkdtemp()\nclass_json_path = 'class.json'\n</pre> #training parameters data_dir=None batch_size = 16 epochs = 13 image_size=512 num_classes=90 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None model_dir=tempfile.mkdtemp() class_json_path = 'class.json' <p>The <code>class.json</code> includes a map for the class number and what object the number corresponds to.</p> In\u00a0[3]: Copied! <pre>import json\n\nwith open(class_json_path, 'r') as f:\n    class_map = json.load(f)\n</pre> import json  with open(class_json_path, 'r') as f:     class_map = json.load(f) <p>We call our <code>mscoco</code> data API to obtain the training and validation set:</p> <ul> <li>118287 images for training</li> <li>5000 images for validation</li> </ul> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\n\ntrain_ds, eval_ds = mscoco.load_data(root_dir=data_dir)\nprint(len(train_ds)) # 118287\nprint(len(eval_ds)) # 5000\n</pre> from fastestimator.dataset.data import mscoco  train_ds, eval_ds = mscoco.load_data(root_dir=data_dir) print(len(train_ds)) # 118287 print(len(eval_ds)) # 5000 <p>Anchors are predefined for each pixel in the feature map. In this apphub our backbone is ResNet-50, so we can precompute all the anchors we need for training.</p> In\u00a0[5]: Copied! <pre>def _get_fpn_anchor_box(width, height):\n    assert height % 32 == 0 and width % 32 == 0\n    shapes = [(int(height / 8), int(width / 8))]  # P3\n    num_pixel = [np.prod(shapes)]\n    anchor_lengths = [32, 64, 128, 256, 512]\n    for _ in range(4):  # P4 through P7\n        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n        num_pixel.append(np.prod(shapes[-1]))\n    total_num_pixels = np.sum(num_pixel)\n    anchorbox = np.zeros((9 * total_num_pixels, 4))\n    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n    anchor_idx = 0\n    for shape, anchor_length in zip(shapes, anchor_lengths):\n        p_h, p_w = shape\n        base_y = 2**np.ceil(np.log2(height / p_h))\n        base_x = 2**np.ceil(np.log2(width / p_w))\n        for i in range(p_h):\n            center_y = (i + 1 / 2) * base_y\n            for j in range(p_w):\n                center_x = (j + 1 / 2) * base_x\n                for anchor_length_multiplier in anchor_length_multipliers:\n                    area = (anchor_length * anchor_length_multiplier)**2\n                    for aspect_ratio in aspect_ratios:\n                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n                        anchorbox[anchor_idx, 0] = x1\n                        anchorbox[anchor_idx, 1] = y1\n                        anchorbox[anchor_idx, 2] = x2 - x1\n                        anchorbox[anchor_idx, 3] = y2 - y1\n                        anchor_idx += 1\n        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n            break\n    return np.float32(anchorbox), np.int32(num_pixel) * 9\n</pre> def _get_fpn_anchor_box(width, height):     assert height % 32 == 0 and width % 32 == 0     shapes = [(int(height / 8), int(width / 8))]  # P3     num_pixel = [np.prod(shapes)]     anchor_lengths = [32, 64, 128, 256, 512]     for _ in range(4):  # P4 through P7         shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))         num_pixel.append(np.prod(shapes[-1]))     total_num_pixels = np.sum(num_pixel)     anchorbox = np.zeros((9 * total_num_pixels, 4))     anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]     aspect_ratios = [1.0, 2.0, 0.5]  #x:y     anchor_idx = 0     for shape, anchor_length in zip(shapes, anchor_lengths):         p_h, p_w = shape         base_y = 2**np.ceil(np.log2(height / p_h))         base_x = 2**np.ceil(np.log2(width / p_w))         for i in range(p_h):             center_y = (i + 1 / 2) * base_y             for j in range(p_w):                 center_x = (j + 1 / 2) * base_x                 for anchor_length_multiplier in anchor_length_multipliers:                     area = (anchor_length * anchor_length_multiplier)**2                     for aspect_ratio in aspect_ratios:                         x1 = center_x - np.sqrt(area * aspect_ratio) / 2                         y1 = center_y - np.sqrt(area / aspect_ratio) / 2                         x2 = center_x + np.sqrt(area * aspect_ratio) / 2                         y2 = center_y + np.sqrt(area / aspect_ratio) / 2                         anchorbox[anchor_idx, 0] = x1                         anchorbox[anchor_idx, 1] = y1                         anchorbox[anchor_idx, 2] = x2 - x1                         anchorbox[anchor_idx, 3] = y2 - y1                         anchor_idx += 1         if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore             break     return np.float32(anchorbox), np.int32(num_pixel) * 9 In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass ShiftLabel(NumpyOp):\n    def forward(self, data, state):\n        # the label of COCO dataset starts from 1, shifting the start to 0\n        bbox = np.array(data, dtype=np.float32)\n        bbox[:, -1] = bbox[:, -1] - 1\n        return bbox\n\n\nclass AnchorBox(NumpyOp):\n    def __init__(self, width, height, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n\n    def forward(self, data, state):\n        target = self._generate_target(data)  # bbox is #obj x 5\n        return np.float32(target)\n\n    def _generate_target(self, bbox):\n        object_boxes = bbox[:, :-1]  # num_obj x 4\n        label = bbox[:, -1]  # num_obj x 1\n        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n        #now for each object in image, assign the anchor box with highest iou to them\n        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n        num_obj = ious.shape[0]\n        for row in range(num_obj):\n            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n        #next, begin the anchor box assignment based on iou\n        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n        cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class\n        cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples\n        #finally, calculate localization target\n        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n        dwidth = np.squeeze(np.log(gt_width / ac_width))\n        dheight = np.squeeze(np.log(gt_height / ac_height))\n        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n\n    @staticmethod\n    def _get_iou(boxes1, boxes2):\n\"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n        Args:\n            box1 (array): first boxes in N x 4\n            box2 (array): second box in M x 4\n        Returns:\n            float: IoU value in N x M\n        \"\"\"\n        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n        x12 = x11 + w1\n        y12 = y11 + h1\n        x22 = x21 + w2\n        y22 = y21 + h2\n        xmin = np.maximum(x11, np.transpose(x21))\n        ymin = np.maximum(y11, np.transpose(y21))\n        xmax = np.minimum(x12, np.transpose(x22))\n        ymax = np.minimum(y12, np.transpose(y22))\n        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n        area1 = (w1 + 1) * (h1 + 1)\n        area2 = (w2 + 1) * (h2 + 1)\n        iou = inter_area / (area1 + area2.T - inter_area)\n        return iou\n</pre> from fastestimator.op.numpyop import NumpyOp  class ShiftLabel(NumpyOp):     def forward(self, data, state):         # the label of COCO dataset starts from 1, shifting the start to 0         bbox = np.array(data, dtype=np.float32)         bbox[:, -1] = bbox[:, -1] - 1         return bbox   class AnchorBox(NumpyOp):     def __init__(self, width, height, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4      def forward(self, data, state):         target = self._generate_target(data)  # bbox is #obj x 5         return np.float32(target)      def _generate_target(self, bbox):         object_boxes = bbox[:, :-1]  # num_obj x 4         label = bbox[:, -1]  # num_obj x 1         ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor         #now for each object in image, assign the anchor box with highest iou to them         anchorbox_best_iou_idx = np.argmax(ious, axis=1)         num_obj = ious.shape[0]         for row in range(num_obj):             ious[row, anchorbox_best_iou_idx[row]] = 0.99         #next, begin the anchor box assignment based on iou         anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1         anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1         cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1         cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class         cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples         #finally, calculate localization target         single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4         gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)         ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)         dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)         dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)         dwidth = np.squeeze(np.log(gt_width / ac_width))         dheight = np.squeeze(np.log(gt_height / ac_height))         return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5      @staticmethod     def _get_iou(boxes1, boxes2):         \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.         Args:             box1 (array): first boxes in N x 4             box2 (array): second box in M x 4         Returns:             float: IoU value in N x M         \"\"\"         x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)         x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)         x12 = x11 + w1         y12 = y11 + h1         x22 = x21 + w2         y22 = y21 + h2         xmin = np.maximum(x11, np.transpose(x21))         ymin = np.maximum(y11, np.transpose(y21))         xmax = np.minimum(x12, np.transpose(x22))         ymax = np.minimum(y12, np.transpose(y22))         inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)         area1 = (w1 + 1) * (h1 + 1)         area2 = (w2 + 1) * (h2 + 1)         iou = inter_area / (area1 + area2.T - inter_area)         return iou <p>For our data pipeline, we resize the images such that the longer side is 512 pixels. We keep the aspect ratio the same as original image, so on the shorter side we pad zeros. The resized image is 512 by 512. For data augmentation we only flip the image horizontally.</p> In\u00a0[7]: Copied! <pre>from albumentations import BboxParams\n\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=eval_ds,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        LongestMaxSize(image_size, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params=BboxParams(\"coco\", min_area=1.0)),\n        PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\",bbox_params=BboxParams(\"coco\", min_area=1.0)),\n        Sometimes(HorizontalFlip(mode=\"train\", image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params='coco')),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        ShiftLabel(inputs=\"bbox\", outputs=\"bbox\"),\n        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=image_size, height=image_size),\n        ChannelTranspose(inputs=\"image\", outputs=\"image\")],\n    pad_value=0)\n</pre> from albumentations import BboxParams  from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=eval_ds,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         LongestMaxSize(image_size, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params=BboxParams(\"coco\", min_area=1.0)),         PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\",bbox_params=BboxParams(\"coco\", min_area=1.0)),         Sometimes(HorizontalFlip(mode=\"train\", image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params='coco')),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         ShiftLabel(inputs=\"bbox\", outputs=\"bbox\"),         AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=image_size, height=image_size),         ChannelTranspose(inputs=\"image\", outputs=\"image\")],     pad_value=0) In\u00a0[8]: Copied! <pre>batch_data = pipeline.get_results(mode='eval', num_steps=2)\n</pre> batch_data = pipeline.get_results(mode='eval', num_steps=2) In\u00a0[9]: Copied! <pre>import matplotlib.patches as patches\n\nstep_index = 0\nbatch_index = 7\n\nimg = batch_data[step_index]['image'][batch_index].numpy()\nimg = ((img + 1)/2 * 255).astype(np.uint8)\nimg = np.transpose(img, [1, 2, 0])\n\nkeep = ~np.all(batch_data[step_index]['bbox'][batch_index].numpy() == 0, axis=1)\nx1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.imshow(img)\nfor j in range(len(x1)):\n    rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\n    ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]+1))], color=(1, 0, 0), fontsize=14)\n\n\nprint(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))\n</pre> import matplotlib.patches as patches  step_index = 0 batch_index = 7  img = batch_data[step_index]['image'][batch_index].numpy() img = ((img + 1)/2 * 255).astype(np.uint8) img = np.transpose(img, [1, 2, 0])  keep = ~np.all(batch_data[step_index]['bbox'][batch_index].numpy() == 0, axis=1) x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T  fig, ax = plt.subplots(figsize=(10, 10)) ax.imshow(img) for j in range(len(x1)):     rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='r',facecolor='none')     ax.add_patch(rect)     ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]+1))], color=(1, 0, 0), fontsize=14)   print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy())) <pre>id = 312586\n</pre> <p>We define the classification (class) subnet and regression (box) subnet. See Fig. 3 of the original paper.</p> In\u00a0[10]: Copied! <pre>class ClassificationSubNet(nn.Module):\n    def __init__(self, in_channels, num_classes, num_anchors=9):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_1.bias.data)\n        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_2.bias.data)\n        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_3.bias.data)\n        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_4.bias.data)\n        self.conv2d_5 = nn.Conv2d(256, num_classes * num_anchors, 3, padding=1)\n        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n        nn.init.constant_(self.conv2d_5.bias.data, val=np.log(1 / 99))\n\n    def forward(self, x):\n        x = self.conv2d_1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_2(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_3(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_4(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_5(x)\n        x = torch.sigmoid(x)\n        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position\n        return x.reshape(x.size(0), -1, self.num_classes)  # the output dimension is [batch, #anchor, #classes]\n\n\nclass RegressionSubNet(nn.Module):\n    def __init__(self, in_channels, num_anchors=9):\n        super().__init__()\n        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_1.bias.data)\n        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_2.bias.data)\n        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_3.bias.data)\n        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_4.bias.data)\n        self.conv2d_5 = nn.Conv2d(256, 4 * num_anchors, 3, padding=1)\n        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_5.bias.data)\n\n    def forward(self, x):\n        x = self.conv2d_1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_2(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_3(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_4(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_5(x)\n        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position\n        return x.reshape(x.size(0), -1, 4)  # the output dimension is [batch, #anchor, 4]\n</pre> class ClassificationSubNet(nn.Module):     def __init__(self, in_channels, num_classes, num_anchors=9):         super().__init__()         self.num_classes = num_classes         self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)         nn.init.normal_(self.conv2d_1.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_1.bias.data)         self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_2.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_2.bias.data)         self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_3.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_3.bias.data)         self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_4.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_4.bias.data)         self.conv2d_5 = nn.Conv2d(256, num_classes * num_anchors, 3, padding=1)         nn.init.normal_(self.conv2d_5.weight.data, std=0.01)         nn.init.constant_(self.conv2d_5.bias.data, val=np.log(1 / 99))      def forward(self, x):         x = self.conv2d_1(x)         x = nn.functional.relu(x)         x = self.conv2d_2(x)         x = nn.functional.relu(x)         x = self.conv2d_3(x)         x = nn.functional.relu(x)         x = self.conv2d_4(x)         x = nn.functional.relu(x)         x = self.conv2d_5(x)         x = torch.sigmoid(x)         x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position         return x.reshape(x.size(0), -1, self.num_classes)  # the output dimension is [batch, #anchor, #classes]   class RegressionSubNet(nn.Module):     def __init__(self, in_channels, num_anchors=9):         super().__init__()         self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)         nn.init.normal_(self.conv2d_1.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_1.bias.data)         self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_2.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_2.bias.data)         self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_3.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_3.bias.data)         self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_4.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_4.bias.data)         self.conv2d_5 = nn.Conv2d(256, 4 * num_anchors, 3, padding=1)         nn.init.normal_(self.conv2d_5.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_5.bias.data)      def forward(self, x):         x = self.conv2d_1(x)         x = nn.functional.relu(x)         x = self.conv2d_2(x)         x = nn.functional.relu(x)         x = self.conv2d_3(x)         x = nn.functional.relu(x)         x = self.conv2d_4(x)         x = nn.functional.relu(x)         x = self.conv2d_5(x)         x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position         return x.reshape(x.size(0), -1, 4)  # the output dimension is [batch, #anchor, 4] <p>We use ResNet-50 as our backbone.</p> In\u00a0[11]: Copied! <pre>class RetinaNet(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        res50_layers = list(torchvision.models.resnet50(pretrained=True).children())\n        self.res50_in_C3 = nn.Sequential(*(res50_layers[:6]))\n        self.res50_C3_C4 = nn.Sequential(*(res50_layers[6]))\n        self.res50_C4_C5 = nn.Sequential(*(res50_layers[7]))\n        self.conv2d_C5 = nn.Conv2d(2048, 256, 1)\n        self.conv2d_C4 = nn.Conv2d(1024, 256, 1)\n        self.conv2d_C3 = nn.Conv2d(512, 256, 1)\n        self.conv2d_P6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)\n        self.conv2d_P7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n        self.conv2d_P5 = nn.Conv2d(256, 256, 3, padding=1)\n        self.conv2d_P4 = nn.Conv2d(256, 256, 3, padding=1)\n        self.conv2d_P3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.cls_net = ClassificationSubNet(in_channels=256, num_classes=num_classes)\n        self.reg_net = RegressionSubNet(in_channels=256)\n\n    def forward(self, x):\n        C3 = self.res50_in_C3(x)\n        C4 = self.res50_C3_C4(C3)\n        C5 = self.res50_C4_C5(C4)\n        P5 = self.conv2d_C5(C5)\n        P5_upsampling = nn.functional.interpolate(P5, scale_factor=2)\n        P4 = self.conv2d_C4(C4)\n        P4 = P5_upsampling + P4\n        P4_upsampling = nn.functional.interpolate(P4, scale_factor=2)\n        P3 = self.conv2d_C3(C3)\n        P3 = P4_upsampling + P3\n        P6 = self.conv2d_P6(C5)\n        P7 = nn.functional.relu(P6)\n        P7 = self.conv2d_P7(P7)\n        P5 = self.conv2d_P5(P5)\n        P4 = self.conv2d_P4(P4)\n        P3 = self.conv2d_P3(P3)\n        pyramid = [P3, P4, P5, P6, P7]\n        cls_output = torch.cat([self.cls_net(x) for x in pyramid], dim=-2)\n        loc_output = torch.cat([self.reg_net(x) for x in pyramid], dim=-2)\n        return cls_output, loc_output\n</pre> class RetinaNet(nn.Module):     def __init__(self, num_classes):         super().__init__()         res50_layers = list(torchvision.models.resnet50(pretrained=True).children())         self.res50_in_C3 = nn.Sequential(*(res50_layers[:6]))         self.res50_C3_C4 = nn.Sequential(*(res50_layers[6]))         self.res50_C4_C5 = nn.Sequential(*(res50_layers[7]))         self.conv2d_C5 = nn.Conv2d(2048, 256, 1)         self.conv2d_C4 = nn.Conv2d(1024, 256, 1)         self.conv2d_C3 = nn.Conv2d(512, 256, 1)         self.conv2d_P6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)         self.conv2d_P7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)         self.conv2d_P5 = nn.Conv2d(256, 256, 3, padding=1)         self.conv2d_P4 = nn.Conv2d(256, 256, 3, padding=1)         self.conv2d_P3 = nn.Conv2d(256, 256, 3, padding=1)         self.cls_net = ClassificationSubNet(in_channels=256, num_classes=num_classes)         self.reg_net = RegressionSubNet(in_channels=256)      def forward(self, x):         C3 = self.res50_in_C3(x)         C4 = self.res50_C3_C4(C3)         C5 = self.res50_C4_C5(C4)         P5 = self.conv2d_C5(C5)         P5_upsampling = nn.functional.interpolate(P5, scale_factor=2)         P4 = self.conv2d_C4(C4)         P4 = P5_upsampling + P4         P4_upsampling = nn.functional.interpolate(P4, scale_factor=2)         P3 = self.conv2d_C3(C3)         P3 = P4_upsampling + P3         P6 = self.conv2d_P6(C5)         P7 = nn.functional.relu(P6)         P7 = self.conv2d_P7(P7)         P5 = self.conv2d_P5(P5)         P4 = self.conv2d_P4(P4)         P3 = self.conv2d_P3(P3)         pyramid = [P3, P4, P5, P6, P7]         cls_output = torch.cat([self.cls_net(x) for x in pyramid], dim=-2)         loc_output = torch.cat([self.reg_net(x) for x in pyramid], dim=-2)         return cls_output, loc_output <p>We use focal loss for classification and smooth L1 for regression loss.</p> In\u00a0[12]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass RetinaLoss(TensorOp):\n    def forward(self, data, state):\n        anchorbox, cls_pred, loc_pred = data\n        batch_size = anchorbox.size(0)\n        focal_loss, l1_loss = 0.0, 0.0\n        for idx in range(batch_size):\n            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], anchorbox[idx][:, -1].long()\n            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n            single_focal_loss, anchor_obj_bool = self.focal_loss(single_cls_gt, single_cls_pred)\n            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_bool)\n            focal_loss += single_focal_loss\n            l1_loss += single_l1_loss\n        focal_loss = focal_loss / batch_size\n        l1_loss = l1_loss / batch_size\n        total_loss = focal_loss + l1_loss\n        return total_loss, focal_loss, l1_loss\n\n    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n        num_classes = single_cls_pred.size(-1)\n        # gather the objects and background, discard the rest\n        anchor_obj_bool = single_cls_gt &gt;= 0\n        anchor_background_obj_bool = single_cls_gt &gt;= -1\n        anchor_background_bool = single_cls_gt == -1\n        # create one hot encoder, make -1 (background) and -2 (ignore) encoded as 0 in ground truth\n        single_cls_gt[single_cls_gt &lt; 0] = 0\n        single_cls_gt = nn.functional.one_hot(single_cls_gt, num_classes=num_classes)\n        single_cls_gt[anchor_background_bool] = 0\n        single_cls_gt = single_cls_gt[anchor_background_obj_bool]  # remove all ignore cases\n        single_cls_gt = single_cls_gt.view(-1)\n        single_cls_pred = single_cls_pred[anchor_background_obj_bool]\n        single_cls_pred = single_cls_pred.view(-1)\n        # compute the focal weight on each selected anchor box\n        alpha_factor = torch.ones_like(single_cls_gt) * alpha\n        alpha_factor = torch.where(single_cls_gt == 1, alpha_factor, 1 - alpha_factor)\n        focal_weight = torch.where(single_cls_gt == 1, 1 - single_cls_pred, single_cls_pred)\n        focal_weight = alpha_factor * focal_weight**gamma / torch.sum(anchor_obj_bool)\n        focal_loss = nn.functional.binary_cross_entropy(input=single_cls_pred,\n                                                        target=single_cls_gt.float(),\n                                                        weight=focal_weight.detach(),\n                                                        reduction=\"sum\")\n        return focal_loss, anchor_obj_bool\n\n    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_bool, beta=0.1):\n        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n        single_loc_pred = single_loc_pred[anchor_obj_bool]  # anchor_obj_count x 4\n        single_loc_gt = single_loc_gt[anchor_obj_bool]  # anchor_obj_count x 4\n        single_loc_pred = single_loc_pred.view(-1)\n        single_loc_gt = single_loc_gt.view(-1)\n        loc_diff = torch.abs(single_loc_gt - single_loc_pred)\n        loc_loss = torch.where(loc_diff &lt; beta, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n        loc_loss = torch.sum(loc_loss) / torch.sum(anchor_obj_bool)\n        return loc_loss\n</pre> from fastestimator.op.tensorop import TensorOp  class RetinaLoss(TensorOp):     def forward(self, data, state):         anchorbox, cls_pred, loc_pred = data         batch_size = anchorbox.size(0)         focal_loss, l1_loss = 0.0, 0.0         for idx in range(batch_size):             single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], anchorbox[idx][:, -1].long()             single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]             single_focal_loss, anchor_obj_bool = self.focal_loss(single_cls_gt, single_cls_pred)             single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_bool)             focal_loss += single_focal_loss             l1_loss += single_l1_loss         focal_loss = focal_loss / batch_size         l1_loss = l1_loss / batch_size         total_loss = focal_loss + l1_loss         return total_loss, focal_loss, l1_loss      def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):         # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]         num_classes = single_cls_pred.size(-1)         # gather the objects and background, discard the rest         anchor_obj_bool = single_cls_gt &gt;= 0         anchor_background_obj_bool = single_cls_gt &gt;= -1         anchor_background_bool = single_cls_gt == -1         # create one hot encoder, make -1 (background) and -2 (ignore) encoded as 0 in ground truth         single_cls_gt[single_cls_gt &lt; 0] = 0         single_cls_gt = nn.functional.one_hot(single_cls_gt, num_classes=num_classes)         single_cls_gt[anchor_background_bool] = 0         single_cls_gt = single_cls_gt[anchor_background_obj_bool]  # remove all ignore cases         single_cls_gt = single_cls_gt.view(-1)         single_cls_pred = single_cls_pred[anchor_background_obj_bool]         single_cls_pred = single_cls_pred.view(-1)         # compute the focal weight on each selected anchor box         alpha_factor = torch.ones_like(single_cls_gt) * alpha         alpha_factor = torch.where(single_cls_gt == 1, alpha_factor, 1 - alpha_factor)         focal_weight = torch.where(single_cls_gt == 1, 1 - single_cls_pred, single_cls_pred)         focal_weight = alpha_factor * focal_weight**gamma / torch.sum(anchor_obj_bool)         focal_loss = nn.functional.binary_cross_entropy(input=single_cls_pred,                                                         target=single_cls_gt.float(),                                                         weight=focal_weight.detach(),                                                         reduction=\"sum\")         return focal_loss, anchor_obj_bool      def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_bool, beta=0.1):         # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]         single_loc_pred = single_loc_pred[anchor_obj_bool]  # anchor_obj_count x 4         single_loc_gt = single_loc_gt[anchor_obj_bool]  # anchor_obj_count x 4         single_loc_pred = single_loc_pred.view(-1)         single_loc_gt = single_loc_gt.view(-1)         loc_diff = torch.abs(single_loc_gt - single_loc_pred)         loc_loss = torch.where(loc_diff &lt; beta, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)         loc_loss = torch.sum(loc_loss) / torch.sum(anchor_obj_bool)         return loc_loss <p>The learning rate has a warm up phase when step number is &lt; 1000. After that, we reduce the learning rate by 10 at 60k and 80k respectively. The original batch size in the paper is 16 for 8 GPUs. Here we are using 1GPU, with batch size 16 due to a smaller image size.</p> In\u00a0[13]: Copied! <pre>def lr_fn(step):\n    if step &lt; 1000:\n        lr = (0.01 - 0.0002) / 1000 * step + 0.0002\n    elif step &lt; 60000:\n        lr = 0.01\n    elif step &lt; 80000:\n        lr = 0.001\n    else:\n        lr = 0.0001\n    return lr\n</pre> def lr_fn(step):     if step &lt; 1000:         lr = (0.01 - 0.0002) / 1000 * step + 0.0002     elif step &lt; 60000:         lr = 0.01     elif step &lt; 80000:         lr = 0.001     else:         lr = 0.0001     return lr   In\u00a0[14]: Copied! <pre>model = fe.build(model_fn=lambda: RetinaNet(num_classes=num_classes), optimizer_fn=lambda x: torch.optim.SGD(x, lr=2e-4, momentum=0.9, weight_decay=0.0001))\n</pre> model = fe.build(model_fn=lambda: RetinaNet(num_classes=num_classes), optimizer_fn=lambda x: torch.optim.SGD(x, lr=2e-4, momentum=0.9, weight_decay=0.0001)) <p>During evaluation, testing and inferencing, some additional postprocessing stes are required, for example, filter out lower scores and perform Non-maximal suppression on bounding box predictions. These postprocessing steps are implemented as <code>TensorOp</code> named <code>PredictBox</code>.</p> In\u00a0[15]: Copied! <pre>class PredictBox(TensorOp):\n\"\"\"Convert network output to bounding boxes.\n        \"\"\"\n    def __init__(self,\n                 inputs=None,\n                 outputs=None,\n                 mode=None,\n                 input_shape=(512, 512, 3),\n                 select_top_k=1000,\n                 nms_max_outputs=100,\n                 score_threshold=0.05):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.input_shape = input_shape\n        self.select_top_k = select_top_k\n        self.nms_max_outputs = nms_max_outputs\n        self.score_threshold = score_threshold\n        self.all_anchors, self.num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n        self.all_anchors = torch.Tensor(self.all_anchors)\n        if torch.cuda.is_available():\n            self.all_anchors = self.all_anchors.to(\"cuda\")\n\n    def forward(self, data, state):\n        cls_pred, loc_pred = data  # [Batch, #anchor, #num_classes], [Batch, #anchor, 4]\n        batch_size = cls_pred.size(0)\n        scores_pred, labels_pred = torch.max(cls_pred, dim=-1)\n        # loc_pred -&gt; loc_abs\n        x1_abs = loc_pred[..., 0] * self.all_anchors[..., 2] + self.all_anchors[..., 0]\n        y1_abs = loc_pred[..., 1] * self.all_anchors[..., 3] + self.all_anchors[..., 1]\n        w_abs = torch.exp(loc_pred[..., 2]) * self.all_anchors[..., 2]\n        h_abs = torch.exp(loc_pred[..., 3]) * self.all_anchors[..., 3]\n        x2_abs, y2_abs = x1_abs + w_abs, y1_abs + h_abs\n        # iterate over images\n        final_results = []\n        for idx in range(batch_size):\n            scores_pred_single = scores_pred[idx]\n            boxes_pred_single = torch.stack([x1_abs[idx], y1_abs[idx], x2_abs[idx], y2_abs[idx]], dim=-1)\n            # iterate over each pyramid to select top 1000 anchor boxes\n            start = 0\n            top_idx = []\n            for num_anchors_fpn_level in self.num_anchors_per_level:\n                fpn_scores = scores_pred_single[start:start + num_anchors_fpn_level]\n                _, selected_index = torch.topk(fpn_scores, min(self.select_top_k, int(num_anchors_fpn_level)))\n                top_idx.append(selected_index + start)\n                start += num_anchors_fpn_level\n            top_idx = torch.cat([x.long() for x in top_idx])\n            # perform nms\n            nms_keep = torchvision.ops.nms(boxes_pred_single[top_idx], scores_pred_single[top_idx], iou_threshold=0.5)\n            nms_keep = nms_keep[:self.nms_max_outputs]  # select the top nms outputs\n            top_idx = top_idx[nms_keep]  # narrow the keep index\n            results_single = [\n                x1_abs[idx][top_idx],\n                y1_abs[idx][top_idx],\n                w_abs[idx][top_idx],\n                h_abs[idx][top_idx],\n                labels_pred[idx][top_idx].float(),\n                scores_pred[idx][top_idx],\n                torch.ones_like(x1_abs[idx][top_idx])\n            ]\n            # clip bounding boxes to image size\n            results_single[0] = torch.clamp(results_single[0], min=0, max=self.input_shape[1])\n            results_single[1] = torch.clamp(results_single[1], min=0, max=self.input_shape[0])\n            results_single[2] = torch.clamp(results_single[2], min=0)\n            results_single[2] = torch.where(results_single[2] &gt; self.input_shape[1] - results_single[0],\n                                            self.input_shape[1] - results_single[0],\n                                            results_single[2])\n            results_single[3] = torch.clamp(results_single[3], min=0)\n            results_single[3] = torch.where(results_single[3] &gt; self.input_shape[0] - results_single[1],\n                                            self.input_shape[0] - results_single[1],\n                                            results_single[3])\n            # mark the select as 0 for any anchorbox with score lower than threshold\n            results_single[-1] = torch.where(results_single[-2] &gt; self.score_threshold,\n                                             results_single[-1],\n                                             torch.zeros_like(results_single[-1]))\n            final_results.append(torch.stack(results_single, dim=-1))\n        return torch.stack(final_results)\n</pre> class PredictBox(TensorOp):     \"\"\"Convert network output to bounding boxes.         \"\"\"     def __init__(self,                  inputs=None,                  outputs=None,                  mode=None,                  input_shape=(512, 512, 3),                  select_top_k=1000,                  nms_max_outputs=100,                  score_threshold=0.05):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.input_shape = input_shape         self.select_top_k = select_top_k         self.nms_max_outputs = nms_max_outputs         self.score_threshold = score_threshold         self.all_anchors, self.num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])         self.all_anchors = torch.Tensor(self.all_anchors)         if torch.cuda.is_available():             self.all_anchors = self.all_anchors.to(\"cuda\")      def forward(self, data, state):         cls_pred, loc_pred = data  # [Batch, #anchor, #num_classes], [Batch, #anchor, 4]         batch_size = cls_pred.size(0)         scores_pred, labels_pred = torch.max(cls_pred, dim=-1)         # loc_pred -&gt; loc_abs         x1_abs = loc_pred[..., 0] * self.all_anchors[..., 2] + self.all_anchors[..., 0]         y1_abs = loc_pred[..., 1] * self.all_anchors[..., 3] + self.all_anchors[..., 1]         w_abs = torch.exp(loc_pred[..., 2]) * self.all_anchors[..., 2]         h_abs = torch.exp(loc_pred[..., 3]) * self.all_anchors[..., 3]         x2_abs, y2_abs = x1_abs + w_abs, y1_abs + h_abs         # iterate over images         final_results = []         for idx in range(batch_size):             scores_pred_single = scores_pred[idx]             boxes_pred_single = torch.stack([x1_abs[idx], y1_abs[idx], x2_abs[idx], y2_abs[idx]], dim=-1)             # iterate over each pyramid to select top 1000 anchor boxes             start = 0             top_idx = []             for num_anchors_fpn_level in self.num_anchors_per_level:                 fpn_scores = scores_pred_single[start:start + num_anchors_fpn_level]                 _, selected_index = torch.topk(fpn_scores, min(self.select_top_k, int(num_anchors_fpn_level)))                 top_idx.append(selected_index + start)                 start += num_anchors_fpn_level             top_idx = torch.cat([x.long() for x in top_idx])             # perform nms             nms_keep = torchvision.ops.nms(boxes_pred_single[top_idx], scores_pred_single[top_idx], iou_threshold=0.5)             nms_keep = nms_keep[:self.nms_max_outputs]  # select the top nms outputs             top_idx = top_idx[nms_keep]  # narrow the keep index             results_single = [                 x1_abs[idx][top_idx],                 y1_abs[idx][top_idx],                 w_abs[idx][top_idx],                 h_abs[idx][top_idx],                 labels_pred[idx][top_idx].float(),                 scores_pred[idx][top_idx],                 torch.ones_like(x1_abs[idx][top_idx])             ]             # clip bounding boxes to image size             results_single[0] = torch.clamp(results_single[0], min=0, max=self.input_shape[1])             results_single[1] = torch.clamp(results_single[1], min=0, max=self.input_shape[0])             results_single[2] = torch.clamp(results_single[2], min=0)             results_single[2] = torch.where(results_single[2] &gt; self.input_shape[1] - results_single[0],                                             self.input_shape[1] - results_single[0],                                             results_single[2])             results_single[3] = torch.clamp(results_single[3], min=0)             results_single[3] = torch.where(results_single[3] &gt; self.input_shape[0] - results_single[1],                                             self.input_shape[0] - results_single[1],                                             results_single[3])             # mark the select as 0 for any anchorbox with score lower than threshold             results_single[-1] = torch.where(results_single[-2] &gt; self.score_threshold,                                              results_single[-1],                                              torch.zeros_like(results_single[-1]))             final_results.append(torch.stack(results_single, dim=-1))         return torch.stack(final_results) In\u00a0[16]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n    UpdateOp(model=model, loss_name=\"total_loss\"),\n    PredictBox(input_shape=(image_size, image_size, 3), inputs=[\"cls_pred\", \"loc_pred\"], outputs=\"pred\", mode=\"!train\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),     RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),     UpdateOp(model=model, loss_name=\"total_loss\"),     PredictBox(input_shape=(image_size, image_size, 3), inputs=[\"cls_pred\", \"loc_pred\"], outputs=\"pred\", mode=\"!train\") ]) In\u00a0[17]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import MeanAveragePrecision\n\ntraces = [\n    LRScheduler(model=model, lr_fn=lr_fn),\n    BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\"),\n    MeanAveragePrecision(num_classes=num_classes, true_key='bbox', pred_key='pred', mode=\"eval\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         monitor_names=[\"l1_loss\", \"focal_loss\"])\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import MeanAveragePrecision  traces = [     LRScheduler(model=model, lr_fn=lr_fn),     BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\"),     MeanAveragePrecision(num_classes=num_classes, true_key='bbox', pred_key='pred', mode=\"eval\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          monitor_names=[\"l1_loss\", \"focal_loss\"]) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <p>We select 4 images and visualize the model predictions.</p> In\u00a0[\u00a0]: Copied! <pre>num_images = 4\nbatch_idx = 1\nsample_data = pipeline.get_results(mode=\"eval\", num_steps=batch_idx+1)[batch_idx]\nnetwork_out = network.transform(data=sample_data, mode=\"eval\")\n</pre> num_images = 4 batch_idx = 1 sample_data = pipeline.get_results(mode=\"eval\", num_steps=batch_idx+1)[batch_idx] network_out = network.transform(data=sample_data, mode=\"eval\") In\u00a0[41]: Copied! <pre>fig, ax = plt.subplots(num_images, 2, figsize=(20, 30))\nfor batch_index in range(num_images):\n    img = network_out['image'].numpy()[batch_index, ...]\n    img = ((img + 1) / 2 * 255).astype(np.uint8)\n    img = np.transpose(img, [1, 2, 0])\n\n    img2 = img.copy()\n    keep = ~np.all(network_out['bbox'][batch_index].numpy() == 0, axis=1)\n    gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n\n    scores = network_out['pred'][batch_index].numpy()[..., -2]\n    labels = network_out['pred'][batch_index].numpy()[..., -3]\n    keep = scores &gt; 0.5\n    x1, y1, w, h, label, _, _ = network_out['pred'][batch_index].numpy()[keep].T\n\n    for i in range(len(gt_x1)):\n        rect = patches.Rectangle((gt_x1[i], gt_y1[i]),gt_w[i],gt_h[i],linewidth=1,edgecolor='r',facecolor='none')\n        ax[batch_index, 0].add_patch(rect)\n        ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n        ax[batch_index, 0].text(gt_x1[i] + 3,\n                   gt_y1[i] + 12,\n                   class_map[str(int(gt_label[i])+1)],\n                   color=(1, 0, 0),\n                   fontsize=14,\n                   fontweight='bold')\n    for j in range(len(x1)):\n        rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='b',facecolor='none')\n        ax[batch_index, 1].add_patch(rect)\n        ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n        ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]) + 1)], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n\n    ax[batch_index, 0].imshow(img)\n    ax[batch_index, 1].imshow(img2)\n\nplt.tight_layout()\n</pre> fig, ax = plt.subplots(num_images, 2, figsize=(20, 30)) for batch_index in range(num_images):     img = network_out['image'].numpy()[batch_index, ...]     img = ((img + 1) / 2 * 255).astype(np.uint8)     img = np.transpose(img, [1, 2, 0])      img2 = img.copy()     keep = ~np.all(network_out['bbox'][batch_index].numpy() == 0, axis=1)     gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T      scores = network_out['pred'][batch_index].numpy()[..., -2]     labels = network_out['pred'][batch_index].numpy()[..., -3]     keep = scores &gt; 0.5     x1, y1, w, h, label, _, _ = network_out['pred'][batch_index].numpy()[keep].T      for i in range(len(gt_x1)):         rect = patches.Rectangle((gt_x1[i], gt_y1[i]),gt_w[i],gt_h[i],linewidth=1,edgecolor='r',facecolor='none')         ax[batch_index, 0].add_patch(rect)         ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')         ax[batch_index, 0].text(gt_x1[i] + 3,                    gt_y1[i] + 12,                    class_map[str(int(gt_label[i])+1)],                    color=(1, 0, 0),                    fontsize=14,                    fontweight='bold')     for j in range(len(x1)):         rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='b',facecolor='none')         ax[batch_index, 1].add_patch(rect)         ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')         ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]) + 1)], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')      ax[batch_index, 0].imshow(img)     ax[batch_index, 1].imshow(img2)  plt.tight_layout()"}, {"location": "apphub/instance_detection/retinanet/retinanet.html#instance-detection-with-retinanet", "title": "Instance Detection with RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#load-data", "title": "Load data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#generate-anchors", "title": "Generate Anchors\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#pipeline", "title": "<code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualization-of-batch-data", "title": "Visualization of batch data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#class-and-box-subnets", "title": "Class and box subnets\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#retinanet", "title": "RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#loss-functions", "title": "Loss functions\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#learning-rate", "title": "Learning rate\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#predict-box", "title": "Predict Box\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#start-training", "title": "Start Training\u00b6", "text": "<p>The training will take ~14 hours on single V100 GPU.</p>"}, {"location": "apphub/instance_detection/retinanet/retinanet.html#inference", "title": "Inference\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualize-predictions", "title": "Visualize predictions\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html", "title": "Multi-Task Learning using Uncertainty Weighted Loss", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nfrom torch.nn.init import kaiming_normal_ as he_normal\nfrom torchvision import models\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy, Dice\n</pre> import os import tempfile  import cv2 import torch import torch.nn as nn import torch.nn.functional as fn from torch.nn.init import kaiming_normal_ as he_normal from torchvision import models  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy, Dice In\u00a0[2]: parameters Copied! <pre>#parameters\nepochs = 25\nbatch_size = 8\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> #parameters epochs = 25 batch_size = 8 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cub200\n\ntrain_data = cub200.load_data(root_dir=data_dir)\neval_data = train_data.split(0.3)\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cub200  train_data = cub200.load_data(root_dir=data_dir) eval_data = train_data.split(0.3) test_data = eval_data.split(0.5)  In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(batch_size=batch_size,\n                       train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       ops=[\n                           ReadImage(inputs=\"image\",\n                                     outputs=\"image\",\n                                     parent_path=train_data.parent_path),\n                           Normalize(inputs=\"image\",\n                                     outputs=\"image\",\n                                     mean=1.0,\n                                     std=1.0,\n                                     max_pixel_value=127.5),\n                           ReadMat(file='annotation',\n                                   keys=\"seg\",\n                                   parent_path=train_data.parent_path),\n                           Delete(keys=\"annotation\"),\n                           LongestMaxSize(max_size=512,\n                                          image_in=\"image\",\n                                          image_out=\"image\",\n                                          mask_in=\"seg\",\n                                          mask_out=\"seg\"),\n                           PadIfNeeded(min_height=512,\n                                       min_width=512,\n                                       image_in=\"image\",\n                                       image_out=\"image\",\n                                       mask_in=\"seg\",\n                                       mask_out=\"seg\",\n                                       border_mode=cv2.BORDER_CONSTANT,\n                                       value=0,\n                                       mask_value=0),\n                           ShiftScaleRotate(\n                               image_in=\"image\",\n                               mask_in=\"seg\",\n                               image_out=\"image\",\n                               mask_out=\"seg\",\n                               mode=\"train\",\n                               shift_limit=0.2,\n                               rotate_limit=15.0,\n                               scale_limit=0.2,\n                               border_mode=cv2.BORDER_CONSTANT,\n                               value=0,\n                               mask_value=0),\n                           Sometimes(\n                               HorizontalFlip(image_in=\"image\",\n                                              mask_in=\"seg\",\n                                              image_out=\"image\",\n                                              mask_out=\"seg\",\n                                              mode=\"train\")),\n                           ChannelTranspose(inputs=\"image\",\n                                            outputs=\"image\"),\n                           Reshape(shape=(1, 512, 512),\n                                   inputs=\"seg\",\n                                   outputs=\"seg\")\n                       ])\n</pre> pipeline = fe.Pipeline(batch_size=batch_size,                        train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        ops=[                            ReadImage(inputs=\"image\",                                      outputs=\"image\",                                      parent_path=train_data.parent_path),                            Normalize(inputs=\"image\",                                      outputs=\"image\",                                      mean=1.0,                                      std=1.0,                                      max_pixel_value=127.5),                            ReadMat(file='annotation',                                    keys=\"seg\",                                    parent_path=train_data.parent_path),                            Delete(keys=\"annotation\"),                            LongestMaxSize(max_size=512,                                           image_in=\"image\",                                           image_out=\"image\",                                           mask_in=\"seg\",                                           mask_out=\"seg\"),                            PadIfNeeded(min_height=512,                                        min_width=512,                                        image_in=\"image\",                                        image_out=\"image\",                                        mask_in=\"seg\",                                        mask_out=\"seg\",                                        border_mode=cv2.BORDER_CONSTANT,                                        value=0,                                        mask_value=0),                            ShiftScaleRotate(                                image_in=\"image\",                                mask_in=\"seg\",                                image_out=\"image\",                                mask_out=\"seg\",                                mode=\"train\",                                shift_limit=0.2,                                rotate_limit=15.0,                                scale_limit=0.2,                                border_mode=cv2.BORDER_CONSTANT,                                value=0,                                mask_value=0),                            Sometimes(                                HorizontalFlip(image_in=\"image\",                                               mask_in=\"seg\",                                               image_out=\"image\",                                               mask_out=\"seg\",                                               mode=\"train\")),                            ChannelTranspose(inputs=\"image\",                                             outputs=\"image\"),                            Reshape(shape=(1, 512, 512),                                    inputs=\"seg\",                                    outputs=\"seg\")                        ]) In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef Minmax(data):\n    data_max = np.max(data)\n    data_min = np.min(data)\n    data = (data - data_min) / max((data_max - data_min), 1e-7)\n    return data\n\n\ndef visualize_image_mask(img, mask):\n    img = (img*255).astype(np.uint8)\n    \n    mask = mask.astype(np.uint8)\n    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n    \n    ret, mask_thres = cv2.threshold(mask, 0.5,1, cv2.THRESH_BINARY)\n    mask_overlay = mask * mask_thres\n    mask_overlay = np.where( mask_overlay != [0,0,0], [255,0,0] ,[0,0,0])\n    mask_overlay = mask_overlay.astype(np.uint8)\n    img_with_mask = cv2.addWeighted(img, 0.7, mask_overlay, 0.3,0 )\n\n    maskgt_with_maskpred = cv2.addWeighted(mask, 0.7, mask_overlay, 0.3, 0)\n\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(18,8))\n    ax[0].imshow(img)\n    ax[0].set_title('original image')\n    ax[1].imshow(img_with_mask)\n    ax[1].set_title('img - mask overlay')\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def Minmax(data):     data_max = np.max(data)     data_min = np.min(data)     data = (data - data_min) / max((data_max - data_min), 1e-7)     return data   def visualize_image_mask(img, mask):     img = (img*255).astype(np.uint8)          mask = mask.astype(np.uint8)     mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)          ret, mask_thres = cv2.threshold(mask, 0.5,1, cv2.THRESH_BINARY)     mask_overlay = mask * mask_thres     mask_overlay = np.where( mask_overlay != [0,0,0], [255,0,0] ,[0,0,0])     mask_overlay = mask_overlay.astype(np.uint8)     img_with_mask = cv2.addWeighted(img, 0.7, mask_overlay, 0.3,0 )      maskgt_with_maskpred = cv2.addWeighted(mask, 0.7, mask_overlay, 0.3, 0)      fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(18,8))     ax[0].imshow(img)     ax[0].set_title('original image')     ax[1].imshow(img_with_mask)     ax[1].set_title('img - mask overlay')     plt.show() In\u00a0[6]: Copied! <pre>idx = np.random.randint(low=0, high=batch_size)\nresult = pipeline.get_results()\nimg = Minmax(result[\"image\"][idx].numpy())\nmsk = np.squeeze(result[\"seg\"][idx].numpy())\nimg = np.transpose(img, (1, 2, 0))\n\nvisualize_image_mask(img, msk)\n</pre> idx = np.random.randint(low=0, high=batch_size) result = pipeline.get_results() img = Minmax(result[\"image\"][idx].numpy()) msk = np.squeeze(result[\"seg\"][idx].numpy()) img = np.transpose(img, (1, 2, 0))  visualize_image_mask(img, msk) In\u00a0[7]: Copied! <pre>class Upsample2D(nn.Module):\n\"\"\"Upsampling Block\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.upsample = nn.Sequential(\n            nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.upsample:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass DecBlock(nn.Module):\n\"\"\"Decoder Block\"\"\"\n    def __init__(self, upsample_in_ch, conv_in_ch, out_ch):\n        super().__init__()\n        self.upsample = Upsample2D(upsample_in_ch, out_ch)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.conv_layers:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x_up, x_down):\n        x = self.upsample(x_up)\n        x = torch.cat([x, x_down], 1)\n        x = self.conv_layers(x)\n        return x\n\n\nclass ResUnet50(nn.Module):\n\"\"\"Network Architecture\"\"\"\n    def __init__(self, num_classes=200):\n        super().__init__()\n        base_model = models.resnet50(pretrained=True)\n\n        self.enc1 = nn.Sequential(*list(base_model.children())[:3])\n        self.input_pool = list(base_model.children())[3]\n        self.enc2 = nn.Sequential(*list(base_model.children())[4])\n        self.enc3 = nn.Sequential(*list(base_model.children())[5])\n        self.enc4 = nn.Sequential(*list(base_model.children())[6])\n        self.enc5 = nn.Sequential(*list(base_model.children())[7])\n        self.fc = nn.Linear(2048, num_classes)\n\n        self.dec6 = DecBlock(2048, 1536, 512)\n        self.dec7 = DecBlock(512, 768, 256)\n        self.dec8 = DecBlock(256, 384, 128)\n        self.dec9 = DecBlock(128, 128, 64)\n        self.dec10 = Upsample2D(64, 2)\n        self.mask = nn.Conv2d(2, 1, kernel_size=1)\n\n    def forward(self, x):\n        x_e1 = self.enc1(x)\n        x_e1_1 = self.input_pool(x_e1)\n        x_e2 = self.enc2(x_e1_1)\n        x_e3 = self.enc3(x_e2)\n        x_e4 = self.enc4(x_e3)\n        x_e5 = self.enc5(x_e4)\n\n        x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])\n        x_label = x_label.view(x_label.shape[0], -1)\n        x_label = self.fc(x_label)\n        x_label = torch.softmax(x_label, dim=-1)\n\n        x_d6 = self.dec6(x_e5, x_e4)\n        x_d7 = self.dec7(x_d6, x_e3)\n        x_d8 = self.dec8(x_d7, x_e2)\n        x_d9 = self.dec9(x_d8, x_e1)\n        x_d10 = self.dec10(x_d9)\n        x_mask = self.mask(x_d10)\n        x_mask = torch.sigmoid(x_mask)\n        return x_label, x_mask\n</pre> class Upsample2D(nn.Module):     \"\"\"Upsampling Block\"\"\"     def __init__(self, in_channels, out_channels):         super().__init__()         self.upsample = nn.Sequential(             nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.upsample:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x):         return self.upsample(x)   class DecBlock(nn.Module):     \"\"\"Decoder Block\"\"\"     def __init__(self, upsample_in_ch, conv_in_ch, out_ch):         super().__init__()         self.upsample = Upsample2D(upsample_in_ch, out_ch)         self.conv_layers = nn.Sequential(             nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True),             nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.conv_layers:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x_up, x_down):         x = self.upsample(x_up)         x = torch.cat([x, x_down], 1)         x = self.conv_layers(x)         return x   class ResUnet50(nn.Module):     \"\"\"Network Architecture\"\"\"     def __init__(self, num_classes=200):         super().__init__()         base_model = models.resnet50(pretrained=True)          self.enc1 = nn.Sequential(*list(base_model.children())[:3])         self.input_pool = list(base_model.children())[3]         self.enc2 = nn.Sequential(*list(base_model.children())[4])         self.enc3 = nn.Sequential(*list(base_model.children())[5])         self.enc4 = nn.Sequential(*list(base_model.children())[6])         self.enc5 = nn.Sequential(*list(base_model.children())[7])         self.fc = nn.Linear(2048, num_classes)          self.dec6 = DecBlock(2048, 1536, 512)         self.dec7 = DecBlock(512, 768, 256)         self.dec8 = DecBlock(256, 384, 128)         self.dec9 = DecBlock(128, 128, 64)         self.dec10 = Upsample2D(64, 2)         self.mask = nn.Conv2d(2, 1, kernel_size=1)      def forward(self, x):         x_e1 = self.enc1(x)         x_e1_1 = self.input_pool(x_e1)         x_e2 = self.enc2(x_e1_1)         x_e3 = self.enc3(x_e2)         x_e4 = self.enc4(x_e3)         x_e5 = self.enc5(x_e4)          x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])         x_label = x_label.view(x_label.shape[0], -1)         x_label = self.fc(x_label)         x_label = torch.softmax(x_label, dim=-1)          x_d6 = self.dec6(x_e5, x_e4)         x_d7 = self.dec7(x_d6, x_e3)         x_d8 = self.dec8(x_d7, x_e2)         x_d9 = self.dec9(x_d8, x_e1)         x_d10 = self.dec10(x_d9)         x_mask = self.mask(x_d10)         x_mask = torch.sigmoid(x_mask)         return x_label, x_mask <p>Other than the ResUnet50, we will have another network to contain the trainable weighted parameter in the weighted loss. We call it our uncertainty model. In the network <code>ops</code>, ResUnet50 produces both a predicted label and predicted mask. These two predictions are then fed to classification loss and segmentation loss operators respectively. Finally, both losses are passed to the uncertainty model to create a final loss.</p> In\u00a0[8]: Copied! <pre>class UncertaintyLossNet(nn.Module):\n\"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.w1 = nn.Parameter(torch.zeros(1))\n        self.w2 = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(\n            -self.w2) * x[1] + self.w2\n        return loss\n</pre> class UncertaintyLossNet(nn.Module):     \"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115     \"\"\"     def __init__(self):         super().__init__()         self.w1 = nn.Parameter(torch.zeros(1))         self.w2 = nn.Parameter(torch.zeros(1))      def forward(self, x):         loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(             -self.w2) * x[1] + self.w2         return loss <p>We also implement a <code>TensorOp</code> to average the output of <code>UncertaintyLossNet</code> for each batch:</p> In\u00a0[9]: Copied! <pre>class ReduceLoss(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> class ReduceLoss(TensorOp):     def forward(self, data, state):         return reduce_mean(data) In\u00a0[10]: Copied! <pre>resunet50 = fe.build(model_fn=ResUnet50,\n                     model_name=\"resunet50\",\n                     optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4))\nuncertainty = fe.build(model_fn=UncertaintyLossNet,\n                       model_name=\"uncertainty\",\n                       optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs='image',\n            model=resunet50,\n            outputs=[\"label_pred\", \"mask_pred\"]),\n    CrossEntropy(inputs=[\"label_pred\", \"label\"],\n                 outputs=\"cls_loss\",\n                 form=\"sparse\",\n                 average_loss=False),\n    CrossEntropy(inputs=[\"mask_pred\", \"seg\"],\n                 outputs=\"seg_loss\",\n                 form=\"binary\",\n                 average_loss=False),\n    ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],\n            model=uncertainty,\n            outputs=\"total_loss\"),\n    ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),\n    UpdateOp(model=resunet50, loss_name=\"total_loss\"),\n    UpdateOp(model=uncertainty, loss_name=\"total_loss\")\n])\n</pre> resunet50 = fe.build(model_fn=ResUnet50,                      model_name=\"resunet50\",                      optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4)) uncertainty = fe.build(model_fn=UncertaintyLossNet,                        model_name=\"uncertainty\",                        optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))  network = fe.Network(ops=[     ModelOp(inputs='image',             model=resunet50,             outputs=[\"label_pred\", \"mask_pred\"]),     CrossEntropy(inputs=[\"label_pred\", \"label\"],                  outputs=\"cls_loss\",                  form=\"sparse\",                  average_loss=False),     CrossEntropy(inputs=[\"mask_pred\", \"seg\"],                  outputs=\"seg_loss\",                  form=\"binary\",                  average_loss=False),     ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],             model=uncertainty,             outputs=\"total_loss\"),     ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),     UpdateOp(model=resunet50, loss_name=\"total_loss\"),     UpdateOp(model=uncertainty, loss_name=\"total_loss\") ]) In\u00a0[11]: Copied! <pre>traces = [\n    Accuracy(true_key=\"label\", pred_key=\"label_pred\"),\n    Dice(true_key=\"seg\", pred_key='mask_pred'),\n    BestModelSaver(model=resunet50,\n                   save_dir=save_dir,\n                   metric=\"total_loss\",\n                   save_best_mode=\"min\"),\n    LRScheduler(model=resunet50,\n                lr_fn=lambda step: cosine_decay(\n                    step, cycle_length=13200, init_lr=1e-4))\n]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=traces,\n                         epochs=epochs,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         log_steps=500)\n</pre> traces = [     Accuracy(true_key=\"label\", pred_key=\"label_pred\"),     Dice(true_key=\"seg\", pred_key='mask_pred'),     BestModelSaver(model=resunet50,                    save_dir=save_dir,                    metric=\"total_loss\",                    save_best_mode=\"min\"),     LRScheduler(model=resunet50,                 lr_fn=lambda step: cosine_decay(                     step, cycle_length=13200, init_lr=1e-4)) ] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=traces,                          epochs=epochs,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          log_steps=500) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; resunet50_lr: 0.0001; uncertainty_lr: 1e-05; \nFastEstimator-Train: step: 1; total_loss: 8.121616; resunet50_lr: 1e-04; \nFastEstimator-Train: step: 500; total_loss: 4.7089643; steps/sec: 3.17; resunet50_lr: 9.9651326e-05; \nFastEstimator-Train: step: 528; epoch: 1; epoch_time: 167.79 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 528; epoch: 1; total_loss: 3.9346602; min_total_loss: 3.9346602; since_best: 0; accuracy: 0.16022099447513813; Dice: 0.7908390168388019; \nFastEstimator-Train: step: 1000; total_loss: 2.6967134; steps/sec: 3.13; resunet50_lr: 9.860745e-05; \nFastEstimator-Train: step: 1056; epoch: 2; epoch_time: 168.37 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1056; epoch: 2; total_loss: 2.358711; min_total_loss: 2.358711; since_best: 0; accuracy: 0.430939226519337; Dice: 0.8358255033320947; \nFastEstimator-Train: step: 1500; total_loss: 2.5349426; steps/sec: 3.14; resunet50_lr: 9.688313e-05; \nFastEstimator-Train: step: 1584; epoch: 3; epoch_time: 168.17 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1584; epoch: 3; total_loss: 1.8696523; min_total_loss: 1.8696523; since_best: 0; accuracy: 0.5138121546961326; Dice: 0.823370761791696; \nFastEstimator-Train: step: 2000; total_loss: 1.288121; steps/sec: 3.14; resunet50_lr: 9.450275e-05; \nFastEstimator-Train: step: 2112; epoch: 4; epoch_time: 168.15 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 2112; epoch: 4; total_loss: 1.5971442; min_total_loss: 1.5971442; since_best: 0; accuracy: 0.6077348066298343; Dice: 0.8283221605740853; \nFastEstimator-Train: step: 2500; total_loss: 1.1395; steps/sec: 3.14; resunet50_lr: 9.149999e-05; \nFastEstimator-Train: step: 2640; epoch: 5; epoch_time: 168.23 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 2640; epoch: 5; total_loss: 1.2618546; min_total_loss: 1.2618546; since_best: 0; accuracy: 0.6696132596685083; Dice: 0.8558863977253409; \nFastEstimator-Train: step: 3000; total_loss: 0.4348533; steps/sec: 3.14; resunet50_lr: 8.791732e-05; \nFastEstimator-Train: step: 3168; epoch: 6; epoch_time: 168.23 sec; \nFastEstimator-Eval: step: 3168; epoch: 6; total_loss: 1.269778; min_total_loss: 1.2618546; since_best: 1; accuracy: 0.6828729281767956; Dice: 0.8506440013132466; \nFastEstimator-Train: step: 3500; total_loss: 1.0252838; steps/sec: 3.14; resunet50_lr: 8.3805404e-05; \nFastEstimator-Train: step: 3696; epoch: 7; epoch_time: 168.16 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 3696; epoch: 7; total_loss: 1.1601604; min_total_loss: 1.1601604; since_best: 0; accuracy: 0.7071823204419889; Dice: 0.8529923493590581; \nFastEstimator-Train: step: 4000; total_loss: 0.02089151; steps/sec: 3.14; resunet50_lr: 7.922241e-05; \nFastEstimator-Train: step: 4224; epoch: 8; epoch_time: 168.09 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 4224; epoch: 8; total_loss: 1.0023108; min_total_loss: 1.0023108; since_best: 0; accuracy: 0.7370165745856354; Dice: 0.8545488100818929; \nFastEstimator-Train: step: 4500; total_loss: 0.8392416; steps/sec: 3.14; resunet50_lr: 7.423316e-05; \nFastEstimator-Train: step: 4752; epoch: 9; epoch_time: 167.95 sec; \nFastEstimator-Eval: step: 4752; epoch: 9; total_loss: 1.1233779; min_total_loss: 1.0023108; since_best: 1; accuracy: 0.7303867403314918; Dice: 0.8615670240361439; \nFastEstimator-Train: step: 5000; total_loss: 0.16346264; steps/sec: 3.14; resunet50_lr: 6.890823e-05; \nFastEstimator-Train: step: 5280; epoch: 10; epoch_time: 168.11 sec; \nFastEstimator-Eval: step: 5280; epoch: 10; total_loss: 1.087766; min_total_loss: 1.0023108; since_best: 2; accuracy: 0.712707182320442; Dice: 0.8528682840144792; \nFastEstimator-Train: step: 5500; total_loss: 0.06831953; steps/sec: 3.14; resunet50_lr: 6.332292e-05; \nFastEstimator-Train: step: 5808; epoch: 11; epoch_time: 168.11 sec; \nFastEstimator-Eval: step: 5808; epoch: 11; total_loss: 1.0286766; min_total_loss: 1.0023108; since_best: 3; accuracy: 0.7535911602209945; Dice: 0.8548840333571784; \nFastEstimator-Train: step: 6000; total_loss: 0.7263755; steps/sec: 3.14; resunet50_lr: 5.7556244e-05; \nFastEstimator-Train: step: 6336; epoch: 12; epoch_time: 168.1 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 6336; epoch: 12; total_loss: 0.9133773; min_total_loss: 0.9133773; since_best: 0; accuracy: 0.7701657458563536; Dice: 0.8557864413210411; \nFastEstimator-Train: step: 6500; total_loss: 0.054296676; steps/sec: 3.14; resunet50_lr: 5.1689763e-05; \nFastEstimator-Train: step: 6864; epoch: 13; epoch_time: 168.02 sec; \nFastEstimator-Eval: step: 6864; epoch: 13; total_loss: 0.9712434; min_total_loss: 0.9133773; since_best: 1; accuracy: 0.7558011049723757; Dice: 0.8675903102123168; \nFastEstimator-Train: step: 7000; total_loss: -0.041223913; steps/sec: 3.14; resunet50_lr: 4.5806453e-05; \nFastEstimator-Train: step: 7392; epoch: 14; epoch_time: 168.12 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 7392; epoch: 14; total_loss: 0.9070398; min_total_loss: 0.9070398; since_best: 0; accuracy: 0.7668508287292818; Dice: 0.8670388874359952; \nFastEstimator-Train: step: 7500; total_loss: -0.053999424; steps/sec: 3.14; resunet50_lr: 3.998953e-05; \nFastEstimator-Train: step: 7920; epoch: 15; epoch_time: 168.13 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 7920; epoch: 15; total_loss: 0.7376718; min_total_loss: 0.7376718; since_best: 0; accuracy: 0.8077348066298342; Dice: 0.8687608480992627; \nFastEstimator-Train: step: 8000; total_loss: -0.053805795; steps/sec: 3.14; resunet50_lr: 3.432127e-05; \nFastEstimator-Train: step: 8448; epoch: 16; epoch_time: 168.08 sec; \nFastEstimator-Eval: step: 8448; epoch: 16; total_loss: 0.7989601; min_total_loss: 0.7376718; since_best: 1; accuracy: 0.7790055248618785; Dice: 0.8679111249415221; \nFastEstimator-Train: step: 8500; total_loss: 0.09030403; steps/sec: 3.14; resunet50_lr: 2.8881845e-05; \nFastEstimator-Train: step: 8976; epoch: 17; epoch_time: 168.27 sec; \nFastEstimator-Eval: step: 8976; epoch: 17; total_loss: 0.78494877; min_total_loss: 0.7376718; since_best: 2; accuracy: 0.7977900552486188; Dice: 0.8651717848164342; \nFastEstimator-Train: step: 9000; total_loss: -0.03528821; steps/sec: 3.14; resunet50_lr: 2.374819e-05; \nFastEstimator-Train: step: 9500; total_loss: -0.08958718; steps/sec: 3.15; resunet50_lr: 1.8992921e-05; \nFastEstimator-Train: step: 9504; epoch: 18; epoch_time: 168.3 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 9504; epoch: 18; total_loss: 0.6974553; min_total_loss: 0.6974553; since_best: 0; accuracy: 0.8121546961325967; Dice: 0.8689569823798876; \nFastEstimator-Train: step: 10000; total_loss: -0.09731047; steps/sec: 3.14; resunet50_lr: 1.4683296e-05; \nFastEstimator-Train: step: 10032; epoch: 19; epoch_time: 168.06 sec; \nFastEstimator-Eval: step: 10032; epoch: 19; total_loss: 0.70396554; min_total_loss: 0.6974553; since_best: 1; accuracy: 0.8187845303867404; Dice: 0.8683503213115263; \nFastEstimator-Train: step: 10500; total_loss: -0.11295703; steps/sec: 3.13; resunet50_lr: 1.088027e-05; \nFastEstimator-Train: step: 10560; epoch: 20; epoch_time: 168.39 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 10560; epoch: 20; total_loss: 0.6501377; min_total_loss: 0.6501377; since_best: 0; accuracy: 0.8209944751381215; Dice: 0.8626990602523178; \nFastEstimator-Train: step: 11000; total_loss: -0.13782492; steps/sec: 3.14; resunet50_lr: 7.637635e-06; \nFastEstimator-Train: step: 11088; epoch: 21; epoch_time: 168.3 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11088; epoch: 21; total_loss: 0.606383; min_total_loss: 0.606383; since_best: 0; accuracy: 0.8375690607734807; Dice: 0.8694501119973564; \nFastEstimator-Train: step: 11500; total_loss: -0.1537084; steps/sec: 3.14; resunet50_lr: 5.001254e-06; \nFastEstimator-Train: step: 11616; epoch: 22; epoch_time: 168.09 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11616; epoch: 22; total_loss: 0.6024005; min_total_loss: 0.6024005; since_best: 0; accuracy: 0.8320441988950277; Dice: 0.8696893037950605; \nFastEstimator-Train: step: 12000; total_loss: -0.15201315; steps/sec: 3.14; resunet50_lr: 3.0084182e-06; \nFastEstimator-Train: step: 12144; epoch: 23; epoch_time: 168.18 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12144; epoch: 23; total_loss: 0.5858978; min_total_loss: 0.5858978; since_best: 0; accuracy: 0.8320441988950277; Dice: 0.8712486869963797; \nFastEstimator-Train: step: 12500; total_loss: -0.14848635; steps/sec: 3.14; resunet50_lr: 1.6873145e-06; \nFastEstimator-Train: step: 12672; epoch: 24; epoch_time: 168.18 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12672; epoch: 24; total_loss: 0.581293; min_total_loss: 0.581293; since_best: 0; accuracy: 0.8386740331491712; Dice: 0.8710742498357834; \nFastEstimator-Train: step: 13000; total_loss: -0.13124007; steps/sec: 3.14; resunet50_lr: 1.0566287e-06; \nFastEstimator-Train: step: 13200; epoch: 25; epoch_time: 167.95 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 13200; epoch: 25; total_loss: 0.5735652; min_total_loss: 0.5735652; since_best: 0; accuracy: 0.8353591160220994; Dice: 0.8705187608361179; \nFastEstimator-Finish: step: 13200; total_time: 4575.41 sec; resunet50_lr: 1.0000014e-06; uncertainty_lr: 1e-05; \n</pre> <p>Let's load the model with best loss and check our performance on the test set:</p> In\u00a0[13]: Copied! <pre>fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt'))\nestimator.test()\n</pre> fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt')) estimator.test() <pre>Loaded model weights from /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Test: epoch: 25; accuracy: 0.8342541436464088; Dice: 0.8676798139291644; \n</pre> <p>We randomly select an image from the test dataset and use <code>pipeline.transform</code> to process the image. We generate the results using <code>network.transform</code> and visualize the prediction.</p> In\u00a0[14]: Copied! <pre>data = test_data[np.random.randint(low=0, high=len(test_data))]\nresult = pipeline.transform(data, mode=\"infer\")\n\nimg = np.squeeze(result[\"image\"])\nimg = np.transpose(img, (1, 2, 0))\nmask_gt = np.squeeze(result[\"seg\"])\n</pre> data = test_data[np.random.randint(low=0, high=len(test_data))] result = pipeline.transform(data, mode=\"infer\")  img = np.squeeze(result[\"image\"]) img = np.transpose(img, (1, 2, 0)) mask_gt = np.squeeze(result[\"seg\"]) In\u00a0[15]: Copied! <pre>visualize_image_mask(Minmax(img), mask_gt)\n</pre> visualize_image_mask(Minmax(img), mask_gt) In\u00a0[16]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"])\n])\n\npredictions = network.transform(result, mode=\"infer\")\npredicted_mask = predictions[\"mask_pred\"].numpy() \npred_mask = np.squeeze(predicted_mask)\npred_mask = np.round(pred_mask).astype(mask_gt.dtype)\n\nvisualize_image_mask(Minmax(img), pred_mask)\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"]) ])  predictions = network.transform(result, mode=\"infer\") predicted_mask = predictions[\"mask_pred\"].numpy()  pred_mask = np.squeeze(predicted_mask) pred_mask = np.round(pred_mask).astype(mask_gt.dtype)  visualize_image_mask(Minmax(img), pred_mask)"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#multi-task-learning-using-uncertainty-weighted-loss", "title": "Multi-Task Learning using Uncertainty Weighted Loss\u00b6", "text": "<p>Multi-task learning is popular in many deep learning applications. For example, in object detection the network performs both classification and localization for each object. As a result, the final loss will be a combination of classification loss and regression loss. The most frequent way of combining two losses is by simply adding them together:</p> <p>$loss_{total} = loss_1 + loss_2$</p> <p>However, a problem emerges when the two losses are on different numerical scales. To resolve this issue, people usually manually design/experimentally determine the best weight, which is very time consuming and computationally expensive:</p> <p>$loss_{total} = w_1loss_1 + w_2loss_2$</p> <p>This paper presents an interesting idea: make the weights w1 and w2 trainable parameters based on the uncertainty of each task, such that the network can dynamically focus more on the task with higher uncertainty.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#dataset", "title": "Dataset\u00b6", "text": "<p>We will use the CUB200 2010 dataset by Caltech. It contains 6033 bird images from 200 categories, where each image also has a corresponding mask. Therefore, our task is to classify and segment the bird given the image.</p> <p>We use a FastEstimator API to load the CUB200 dataset and split the dataset to get train, evaluation and test sets.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We read the images with <code>ReadImage</code>, and the masks stored in a MAT file with <code>ReadMat</code>. There is other information stored in the MAT file, so we specify the key <code>seg</code> to retrieve the mask only.</p> <p>Here the main task is to resize the images and masks into 512 by 512 pixels. We use <code>LongestMaxSize</code> (to preserve the aspect ratio) and <code>PadIfNeeded</code> to resize the image. We will augment both image and mask in the same way and rescale the image pixel values between -1 and 1 since we are using pre-trained ImageNet weights.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#lets-visualize-our-pipeline-results", "title": "Let's visualize our <code>Pipeline</code> results\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>In this implementation, the network architecture is not the focus. Therefore, we are going to create something out of the blue :). How about a combination of resnet50 and Unet that can do both classification and segmentation? We can call it - ResUnet50</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": "<p>We will have four different traces to control/monitor the training: <code>Dice</code> and <code>Accuracy</code> will be used to measure segmentation and classification results, <code>BestModelSaver</code> will save the model with best loss, and <code>LRScheduler</code> will apply a cosine learning rate decay throughout the training loop.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#training-and-testing", "title": "Training and Testing\u00b6", "text": "<p>The whole training (25 epochs) will take about 1 hour 20 mins on single V100 GPU. We are going to reach ~0.87 dice and ~83% accuracy by the end of the training.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-ground-truth", "title": "Visualize Ground Truth\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-prediction", "title": "Visualize Prediction\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html", "title": "One-Shot Learning using a Siamese Network in FastEstimator", "text": "<p>This notebook demonstrates how to perform one-shot learning using a Siamese Network in FastEstimator.</p> <p>In one-shot learning we classify based on only a single example of each class. This ability to learn from very little data could be useful in many machine learning problems. The details of the method are presented in Siamese neural networks for one-shot image recognition.</p> <p>We will use the Omniglot dataset for training and evaluation. The Omniglot dataset consists of 50 different alphabets split into background (30 alphabets) and evaluation (20 alphabets) sets. Each alphabet has a number of characters, with 20 images for each character.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport os\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\nimport fastestimator as fe\n\nfrom matplotlib import pyplot as plt\n</pre> import tempfile  import os import cv2 import numpy as np  import tensorflow as tf import fastestimator as fe  from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs = 200\nbatch_size = 128\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> # Parameters epochs = 200 batch_size = 128 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import omniglot\n\ntrain_data, eval_data = omniglot.load_data(root_dir=data_dir)\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import omniglot  train_data, eval_data = omniglot.load_data(root_dir=data_dir) test_data = eval_data.split(0.5) <p>For training, batches of data are created with half of the batch consisting of image pairs drawn from the same character, and the other half consisting of image pairs drawn from different characters. The target label is 1 for image pairs from the same character and 0 otherwise. The aim is to learn to quantify similarity between any given pair of images.</p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),\n        ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_a\",\n                             image_out=\"x_a\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_b\",\n                             image_out=\"x_b\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Minmax(inputs=\"x_a\", outputs=\"x_a\"),\n        Minmax(inputs=\"x_b\", outputs=\"x_b\")\n    ])\n</pre> from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import ShiftScaleRotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),         ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),         Sometimes(             ShiftScaleRotate(image_in=\"x_a\",                              image_out=\"x_a\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Sometimes(             ShiftScaleRotate(image_in=\"x_b\",                              image_out=\"x_b\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Minmax(inputs=\"x_a\", outputs=\"x_a\"),         Minmax(inputs=\"x_b\", outputs=\"x_b\")     ]) <p>We can visualize sample images from the <code>Pipeline</code> using the <code>get_results</code> method:</p> In\u00a0[5]: Copied! <pre>sample_batch = pipeline.get_results()\n\npair1_img_a = sample_batch[\"x_a\"][0]\npair1_img_b = sample_batch[\"x_b\"][0]\n\npair2_img_a = sample_batch[\"x_a\"][1]\npair2_img_b = sample_batch[\"x_b\"][1]\n\nif sample_batch[\"y\"][0] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair1_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair1_img_b))\n\nplt.show()\n    \nif sample_batch[\"y\"][1] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair2_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair2_img_b))\n\nplt.show()\n</pre> sample_batch = pipeline.get_results()  pair1_img_a = sample_batch[\"x_a\"][0] pair1_img_b = sample_batch[\"x_b\"][0]  pair2_img_a = sample_batch[\"x_a\"][1] pair2_img_b = sample_batch[\"x_b\"][1]  if sample_batch[\"y\"][0] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair1_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair1_img_b))  plt.show()      if sample_batch[\"y\"][1] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair2_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair2_img_b))  plt.show() <pre>Image pair from same character\n</pre> <pre>Image pair from different characters\n</pre> <p>Our siamese network has two convolutional arms which accept distinct inputs. However, the weights on both these convolutional arms are shared. Each convolutional arm works as a feature extractor which produces a feature vector. L1 component-wise distance between these vectors is computed which is used to classify whether the image pair belongs to the same or different classes (characters).</p> In\u00a0[6]: Copied! <pre>from tensorflow.python.keras import Model, Sequential, layers\nfrom tensorflow.python.keras.initializers import RandomNormal\nfrom tensorflow.python.keras.regularizers import l2\n    \n\ndef siamese_network(input_shape=(105, 105, 1), classes=1):\n\"\"\"Network Architecture\"\"\"\n    left_input = layers.Input(shape=input_shape)\n    right_input = layers.Input(shape=input_shape)\n\n    #Creating the convnet which shares weights between the left and right legs of Siamese network\n    siamese_convnet = Sequential()\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=64,\n                      kernel_size=10,\n                      strides=1,\n                      input_shape=input_shape,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=7,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=256,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.Flatten())\n\n    siamese_convnet.add(\n        layers.Dense(4096,\n                     activation='sigmoid',\n                     kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                     kernel_regularizer=l2(1e-4),\n                     bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    encoded_left_input = siamese_convnet(left_input)\n    encoded_right_input = siamese_convnet(right_input)\n\n    l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])\n\n    output = layers.Dense(classes,\n                          activation='sigmoid',\n                          kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                          bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)\n\n    return Model(inputs=[left_input, right_input], outputs=output)\n</pre> from tensorflow.python.keras import Model, Sequential, layers from tensorflow.python.keras.initializers import RandomNormal from tensorflow.python.keras.regularizers import l2       def siamese_network(input_shape=(105, 105, 1), classes=1):     \"\"\"Network Architecture\"\"\"     left_input = layers.Input(shape=input_shape)     right_input = layers.Input(shape=input_shape)      #Creating the convnet which shares weights between the left and right legs of Siamese network     siamese_convnet = Sequential()      siamese_convnet.add(         layers.Conv2D(filters=64,                       kernel_size=10,                       strides=1,                       input_shape=input_shape,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=7,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=256,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.Flatten())      siamese_convnet.add(         layers.Dense(4096,                      activation='sigmoid',                      kernel_initializer=RandomNormal(mean=0, stddev=0.2),                      kernel_regularizer=l2(1e-4),                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      encoded_left_input = siamese_convnet(left_input)     encoded_right_input = siamese_convnet(right_input)      l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])      output = layers.Dense(classes,                           activation='sigmoid',                           kernel_initializer=RandomNormal(mean=0, stddev=0.2),                           bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)      return Model(inputs=[left_input, right_input], outputs=output) <p>We now prepare the <code>model</code> and define a <code>Network</code> object.</p> In\u00a0[7]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nmodel = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   model = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")  network = fe.Network(ops=[     ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>LRScheduler with a constant decay schedule as described in the paper.</li> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>EarlyStopping for stopping training if the monitored metric doesn't improve within a specified number of epochs.</li> <li>A custom trace to calculate one shot classification accuracy as described in the paper. This trace performs a 20-way within-alphabet classification task in which an alphabet is first chosen from among those reserved for the evaluation set. Then, nineteen other characters are taken uniformly at random from the alphabet. The first character's image is compared with another image of the same character and with images of the other nineteen characters. This is called a one-shot trial. The trial is considered a success if the network outputs the highest similarity (probability) score for the image pair belonging to same character.</li> </ol> In\u00a0[8]: Copied! <pre>from fastestimator.backend import feed_forward\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.adapt import EarlyStopping, LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import Data\n\n\ndef lr_schedule(epoch):\n\"\"\"Learning rate schedule\"\"\"\n    lr = 0.0001*np.power(0.99, epoch)\n    return lr\n\n\nclass OneShotAccuracy(Trace):\n\"\"\"Trace for calculating one shot accuracy\"\"\"\n    def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):\n\n        super().__init__(mode=mode, outputs=output_name)\n        self.dataset = dataset\n        self.model = model\n        self.total = 0\n        self.correct = 0\n        self.output_name = output_name\n        self.N = N\n        self.trials = trials\n\n    def on_epoch_begin(self, data: Data):\n        self.total = 0\n        self.correct = 0\n\n    def on_epoch_end(self, data: Data):\n        for _ in range(self.trials):\n            img_path = self.dataset.one_shot_trial(self.N)\n            input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                                  dtype=np.float32),\n                         np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                                  dtype=np.float32))\n            prediction_score = feed_forward(self.model, input_img, training=False).numpy()\n\n            if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:\n                self.correct += 1\n\n            self.total += 1\n\n        data.write_with_log(self.outputs[0], self.correct / self.total)\n\n        \ntraces = [\n    LRScheduler(model=model, lr_fn=lr_schedule),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),\n    EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\")\n]\n</pre> from fastestimator.backend import feed_forward from fastestimator.trace import Trace from fastestimator.trace.adapt import EarlyStopping, LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import Data   def lr_schedule(epoch):     \"\"\"Learning rate schedule\"\"\"     lr = 0.0001*np.power(0.99, epoch)     return lr   class OneShotAccuracy(Trace):     \"\"\"Trace for calculating one shot accuracy\"\"\"     def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):          super().__init__(mode=mode, outputs=output_name)         self.dataset = dataset         self.model = model         self.total = 0         self.correct = 0         self.output_name = output_name         self.N = N         self.trials = trials      def on_epoch_begin(self, data: Data):         self.total = 0         self.correct = 0      def on_epoch_end(self, data: Data):         for _ in range(self.trials):             img_path = self.dataset.one_shot_trial(self.N)             input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                                   dtype=np.float32),                          np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                                   dtype=np.float32))             prediction_score = feed_forward(self.model, input_img, training=False).numpy()              if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:                 self.correct += 1              self.total += 1          data.write_with_log(self.outputs[0], self.correct / self.total)           traces = [     LRScheduler(model=model, lr_fn=lr_schedule),     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),     BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),     EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\") ] In\u00a0[9]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[10]: Copied! <pre># Training\nestimator.fit()\n</pre> # Training estimator.fit() <p>Now, we can load the best model to check its one-shot accuracy on the test set:</p> In\u00a0[11]: Copied! <pre># Testing\nfe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5'))\nestimator.test()\n</pre> # Testing fe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5')) estimator.test() <pre>Loaded model weights from /tmp/tmptw2czltd/siamese_net_best_one_shot_accuracy.h5\nFastEstimator-Test: epoch: 110; accuracy: 0.9256060606060607; one_shot_accuracy: 0.7875; \n</pre> <p>Let's perform inferencing on some elements in the test dataset. Here, we generate a 5-way one shot trial for demo purposes.</p> In\u00a0[12]: Copied! <pre>#Generating one-shot trial set for 5-way one shot trial\nimg_path = test_data.one_shot_trial(5)\ninput_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                      dtype=np.float32),\n             np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                      dtype=np.float32))\n\nprediction_score = feed_forward(model, input_img, training=False).numpy()\n</pre> #Generating one-shot trial set for 5-way one shot trial img_path = test_data.one_shot_trial(5) input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                       dtype=np.float32),              np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                       dtype=np.float32))  prediction_score = feed_forward(model, input_img, training=False).numpy() <p>The test image is predicted to be belonging to the class with the maximum similarity.</p> In\u00a0[13]: Copied! <pre>plt.figure(figsize=(4, 4))\nplt.imshow(np.squeeze(input_img[0][0]));\nplt.title('test image')\nplt.axis('off');\nplt.show()\n\nplt.figure(figsize=(18, 18))\nplt.subplot(151)\nplt.imshow(np.squeeze(input_img[1][0]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0]))\nplt.axis('off');\n\nplt.subplot(152)\nplt.imshow(np.squeeze(input_img[1][1]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0]))\nplt.axis('off');\n\nplt.subplot(153)\nplt.imshow(np.squeeze(input_img[1][2]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0]))\nplt.axis('off');\n\nplt.subplot(154)\nplt.imshow(np.squeeze(input_img[1][3]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0]))\nplt.axis('off');\n\nplt.subplot(155)\nplt.imshow(np.squeeze(input_img[1][4]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0]))\nplt.axis('off');\n\nplt.tight_layout()\n</pre> plt.figure(figsize=(4, 4)) plt.imshow(np.squeeze(input_img[0][0])); plt.title('test image') plt.axis('off'); plt.show()  plt.figure(figsize=(18, 18)) plt.subplot(151) plt.imshow(np.squeeze(input_img[1][0])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0])) plt.axis('off');  plt.subplot(152) plt.imshow(np.squeeze(input_img[1][1])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0])) plt.axis('off');  plt.subplot(153) plt.imshow(np.squeeze(input_img[1][2])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0])) plt.axis('off');  plt.subplot(154) plt.imshow(np.squeeze(input_img[1][3])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0])) plt.axis('off');  plt.subplot(155) plt.imshow(np.squeeze(input_img[1][4])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0])) plt.axis('off');  plt.tight_layout()"}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#one-shot-learning-using-a-siamese-network-in-fastestimator", "title": "One-Shot Learning using a Siamese Network in FastEstimator\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#training-and-testing", "title": "Training and Testing\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html", "title": "Lung Segmentation Using the Montgomery Dataset", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\nfrom typing import Any, Dict, List\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt\n\nimport fastestimator as fe\nfrom fastestimator.architecture.pytorch import UNet\nfrom fastestimator.dataset.data import montgomery\nfrom fastestimator.op.numpyop import Delete, NumpyOp\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Dice\n</pre> import os import tempfile from typing import Any, Dict, List  import cv2 import numpy as np import pandas as pd import torch from matplotlib import pyplot as plt  import fastestimator as fe from fastestimator.architecture.pytorch import UNet from fastestimator.dataset.data import montgomery from fastestimator.op.numpyop import Delete, NumpyOp from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Dice In\u00a0[2]: Copied! <pre>pd.set_option('display.max_colwidth', 500)\n</pre> pd.set_option('display.max_colwidth', 500) In\u00a0[3]: parameters Copied! <pre>batch_size = 4\nepochs = 25\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> batch_size = 4 epochs = 25 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We download the Montgomery data first:</p> In\u00a0[4]: Copied! <pre>csv = montgomery.load_data(root_dir=data_dir)\n</pre> csv = montgomery.load_data(root_dir=data_dir) <p>This creates a <code>CSVDataset</code>. Let's see what is inside:</p> In\u00a0[5]: Copied! <pre>df = pd.DataFrame.from_dict(csv.data, orient='index')\n</pre> df = pd.DataFrame.from_dict(csv.data, orient='index') In\u00a0[6]: Copied! <pre>df.head()\n</pre> df.head() Out[6]: image mask_left mask_right 0 MontgomerySet/CXR_png/MCUCXR_0243_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0243_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0243_1.png 1 MontgomerySet/CXR_png/MCUCXR_0022_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0022_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0022_0.png 2 MontgomerySet/CXR_png/MCUCXR_0086_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0086_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0086_0.png 3 MontgomerySet/CXR_png/MCUCXR_0008_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0008_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0008_0.png 4 MontgomerySet/CXR_png/MCUCXR_0094_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0094_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0094_0.png <p>Now let's set the stage for training:</p> In\u00a0[7]: Copied! <pre>class CombineLeftRightMask(NumpyOp):    \n    def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n        mask_left, mask_right = data\n        data = mask_left + mask_right\n        return data\n</pre> class CombineLeftRightMask(NumpyOp):         def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:         mask_left, mask_right = data         data = mask_left + mask_right         return data In\u00a0[8]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=csv,\n    eval_data=csv.split(0.2),\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),\n        ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),\n        ReadImage(inputs=\"mask_right\",\n                  parent_path=csv.parent_path,\n                  outputs=\"mask_right\",\n                  color_flag=\"gray\",\n                  mode='!infer'),\n        CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),\n        Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),\n        Resize(image_in=\"image\", width=512, height=512),\n        Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),\n        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),\n        Sometimes(numpy_op=Rotate(\n            image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),\n        Minmax(inputs=\"image\", outputs=\"image\"),\n        Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),\n        Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),\n        Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=csv,     eval_data=csv.split(0.2),     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),         ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),         ReadImage(inputs=\"mask_right\",                   parent_path=csv.parent_path,                   outputs=\"mask_right\",                   color_flag=\"gray\",                   mode='!infer'),         CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),         Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),         Resize(image_in=\"image\", width=512, height=512),         Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),         Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),         Sometimes(numpy_op=Rotate(             image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),         Minmax(inputs=\"image\", outputs=\"image\"),         Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),         Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),         Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')     ]) <p>Let's see if the <code>Pipeline</code> output is reasonable. We call <code>get_results</code> to get outputs from <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre>batch_data = pipeline.get_results()\n</pre> batch_data = pipeline.get_results() In\u00a0[10]: Copied! <pre>batch_index = 1\nfig, ax = plt.subplots(1, 2, figsize=(15,6))\nax[0].imshow(np.squeeze(batch_data['image'][batch_index]), cmap='gray')\nax[1].imshow(np.squeeze(batch_data['mask'][batch_index]), cmap='gray')\n</pre> batch_index = 1 fig, ax = plt.subplots(1, 2, figsize=(15,6)) ax[0].imshow(np.squeeze(batch_data['image'][batch_index]), cmap='gray') ax[1].imshow(np.squeeze(batch_data['mask'][batch_index]), cmap='gray') Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x7f62440ace80&gt;</pre> In\u00a0[11]: Copied! <pre>model = fe.build(\n    model_fn=lambda: UNet(input_size=(1, 512, 512)),\n    optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n    model_name=\"lung_segmentation\"\n)\n</pre> model = fe.build(     model_fn=lambda: UNet(input_size=(1, 512, 512)),     optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),     model_name=\"lung_segmentation\" ) In\u00a0[12]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),\n    CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),     CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) In\u00a0[13]: Copied! <pre>traces = [\n    Dice(true_key=\"mask\", pred_key=\"pred_segment\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max')\n]\n</pre> traces = [     Dice(true_key=\"mask\", pred_key=\"pred_segment\"),     BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max') ] In\u00a0[14]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         log_steps=20,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          log_steps=20,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[15]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 0; lung_segmentation_lr: 0.0001; \nFastEstimator-Train: step: 0; loss: 0.65074736; \nFastEstimator-Train: step: 20; loss: 0.32184097; steps/sec: 2.84; \nFastEstimator-Train: step: 28; epoch: 0; epoch_time: 12.08 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 28; epoch: 0; loss: 0.6737924; min_loss: 0.6737924; since_best: 0; dice: 1.7335249448973935e-13; \nFastEstimator-Train: step: 40; loss: 0.3740636; steps/sec: 2.14; \nFastEstimator-Train: step: 56; epoch: 1; epoch_time: 12.1 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 56; epoch: 1; loss: 0.32339695; min_loss: 0.32339695; since_best: 0; dice: 0.07768369491807968; \nFastEstimator-Train: step: 60; loss: 0.44676578; steps/sec: 2.16; \nFastEstimator-Train: step: 80; loss: 0.14473557; steps/sec: 2.83; \nFastEstimator-Train: step: 84; epoch: 2; epoch_time: 12.12 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 84; epoch: 2; loss: 0.12985013; min_loss: 0.12985013; since_best: 0; dice: 0.8881443049809153; \nFastEstimator-Train: step: 100; loss: 0.1680123; steps/sec: 2.12; \nFastEstimator-Train: step: 112; epoch: 3; epoch_time: 12.24 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 112; epoch: 3; loss: 0.12533109; min_loss: 0.12533109; since_best: 0; dice: 0.8912838752069556; \nFastEstimator-Train: step: 120; loss: 0.09625991; steps/sec: 2.11; \nFastEstimator-Train: step: 140; epoch: 4; epoch_time: 12.45 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 140; epoch: 4; loss: 0.1327874; min_loss: 0.12533109; since_best: 1; dice: 0.9037457108044139; \nFastEstimator-Train: step: 140; loss: 0.100700244; steps/sec: 2.11; \nFastEstimator-Train: step: 160; loss: 0.076236516; steps/sec: 2.78; \nFastEstimator-Train: step: 168; epoch: 5; epoch_time: 12.42 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 168; epoch: 5; loss: 0.12693708; min_loss: 0.12533109; since_best: 2; dice: 0.9092143670270832; \nFastEstimator-Train: step: 180; loss: 0.074325085; steps/sec: 2.11; \nFastEstimator-Train: step: 196; epoch: 6; epoch_time: 12.26 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 196; epoch: 6; loss: 0.08061478; min_loss: 0.08061478; since_best: 0; dice: 0.9364716513786071; \nFastEstimator-Train: step: 200; loss: 0.050859306; steps/sec: 2.04; \nFastEstimator-Train: step: 220; loss: 0.0795434; steps/sec: 2.81; \nFastEstimator-Train: step: 224; epoch: 7; epoch_time: 12.73 sec; \nFastEstimator-Eval: step: 224; epoch: 7; loss: 0.077932164; min_loss: 0.077932164; since_best: 0; dice: 0.9363544687013216; \nFastEstimator-Train: step: 240; loss: 0.04766827; steps/sec: 2.09; \nFastEstimator-Train: step: 252; epoch: 8; epoch_time: 12.39 sec; \nFastEstimator-Eval: step: 252; epoch: 8; loss: 0.08354305; min_loss: 0.077932164; since_best: 1; dice: 0.9357203667668574; \nFastEstimator-Train: step: 260; loss: 0.07521939; steps/sec: 1.98; \nFastEstimator-Train: step: 280; epoch: 9; epoch_time: 12.96 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 280; epoch: 9; loss: 0.07668398; min_loss: 0.07668398; since_best: 0; dice: 0.9423949873504999; \nFastEstimator-Train: step: 280; loss: 0.040693548; steps/sec: 2.13; \nFastEstimator-Train: step: 300; loss: 0.056012712; steps/sec: 2.8; \nFastEstimator-Train: step: 308; epoch: 10; epoch_time: 12.25 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 308; epoch: 10; loss: 0.07509731; min_loss: 0.07509731; since_best: 0; dice: 0.9461569423558578; \nFastEstimator-Train: step: 320; loss: 0.04584109; steps/sec: 1.99; \nFastEstimator-Train: step: 336; epoch: 11; epoch_time: 12.89 sec; \nFastEstimator-Eval: step: 336; epoch: 11; loss: 0.071265906; min_loss: 0.071265906; since_best: 0; dice: 0.9425017790028412; \nFastEstimator-Train: step: 340; loss: 0.06591544; steps/sec: 2.06; \nFastEstimator-Train: step: 360; loss: 0.039461873; steps/sec: 2.77; \nFastEstimator-Train: step: 364; epoch: 12; epoch_time: 12.69 sec; \nFastEstimator-Eval: step: 364; epoch: 12; loss: 0.06946295; min_loss: 0.06946295; since_best: 0; dice: 0.9453714568046383; \nFastEstimator-Train: step: 380; loss: 0.053401247; steps/sec: 2.03; \nFastEstimator-Train: step: 392; epoch: 13; epoch_time: 12.69 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 392; epoch: 13; loss: 0.06035184; min_loss: 0.06035184; since_best: 0; dice: 0.9530510743823034; \nFastEstimator-Train: step: 400; loss: 0.032455094; steps/sec: 2.12; \nFastEstimator-Train: step: 420; epoch: 14; epoch_time: 12.33 sec; \nFastEstimator-Eval: step: 420; epoch: 14; loss: 0.072278894; min_loss: 0.06035184; since_best: 1; dice: 0.9376849732175224; \nFastEstimator-Train: step: 420; loss: 0.053798117; steps/sec: 2.07; \nFastEstimator-Train: step: 440; loss: 0.038134858; steps/sec: 2.77; \nFastEstimator-Train: step: 448; epoch: 15; epoch_time: 12.62 sec; \nFastEstimator-Eval: step: 448; epoch: 15; loss: 0.06692966; min_loss: 0.06035184; since_best: 2; dice: 0.9515013302881467; \nFastEstimator-Train: step: 460; loss: 0.039841093; steps/sec: 2.09; \nFastEstimator-Train: step: 476; epoch: 16; epoch_time: 12.42 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 476; epoch: 16; loss: 0.062103588; min_loss: 0.06035184; since_best: 3; dice: 0.9542675866982232; \nFastEstimator-Train: step: 480; loss: 0.029947285; steps/sec: 2.14; \nFastEstimator-Train: step: 500; loss: 0.027964272; steps/sec: 2.78; \nFastEstimator-Train: step: 504; epoch: 17; epoch_time: 12.24 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 504; epoch: 17; loss: 0.05789433; min_loss: 0.05789433; since_best: 0; dice: 0.9561937779850502; \nFastEstimator-Train: step: 520; loss: 0.030686911; steps/sec: 2.12; \nFastEstimator-Train: step: 532; epoch: 18; epoch_time: 12.29 sec; \nFastEstimator-Eval: step: 532; epoch: 18; loss: 0.05837614; min_loss: 0.05789433; since_best: 1; dice: 0.9551540080564349; \nFastEstimator-Train: step: 540; loss: 0.02688715; steps/sec: 2.12; \nFastEstimator-Train: step: 560; epoch: 19; epoch_time: 12.26 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 560; epoch: 19; loss: 0.05515228; min_loss: 0.05515228; since_best: 0; dice: 0.9569575317963865; \nFastEstimator-Train: step: 560; loss: 0.045194946; steps/sec: 2.12; \nFastEstimator-Train: step: 580; loss: 0.029270299; steps/sec: 2.81; \nFastEstimator-Train: step: 588; epoch: 20; epoch_time: 12.29 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 588; epoch: 20; loss: 0.054214593; min_loss: 0.054214593; since_best: 0; dice: 0.9588091257130298; \nFastEstimator-Train: step: 600; loss: 0.03187306; steps/sec: 2.15; \nFastEstimator-Train: step: 616; epoch: 21; epoch_time: 12.14 sec; \nFastEstimator-Eval: step: 616; epoch: 21; loss: 0.056209736; min_loss: 0.054214593; since_best: 1; dice: 0.9565499194603396; \nFastEstimator-Train: step: 620; loss: 0.03272804; steps/sec: 2.16; \nFastEstimator-Train: step: 640; loss: 0.034923207; steps/sec: 2.82; \nFastEstimator-Train: step: 644; epoch: 22; epoch_time: 12.12 sec; \nFastEstimator-Eval: step: 644; epoch: 22; loss: 0.05843257; min_loss: 0.054214593; since_best: 2; dice: 0.9549386241705727; \nFastEstimator-Train: step: 660; loss: 0.03908976; steps/sec: 2.14; \nFastEstimator-Train: step: 672; epoch: 23; epoch_time: 12.16 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 672; epoch: 23; loss: 0.05370887; min_loss: 0.05370887; since_best: 0; dice: 0.9596281608464776; \nFastEstimator-Train: step: 680; loss: 0.031742807; steps/sec: 2.15; \nFastEstimator-Train: step: 700; epoch: 24; epoch_time: 12.16 sec; \nFastEstimator-Eval: step: 700; epoch: 24; loss: 0.054277744; min_loss: 0.05370887; since_best: 1; dice: 0.9582266396901049; \nFastEstimator-Finish: step: 700; total_time: 392.64 sec; lung_segmentation_lr: 0.0001; \n</pre> <p>Let's visualize the prediction from the neural network. We select a random image from the dataset:</p> In\u00a0[16]: Copied! <pre>image_path = df['image'].sample(random_state=3).values[0]\n</pre> image_path = df['image'].sample(random_state=3).values[0] <p>We create a data dict, and call <code>Pipeline.transform()</code>.</p> In\u00a0[17]: Copied! <pre>data = {'image': image_path}\ndata = pipeline.transform(data, mode=\"infer\")\n</pre> data = {'image': image_path} data = pipeline.transform(data, mode=\"infer\") <p>After the <code>Pipeline</code>, we rebuild our model by providing the trained weights path and pass it to a new <code>Network</code>:</p> In\u00a0[18]: Copied! <pre>weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path\n\nmodel = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),\n                 optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n                 model_name=\"lung_segmentation\",\n                 weights_path=weights_path)\n</pre> weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path  model = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),                  optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),                  model_name=\"lung_segmentation\",                  weights_path=weights_path) <pre>Loaded model weights from /tmp/tmpsqurxgnr/lung_segmentation_best_dice.pt\n</pre> In\u00a0[19]: Copied! <pre>network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")])\n</pre> network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")]) <p>We call <code>Network.transform()</code> to get outputs from our <code>Network</code>:</p> In\u00a0[20]: Copied! <pre>pred = network.transform(data, mode=\"infer\")\n</pre> pred = network.transform(data, mode=\"infer\") In\u00a0[21]: Copied! <pre>img = np.squeeze(pred['image'].numpy())\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\nimg_rgb = (img_rgb * 255).astype(np.uint8)\n</pre> img = np.squeeze(pred['image'].numpy()) img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) img_rgb = (img_rgb * 255).astype(np.uint8) In\u00a0[22]: Copied! <pre>mask = pred['pred_segment'].numpy()\nmask = np.squeeze(mask)\nmask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\nmask_rgb = (mask_rgb * 255).astype(np.uint8)\n</pre> mask = pred['pred_segment'].numpy() mask = np.squeeze(mask) mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) mask_rgb = (mask_rgb * 255).astype(np.uint8) In\u00a0[23]: Copied! <pre>_, mask_thres = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY)\nmask_overlay = mask_rgb * np.expand_dims(mask_thres, axis=-1)\nmask_overlay = np.where(mask_overlay != [0, 0, 0], [255, 0, 0], [0, 0, 0])\nmask_overlay = mask_overlay.astype(np.uint8)\nimg_with_mask = cv2.addWeighted(img_rgb, 0.7, mask_overlay, 0.3, 0)\nmaskgt_with_maskpred = cv2.addWeighted(mask_rgb, 0.7, mask_overlay, 0.3, 0)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\nax[0].imshow(img_rgb)\nax[0].set_title('original lung')\nax[1].imshow(img_with_mask)\nax[1].set_title('predict mask ')\nplt.show()\n</pre> _, mask_thres = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY) mask_overlay = mask_rgb * np.expand_dims(mask_thres, axis=-1) mask_overlay = np.where(mask_overlay != [0, 0, 0], [255, 0, 0], [0, 0, 0]) mask_overlay = mask_overlay.astype(np.uint8) img_with_mask = cv2.addWeighted(img_rgb, 0.7, mask_overlay, 0.3, 0) maskgt_with_maskpred = cv2.addWeighted(mask_rgb, 0.7, mask_overlay, 0.3, 0)  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6)) ax[0].imshow(img_rgb) ax[0].set_title('original lung') ax[1].imshow(img_with_mask) ax[1].set_title('predict mask ') plt.show()"}, {"location": "apphub/semantic_segmentation/unet/unet.html#lung-segmentation-using-the-montgomery-dataset", "title": "Lung Segmentation Using the Montgomery Dataset\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#download-data", "title": "Download Data\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#pass-the-image-through-pipeline-and-network", "title": "Pass the image through <code>Pipeline</code> and <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#visualize-outputs", "title": "Visualize Outputs\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html", "title": "Fast Style Transfer with FastEstimator", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport cv2\nimport tensorflow as tf\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage\nfrom fastestimator.trace.io import ModelSaver\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n</pre> import tempfile import os import cv2 import tensorflow as tf import numpy as np  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import Normalize, ReadImage from fastestimator.trace.io import ModelSaver  import matplotlib from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre>#Parameters\nbatch_size = 4\nepochs = 2\nmax_train_steps_per_epoch = None\nlog_steps = 2000\nstyle_weight=5.0\ncontent_weight=1.0\ntv_weight=1e-4\nsave_dir = tempfile.mkdtemp()\nstyle_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg'\ntest_img_path = 'panda.jpeg'\ndata_dir = None\n</pre> #Parameters batch_size = 4 epochs = 2 max_train_steps_per_epoch = None log_steps = 2000 style_weight=5.0 content_weight=1.0 tv_weight=1e-4 save_dir = tempfile.mkdtemp() style_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg' test_img_path = 'panda.jpeg' data_dir = None <p>In this notebook we will use Vassily Kandinsky's Composition 7 as a style image. We will also resize the style image to $256 \\times 256$ to make the dimension consistent with that of COCO images.</p> In\u00a0[3]: Copied! <pre>style_img = cv2.imread(style_img_path)\nassert style_img is not None, \"cannot load the style image, please go to the folder with style image\"\nstyle_img = cv2.resize(style_img, (256, 256))\nstyle_img = (style_img.astype(np.float32) - 127.5) / 127.5\n\nstyle_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB)\nplt.imshow(style_img_disp)\nplt.title('Vassily Kandinsky\\'s Composition 7')\nplt.axis('off');\n</pre> style_img = cv2.imread(style_img_path) assert style_img is not None, \"cannot load the style image, please go to the folder with style image\" style_img = cv2.resize(style_img, (256, 256)) style_img = (style_img.astype(np.float32) - 127.5) / 127.5  style_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB) plt.imshow(style_img_disp) plt.title('Vassily Kandinsky\\'s Composition 7') plt.axis('off'); In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\ntrain_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False)\n</pre> from fastestimator.dataset.data import mscoco train_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False) In\u00a0[5]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        Resize(height=256, width=256, image_in=\"image\", image_out=\"image\"),\n        LambdaOp(fn=lambda: style_img, outputs=\"style_image\"),\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         Resize(height=256, width=256, image_in=\"image\", image_out=\"image\"),         LambdaOp(fn=lambda: style_img, outputs=\"style_image\"),     ]) <p>We can visualize sample images from our <code>Pipeline</code> using the 'get_results' method:</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef Minmax(data):\n    data_max = np.max(data)\n    data_min = np.min(data)\n    data = (data - data_min) / max((data_max - data_min), 1e-7)\n    return data\n\nsample_batch = pipeline.get_results()\nimg = Minmax(sample_batch[\"image\"][0].numpy())\nplt.imshow(img)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def Minmax(data):     data_max = np.max(data)     data_min = np.min(data)     data = (data - data_min) / max((data_max - data_min), 1e-7)     return data  sample_batch = pipeline.get_results() img = Minmax(sample_batch[\"image\"][0].numpy()) plt.imshow(img) plt.show() In\u00a0[7]: Copied! <pre>from typing import Dict, List, Tuple, Union\n\nimport tensorflow as tf\n\nfrom fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D\n\n\ndef _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x0)\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.Add()([x, x0_cropped])\n    return x\n\n\ndef _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    if apply_relu:\n        x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2DTranspose(filters=num_filter,\n                                        kernel_size=kernel_size,\n                                        strides=strides,\n                                        padding=padding,\n                                        kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):\n\"\"\"Creates the Style Transfer Network.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    x = ReflectionPadding2D(padding=(40, 40))(x0)\n    x = _conv_block(x, num_filter=32)\n    x = _downsample(x, num_filter=64)\n    x = _downsample(x, num_filter=128)\n\n    for _ in range(num_resblock):\n        x = _residual_block(x, num_filter=128)\n\n    x = _upsample(x, num_filter=64)\n    x = _upsample(x, num_filter=32)\n    x = _conv_block(x, num_filter=3, apply_relu=False)\n    x = tf.keras.layers.Activation(\"tanh\")(x)\n    return tf.keras.Model(inputs=x0, outputs=x)\n\n\ndef LossNet(input_shape=(256, 256, 3),\n            style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],\n            content_layers=[\"block3_conv3\"]):\n\"\"\"Creates the network to compute the style loss.\n    This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16\n    for each.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)\n    # Compute style loss\n    style_output = [mdl.get_layer(name).output for name in style_layers]\n    content_output = [mdl.get_layer(name).output for name in content_layers]\n    output = {\"style\": style_output, \"content\": content_output}\n    return tf.keras.Model(inputs=x0, outputs=output)\n</pre> from typing import Dict, List, Tuple, Union  import tensorflow as tf  from fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D   def _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):     initializer = tf.random_normal_initializer(0., 0.02)     x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x0)     x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x)      x = InstanceNormalization()(x)     x = tf.keras.layers.Add()([x, x0_cropped])     return x   def _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     if apply_relu:         x = tf.keras.layers.ReLU()(x)     return x   def _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2DTranspose(filters=num_filter,                                         kernel_size=kernel_size,                                         strides=strides,                                         padding=padding,                                         kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):     \"\"\"Creates the Style Transfer Network.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     x = ReflectionPadding2D(padding=(40, 40))(x0)     x = _conv_block(x, num_filter=32)     x = _downsample(x, num_filter=64)     x = _downsample(x, num_filter=128)      for _ in range(num_resblock):         x = _residual_block(x, num_filter=128)      x = _upsample(x, num_filter=64)     x = _upsample(x, num_filter=32)     x = _conv_block(x, num_filter=3, apply_relu=False)     x = tf.keras.layers.Activation(\"tanh\")(x)     return tf.keras.Model(inputs=x0, outputs=x)   def LossNet(input_shape=(256, 256, 3),             style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],             content_layers=[\"block3_conv3\"]):     \"\"\"Creates the network to compute the style loss.     This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16     for each.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)     # Compute style loss     style_output = [mdl.get_layer(name).output for name in style_layers]     content_output = [mdl.get_layer(name).output for name in content_layers]     output = {\"style\": style_output, \"content\": content_output}     return tf.keras.Model(inputs=x0, outputs=output) In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=StyleTransferNet, \n                 model_name=\"style_transfer_net\",\n                 optimizer_fn=lambda: tf.optimizers.Adam(1e-3))\n</pre> model = fe.build(model_fn=StyleTransferNet,                   model_name=\"style_transfer_net\",                  optimizer_fn=lambda: tf.optimizers.Adam(1e-3)) In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass ExtractVGGFeatures(TensorOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.vgg = LossNet()\n\n    def forward(self, data, state):\n        return self.vgg(data)\n\n\nclass StyleContentLoss(TensorOp):\n    def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.style_weight = style_weight\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n        self.average_loss = average_loss\n\n    def calculate_style_recon_loss(self, y_true, y_pred):\n        y_true_gram = self.calculate_gram_matrix(y_true)\n        y_pred_gram = self.calculate_gram_matrix(y_pred)\n        y_diff_gram = y_pred_gram - y_true_gram\n        y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))\n        return y_norm\n\n    def calculate_feature_recon_loss(self, y_true, y_pred):\n        y_diff = y_pred - y_true\n        num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)\n        y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts\n        return y_diff_norm\n\n    def calculate_gram_matrix(self, x):\n        x = tf.cast(x, tf.float32)\n        num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)\n        gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)\n        gram_matrix /= num_elts\n        return gram_matrix\n\n    def calculate_total_variation(self, y_pred):\n        return tf.image.total_variation(y_pred)\n\n    def forward(self, data, state):\n        y_pred, y_style, y_content, image_out = data\n\n        style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]\n        style_loss = tf.add_n(style_loss)\n        style_loss *= self.style_weight\n\n        content_loss = [\n            self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])\n        ]\n        content_loss = tf.add_n(content_loss)\n        content_loss *= self.content_weight\n\n        total_variation_reg = self.calculate_total_variation(image_out)\n        total_variation_reg *= self.tv_weight\n        loss = style_loss + content_loss + total_variation_reg\n\n        if self.average_loss:\n            loss = reduce_mean(loss)\n\n        return loss\n</pre> from fastestimator.op.tensorop import TensorOp  class ExtractVGGFeatures(TensorOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs, outputs, mode)         self.vgg = LossNet()      def forward(self, data, state):         return self.vgg(data)   class StyleContentLoss(TensorOp):     def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.style_weight = style_weight         self.content_weight = content_weight         self.tv_weight = tv_weight         self.average_loss = average_loss      def calculate_style_recon_loss(self, y_true, y_pred):         y_true_gram = self.calculate_gram_matrix(y_true)         y_pred_gram = self.calculate_gram_matrix(y_pred)         y_diff_gram = y_pred_gram - y_true_gram         y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))         return y_norm      def calculate_feature_recon_loss(self, y_true, y_pred):         y_diff = y_pred - y_true         num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)         y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts         return y_diff_norm      def calculate_gram_matrix(self, x):         x = tf.cast(x, tf.float32)         num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)         gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)         gram_matrix /= num_elts         return gram_matrix      def calculate_total_variation(self, y_pred):         return tf.image.total_variation(y_pred)      def forward(self, data, state):         y_pred, y_style, y_content, image_out = data          style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]         style_loss = tf.add_n(style_loss)         style_loss *= self.style_weight          content_loss = [             self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])         ]         content_loss = tf.add_n(content_loss)         content_loss *= self.content_weight          total_variation_reg = self.calculate_total_variation(image_out)         total_variation_reg *= self.tv_weight         loss = style_loss + content_loss + total_variation_reg          if self.average_loss:             loss = reduce_mean(loss)          return loss <p>We now define the <code>Network</code> object:</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),\n    ExtractVGGFeatures(inputs=\"style_image\", outputs=\"y_style\"),\n    ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),\n    ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),\n    StyleContentLoss(style_weight=style_weight,\n                     content_weight=content_weight,\n                     tv_weight=tv_weight,\n                     inputs=('y_pred', 'y_style', 'y_content', 'image_out'),\n                     outputs='loss'),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),     ExtractVGGFeatures(inputs=\"style_image\", outputs=\"y_style\"),     ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),     ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),     StyleContentLoss(style_weight=style_weight,                      content_weight=content_weight,                      tv_weight=tv_weight,                      inputs=('y_pred', 'y_style', 'y_content', 'image_out'),                      outputs='loss'),     UpdateOp(model=model, loss_name=\"loss\") ]) In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),\n                         epochs=epochs,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         log_steps=log_steps)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),                          epochs=epochs,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          log_steps=log_steps) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; style_transfer_net_lr: 0.001; \nFastEstimator-Train: step: 1; loss: 686.76025; \nFastEstimator-Train: step: 2000; loss: 194.85736; steps/sec: 10.82; \nFastEstimator-Train: step: 4000; loss: 172.5411; steps/sec: 10.81; \nFastEstimator-Train: step: 6000; loss: 173.80026; steps/sec: 10.81; \nFastEstimator-Train: step: 8000; loss: 168.31807; steps/sec: 10.81; \nFastEstimator-Train: step: 10000; loss: 169.65088; steps/sec: 10.81; \nFastEstimator-Train: step: 12000; loss: 157.52707; steps/sec: 10.81; \nFastEstimator-Train: step: 14000; loss: 157.95462; steps/sec: 10.82; \nFastEstimator-Train: step: 16000; loss: 156.0791; steps/sec: 10.82; \nFastEstimator-Train: step: 18000; loss: 141.07487; steps/sec: 10.82; \nFastEstimator-Train: step: 20000; loss: 165.1513; steps/sec: 10.82; \nFastEstimator-Train: step: 22000; loss: 142.25858; steps/sec: 10.82; \nFastEstimator-Train: step: 24000; loss: 141.33316; steps/sec: 10.82; \nFastEstimator-Train: step: 26000; loss: 136.16362; steps/sec: 10.82; \nFastEstimator-Train: step: 28000; loss: 159.05832; steps/sec: 10.82; \nFastEstimator-ModelSaver: saved model to /tmp/tmpyby4rzao/style_transfer_net_epoch_1.h5\nFastEstimator-Train: step: 29572; epoch: 1; epoch_time: 2746.03 sec; \nFastEstimator-Train: step: 30000; loss: 140.72166; steps/sec: 10.52; \nFastEstimator-Train: step: 32000; loss: 153.47067; steps/sec: 10.82; \nFastEstimator-Train: step: 34000; loss: 157.17432; steps/sec: 10.82; \nFastEstimator-Train: step: 36000; loss: 145.79706; steps/sec: 10.82; \nFastEstimator-Train: step: 38000; loss: 152.90091; steps/sec: 10.81; \nFastEstimator-Train: step: 40000; loss: 146.31168; steps/sec: 10.81; \nFastEstimator-Train: step: 42000; loss: 148.24469; steps/sec: 10.82; \nFastEstimator-Train: step: 44000; loss: 149.6113; steps/sec: 10.82; \nFastEstimator-Train: step: 46000; loss: 136.75755; steps/sec: 10.82; \nFastEstimator-Train: step: 48000; loss: 149.79001; steps/sec: 10.82; \nFastEstimator-Train: step: 50000; loss: 144.61955; steps/sec: 10.82; \nFastEstimator-Train: step: 52000; loss: 138.5368; steps/sec: 10.82; \nFastEstimator-Train: step: 54000; loss: 144.22594; steps/sec: 10.82; \nFastEstimator-Train: step: 56000; loss: 140.38748; steps/sec: 10.82; \nFastEstimator-Train: step: 58000; loss: 150.7169; steps/sec: 10.82; \nFastEstimator-ModelSaver: saved model to /tmp/tmpyby4rzao/style_transfer_net_epoch_2.h5\nFastEstimator-Train: step: 59144; epoch: 2; epoch_time: 2734.43 sec; \nFastEstimator-Finish: step: 59144; total_time: 5480.64 sec; style_transfer_net_lr: 0.001; \n</pre> In\u00a0[13]: Copied! <pre>data = {\"image\":test_img_path}\nresult = pipeline.transform(data, mode=\"infer\")\ntest_img = np.squeeze(result[\"image\"])\n</pre> data = {\"image\":test_img_path} result = pipeline.transform(data, mode=\"infer\") test_img = np.squeeze(result[\"image\"]) In\u00a0[14]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=model, outputs=\"image_out\")\n])\n\npredictions = network.transform(result, mode=\"infer\")\noutput_img = np.squeeze(predictions[\"image_out\"])\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=model, outputs=\"image_out\") ])  predictions = network.transform(result, mode=\"infer\") output_img = np.squeeze(predictions[\"image_out\"]) In\u00a0[15]: Copied! <pre>output_img_disp = (output_img + 1) * 0.5\ntest_img_disp = (test_img + 1) * 0.5\nplt.figure(figsize=(20,20))\n\nplt.subplot(131)\nplt.imshow(cv2.cvtColor(test_img_disp, cv2.COLOR_BGR2RGB))\nplt.title('Original Image')\nplt.axis('off');\n\nplt.subplot(132)\nplt.imshow(style_img_disp)\nplt.title('Style Image')\nplt.axis('off');\n\nplt.subplot(133)\nplt.imshow(cv2.cvtColor(output_img_disp, cv2.COLOR_BGR2RGB));\nplt.title('Transferred Image')\nplt.axis('off');\n</pre> output_img_disp = (output_img + 1) * 0.5 test_img_disp = (test_img + 1) * 0.5 plt.figure(figsize=(20,20))  plt.subplot(131) plt.imshow(cv2.cvtColor(test_img_disp, cv2.COLOR_BGR2RGB)) plt.title('Original Image') plt.axis('off');  plt.subplot(132) plt.imshow(style_img_disp) plt.title('Style Image') plt.axis('off');  plt.subplot(133) plt.imshow(cv2.cvtColor(output_img_disp, cv2.COLOR_BGR2RGB)); plt.title('Transferred Image') plt.axis('off');"}, {"location": "apphub/style_transfer/fst_coco/fst.html#fast-style-transfer-with-fastestimator", "title": "Fast Style Transfer with FastEstimator\u00b6", "text": "<p>In this notebook we will demonstrate how to do a neural image style transfer with perceptual loss as described in Perceptual Losses for Real-Time Style Transfer and Super-Resolution. Typical neural style transfer involves two images: an image containing semantics that you want to preserve, and another image serving as a reference style. The first image is often referred as the content image and the other image as the style image. In this paper training images from the COCO2014 dataset are used to learn style transfer from any content image.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download training images from the COCO2014 dataset via our dataset API. Downloading the images will take a while.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model is a modified ResNet:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#defining-loss", "title": "Defining Loss\u00b6", "text": "<p>The perceptual loss described in the paper is computed based on intermediate layers of VGG16 pretrained on ImageNet; specifically, <code>relu1_2</code>, <code>relu2_2</code>, <code>relu3_3</code>, and <code>relu4_3</code> of VGG16 are used.</p> <p>The style loss term is computed as the squared l2 norm of the difference in Gram Matrix of these feature maps between an input image and the reference style image.</p> <p>The content loss is simply the l2 norm of the difference in <code>relu3_3</code> of the input image and the reference style image. In addition, the method also uses total variation loss to enforce spatial smoothness in the output image.</p> <p>The final loss is a weighted sum of the style loss term, the content loss term (feature reconstruction term in the paper), and the total variation term.</p> <p>We first define a custom <code>TensorOp</code> that outputs intermediate layers of VGG16. Given these intermediate layers returned by the loss network as a dictionary, we define a custom <code>StyleContentLoss</code> class that encapsulates all the logic of the loss calculation.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-3-estimator", "title": "Step 3: Estimator\u00b6", "text": "<p>We can now define the <code>Estimator</code>. We will use <code>Trace</code> to save intermediate models:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will apply the model to perform style transfer on arbitrary images. Here we use a photo of a panda.</p>"}, {"location": "apphub/tabular/dnn/dnn.html", "title": "Breast Cancer Detection", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import breast_cancer\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  import tensorflow as tf import pandas as pd from sklearn.preprocessing import StandardScaler  import fastestimator as fe from fastestimator.dataset.data import breast_cancer from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre>#training parameters\nbatch_size = 4\nepochs = 10\nsave_dir = tempfile.mkdtemp()\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\n</pre> #training parameters batch_size = 4 epochs = 10 save_dir = tempfile.mkdtemp() max_train_steps_per_epoch = None max_eval_steps_per_epoch = None <p>This downloads some tabular data with different features stored in numerical format in a table. We then split the data into train, evaluation, and testing data sets.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = breast_cancer.load_data()\ntest_data = eval_data.split(0.5)\n</pre> train_data, eval_data = breast_cancer.load_data() test_data = eval_data.split(0.5) <p>This is what the raw data looks like:</p> In\u00a0[4]: Copied! <pre>df = pd.DataFrame.from_dict(train_data.data, orient='index')\ndf.head()\n</pre> df = pd.DataFrame.from_dict(train_data.data, orient='index') df.head() Out[4]: x y 0 [9.029, 17.33, 58.79, 250.5, 0.1066, 0.1413, 0... 1 1 [21.09, 26.57, 142.7, 1311.0, 0.1141, 0.2832, ... 0 2 [9.173, 13.86, 59.2, 260.9, 0.07721, 0.08751, ... 1 3 [10.65, 25.22, 68.01, 347.0, 0.09657, 0.07234,... 1 4 [10.17, 14.88, 64.55, 311.9, 0.1134, 0.08061, ... 1 In\u00a0[5]: Copied! <pre>scaler = StandardScaler()\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\ntest_data[\"x\"] = scaler.transform(test_data[\"x\"])\n</pre> scaler = StandardScaler() train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) test_data[\"x\"] = scaler.transform(test_data[\"x\"]) <p>We create the <code>Pipeline</code> with the usual train, eval, and test data along with the batch size:</p> In\u00a0[6]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size)\n</pre> pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size) <p>We first define the neural network in a function that can then be passed on to the FastEstimator <code>Network</code>:</p> In\u00a0[7]: Copied! <pre>def create_dnn():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    return model\n</pre> def create_dnn():     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(16, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))     return model In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\")\n])\n</pre> model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\") ]) In\u00a0[9]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         log_steps=10,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          log_steps=10,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[10]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \nFastEstimator-Train: step: 1; ce: 0.58930933; \nFastEstimator-Train: step: 10; ce: 1.2191963; steps/sec: 342.02; \nFastEstimator-Train: step: 20; ce: 0.6330318; steps/sec: 422.95; \nFastEstimator-Train: step: 30; ce: 0.68403095; steps/sec: 400.86; \nFastEstimator-Train: step: 40; ce: 0.70622563; steps/sec: 277.93; \nFastEstimator-Train: step: 50; ce: 0.7649698; steps/sec: 443.68; \nFastEstimator-Train: step: 60; ce: 0.70189; steps/sec: 455.18; \nFastEstimator-Train: step: 70; ce: 0.6120157; steps/sec: 486.19; \nFastEstimator-Train: step: 80; ce: 0.6461396; steps/sec: 495.01; \nFastEstimator-Train: step: 90; ce: 0.709924; steps/sec: 397.38; \nFastEstimator-Train: step: 100; ce: 0.69695604; steps/sec: 545.18; \nFastEstimator-Train: step: 110; ce: 0.5406225; steps/sec: 587.24; \nFastEstimator-Train: step: 114; epoch: 1; epoch_time: 3.3 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 114; epoch: 1; ce: 0.49621966; min_ce: 0.49621966; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 120; ce: 0.55340487; steps/sec: 7.94; \nFastEstimator-Train: step: 130; ce: 0.31839967; steps/sec: 308.44; \nFastEstimator-Train: step: 140; ce: 0.16889682; steps/sec: 482.01; \nFastEstimator-Train: step: 150; ce: 0.5158031; steps/sec: 407.94; \nFastEstimator-Train: step: 160; ce: 0.6378304; steps/sec: 386.75; \nFastEstimator-Train: step: 170; ce: 1.1647241; steps/sec: 447.16; \nFastEstimator-Train: step: 180; ce: 0.5274984; steps/sec: 500.64; \nFastEstimator-Train: step: 190; ce: 0.68258667; steps/sec: 481.22; \nFastEstimator-Train: step: 200; ce: 0.35559005; steps/sec: 476.14; \nFastEstimator-Train: step: 210; ce: 0.46034247; steps/sec: 508.85; \nFastEstimator-Train: step: 220; ce: 0.95580393; steps/sec: 548.84; \nFastEstimator-Train: step: 228; epoch: 2; epoch_time: 1.39 sec; \nFastEstimator-Eval: step: 228; epoch: 2; ce: 0.3193086; min_ce: 0.3193086; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 230; ce: 0.33260575; steps/sec: 8.49; \nFastEstimator-Train: step: 240; ce: 0.2510308; steps/sec: 232.08; \nFastEstimator-Train: step: 250; ce: 0.2878321; steps/sec: 666.82; \nFastEstimator-Train: step: 260; ce: 0.1154226; steps/sec: 368.11; \nFastEstimator-Train: step: 270; ce: 0.26300237; steps/sec: 414.99; \nFastEstimator-Train: step: 280; ce: 0.5653368; steps/sec: 421.39; \nFastEstimator-Train: step: 290; ce: 0.5872185; steps/sec: 402.68; \nFastEstimator-Train: step: 300; ce: 0.27621573; steps/sec: 440.24; \nFastEstimator-Train: step: 310; ce: 0.5477217; steps/sec: 481.69; \nFastEstimator-Train: step: 320; ce: 0.4602429; steps/sec: 398.85; \nFastEstimator-Train: step: 330; ce: 0.38244748; steps/sec: 546.57; \nFastEstimator-Train: step: 340; ce: 0.5337428; steps/sec: 571.02; \nFastEstimator-Train: step: 342; epoch: 3; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 342; epoch: 3; ce: 0.18308732; min_ce: 0.18308732; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 350; ce: 0.13466343; steps/sec: 8.53; \nFastEstimator-Train: step: 360; ce: 0.22628057; steps/sec: 368.34; \nFastEstimator-Train: step: 370; ce: 0.5836228; steps/sec: 485.06; \nFastEstimator-Train: step: 380; ce: 0.37300625; steps/sec: 409.87; \nFastEstimator-Train: step: 390; ce: 0.2717349; steps/sec: 413.59; \nFastEstimator-Train: step: 400; ce: 0.07554119; steps/sec: 433.84; \nFastEstimator-Train: step: 410; ce: 0.20552614; steps/sec: 439.36; \nFastEstimator-Train: step: 420; ce: 0.28509304; steps/sec: 448.96; \nFastEstimator-Train: step: 430; ce: 0.32158756; steps/sec: 492.58; \nFastEstimator-Train: step: 440; ce: 1.1102628; steps/sec: 525.49; \nFastEstimator-Train: step: 450; ce: 0.31964102; steps/sec: 548.06; \nFastEstimator-Train: step: 456; epoch: 4; epoch_time: 1.4 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 456; epoch: 4; ce: 0.105911165; min_ce: 0.105911165; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 460; ce: 0.4391592; steps/sec: 8.37; \nFastEstimator-Train: step: 470; ce: 0.29870045; steps/sec: 297.42; \nFastEstimator-Train: step: 480; ce: 0.03247342; steps/sec: 597.74; \nFastEstimator-Train: step: 490; ce: 0.13323224; steps/sec: 393.92; \nFastEstimator-Train: step: 500; ce: 0.58429027; steps/sec: 405.0; \nFastEstimator-Train: step: 510; ce: 0.2376658; steps/sec: 455.97; \nFastEstimator-Train: step: 520; ce: 0.4150503; steps/sec: 424.88; \nFastEstimator-Train: step: 530; ce: 0.22695109; steps/sec: 451.62; \nFastEstimator-Train: step: 540; ce: 0.42051294; steps/sec: 402.12; \nFastEstimator-Train: step: 550; ce: 0.17364319; steps/sec: 389.83; \nFastEstimator-Train: step: 560; ce: 0.06320181; steps/sec: 466.97; \nFastEstimator-Train: step: 570; ce: 0.13996354; steps/sec: 518.8; \nFastEstimator-Train: step: 570; epoch: 5; epoch_time: 1.46 sec; \nFastEstimator-Eval: step: 570; epoch: 5; ce: 0.066059396; min_ce: 0.066059396; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 580; ce: 0.12985338; steps/sec: 8.17; \nFastEstimator-Train: step: 590; ce: 0.6419388; steps/sec: 373.15; \nFastEstimator-Train: step: 600; ce: 0.2857446; steps/sec: 404.92; \nFastEstimator-Train: step: 610; ce: 0.21400735; steps/sec: 381.65; \nFastEstimator-Train: step: 620; ce: 0.27899668; steps/sec: 394.87; \nFastEstimator-Train: step: 630; ce: 0.31599885; steps/sec: 472.31; \nFastEstimator-Train: step: 640; ce: 0.036415085; steps/sec: 457.09; \nFastEstimator-Train: step: 650; ce: 0.10052729; steps/sec: 461.82; \nFastEstimator-Train: step: 660; ce: 0.40688303; steps/sec: 474.46; \nFastEstimator-Train: step: 670; ce: 0.40816957; steps/sec: 517.75; \nFastEstimator-Train: step: 680; ce: 0.40120217; steps/sec: 555.53; \nFastEstimator-Train: step: 684; epoch: 6; epoch_time: 1.44 sec; \nFastEstimator-Eval: step: 684; epoch: 6; ce: 0.04396173; min_ce: 0.04396173; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 690; ce: 0.20741543; steps/sec: 8.33; \nFastEstimator-Train: step: 700; ce: 0.12485474; steps/sec: 324.64; \nFastEstimator-Train: step: 710; ce: 2.8970864e-05; steps/sec: 534.71; \nFastEstimator-Train: step: 720; ce: 0.110491954; steps/sec: 402.98; \nFastEstimator-Train: step: 730; ce: 0.10486858; steps/sec: 432.34; \nFastEstimator-Train: step: 740; ce: 0.2951797; steps/sec: 421.45; \nFastEstimator-Train: step: 750; ce: 0.65293443; steps/sec: 433.71; \nFastEstimator-Train: step: 760; ce: 0.32570755; steps/sec: 461.43; \nFastEstimator-Train: step: 770; ce: 0.35400242; steps/sec: 433.46; \nFastEstimator-Train: step: 780; ce: 0.023054674; steps/sec: 483.0; \nFastEstimator-Train: step: 790; ce: 0.16433364; steps/sec: 540.17; \nFastEstimator-Train: step: 798; epoch: 7; epoch_time: 1.43 sec; \nFastEstimator-Eval: step: 798; epoch: 7; ce: 0.040205613; min_ce: 0.040205613; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 800; ce: 0.42427045; steps/sec: 8.52; \nFastEstimator-Train: step: 810; ce: 0.39827985; steps/sec: 266.34; \nFastEstimator-Train: step: 820; ce: 0.43165076; steps/sec: 775.07; \nFastEstimator-Train: step: 830; ce: 0.06976031; steps/sec: 412.9; \nFastEstimator-Train: step: 840; ce: 0.37039524; steps/sec: 441.05; \nFastEstimator-Train: step: 850; ce: 0.10960688; steps/sec: 418.01; \nFastEstimator-Train: step: 860; ce: 0.0070317476; steps/sec: 450.1; \nFastEstimator-Train: step: 870; ce: 0.020452987; steps/sec: 434.06; \nFastEstimator-Train: step: 880; ce: 0.12914097; steps/sec: 476.49; \nFastEstimator-Train: step: 890; ce: 0.25528443; steps/sec: 466.42; \nFastEstimator-Train: step: 900; ce: 0.18017673; steps/sec: 549.95; \nFastEstimator-Train: step: 910; ce: 0.31777602; steps/sec: 583.67; \nFastEstimator-Train: step: 912; epoch: 8; epoch_time: 1.41 sec; \nFastEstimator-Eval: step: 912; epoch: 8; ce: 0.028554583; min_ce: 0.028554583; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 920; ce: 0.24684253; steps/sec: 8.42; \nFastEstimator-Train: step: 930; ce: 0.19438684; steps/sec: 365.05; \nFastEstimator-Train: step: 940; ce: 0.1568121; steps/sec: 477.85; \nFastEstimator-Train: step: 950; ce: 0.3368427; steps/sec: 371.39; \nFastEstimator-Train: step: 960; ce: 0.20518681; steps/sec: 411.72; \nFastEstimator-Train: step: 970; ce: 0.13320616; steps/sec: 401.91; \nFastEstimator-Train: step: 980; ce: 0.1800138; steps/sec: 470.79; \nFastEstimator-Train: step: 990; ce: 0.10868286; steps/sec: 421.41; \nFastEstimator-Train: step: 1000; ce: 0.040300086; steps/sec: 467.66; \nFastEstimator-Train: step: 1010; ce: 0.42622733; steps/sec: 505.31; \nFastEstimator-Train: step: 1020; ce: 0.06701453; steps/sec: 530.27; \nFastEstimator-Train: step: 1026; epoch: 9; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 1026; epoch: 9; ce: 0.019402837; min_ce: 0.019402837; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 1030; ce: 0.27714887; steps/sec: 8.63; \nFastEstimator-Train: step: 1040; ce: 0.074241355; steps/sec: 302.11; \nFastEstimator-Train: step: 1050; ce: 0.025415465; steps/sec: 640.07; \nFastEstimator-Train: step: 1060; ce: 0.21693969; steps/sec: 447.73; \nFastEstimator-Train: step: 1070; ce: 0.120441705; steps/sec: 432.52; \nFastEstimator-Train: step: 1080; ce: 0.25360084; steps/sec: 459.08; \nFastEstimator-Train: step: 1090; ce: 0.22401881; steps/sec: 493.29; \nFastEstimator-Train: step: 1100; ce: 0.112028226; steps/sec: 485.33; \nFastEstimator-Train: step: 1110; ce: 2.4293017; steps/sec: 446.65; \nFastEstimator-Train: step: 1120; ce: 0.19810674; steps/sec: 514.16; \nFastEstimator-Train: step: 1130; ce: 0.12599353; steps/sec: 529.87; \nFastEstimator-Train: step: 1140; ce: 0.23468983; steps/sec: 580.97; \nFastEstimator-Train: step: 1140; epoch: 10; epoch_time: 1.38 sec; \nFastEstimator-Eval: step: 1140; epoch: 10; ce: 0.017586827; min_ce: 0.017586827; since_best: 0; accuracy: 1.0; \nFastEstimator-Finish: step: 1140; total_time: 28.21 sec; model_lr: 0.001; \n</pre> In\u00a0[11]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: epoch: 10; accuracy: 0.9649122807017544; \n</pre>"}, {"location": "apphub/tabular/dnn/dnn.html#breast-cancer-detection", "title": "Breast Cancer Detection\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#download-data", "title": "Download data\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing with the test dataset that was specified in our <code>Pipeline</code>. We can use this to evaluate our model's accuracy on previously unseen data:</p>"}, {"location": "fastestimator/estimator.html", "title": "estimator", "text": ""}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.EarlyStop", "title": "<code>EarlyStop</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>class EarlyStop(Exception):\n\"\"\"An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.\n    This class is intentionally not @traceable.\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator", "title": "<code>Estimator</code>", "text": "<p>One class to rule them all.</p> <p>Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train (estimator.fit) or test (estimator.test) models. It wraps <code>Pipeline</code>, <code>Network</code>, <code>Trace</code> objects together and defines the whole optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>An fe.Pipeline object that defines the data processing workflow.</p> required <code>network</code> <code>BaseNetwork</code> <p>An fe.Network object that contains models and other training graph definitions.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to run.</p> required <code>max_train_steps_per_epoch</code> <code>Optional[int]</code> <p>Training will complete after n steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>max_eval_steps_per_epoch</code> <code>Optional[int]</code> <p>Evaluation will complete after n steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>traces</code> <code>Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]]</code> <p>What Traces to run during training. If None, only the system's default Traces will be included.</p> <code>None</code> <code>log_steps</code> <code>Optional[int]</code> <p>Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch information will still print). None to completely disable printing.</p> <code>100</code> <code>monitor_names</code> <code>Union[None, str, Iterable[str]]</code> <p>Additional keys from the data dictionary to be written into the logs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>@traceable()\nclass Estimator:\n\"\"\"One class to rule them all.\n    Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train\n    (estimator.fit) or test (estimator.test) models. It wraps `Pipeline`, `Network`, `Trace` objects together and\n    defines the whole optimization process.\n    Args:\n        pipeline: An fe.Pipeline object that defines the data processing workflow.\n        network: An fe.Network object that contains models and other training graph definitions.\n        epochs: The number of epochs to run.\n        max_train_steps_per_epoch: Training will complete after n steps even if loader is not yet exhausted. If None,\n            all data will be used.\n        max_eval_steps_per_epoch: Evaluation will complete after n steps even if loader is not yet exhausted. If None,\n            all data will be used.\n        traces: What Traces to run during training. If None, only the system's default Traces will be included.\n        log_steps: Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch\n            information will still print). None to completely disable printing.\n        monitor_names: Additional keys from the data dictionary to be written into the logs.\n    \"\"\"\nmonitor_names: Set[str]\ntraces_in_use: List[Union[Trace, Scheduler[Trace]]]\nsystem: System\nfilepath: str\ndef __init__(self,\npipeline: Pipeline,\nnetwork: BaseNetwork,\nepochs: int,\nmax_train_steps_per_epoch: Optional[int] = None,\nmax_eval_steps_per_epoch: Optional[int] = None,\ntraces: Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]] = None,\nlog_steps: Optional[int] = 100,\nmonitor_names: Union[None, str, Iterable[str]] = None):\nself.traces_in_use = []\nself.filepath = os.path.realpath(inspect.stack()[2].filename)  # Record this for history tracking\nassert log_steps is None or log_steps &gt;= 0, \\\n            \"log_steps must be None or positive (or 0 to disable only train logging)\"\nself.monitor_names = to_set(monitor_names) | network.get_loss_keys()\nself.system = System(network=network,\npipeline=pipeline,\ntraces=to_list(traces),\nlog_steps=log_steps,\ntotal_epochs=epochs,\nmax_train_steps_per_epoch=max_train_steps_per_epoch,\nmax_eval_steps_per_epoch=max_eval_steps_per_epoch,\nsystem_config=self.fe_summary())\n@property\ndef pipeline(self) -&gt; Pipeline:\nreturn self.system.pipeline\n@property\ndef network(self) -&gt; BaseNetwork:\nreturn self.system.network\n@property\ndef traces(self) -&gt; List[Union[Trace, Scheduler[Trace]]]:\nreturn self.system.traces\ndef fit(self, summary: Optional[str] = None, warmup: Union[bool, str] = True) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training.\n            warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n                epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n                also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n                mismatch. When set to \"debug\", the warmup will be performed in eager execution for easier debugging.\n        Returns:\n            A summary object containing the training history for this session iff a `summary` name was provided.\n        \"\"\"\ndraw()\nself.system.reset(summary, self.fe_summary())\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup(warmup=warmup)\nself._start(run_modes={\"train\", \"eval\"})\nreturn self.system.summary or None\ndef _prepare_traces(self, run_modes: Set[str]) -&gt; None:\n\"\"\"Prepare information about the traces for training.\n        Add default traces into the traces_in_use list, also prints a warning if no model saver trace is detected.\n        Args:\n            run_modes: The current execution modes.\n        \"\"\"\nself.traces_in_use = [trace for trace in self.traces]\nif self.system.log_steps is not None:\nself.traces_in_use.append(Logger())\n# Look for any monitor names which should be automagically added.\ntrace_outputs = set()\nextra_monitor_keys = set()\nfor trace in sort_traces(get_current_items(self.traces_in_use, run_modes=run_modes)):\ntrace_outputs.update(trace.outputs)\nextra_monitor_keys.update(trace.fe_monitor_names - trace_outputs)\n# Add the essential traces\nif \"train\" in run_modes:\nself.traces_in_use.insert(0, TrainEssential(monitor_names=self.monitor_names.union(extra_monitor_keys)))\nno_save_warning = True\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\nif isinstance(trace, (ModelSaver, BestModelSaver)):\nno_save_warning = False\nif no_save_warning:\nprint(\"FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\")\nif \"eval\" in run_modes and \"eval\" in self.pipeline.get_modes():\nself.traces_in_use.insert(1, EvalEssential(monitor_names=self.monitor_names.union(extra_monitor_keys)))\n# insert system instance to trace\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\ntrace.system = self.system\ndef test(self, summary: Optional[str] = None) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training. If None, the default value will be whatever `summary` name was\n                most recently provided to this Estimator's .fit() or .test() methods.\n        Returns:\n            A summary object containing the training history for this session iff the `summary` name is not None (after\n            considering the default behavior above).\n        \"\"\"\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"})\nreturn self.system.summary or None\ndef _warmup(self, warmup: Union[bool, str]) -&gt; None:\n\"\"\"Perform a test run of each pipeline and network signature epoch to make sure that training won't fail later.\n        Traces are not executed in the warmup since they are likely to contain state variables which could become\n        corrupted by running extra steps.\n        Args:\n            warmup: Warmup arg specified by estimator.fit.\n        \"\"\"\nall_traces = get_current_items(self.traces_in_use, run_modes={\"train\", \"eval\"})\nsort_traces(all_traces)  # This ensures that the traces can sort properly for on_begin and on_end\nmonitor_names = self.monitor_names\nfor mode in self.pipeline.get_modes() - {\"test\"}:\nscheduled_items = self.pipeline.get_scheduled_items(mode) + self.network.get_scheduled_items(\nmode) + self.get_scheduled_items(mode)\nsignature_epochs = get_signature_epochs(scheduled_items, self.system.total_epochs, mode=mode)\nepochs_with_data = self.pipeline.get_epochs_with_data(total_epochs=self.system.total_epochs, mode=mode)\nfor epoch in signature_epochs:\nif epoch not in epochs_with_data:\ncontinue\n# key checking\nloader = self._configure_loader(self.pipeline.get_loader(mode, epoch))\nwith Suppressor():\nif isinstance(loader, tf.data.Dataset):\nbatch = list(loader.take(1))[0]\nelse:\nbatch = next(iter(loader))\nbatch = self._configure_tensor(loader, batch)\nassert isinstance(batch, dict), \"please make sure data output format is dictionary\"\npipeline_output_keys = to_set(batch.keys())\nnetwork_output_keys = self.network.get_all_output_keys(mode, epoch)\ntrace_input_keys = set()\ntrace_output_keys = {\"*\"}\ntraces = get_current_items(self.traces_in_use, run_modes=mode, epoch=epoch)\nfor idx, trace in enumerate(traces):\nif idx &gt; 0:  # ignore TrainEssential and EvalEssential's inputs for unmet requirement checking\ntrace_input_keys.update(trace.inputs)\ntrace_output_keys.update(trace.outputs)\nmonitor_names = monitor_names - (pipeline_output_keys | network_output_keys)\nunmet_requirements = trace_input_keys - (pipeline_output_keys | network_output_keys | trace_output_keys)\nassert not unmet_requirements, \\\n                    \"found missing key(s) during epoch {} mode {}: {}\".format(epoch, mode, unmet_requirements)\nsort_traces(traces, available_outputs=pipeline_output_keys | network_output_keys)\ntrace_input_keys.update(traces[0].inputs)\nself.network.load_epoch(mode, epoch, output_keys=trace_input_keys, warmup=warmup)\nself.network.run_step(batch)\nself.network.unload_epoch()\nassert not monitor_names, \"found missing key(s): {}\".format(monitor_names)\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in estimator.\n        \"\"\"\nreturn self.traces_in_use\ndef _start(self, run_modes: Set[str]) -&gt; None:\n\"\"\"The outer training loop.\n        This method invokes the trace on_begin method, runs the necessary 'train' and 'eval' epochs, and then invokes\n        the trace on_end method.\n        Args:\n            run_modes: The current execution modes.\n        \"\"\"\nall_traces = sort_traces(get_current_items(self.traces_in_use, run_modes=run_modes))\nwith NonContext() if fe.fe_history_path is False else HistoryRecorder(\nself.system, self.filepath, db_path=fe.fe_history_path):\ntry:\nself._run_traces_on_begin(traces=all_traces)\nif \"train\" in run_modes or \"eval\" in run_modes:\n# If the training is re-starting from a restore wizard, it should re-run the last eval epoch\nif self.system.epoch_idx &gt; 0 and \"eval\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"eval\"\nself._run_epoch()\nfor self.system.epoch_idx in range(self.system.epoch_idx + 1, self.system.total_epochs + 1):\nif \"train\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"train\"\nself._run_epoch()\nif \"eval\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"eval\"\nself._run_epoch()\nelse:\nself._run_epoch()\nexcept EarlyStop:\npass  # On early stopping we still want to run the final traces and return results\nself._run_traces_on_end(traces=all_traces)\ndef _run_epoch(self) -&gt; None:\n\"\"\"A method to perform an epoch of activity.\n        This method requires that the current mode and epoch already be specified within the self.system object.\n        \"\"\"\ntraces = get_current_items(self.traces_in_use, run_modes=self.system.mode, epoch=self.system.epoch_idx)\ntrace_input_keys = set()\nfor trace in traces:\ntrace_input_keys.update(trace.inputs)\nloader = self._configure_loader(self.pipeline.get_loader(self.system.mode, self.system.epoch_idx))\niterator = iter(loader)\nself.network.load_epoch(mode=self.system.mode, epoch=self.system.epoch_idx, output_keys=trace_input_keys)\nself.system.batch_idx = None\nwith Suppressor():\nbatch = next(iterator)\ntraces = sort_traces(\ntraces,\navailable_outputs=to_set(batch.keys())\n| self.network.get_all_output_keys(self.system.mode, self.system.epoch_idx))\nself._run_traces_on_epoch_begin(traces=traces)\nwhile True:\ntry:\nif self.system.mode == \"train\":\nself.system.update_global_step()\nself.system.update_batch_idx()\nbatch = self._configure_tensor(loader, batch)\nself._run_traces_on_batch_begin(batch, traces=traces)\nbatch, prediction = self.network.run_step(batch)\nself._run_traces_on_batch_end(batch, prediction, traces=traces)\nif isinstance(loader, DataLoader) and (\n(self.system.batch_idx == self.system.max_train_steps_per_epoch and self.system.mode == \"train\") or\n(self.system.batch_idx == self.system.max_eval_steps_per_epoch and self.system.mode == \"eval\")):\nraise StopIteration\nwith Suppressor():\nbatch = next(iterator)\nexcept StopIteration:\nbreak\nself._run_traces_on_epoch_end(traces=traces)\nself.network.unload_epoch()\ndef _configure_loader(self, loader: Union[DataLoader, tf.data.Dataset]) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"A method to configure a given dataloader for use with this Estimator's Network.\n        This method will ensure that the `loader` returns the correct data type (tf.Tensor or torch.Tensor) depending on\n         the requirements of the Network. It also handles issues with multi-gpu data sharding.\n        Args:\n            loader: A data loader to be modified.\n        Returns:\n            The potentially modified dataloader to be used for training.\n        \"\"\"\nnew_loader = loader\nif isinstance(new_loader, DataLoader) and isinstance(self.network, TFNetwork):\nadd_batch = True\nif hasattr(loader.dataset, \"dataset\") and isinstance(loader.dataset.dataset, BatchDataset):\nadd_batch = False\nbatch = to_tensor(loader.dataset[0], target_type=\"tf\")\ndata_type = to_type(batch)\ndata_shape = to_shape(batch, add_batch=add_batch, exact_shape=False)\nnew_loader = tf.data.Dataset.from_generator(lambda: loader, data_type, output_shapes=data_shape)\nnew_loader = new_loader.prefetch(1)\nif isinstance(new_loader, tf.data.Dataset):\nif self.system.max_train_steps_per_epoch and self.system.mode == \"train\":\nnew_loader = new_loader.take(self.system.max_train_steps_per_epoch)\nif self.system.max_eval_steps_per_epoch and self.system.mode == \"eval\":\nnew_loader = new_loader.take(self.system.max_eval_steps_per_epoch)\nif isinstance(tf.distribute.get_strategy(),\ntf.distribute.MirroredStrategy) and not isinstance(new_loader, DistributedDataset):\nnew_loader = tf.distribute.get_strategy().experimental_distribute_dataset(new_loader)\nreturn new_loader\ndef _configure_tensor(self, loader: Union[DataLoader, tf.data.Dataset], batch: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"A function to convert a batch of tf.Tensors to torch.Tensors if required.\n        Returns:\n            Either the original `batch`, or the `batch` converted to torch.Tensors if required.\n        \"\"\"\nif isinstance(loader, tf.data.Dataset) and isinstance(self.network, TorchNetwork):\nbatch = to_tensor(batch, target_type=\"torch\")\nreturn batch\ndef _run_traces_on_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nrestore = None\nfor trace in traces:\n# Delay RestoreWizard until the end so that it can overwrite everyone's on_begin methods\nif isinstance(trace, RestoreWizard):\nrestore = trace\ncontinue\n# Restore does need to run before the logger though\nif isinstance(trace, Logger) and restore:\nrestore.on_begin(data)\nrestore = None\ntrace.on_begin(data)\nif restore:\nrestore.on_begin(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_epoch_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_epoch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_begin(self, batch: Dict[str, Any], traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_begin methods of given traces.\n        Args:\n            batch: The batch data which was provided by the pipeline.\n            traces: List of traces.\n        \"\"\"\ndata = Data(batch)\nfor trace in traces:\ntrace.on_batch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_end(self, batch: Dict[str, Any], prediction: Dict[str, Any],\ntraces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_end methods of given traces.\n        Args:\n            batch: The batch data which was provided by the pipeline.\n            prediction: The prediction data which was generated by the network.\n            traces: List of traces.\n        \"\"\"\ndata = Data(ChainMap(prediction, batch))\nfor trace in traces:\ntrace.on_batch_end(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_end(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_epoch_end methods of of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_epoch_end(data)\nself._check_early_exit()\n@staticmethod\ndef _run_traces_on_end(traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_end methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\ntraceability = None\nfor trace in traces:\nif isinstance(trace, Traceability):\n# Delay traceability until the end so that it can capture all data including the total training time\ntraceability = trace\ncontinue\ntrace.on_end(data)\nif traceability:\ntraceability.on_end(data)\ndef _check_early_exit(self) -&gt; None:\n\"\"\"Determine whether training should be prematurely aborted.\n        Raises:\n            EarlyStop: If the system.stop_training flag has been set to True.\n        \"\"\"\nif self.system.stop_training:\nraise EarlyStop\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.fit", "title": "<code>fit</code>", "text": "<p>Train the network for the number of epochs specified by the estimator's constructor.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training.</p> <code>None</code> <code>warmup</code> <code>Union[bool, str]</code> <p>Whether to perform warmup before training begins. The warmup procedure will test one step at every epoch where schedulers cause the execution graph to change. This can take some time up front, but can also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size mismatch. When set to \"debug\", the warmup will be performed in eager execution for easier debugging.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff a <code>summary</code> name was provided.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def fit(self, summary: Optional[str] = None, warmup: Union[bool, str] = True) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training.\n        warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n            epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n            also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n            mismatch. When set to \"debug\", the warmup will be performed in eager execution for easier debugging.\n    Returns:\n        A summary object containing the training history for this session iff a `summary` name was provided.\n    \"\"\"\ndraw()\nself.system.reset(summary, self.fe_summary())\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup(warmup=warmup)\nself._start(run_modes={\"train\", \"eval\"})\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in estimator.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in estimator.\n    \"\"\"\nreturn self.traces_in_use\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.test", "title": "<code>test</code>", "text": "<p>Run the pipeline / network in test mode for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training. If None, the default value will be whatever <code>summary</code> name was most recently provided to this Estimator's .fit() or .test() methods.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff the <code>summary</code> name is not None (after</p> <code>Optional[Summary]</code> <p>considering the default behavior above).</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def test(self, summary: Optional[str] = None) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training. If None, the default value will be whatever `summary` name was\n            most recently provided to this Estimator's .fit() or .test() methods.\n    Returns:\n        A summary object containing the training history for this session iff the `summary` name is not None (after\n        considering the default behavior above).\n    \"\"\"\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"})\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.enable_deterministic", "title": "<code>enable_deterministic</code>", "text": "<p>Invoke to set random seed for deterministic training.</p> <p>The determinism only works for tensorflow &gt;= 2.1 and pytorch &gt;= 1.14, and some model layers don't support.</p> <p>Known failing layers: * tf.keras.layers.UpSampling2D</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed to use for training.</p> required Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def enable_deterministic(seed: int) -&gt; None:\n\"\"\"Invoke to set random seed for deterministic training.\n    The determinism only works for tensorflow &gt;= 2.1 and pytorch &gt;= 1.14, and some model layers don't support.\n    Known failing layers:\n    * tf.keras.layers.UpSampling2D\n    Args:\n        seed: The random seed to use for training.\n    \"\"\"\nfe.fe_deterministic_seed = seed\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = str(1)\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntorch.manual_seed(seed)\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.record_history", "title": "<code>record_history</code>", "text": "<p>Change the default location for history tracking.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[bool, str]</code> <p>The path to save experiment histories. Pass True to use the default location of ~/fastestimator_data/history.db. Pass False to disable history tracking.</p> required Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def record_history(path: Union[bool, str]) -&gt; None:\n\"\"\"Change the default location for history tracking.\n    Args:\n        path: The path to save experiment histories. Pass True to use the default location of\n            ~/fastestimator_data/history.db. Pass False to disable history tracking.\n    \"\"\"\nif path in (None, True):\nfe.fe_history_path = None\nelse:\nfe.fe_history_path = path\n</code></pre>"}, {"location": "fastestimator/network.html", "title": "network", "text": ""}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork", "title": "<code>BaseNetwork</code>", "text": "<p>A base class for Network objects.</p> <p>Networks are used to define the computation graph surrounding one or more models during training.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The operators to be executed throughout training / testing / inference. These are likely to contain one or more model ops, as well as loss ops and update ops.</p> required Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass BaseNetwork:\n\"\"\"A base class for Network objects.\n    Networks are used to define the computation graph surrounding one or more models during training.\n    Args:\n        ops: The operators to be executed throughout training / testing / inference. These are likely to contain one or\n            more model ops, as well as loss ops and update ops.\n    \"\"\"\ndef __init__(self, ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; None:\nself.ops = to_list(ops)\nself.models = to_list(_collect_models(ops))\nself._verify_inputs()\nself.effective_inputs = dict()\nself.effective_outputs = dict()\nself.epoch_ops = []\nself.epoch_models = set()\nself.epoch_state = dict()\nself.scaler = None\ndef _verify_inputs(self) -&gt; None:\n\"\"\"Ensure that all ops are TensorOps.\n        Raises:\n            AssertionError: If any of the ops are not TensorOps.\n        \"\"\"\nfor op in get_current_items(self.ops):\nassert isinstance(op, TensorOp), \"unsupported op format, must provide TensorOp in Network\"\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Network.\n        \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models]\nelse:\nall_items = self.ops\nreturn all_items\ndef load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch.\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(output_keys)\nself.epoch_ops = get_current_items(self.ops, mode, epoch)\nself.epoch_models = set.union(*[op.get_fe_models() for op in self.epoch_ops])\ngradient_ops = [op for op in self.epoch_ops if op.fe_retain_graph() is not None]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.fe_retain_graph(idx != len(gradient_ops) - 1)\nself.epoch_state = {\n\"warmup\": warmup,\n\"mode\": mode,\n\"req_grad\": len(gradient_ops) &gt; 0,\n\"epoch\": epoch,\n\"deferred\": {},\n\"scaler\": self.scaler\n}\n# warmup: bool, mode: str, req_grad: bool, epoch: int, deferred: Dict[str, List[Callable]]]\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        \"\"\"\npass\ndef get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n        Returns:\n            All of the keys associated with model losses in this network.\n        \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nloss_keys |= op.get_fe_loss_keys()\nreturn loss_keys\ndef get_effective_input_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider for determining inputs.\n        Returns:\n            The necessary inputs for the network to execute the given `epoch` and `mode`.\n        \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\ndef get_all_output_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider when searching for outputs.\n        Returns:\n            The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n        \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\noutput_keys.update(op.outputs)\nreturn output_keys\n@staticmethod\ndef _forward_batch(batch: MutableMapping[str, Any], state: Dict[str, Any], ops: List[TensorOp]) -&gt; None:\n\"\"\"Run a forward pass through the network's Op chain given a `batch` of data.\n        Args:\n            batch: A batch of input data. Predictions from the network will be written back into this dictionary.\n            state: A dictionary holding information about the current execution context. The TF gradient tape, for\n                example will be stored here.\n            ops: Which ops to execute.\n        \"\"\"\nfor op in ops:\ndata = get_inputs_by_op(op, batch)\ndata = op.forward(data, state)\nif op.outputs:\nwrite_outputs_by_op(op, batch, data)\nfor fn_list in state['deferred'].values():\nfor fn in fn_list:\nfn()\nstate['deferred'].clear()\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nraise NotImplementedError\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_all_output_keys", "title": "<code>get_all_output_keys</code>", "text": "<p>Get all of the keys that will be generated by the network during the given <code>epoch</code> and <code>mode</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider when searching for outputs.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The keys that will be generated by the network's Ops during the <code>epoch</code> for the given <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_all_output_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider when searching for outputs.\n    Returns:\n        The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n    \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\noutput_keys.update(op.outputs)\nreturn output_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_effective_input_keys", "title": "<code>get_effective_input_keys</code>", "text": "<p>Determine which keys need to be provided as input to the network during the given <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider for determining inputs.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The necessary inputs for the network to execute the given <code>epoch</code> and <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_effective_input_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider for determining inputs.\n    Returns:\n        The necessary inputs for the network to execute the given `epoch` and `mode`.\n    \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_loss_keys", "title": "<code>get_loss_keys</code>", "text": "<p>Find all of the keys associated with model losses.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>All of the keys associated with model losses in this network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n    Returns:\n        All of the keys associated with model losses in this network.\n    \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nloss_keys |= op.get_fe_loss_keys()\nreturn loss_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Network.\n    \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models]\nelse:\nall_items = self.ops\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch.\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n    \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(output_keys)\nself.epoch_ops = get_current_items(self.ops, mode, epoch)\nself.epoch_models = set.union(*[op.get_fe_models() for op in self.epoch_ops])\ngradient_ops = [op for op in self.epoch_ops if op.fe_retain_graph() is not None]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.fe_retain_graph(idx != len(gradient_ops) - 1)\nself.epoch_state = {\n\"warmup\": warmup,\n\"mode\": mode,\n\"req_grad\": len(gradient_ops) &gt; 0,\n\"epoch\": epoch,\n\"deferred\": {},\n\"scaler\": self.scaler\n}\n# warmup: bool, mode: str, req_grad: bool, epoch: int, deferred: Dict[str, List[Callable]]]\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork", "title": "<code>TFNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for TensorFlow models.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The ops defining the execution graph for this Network.</p> required Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass TFNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for TensorFlow models.\n    Args:\n        ops: The ops defining the execution graph for this Network.\n    \"\"\"\ndef __init__(self, ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; None:\nsuper().__init__(ops)\nfor op in get_current_items(self.ops):\nop.build(framework='tf')\ndef load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nself.epoch_state[\"epoch\"] = tf.convert_to_tensor(self.epoch_state[\"epoch\"])\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nif self.epoch_state[\"warmup\"] == \"debug\":\nprediction = strategy.run(\nself._forward_step_eager,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nelse:\nprediction = strategy.run(\nself._forward_step_static,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nbatch = self._per_replica_to_global(batch)\nprediction = self._per_replica_to_global(prediction)\nelse:\nif self.epoch_state[\"warmup\"] == \"debug\":\nprediction = self._forward_step_eager(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nelse:\nprediction = self._forward_step_static(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nreturn batch, prediction\ndef _per_replica_to_global(self, data: T) -&gt; T:\n\"\"\"Combine data from \"per-replica\" values recursively.\n        For multi-GPU training, data are distributed using `tf.distribute.Strategy.experimental_distribute_dataset`.\n        This method collects data from all replicas and combines them into one.\n        Args:\n            data: Distributed data.\n        Returns:\n            Combined data from all replicas.\n        \"\"\"\nif isinstance(data, DistributedValues):\nif data.values[0].shape.rank == 0:\nreturn tf.reduce_mean(tuple(d for d in data.values if not tf.math.is_nan(d)))\nelse:\nreturn tf.concat(data.values, axis=0)\nelif isinstance(data, dict):\nresult = {}\nfor key, val in data.items():\nresult[key] = self._per_replica_to_global(val)\nreturn result\nelif isinstance(data, list):\nreturn [self._per_replica_to_global(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._per_replica_to_global(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._per_replica_to_global(val) for val in data])\nelse:\nreturn data\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Filter input data so that only the data required by the Network is moved onto the GPU.\n        Args:\n            batch: An unfiltered batch of input data.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The filtered input data ready for use on GPU(s).\n        \"\"\"\nnew_batch = {}\nfor key in self.effective_inputs[mode]:\nif key in batch:\nnew_batch[key] = batch[key]\nreturn new_batch\ndef _forward_step_eager(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in eager (non-static graph) mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = ChainMap({}, batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\n@tf.function\ndef _forward_step_static(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in static graph mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = dict(batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nself.load_epoch(mode, epoch, warmup=False)\ndata = to_tensor(data, target_type=\"tf\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\n# handle tensorflow multi-gpu inferencing issue, it will replicate data on each device\nif isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\nprediction = self._subsample_data(prediction, get_batch_size(data))\ndata.update(prediction)\nreturn data\ndef _subsample_data(self, data: T, n: int) -&gt; T:\n\"\"\"Subsample data by selecting the first n indices recursively.\n        Args:\n            data: The data to be subsampled.\n        Returns:\n            Subsampled data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._subsample_data(val, n) for (key, val) in data.items()}\nelif isinstance(data, list):\nreturn [self._subsample_data(val, n) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._subsample_data(val, n) for val in data])\nelif isinstance(data, set):\nreturn set([self._subsample_data(val, n) for val in data])\nelif hasattr(data, \"shape\") and list(data.shape) and data.shape[0] &gt; n:\nreturn data[0:n]\nelse:\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n    \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nself.epoch_state[\"epoch\"] = tf.convert_to_tensor(self.epoch_state[\"epoch\"])\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nif self.epoch_state[\"warmup\"] == \"debug\":\nprediction = strategy.run(\nself._forward_step_eager,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nelse:\nprediction = strategy.run(\nself._forward_step_static,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nbatch = self._per_replica_to_global(batch)\nprediction = self._per_replica_to_global(prediction)\nelse:\nif self.epoch_state[\"warmup\"] == \"debug\":\nprediction = self._forward_step_eager(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nelse:\nprediction = self._forward_step_static(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nreturn batch, prediction\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nself.load_epoch(mode, epoch, warmup=False)\ndata = to_tensor(data, target_type=\"tf\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\n# handle tensorflow multi-gpu inferencing issue, it will replicate data on each device\nif isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\nprediction = self._subsample_data(prediction, get_batch_size(data))\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork", "title": "<code>TorchNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for PyTorch models.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The ops defining the execution graph for this Network.</p> required Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass TorchNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for PyTorch models.\n    Args:\n        ops: The ops defining the execution graph for this Network.\n    \"\"\"\ndef __init__(self, ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; None:\nsuper().__init__(ops)\nfor op in get_current_items(self.ops):\nop.build(framework='torch')\nself.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif any([model.mixed_precision for model in self.models]):\nself.scaler = torch.cuda.amp.GradScaler()\ndef load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\nif model.current_optimizer and mode == \"train\":\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\n# Set all of the contiguous final updates to defer their updates by default to enable things like CycleGan\n# This is not necessary for TF because overriding tf weights does not confuse the gradient tape computation\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop._old_defer = op.defer\nop.defer = True\nelse:\nbreak\ndef _move_optimizer_between_device(self, data: Dict[str, Any], device: Union[str, torch.device]) -&gt; None:\n\"\"\"Move optimizer state between gpu and cpu recursively.\n        Args:\n            data: Optimizer state.\n            device: The target device.\n        \"\"\"\nfor key in data:\nif isinstance(data[key], dict):\nself._move_optimizer_between_device(data[key], device)\nelse:\ntry:\ndata[key] = data[key].to(device)\nexcept:\npass\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        In this case we move all of the models from the GPU(s) back to the CPU.\n        \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\nif model.current_optimizer and self.epoch_state[\"mode\"] == \"train\":\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\n# Set the final update ops back to their original defer status\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop.defer = op.__dict__.get('_old_defer', op.defer)\nelse:\nbreak\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Copy input data from the the CPU onto the GPU(s).\n        This method will filter inputs from the batch so that only data required by the network during execution will be\n        copied to the GPU.\n        Args:\n            batch: The input data to be moved.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The input data ready for use on GPU(s).\n        \"\"\"\nif self.device.type == \"cuda\":\nnew_batch = {\nkey: self._move_tensor_between_device(batch[key], self.device)\nfor key in self.effective_inputs[mode] if key in batch\n}\nelse:\nnew_batch = {key: batch[key] for key in self.effective_inputs[mode] if key in batch}\nreturn new_batch\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nself.epoch_state[\"tape\"] = NonContext()\n# gpu operation\nwith torch.no_grad() if not self.epoch_state[\"req_grad\"] else NonContext():\nwith torch.cuda.amp.autocast() if self.epoch_state[\"scaler\"] is not None else NonContext():\nself._forward_batch(batch_in, self.epoch_state, self.epoch_ops)\n# if the loss scaler is used for training, update the scaler\nif not self.epoch_state[\"warmup\"] and self.epoch_state[\n\"mode\"] == \"train\" and self.epoch_state[\"scaler\"] is not None:\nself.epoch_state[\"scaler\"].update()\n# copy data to cpu\nif self.device.type == \"cuda\":\nprediction = {\nkey: self._move_tensor_between_device(self._detach_tensor(batch_in[key]), \"cpu\")\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nelse:\nprediction = {\nkey: self._detach_tensor(batch_in[key])\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nreturn batch, prediction\ndef _move_tensor_between_device(self, data: T, device: Union[str, torch.device]) -&gt; T:\n\"\"\"Move tensor between gpu and cpu recursively.\n        Args:\n            data: The input data to be moved.\n            device: The target device.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._move_tensor_between_device(value, device) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._move_tensor_between_device(val, device) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, set):\nreturn set([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.to(device)\nelse:\nreturn data\ndef _detach_tensor(self, data: T) -&gt; T:\n\"\"\"Detach tensor from current graph recursively.\n        Args:\n            data: The data to be detached.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._detach_tensor(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._detach_tensor(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._detach_tensor(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._detach_tensor(val) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.detach()\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nself.load_epoch(mode, epoch, warmup=False)\ndata = to_tensor(data, \"torch\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n    \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\nif model.current_optimizer and mode == \"train\":\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\n# Set all of the contiguous final updates to defer their updates by default to enable things like CycleGan\n# This is not necessary for TF because overriding tf weights does not confuse the gradient tape computation\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop._old_defer = op.defer\nop.defer = True\nelse:\nbreak\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nself.epoch_state[\"tape\"] = NonContext()\n# gpu operation\nwith torch.no_grad() if not self.epoch_state[\"req_grad\"] else NonContext():\nwith torch.cuda.amp.autocast() if self.epoch_state[\"scaler\"] is not None else NonContext():\nself._forward_batch(batch_in, self.epoch_state, self.epoch_ops)\n# if the loss scaler is used for training, update the scaler\nif not self.epoch_state[\"warmup\"] and self.epoch_state[\n\"mode\"] == \"train\" and self.epoch_state[\"scaler\"] is not None:\nself.epoch_state[\"scaler\"].update()\n# copy data to cpu\nif self.device.type == \"cuda\":\nprediction = {\nkey: self._move_tensor_between_device(self._detach_tensor(batch_in[key]), \"cpu\")\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nelse:\nprediction = {\nkey: self._detach_tensor(batch_in[key])\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nreturn batch, prediction\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nself.load_epoch(mode, epoch, warmup=False)\ndata = to_tensor(data, \"torch\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> <p>In this case we move all of the models from the GPU(s) back to the CPU.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    In this case we move all of the models from the GPU(s) back to the CPU.\n    \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\nif model.current_optimizer and self.epoch_state[\"mode\"] == \"train\":\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\n# Set the final update ops back to their original defer status\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop.defer = op.__dict__.get('_old_defer', op.defer)\nelse:\nbreak\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.Network", "title": "<code>Network</code>", "text": "<p>A function to automatically instantiate the correct Network derived class based on the given <code>ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch models within the same network.</p> required <p>Returns:</p> Type Description <code>BaseNetwork</code> <p>A network instance containing the given <code>ops</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If TensorFlow and PyTorch models are mixed, or if no models are provided.</p> <code>ValueError</code> <p>If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def Network(ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; BaseNetwork:\n\"\"\"A function to automatically instantiate the correct Network derived class based on the given `ops`.\n    Args:\n        ops: A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all\n            models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch\n            models within the same network.\n    Returns:\n        A network instance containing the given `ops`.\n    Raises:\n        AssertionError: If TensorFlow and PyTorch models are mixed, or if no models are provided.\n        ValueError: If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.\n    \"\"\"\nmodels = _collect_models(ops)\nframework = set()\nmodel_names = set()\nfor model in models:\n# 'Model' and 'model' should not be considered unique in case you are saving on a non-case-sensitive filesystem\nmodel_names.add(model.model_name.lower())\nif isinstance(model, tf.keras.Model):\nframework.add(\"tf\")\nelif isinstance(model, torch.nn.Module):\nframework.add(\"torch\")\nelse:\nframework.add(\"unknown\")\nif len(framework) == 0:\nframework.add('tf')  # We will use tf as default framework if no models are found\nassert len(framework) == 1, \"please make sure either tensorflow or torch model is used in network\"\nassert len(model_names) == len(models), \"all models must have unique model names\"\nframework = framework.pop()\nif framework == \"tf\":\nnetwork = TFNetwork(ops)\nelif framework == \"torch\":\nnetwork = TorchNetwork(ops)\nelse:\nraise ValueError(\"Unknown model type\")\nreturn network\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.build", "title": "<code>build</code>", "text": "<p>Build model instances and associate them with optimizers.</p> <p>This method can be used with TensorFlow models / optimizers: <pre><code>model_def = fe.architecture.tensorflow.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models / optimizers: <pre><code>model_def = fe.architecture.pytorch.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>Callable[[], Union[Model, List[Model]]]</code> <p>A function that define model(s).</p> required <code>optimizer_fn</code> <code>Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None]</code> <p>Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers provided here should match the number of models generated by the <code>model_fn</code>.</p> required <code>model_name</code> <code>Union[str, List[str], None]</code> <p>Name(s) of the model(s) that will be used for logging purpose. If None, a name will be automatically generated and assigned.</p> <code>None</code> <code>weights_path</code> <code>Union[str, None, List[Union[str, None]]]</code> <p>Path(s) from which to load model weights. If not None, then the number of weight paths provided should match the number of models generated by the <code>model_fn</code>.</p> <code>None</code> <code>mixed_precision</code> <code>bool</code> <p>Whether to enable mix precision network operations.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>models</code> <code>Union[Model, List[Model]]</code> <p>The model(s) built by FastEstimator.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def build(model_fn: Callable[[], Union[Model, List[Model]]],\noptimizer_fn: Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None],\nweights_path: Union[str, None, List[Union[str, None]]] = None,\nmodel_name: Union[str, List[str], None] = None,\nmixed_precision: bool = False) -&gt; Union[Model, List[Model]]:\n\"\"\"Build model instances and associate them with optimizers.\n    This method can be used with TensorFlow models / optimizers:\n    ```python\n    model_def = fe.architecture.tensorflow.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n    ```\n    This method can be used with PyTorch models / optimizers:\n    ```python\n    model_def = fe.architecture.pytorch.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n    ```\n    Args:\n        model_fn: A function that define model(s).\n        optimizer_fn: Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers\n            provided here should match the number of models generated by the `model_fn`.\n        model_name: Name(s) of the model(s) that will be used for logging purpose. If None, a name will be\n            automatically generated and assigned.\n        weights_path: Path(s) from which to load model weights. If not None, then the number of weight paths provided\n            should match the number of models generated by the `model_fn`.\n        mixed_precision: Whether to enable mix precision network operations.\n    Returns:\n        models: The model(s) built by FastEstimator.\n    \"\"\"\ndef _generate_model_names(num_names):\nnames = [\"model\" if i + build.count == 0 else \"model{}\".format(i + build.count) for i in range(num_names)]\nbuild.count += num_names\nreturn names\nif not hasattr(build, \"count\"):\nbuild.count = 0\n# Tensorflow models require setting global policies prior to model creation. Since there is no way to know the\n# framework of the model ahead of time, set the policy for both tf and pytorch here.\nif mixed_precision:\nmixed_precision_tf.set_policy(mixed_precision_tf.Policy('mixed_float16'))\nelse:\nmixed_precision_tf.set_policy(mixed_precision_tf.Policy('float32'))\nmodels, optimizer_fn = to_list(model_fn()), to_list(optimizer_fn)\n# fill optimizer\nif not optimizer_fn:\noptimizer_fn = [None]\n# check framework\nif isinstance(models[0], tf.keras.Model):\nframework = \"tf\"\nelif isinstance(models[0], torch.nn.Module):\nframework = \"torch\"\nelse:\nraise ValueError(\"unrecognized model format: {}\".format(type(models[0])))\n# multi-gpu handling\nif torch.cuda.device_count() &gt; 1:\nif framework == \"tf\" and not isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\ntf.distribute.experimental_set_strategy(tf.distribute.MirroredStrategy())\nmodels = to_list(model_fn())\nif framework == \"torch\":\nmodels = [torch.nn.DataParallel(model) for model in models]\n# mark models with its mixed_precision flag\nfor model in models:\nmodel.mixed_precision = mixed_precision\n# generate names\nif not model_name:\nmodel_name = _generate_model_names(len(models))\nmodel_name = to_list(model_name)\n# load weights\nif weights_path:\nweights_path = to_list(weights_path)\nelse:\nweights_path = [None] * len(models)\nassert len(models) == len(optimizer_fn) == len(weights_path) == len(model_name), \\\n        \"Found inconsistency in number of models, optimizers, model_name or weights\"\n# create optimizer\nfor idx, (model, optimizer_def, weight, name) in enumerate(zip(models, optimizer_fn, weights_path, model_name)):\nmodels[idx] = trace_model(_fe_compile(model, optimizer_def, weight, name, framework),\nmodel_idx=idx if len(models) &gt; 1 else -1,\nmodel_fn=model_fn,\noptimizer_fn=optimizer_def,\nweights_path=weight)\nif len(models) == 1:\nmodels = models[0]\nreturn models\n</code></pre>"}, {"location": "fastestimator/pipeline.html", "title": "pipeline", "text": ""}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline", "title": "<code>Pipeline</code>", "text": "<p>A data pipeline class that takes care of data pre-processing.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The training data, or None if no training data is available.</p> <code>None</code> <code>eval_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The evaluation data, or None if no evaluation data is available.</p> <code>None</code> <code>test_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The testing data, or None if no evaluation data is available.</p> <code>None</code> <code>batch_size</code> <code>Union[None, int, Scheduler[int]]</code> <p>The batch size to be used by the pipeline. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>ops</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>num_process</code> <code>Optional[int]</code> <p>Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset. None will default to min(n_cpus, max(32, 32*n_gpus)). Multiprocessing can be disabled by passing 0 here, which can be useful for debugging.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch if the last batch is incomplete.</p> <code>False</code> <code>pad_value</code> <code>Optional[Union[int, float]]</code> <p>The padding value if batch padding is needed. None indicates that no padding is needed. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>collate_fn</code> <code>Optional[Callable]</code> <p>Function to merge data into one batch with input being list of elements.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>@traceable()\nclass Pipeline:\n\"\"\"A data pipeline class that takes care of data pre-processing.\n    Args:\n        train_data: The training data, or None if no training data is available.\n        eval_data: The evaluation data, or None if no evaluation data is available.\n        test_data: The testing data, or None if no evaluation data is available.\n        batch_size: The batch size to be used by the pipeline. NOTE: This argument is only applicable when using a\n            FastEstimator Dataset.\n        ops: NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator\n            Dataset.\n        num_process: Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when\n            using a FastEstimator Dataset. None will default to min(n_cpus, max(32, 32*n_gpus)). Multiprocessing can be\n            disabled by passing 0 here, which can be useful for debugging.\n        drop_last: Whether to drop the last batch if the last batch is incomplete.\n        pad_value: The padding value if batch padding is needed. None indicates that no padding is needed. NOTE: This\n            argument is only applicable when using a FastEstimator Dataset.\n        collate_fn: Function to merge data into one batch with input being list of elements.\n    \"\"\"\nops: List[Union[NumpyOp, Scheduler[NumpyOp]]]\ndef __init__(self,\ntrain_data: Union[None, DataSource, Scheduler[DataSource]] = None,\neval_data: Union[None, DataSource, Scheduler[DataSource]] = None,\ntest_data: Union[None, DataSource, Scheduler[DataSource]] = None,\nbatch_size: Union[None, int, Scheduler[int]] = None,\nops: Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]] = None,\nnum_process: Optional[int] = None,\ndrop_last: bool = False,\npad_value: Optional[Union[int, float]] = None,\ncollate_fn: Optional[Callable] = None):\nself.data = {x: y for (x, y) in zip([\"train\", \"eval\", \"test\"], [train_data, eval_data, test_data]) if y}\nself.batch_size = batch_size\nself.ops = to_list(ops)\nif mp.get_start_method(allow_none=True) is None and os.name != 'nt':\nmp.set_start_method('fork')\nif mp.get_start_method(allow_none=True) != 'fork':\nprint(\"FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\")\nnum_process = 0\nself.num_process = num_process if num_process is not None else min(os.cpu_count(), 32 * get_num_devices())\nself.drop_last = drop_last\nself.pad_value = pad_value\nself.collate_fn = collate_fn\nself._verify_inputs(**{k: v for k, v in locals().items() if k != 'self'})\ndef _verify_inputs(self, **kwargs) -&gt; None:\n\"\"\"A helper method to ensure that the Pipeline inputs are valid.\n        Args:\n            **kwargs: A collection of variable / value pairs to validate.\n        Raises:\n            AssertionError: If `batch_size`, `ops`, or `num_process` were specified in the absence of a FastEstimator\n                Dataset.\n        \"\"\"\nfe_dataset = False\nfor dataset in get_current_items(self.data.values()):\nfe_dataset = self._verify_dataset(dataset, **kwargs) or fe_dataset\nif not fe_dataset:\nassert kwargs['batch_size'] is None, \"Pipeline only supports batch_size with built-in (FE) datasets\"\nassert kwargs['ops'] is None, \"Pipeline only supports ops with built-in (FE) datasets\"\nassert kwargs['num_process'] is None, \"Pipeline only support num_process with built-in (FE) datasets\"\ndef _verify_dataset(self, dataset: DataSource, **kwargs) -&gt; bool:\n\"\"\"A helper function to ensure that all of a dataset's arguments are correct.\n        Args:\n            dataset: The dataset to validate against.\n            **kwargs: A selection of variables and their values which must be validated.\n        Returns:\n            True iff the `dataset` is a PyTorch Dataset (as opposed to a DataLoader or tf.data.Dataset).\n        Raises:\n            AssertionError: If the `kwargs` are found to be invalid based on the given `dataset`.\n            ValueError: If the `dataset` is of an unknown type.\n        \"\"\"\nif isinstance(dataset, Dataset):\n# batch_size check\nfor batch_size in get_current_items(to_list(self.batch_size)):\nassert isinstance(batch_size, int), \"unsupported batch_size format: {}\".format(type(batch_size))\n# ops check\nfor op in get_current_items(self.ops):\nassert isinstance(op, NumpyOp), \"unsupported op format, must provide NumpyOp in Pipeline\"\n# num_process check\nassert isinstance(self.num_process, int), \"number of processes must be an integer\"\nreturn True\nelif isinstance(dataset, (DataLoader, tf.data.Dataset)):\nif kwargs['batch_size'] is not None:\nwarnings.warn(\"batch_size will only be used for built-in dataset\")\nif kwargs['ops'] is not None:\nwarnings.warn(\"ops will only be used for built-in dataset\")\nif kwargs['num_process'] is not None:\nwarnings.warn(\"num_process will only be used for built-in dataset\")\nreturn False\nelse:\nraise ValueError(\"Unsupported dataset type: {}\".format(type(dataset)))\ndef get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n        Args:\n            epoch: The current epoch index\n        Returns:\n            The modes for which the Pipeline has data.\n        \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, dataset in self.data.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\ndef benchmark(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1000, log_interval: int = 100) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n        Args:\n            mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n            num_steps: The maximum number of steps over which to perform the benchmark.\n            log_interval: The logging interval.\n        \"\"\"\nloader = self.get_loader(mode=mode, epoch=epoch)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nprint(\"FastEstimator: Step: {}, Epoch: {}, Steps/sec: {}\".format(idx, epoch, iters_per_sec))\nstart = time.perf_counter()\nif idx == num_steps:\nbreak\n# Pipeline Operations Benchmarking when using FEDataset\nif isinstance(loader, DataLoader) and isinstance(loader.dataset, OpDataset):\nop_list = loader.dataset.ops\nduration_list = np.zeros(shape=(len(op_list)))\ndata_len = len(loader.dataset.dataset)\nif self.batch_size:\nlog_interval = log_interval * self.batch_size\nprint(\"\\nBreakdown of time taken by Pipeline Operations ({} epoch {})\".format(mode, epoch))\nfor _ in range(log_interval):\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif isinstance(loader.dataset.dataset, BatchDataset):\nunique_list = []\nfor item in items:\nif id(item) not in unique_list:\nfor i, op in enumerate(op_list):\nstart = time.perf_counter()\nforward_numpyop([op], item, loader.dataset.mode)\nduration = time.perf_counter() - start\nduration_list[i] += duration\nunique_list.append(id(item))\nelse:\nfor i, op in enumerate(op_list):\nstart = time.perf_counter()\nforward_numpyop([op], items, loader.dataset.mode)\nduration = time.perf_counter() - start\nduration_list[i] += duration\ntotal_time = np.sum(duration_list)\nop_names = [\"Op\"]\nfor op in op_list:\nif isinstance(op, Sometimes) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, OneOf) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelse:\nop_names.append(op.__class__.__name__)\nmax_op_len = max(len(op_name) for op_name in op_names)\nmax_in_len = max([len(\", \".join(op.inputs)) for op in op_list] + [len(\"Inputs\")])\nmax_out_len = max([len(\", \".join(op.outputs)) for op in op_list] + [len(\"Outputs\")])\nprint(\"{}: {}: {}: {}\".format(\"Op\".ljust(max_op_len + 1),\n\"Inputs\".ljust(max_in_len + 1),\n\"Outputs\".ljust(max_out_len + 1),\n\"Time\".rjust(5)))\nprint(\"-\" * (max_op_len + max_in_len + max_out_len + 15))\nfor i, op in enumerate(op_list):\nprint(\"{}: {}: {}: {:5.2f}%\".format(op_names[i + 1].ljust(max_op_len + 1),\n\", \".join(op.inputs).ljust(max_in_len + 1),\n\", \".join(op.outputs).ljust(max_out_len + 1),\n100 * duration_list[i] / total_time))\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Pipeline.\n        \"\"\"\nall_items = self.ops + [self.batch_size] + [self.data[mode]]\nreturn all_items\ndef get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n        Args:\n            total_epochs: Total number of epochs.\n            mode: Current execution mode.\n        Returns:\n            Set of epoch indices.\n        \"\"\"\nepochs_with_data = set()\ndataset = self.data[mode]\nif isinstance(dataset, Scheduler):\nepochs_with_data = set(epoch for epoch in range(1, total_epochs + 1) if dataset.get_current_value(epoch))\nelif dataset:\nepochs_with_data = set(range(1, total_epochs + 1))\nreturn epochs_with_data\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n        Args:\n            data: Input data in dictionary format.\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        Returns:\n            The transformed data.\n        \"\"\"\ndata = deepcopy(data)\nops = get_current_items(self.ops, mode, epoch)\nforward_numpyop(ops, data, mode)\nfor key, value in data.items():\ndata[key] = np.expand_dims(value, 0)\nreturn data\ndef get_results(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n        Args:\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n            num_steps: Number of steps (batches) to get.\n            shuffle: Whether to use shuffling.\n        Returns:\n            A list of batches of Pipeline outputs.\n        \"\"\"\nresults = []\nloader = self.get_loader(mode=mode, epoch=epoch, shuffle=shuffle)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef get_loader(self, mode: str, epoch: int = 1,\nshuffle: Optional[bool] = None) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"Get a data loader from the Pipeline for a given `mode` and `epoch`.\n        Args:\n            mode: The execution mode for the loader. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index for the loader. Note that epoch indices are 1-indexed.\n            shuffle: Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument\n                is only used with FastEstimator Datasets.\n        Returns:\n            A data loader for the given `mode` and `epoch`.\n        \"\"\"\ndata = self.data[mode]\nif isinstance(data, Scheduler):\ndata = data.get_current_value(epoch)\nif isinstance(data, Dataset):\n# batch size\nbatch_size = self.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\n# batch dataset\nif isinstance(data, BatchDataset):\ndata.pad_value = self.pad_value\n# shuffle\nif shuffle is None:\nshuffle = mode == \"train\" and batch_size is not None\n# collate_fn\ncollate_fn = self.collate_fn\nif collate_fn is None and self.pad_value is not None:\ncollate_fn = self._pad_batch_collate\nop_dataset = OpDataset(data, get_current_items(self.ops, mode, epoch), mode)\ndata = DataLoader(op_dataset,\nbatch_size=None if isinstance(data, BatchDataset) else batch_size,\nshuffle=False if isinstance(data, BatchDataset) else shuffle,\nsampler=RandomSampler(op_dataset) if isinstance(data, BatchDataset) and shuffle else None,\nnum_workers=self.num_process,\ndrop_last=self.drop_last,\nworker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),\ncollate_fn=collate_fn)\nreturn data\ndef _pad_batch_collate(self, batch: List[MutableMapping[str, Any]]) -&gt; Dict[str, Any]:\n\"\"\"A collate function which pads a batch of data.\n        Args:\n            batch: The data to be batched and collated.\n        Returns:\n            A padded and collated batch of data.\n        \"\"\"\npad_batch(batch, self.pad_value)\nreturn default_collate(batch)\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.benchmark", "title": "<code>benchmark</code>", "text": "<p>Benchmark the pipeline processing speed.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to benchmark. This can be 'train', 'eval' or 'test'.</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to benchmark. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>num_steps</code> <code>int</code> <p>The maximum number of steps over which to perform the benchmark.</p> <code>1000</code> <code>log_interval</code> <code>int</code> <p>The logging interval.</p> <code>100</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def benchmark(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1000, log_interval: int = 100) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n    Args:\n        mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n        epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n        num_steps: The maximum number of steps over which to perform the benchmark.\n        log_interval: The logging interval.\n    \"\"\"\nloader = self.get_loader(mode=mode, epoch=epoch)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nprint(\"FastEstimator: Step: {}, Epoch: {}, Steps/sec: {}\".format(idx, epoch, iters_per_sec))\nstart = time.perf_counter()\nif idx == num_steps:\nbreak\n# Pipeline Operations Benchmarking when using FEDataset\nif isinstance(loader, DataLoader) and isinstance(loader.dataset, OpDataset):\nop_list = loader.dataset.ops\nduration_list = np.zeros(shape=(len(op_list)))\ndata_len = len(loader.dataset.dataset)\nif self.batch_size:\nlog_interval = log_interval * self.batch_size\nprint(\"\\nBreakdown of time taken by Pipeline Operations ({} epoch {})\".format(mode, epoch))\nfor _ in range(log_interval):\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif isinstance(loader.dataset.dataset, BatchDataset):\nunique_list = []\nfor item in items:\nif id(item) not in unique_list:\nfor i, op in enumerate(op_list):\nstart = time.perf_counter()\nforward_numpyop([op], item, loader.dataset.mode)\nduration = time.perf_counter() - start\nduration_list[i] += duration\nunique_list.append(id(item))\nelse:\nfor i, op in enumerate(op_list):\nstart = time.perf_counter()\nforward_numpyop([op], items, loader.dataset.mode)\nduration = time.perf_counter() - start\nduration_list[i] += duration\ntotal_time = np.sum(duration_list)\nop_names = [\"Op\"]\nfor op in op_list:\nif isinstance(op, Sometimes) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, OneOf) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelse:\nop_names.append(op.__class__.__name__)\nmax_op_len = max(len(op_name) for op_name in op_names)\nmax_in_len = max([len(\", \".join(op.inputs)) for op in op_list] + [len(\"Inputs\")])\nmax_out_len = max([len(\", \".join(op.outputs)) for op in op_list] + [len(\"Outputs\")])\nprint(\"{}: {}: {}: {}\".format(\"Op\".ljust(max_op_len + 1),\n\"Inputs\".ljust(max_in_len + 1),\n\"Outputs\".ljust(max_out_len + 1),\n\"Time\".rjust(5)))\nprint(\"-\" * (max_op_len + max_in_len + max_out_len + 15))\nfor i, op in enumerate(op_list):\nprint(\"{}: {}: {}: {:5.2f}%\".format(op_names[i + 1].ljust(max_op_len + 1),\n\", \".join(op.inputs).ljust(max_in_len + 1),\n\", \".join(op.outputs).ljust(max_out_len + 1),\n100 * duration_list[i] / total_time))\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_epochs_with_data", "title": "<code>get_epochs_with_data</code>", "text": "<p>Get a set of epoch indices that contains data given mode.</p> <p>Parameters:</p> Name Type Description Default <code>total_epochs</code> <code>int</code> <p>Total number of epochs.</p> required <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>Set[int]</code> <p>Set of epoch indices.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n    Args:\n        total_epochs: Total number of epochs.\n        mode: Current execution mode.\n    Returns:\n        Set of epoch indices.\n    \"\"\"\nepochs_with_data = set()\ndataset = self.data[mode]\nif isinstance(dataset, Scheduler):\nepochs_with_data = set(epoch for epoch in range(1, total_epochs + 1) if dataset.get_current_value(epoch))\nelif dataset:\nepochs_with_data = set(range(1, total_epochs + 1))\nreturn epochs_with_data\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_loader", "title": "<code>get_loader</code>", "text": "<p>Get a data loader from the Pipeline for a given <code>mode</code> and <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode for the loader. This can be 'train', 'eval' or 'test'.</p> required <code>epoch</code> <code>int</code> <p>The epoch index for the loader. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>shuffle</code> <code>Optional[bool]</code> <p>Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument is only used with FastEstimator Datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataLoader, tf.data.Dataset]</code> <p>A data loader for the given <code>mode</code> and <code>epoch</code>.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_loader(self, mode: str, epoch: int = 1,\nshuffle: Optional[bool] = None) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"Get a data loader from the Pipeline for a given `mode` and `epoch`.\n    Args:\n        mode: The execution mode for the loader. This can be 'train', 'eval' or 'test'.\n        epoch: The epoch index for the loader. Note that epoch indices are 1-indexed.\n        shuffle: Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument\n            is only used with FastEstimator Datasets.\n    Returns:\n        A data loader for the given `mode` and `epoch`.\n    \"\"\"\ndata = self.data[mode]\nif isinstance(data, Scheduler):\ndata = data.get_current_value(epoch)\nif isinstance(data, Dataset):\n# batch size\nbatch_size = self.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\n# batch dataset\nif isinstance(data, BatchDataset):\ndata.pad_value = self.pad_value\n# shuffle\nif shuffle is None:\nshuffle = mode == \"train\" and batch_size is not None\n# collate_fn\ncollate_fn = self.collate_fn\nif collate_fn is None and self.pad_value is not None:\ncollate_fn = self._pad_batch_collate\nop_dataset = OpDataset(data, get_current_items(self.ops, mode, epoch), mode)\ndata = DataLoader(op_dataset,\nbatch_size=None if isinstance(data, BatchDataset) else batch_size,\nshuffle=False if isinstance(data, BatchDataset) else shuffle,\nsampler=RandomSampler(op_dataset) if isinstance(data, BatchDataset) and shuffle else None,\nnum_workers=self.num_process,\ndrop_last=self.drop_last,\nworker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),\ncollate_fn=collate_fn)\nreturn data\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_modes", "title": "<code>get_modes</code>", "text": "<p>Get the modes for which the Pipeline has data.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>Optional[int]</code> <p>The current epoch index</p> <code>None</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes for which the Pipeline has data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n    Args:\n        epoch: The current epoch index\n    Returns:\n        The modes for which the Pipeline has data.\n    \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, dataset in self.data.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_results", "title": "<code>get_results</code>", "text": "<p>Get sample Pipeline outputs.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>num_steps</code> <code>int</code> <p>Number of steps (batches) to get.</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>Whether to use shuffling.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, Any]], Dict[str, Any]]</code> <p>A list of batches of Pipeline outputs.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_results(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n    Args:\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        num_steps: Number of steps (batches) to get.\n        shuffle: Whether to use shuffling.\n    Returns:\n        A list of batches of Pipeline outputs.\n    \"\"\"\nresults = []\nloader = self.get_loader(mode=mode, epoch=epoch, shuffle=shuffle)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Pipeline.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Pipeline.\n    \"\"\"\nall_items = self.ops + [self.batch_size] + [self.data[mode]]\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.transform", "title": "<code>transform</code>", "text": "<p>Apply all pipeline operations on a given data instance for the specified <code>mode</code> and <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Input data in dictionary format.</p> required <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".</p> required <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The transformed data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n    Args:\n        data: Input data in dictionary format.\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n    Returns:\n        The transformed data.\n    \"\"\"\ndata = deepcopy(data)\nops = get_current_items(self.ops, mode, epoch)\nforward_numpyop(ops, data, mode)\nfor key, value in data.items():\ndata[key] = np.expand_dims(value, 0)\nreturn data\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/pytorch/lenet.html#fastestimator.fastestimator.architecture.pytorch.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>         Bases: <code>torch.nn.Module</code></p> <p>A standard LeNet implementation in pytorch.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>The shape of the model input (channels, height, width).</p> <code>(1, 28, 28)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\lenet.py</code> <pre><code>class LeNet(torch.nn.Module):\n\"\"\"A standard LeNet implementation in pytorch.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: The shape of the model input (channels, height, width).\n        classes: The number of outputs the model should generate.\n    \"\"\"\ndef __init__(self, input_shape: Tuple[int, int, int] = (1, 28, 28), classes: int = 10) -&gt; None:\nsuper().__init__()\nconv_kernel = 3\nself.pool_kernel = 2\nself.conv1 = nn.Conv2d(input_shape[0], 32, conv_kernel)\nself.conv2 = nn.Conv2d(32, 64, conv_kernel)\nself.conv3 = nn.Conv2d(64, 64, conv_kernel)\nflat_x = ((((input_shape[1] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nflat_y = ((((input_shape[2] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nself.fc1 = nn.Linear(flat_x * flat_y * 64, 64)\nself.fc2 = nn.Linear(64, classes)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = fn.relu(self.conv1(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv2(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv3(x))\nx = x.view(x.size(0), -1)\nx = fn.relu(self.fc1(x))\nx = fn.softmax(self.fc2(x), dim=-1)\nreturn x\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNet", "title": "<code>UNet</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A standard UNet implementation in PyTorch.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (channels, height, width).</p> <code>(1, 128, 128)</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNet(nn.Module):\n\"\"\"A standard UNet implementation in PyTorch.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        input_size: The size of the input tensor (channels, height, width).\n    \"\"\"\ndef __init__(self, input_size: Tuple[int, int, int] = (1, 128, 128)) -&gt; None:\nsuper().__init__()\nself.input_size = input_size\nself.enc1 = UNetEncoderBlock(in_channels=input_size[0], out_channels=64)\nself.enc2 = UNetEncoderBlock(in_channels=64, out_channels=128)\nself.enc3 = UNetEncoderBlock(in_channels=128, out_channels=256)\nself.enc4 = UNetEncoderBlock(in_channels=256, out_channels=512)\nself.bottle_neck = UNetDecoderBlock(in_channels=512, mid_channels=1024, out_channels=512)\nself.dec4 = UNetDecoderBlock(in_channels=1024, mid_channels=512, out_channels=256)\nself.dec3 = UNetDecoderBlock(in_channels=512, mid_channels=256, out_channels=128)\nself.dec2 = UNetDecoderBlock(in_channels=256, mid_channels=128, out_channels=64)\nself.dec1 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 1, 1),\nnn.Sigmoid())\nfor layer in self.dec1:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx1, x_e1 = self.enc1(x)\nx2, x_e2 = self.enc2(x_e1)\nx3, x_e3 = self.enc3(x_e2)\nx4, x_e4 = self.enc4(x_e3)\nx_bottle_neck = self.bottle_neck(x_e4)\nx_d4 = self.dec4(torch.cat((x_bottle_neck, x4), 1))\nx_d3 = self.dec3(torch.cat((x_d4, x3), 1))\nx_d2 = self.dec2(torch.cat((x_d3, x2), 1))\nx_out = self.dec1(torch.cat((x_d2, x1), 1))\nreturn x_out\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetDecoderBlock", "title": "<code>UNetDecoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet decoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the decoder.</p> required <code>mid_channels</code> <code>int</code> <p>How many channels are used for the decoder's intermediate layer.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the decoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetDecoderBlock(nn.Module):\n\"\"\"A UNet decoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the decoder.\n        mid_channels: How many channels are used for the decoder's intermediate layer.\n        out_channels: How many channels leave the decoder.\n    \"\"\"\ndef __init__(self, in_channels: int, mid_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(mid_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\nnn.Conv2d(mid_channels, out_channels, 3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.layers(x)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetEncoderBlock", "title": "<code>UNetEncoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet encoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the encoder.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the encoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetEncoderBlock(nn.Module):\n\"\"\"A UNet encoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the encoder.\n        out_channels: How many channels leave the encoder.\n    \"\"\"\ndef __init__(self, in_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\nout = self.layers(x)\nreturn out, F.max_pool2d(out, 2)\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/lenet.html#fastestimator.fastestimator.architecture.tensorflow.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>A standard LeNet implementation in TensorFlow.</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>shape of the input data (height, width, channels).</p> <code>(28, 28, 1)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow LeNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\lenet.py</code> <pre><code>def LeNet(input_shape: Tuple[int, int, int] = (28, 28, 1), classes: int = 10) -&gt; tf.keras.Model:\n\"\"\"A standard LeNet implementation in TensorFlow.\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: shape of the input data (height, width, channels).\n        classes: The number of outputs the model should generate.\n    Returns:\n        A TensorFlow LeNet model.\n    \"\"\"\nmodel = Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(classes, activation='softmax'))\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/unet.html#fastestimator.fastestimator.architecture.tensorflow.unet.UNet", "title": "<code>UNet</code>", "text": "<p>A standard UNet implementation in pytorch</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> <code>(128, 128, 1)</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow LeNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\unet.py</code> <pre><code>def UNet(input_size: Tuple[int, int, int] = (128, 128, 1)) -&gt; tf.keras.Model:\n\"\"\"A standard UNet implementation in pytorch\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n    Returns:\n        A TensorFlow LeNet model.\n    \"\"\"\nconv_config = {'activation': 'relu', 'padding': 'same', 'kernel_initializer': 'he_normal'}\nup_config = {'size': (2, 2), 'interpolation': 'bilinear'}\ninputs = Input(input_size)\nconv1 = Conv2D(64, 3, **conv_config)(inputs)\nconv1 = Conv2D(64, 3, **conv_config)(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = Conv2D(128, 3, **conv_config)(pool1)\nconv2 = Conv2D(128, 3, **conv_config)(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = Conv2D(256, 3, **conv_config)(pool2)\nconv3 = Conv2D(256, 3, **conv_config)(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = Conv2D(512, 3, **conv_config)(pool3)\nconv4 = Conv2D(512, 3, **conv_config)(conv4)\ndrop4 = Dropout(0.5)(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\nconv5 = Conv2D(1024, 3, **conv_config)(pool4)\nconv5 = Conv2D(1024, 3, **conv_config)(conv5)\ndrop5 = Dropout(0.5)(conv5)\nup6 = Conv2D(512, 3, **conv_config)(UpSampling2D(**up_config)(drop5))\nmerge6 = concatenate([drop4, up6], axis=-1)\nconv6 = Conv2D(512, 3, **conv_config)(merge6)\nconv6 = Conv2D(512, 3, **conv_config)(conv6)\nup7 = Conv2D(256, 3, **conv_config)(UpSampling2D(**up_config)(conv6))\nmerge7 = concatenate([conv3, up7], axis=-1)\nconv7 = Conv2D(256, 3, **conv_config)(merge7)\nconv7 = Conv2D(256, 3, **conv_config)(conv7)\nup8 = Conv2D(128, 3, **conv_config)(UpSampling2D(**up_config)(conv7))\nmerge8 = concatenate([conv2, up8], axis=-1)\nconv8 = Conv2D(128, 3, **conv_config)(merge8)\nconv8 = Conv2D(128, 3, **conv_config)(conv8)\nup9 = Conv2D(64, 3, **conv_config)(UpSampling2D(**up_config)(conv8))\nmerge9 = concatenate([conv1, up9], axis=-1)\nconv9 = Conv2D(64, 3, **conv_config)(merge9)\nconv9 = Conv2D(64, 3, **conv_config)(conv9)\nconv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\nmodel = Model(inputs=inputs, outputs=conv10)\nreturn model\n</code></pre>"}, {"location": "fastestimator/backend/abs.html", "title": "abs", "text": ""}, {"location": "fastestimator/backend/abs.html#fastestimator.fastestimator.backend.abs.abs", "title": "<code>abs</code>", "text": "<p>Compute the absolute value of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.abs(n)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.abs(t)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.abs(p)  # [2, 7, 19]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute value of <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\abs.py</code> <pre><code>def abs(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the absolute value of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.abs(n)  # [2, 7, 19]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.abs(t)  # [2, 7, 19]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.abs(p)  # [2, 7, 19]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The absolute value of `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.abs(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.abs(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.abs(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/argmax.html", "title": "argmax", "text": ""}, {"location": "fastestimator/backend/argmax.html#fastestimator.fastestimator.backend.argmax.argmax", "title": "<code>argmax</code>", "text": "<p>Compute the index of the maximum value along a given axis of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>int</code> <p>Which axis to compute the index along.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices corresponding to the maximum values within <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\argmax.py</code> <pre><code>def argmax(tensor: Tensor, axis: int = 0) -&gt; Tensor:\n\"\"\"Compute the index of the maximum value along a given axis of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to compute the index along.\n    Returns:\n        The indices corresponding to the maximum values within `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.argmax(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.max(dim=axis, keepdim=False)[1]\nelif isinstance(tensor, np.ndarray):\nreturn np.argmax(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/binary_crossentropy.html", "title": "binary_crossentropy", "text": ""}, {"location": "fastestimator/backend/binary_crossentropy.html#fastestimator.fastestimator.backend.binary_crossentropy.binary_crossentropy", "title": "<code>binary_crossentropy</code>", "text": "<p>Compute binary crossentropy.</p> <p>This method is applicable when there are only two label classes (zero and one). There should be a single floating point prediction per example.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [1], [0]])\npred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [1], [0]])\npred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (batch, ...). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with the same shape as <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The binary crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a tensor with</p> <code>Tensor</code> <p>the same shape as <code>y_true</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\binary_crossentropy.py</code> <pre><code>def binary_crossentropy(y_pred: Tensor, y_true: Tensor, from_logits: bool = False, average_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute binary crossentropy.\n    This method is applicable when there are only two label classes (zero and one). There should be a single floating\n    point prediction per example.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [1], [0]])\n    pred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [1], [0]])\n    pred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (batch, ...). dtype: float32 or float16.\n        y_true: Ground truth class labels with the same shape as `y_pred`. dtype: int or float32 or float16.\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The binary crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a tensor with\n        the same shape as `y_true`.\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) is type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, torch.Tensor) or tf.is_tensor(y_pred), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, torch.Tensor) or tf.is_tensor(y_true), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.binary_crossentropy(y_pred=y_pred,\ny_true=tf.reshape(y_true, y_pred.shape),\nfrom_logits=from_logits)\nce = tf.reshape(ce, [ce.shape[0], -1])\nce = tf.reduce_mean(ce, 1)\nelse:\ny_true = y_true.to(torch.float)\nif from_logits:\nce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nelse:\nce = torch.nn.BCELoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nce = ce.view(ce.shape[0], -1)\nce = torch.mean(ce, dim=1)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/cast.html", "title": "cast", "text": ""}, {"location": "fastestimator/backend/cast.html#fastestimator.fastestimator.backend.cast.cast", "title": "<code>cast</code>", "text": "<p>Cast the data to a specific data type recursively.</p> This method can be used with Numpy data <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nfe.backend.to_type(data)\n# {'x': dtype('float64'), 'y': [dtype('float64'), dtype('float64')], 'z': {'key': dtype('float64')}}\ndata = fe.backend.cast(data, \"float16\")\nfe.backend.to_type(data)\n# {'x': dtype('float16'), 'y': [dtype('float16'), dtype('float16')], 'z': {'key': dtype('float16')}}\n</code></pre> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\nfe.backend.to_type(data) # {'x': tf.float32, 'y': [tf.float32, tf.float32], 'z': {'key': tf.float32}}\ndata = fe.backend.cast(data, \"uint8\")\nfe.backend.to_type(data) # {'x': tf.uint8, 'y': [tf.uint8, tf.uint8], 'z': {'key': tf.uint8}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nfe.backend.to_type(data) # {'x': torch.float32, 'y': [torch.float32, torch.float32], 'z': {'key': torch.float32}}\ndata = fe.backend.cast(data, \"float64\")\nfe.backend.to_type(data) # {'x': torch.float64, 'y': [torch.float64, torch.float64], 'z': {'key': torch.float64}}\n</code></pre></p> <p>Args:     data: A tensor or possibly nested collection of tensors.     dtype: Target data type, can be one of following: uint8, int8, int16, int32, int64, float16, float32, float64.</p> <p>Returns:     A collection with the same structure as <code>data</code> with target data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\cast.py</code> <pre><code>def cast(data: Union[Collection, Tensor], dtype: str) -&gt; Union[Collection, Tensor]:\n\"\"\"Cast the data to a specific data type recursively.\n   This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    fe.backend.to_type(data)\n    # {'x': dtype('float64'), 'y': [dtype('float64'), dtype('float64')], 'z': {'key': dtype('float64')}}\n    data = fe.backend.cast(data, \"float16\")\n    fe.backend.to_type(data)\n    # {'x': dtype('float16'), 'y': [dtype('float16'), dtype('float16')], 'z': {'key': dtype('float16')}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    fe.backend.to_type(data) # {'x': tf.float32, 'y': [tf.float32, tf.float32], 'z': {'key': tf.float32}}\n    data = fe.backend.cast(data, \"uint8\")\n    fe.backend.to_type(data) # {'x': tf.uint8, 'y': [tf.uint8, tf.uint8], 'z': {'key': tf.uint8}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    fe.backend.to_type(data) # {'x': torch.float32, 'y': [torch.float32, torch.float32], 'z': {'key': torch.float32}}\n    data = fe.backend.cast(data, \"float64\")\n    fe.backend.to_type(data) # {'x': torch.float64, 'y': [torch.float64, torch.float64], 'z': {'key': torch.float64}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        dtype: Target data type, can be one of following: uint8, int8, int16, int32, int64, float16, float32, float64.\n    Returns:\n        A collection with the same structure as `data` with target data type.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: cast(value, dtype) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [cast(val, dtype) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([cast(val, dtype) for val in data])\nelif isinstance(data, set):\nreturn set([cast(val, dtype) for val in data])\nelif tf.is_tensor(data):\nreturn tf.cast(data, STRING_TO_TF_DTYPE[dtype])\nelif isinstance(data, torch.Tensor):\nreturn data.type(STRING_TO_TORCH_DTYPE[dtype])\nelse:\nreturn np.array(data, dtype=dtype)\n</code></pre>"}, {"location": "fastestimator/backend/categorical_crossentropy.html", "title": "categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/categorical_crossentropy.html#fastestimator.fastestimator.backend.categorical_crossentropy.categorical_crossentropy", "title": "<code>categorical_crossentropy</code>", "text": "<p>Compute categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\categorical_crossentropy.py</code> <pre><code>def categorical_crossentropy(y_pred: Tensor, y_true: Tensor, from_logits: bool = False,\naverage_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32 or float16.\n        y_true: Ground truth class labels with a shape like `y_pred`. dtype: int or float32 or float16.\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nelse:\ny_true = y_true.to(torch.float)\nce = _categorical_crossentropy_torch(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/check_nan.html", "title": "check_nan", "text": ""}, {"location": "fastestimator/backend/check_nan.html#fastestimator.fastestimator.backend.check_nan.check_nan", "title": "<code>check_nan</code>", "text": "<p>Checks if the input contains NaN values.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, np.NaN]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[np.NaN, 6.0], [7.0, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [np.NaN, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[int, float, np.ndarray, tf.Tensor, torch.Tensor]</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True iff <code>val</code> contains NaN</p> Source code in <code>fastestimator\\fastestimator\\backend\\check_nan.py</code> <pre><code>def check_nan(val: Union[int, float, np.ndarray, tf.Tensor, torch.Tensor]) -&gt; bool:\n\"\"\"Checks if the input contains NaN values.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, np.NaN]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[np.NaN, 6.0], [7.0, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [np.NaN, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    Args:\n        val: The input value.\n    Returns:\n        True iff `val` contains NaN\n    \"\"\"\nif tf.is_tensor(val):\nreturn tf.reduce_any(tf.math.is_nan(val)) or tf.reduce_any(tf.math.is_inf(val))\nelif isinstance(val, torch.Tensor):\nreturn torch.isnan(val).any() or torch.isinf(val).any()\nelse:\nreturn np.isnan(val).any() or np.isinf(val).any()\n</code></pre>"}, {"location": "fastestimator/backend/clip_by_value.html", "title": "clip_by_value", "text": ""}, {"location": "fastestimator/backend/clip_by_value.html#fastestimator.fastestimator.backend.clip_by_value.clip_by_value", "title": "<code>clip_by_value</code>", "text": "<p>Clip a tensor such that <code>min_value</code> &lt;= tensor &lt;= <code>max_value</code>.</p> <p>Given an interval, values outside the interval are clipped. If <code>min_value</code> or <code>max_value</code> is not provided then clipping is not performed on lower or upper interval edge respectively.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(n, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(t, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(p, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>min_value</code> <code>Union[int, float, Tensor, None]</code> <p>The minimum value to clip to.</p> <code>None</code> <code>max_value</code> <code>Union[int, float, Tensor, None]</code> <p>The maximum value to clip to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code>, with it's values clipped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\clip_by_value.py</code> <pre><code>def clip_by_value(tensor: Tensor,\nmin_value: Union[int, float, Tensor, None] = None,\nmax_value: Union[int, float, Tensor, None] = None) -&gt; Tensor:\n\"\"\"Clip a tensor such that `min_value` &lt;= tensor &lt;= `max_value`.\n    Given an interval, values outside the interval are clipped. If `min_value` or `max_value` is not provided then\n    clipping is not performed on lower or upper interval edge respectively.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(n, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(t, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(p, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    Args:\n        tensor: The input value.\n        min_value: The minimum value to clip to.\n        max_value: The maximum value to clip to.\n    Returns:\n        The `tensor`, with it's values clipped.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nassert min_value is not None or max_value is not None, \"Both min_value and max_value must not be NoneType\"\nif tf.is_tensor(tensor):\nif min_value is None:\nreturn tf.math.minimum(tensor, max_value)\nelif max_value is None:\nreturn tf.math.maximum(tensor, min_value)\nelse:\nreturn tf.clip_by_value(tensor, clip_value_min=min_value, clip_value_max=max_value)\nelif isinstance(tensor, torch.Tensor):\nif isinstance(min_value, torch.Tensor):\nmin_value = min_value.item()\nif isinstance(max_value, torch.Tensor):\nmax_value = max_value.item()\nreturn tensor.clamp(min=min_value, max=max_value)\nelif isinstance(tensor, np.ndarray):\nreturn np.clip(tensor, a_min=min_value, a_max=max_value)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/concat.html", "title": "concat", "text": ""}, {"location": "fastestimator/backend/concat.html#fastestimator.fastestimator.backend.concat.concat", "title": "<code>concat</code>", "text": "<p>Concatenate a list of <code>tensors</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\nb = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\nb = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\nb = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>List[Tensor]</code> <p>A list of tensors to be concatenated.</p> required <code>axis</code> <code>int</code> <p>The axis along which to concatenate the input.</p> <code>0</code> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\concat.py</code> <pre><code>def concat(tensors: List[Tensor], axis: int = 0) -&gt; Optional[Tensor]:\n\"\"\"Concatenate a list of `tensors` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\n    b = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\n    b = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\n    b = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    Args:\n        tensors: A list of tensors to be concatenated.\n        axis: The axis along which to concatenate the input.\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensors` is an unacceptable data type.\n    \"\"\"\nif len(tensors) == 0:\nreturn None\nif tf.is_tensor(tensors[0]):\nreturn tf.concat(tensors, axis=axis)\nelif isinstance(tensors[0], torch.Tensor):\nreturn torch.cat(tensors, dim=axis)\nelif isinstance(tensors[0], np.ndarray):\nreturn np.concatenate(tensors, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensors[0])))\n</code></pre>"}, {"location": "fastestimator/backend/expand_dims.html", "title": "expand_dims", "text": ""}, {"location": "fastestimator/backend/expand_dims.html#fastestimator.fastestimator.backend.expand_dims.expand_dims", "title": "<code>expand_dims</code>", "text": "<p>Create a new dimension in <code>tensor</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([2,7,5])\nb = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([2,7,5])\nb = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([2,7,5])\nb = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input to be modified, having n dimensions.</p> required <code>axis</code> <code>int</code> <p>Which axis should the new axis be inserted along. Must be in the range [-n-1, n].</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\expand_dims.py</code> <pre><code>def expand_dims(tensor: Tensor, axis: int = 1) -&gt; Tensor:\n\"\"\"Create a new dimension in `tensor` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([2,7,5])\n    b = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([2,7,5])\n    b = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([2,7,5])\n    b = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n    ```\n    Args:\n        tensor: The input to be modified, having n dimensions.\n        axis: Which axis should the new axis be inserted along. Must be in the range [-n-1, n].\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.expand_dims(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.unsqueeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.expand_dims(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/feed_forward.html", "title": "feed_forward", "text": ""}, {"location": "fastestimator/backend/feed_forward.html#fastestimator.fastestimator.backend.feed_forward.feed_forward", "title": "<code>feed_forward</code>", "text": "<p>Run a forward step on a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.architecture.tensorflow.LeNet(classes=2)\nx = tf.ones((3,28,28,1))  # (batch, height, width, channels)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.architecture.pytorch.LeNet(classes=2)\nx = torch.ones((3,1,28,28))  # (batch, channels, height, width)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network to run the forward step through.</p> required <code>x</code> <code>Union[Tensor, np.ndarray]</code> <p>An input tensor for the <code>model</code>. This value will be auto-cast to either a tf.Tensor or torch.Tensor as applicable for the <code>model</code>.</p> required <code>training</code> <code>bool</code> <p>Whether this forward step is part of training or not. This may impact the behavior of <code>model</code> layers such as dropout.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The result of <code>model(x)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\feed_forward.py</code> <pre><code>def feed_forward(model: Union[tf.keras.Model, torch.nn.Module], x: Union[Tensor, np.ndarray],\ntraining: bool = True) -&gt; Tensor:\n\"\"\"Run a forward step on a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.architecture.tensorflow.LeNet(classes=2)\n    x = tf.ones((3,28,28,1))  # (batch, height, width, channels)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.architecture.pytorch.LeNet(classes=2)\n    x = torch.ones((3,1,28,28))  # (batch, channels, height, width)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    Args:\n        model: A neural network to run the forward step through.\n        x: An input tensor for the `model`. This value will be auto-cast to either a tf.Tensor or torch.Tensor as\n            applicable for the `model`.\n        training: Whether this forward step is part of training or not. This may impact the behavior of `model` layers\n            such as dropout.\n    Returns:\n        The result of `model(x)`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nif isinstance(model, tf.keras.Model):\nif not tf.is_tensor(x):\nx = to_tensor(x, \"tf\")\nx = model(x, training=training)\nelif isinstance(model, torch.nn.Module):\nmodel.train(mode=training)\nif not isinstance(x, torch.Tensor):\nx = to_tensor(x, \"torch\")\nx = model(x)\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn x\n</code></pre>"}, {"location": "fastestimator/backend/gather.html", "title": "gather", "text": ""}, {"location": "fastestimator/backend/gather.html#fastestimator.fastestimator.backend.gather.gather", "title": "<code>gather</code>", "text": "<p>Gather specific indices from a tensor.</p> <p>The <code>indices</code> will automatically be cast to the correct type (tf, torch, np) based on the type of the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>ind = np.array([1, 0, 1])\nn = np.array([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(n, ind)  # [[2, 3], [0, 1], [2, 3]]\nn = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(n, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>ind = tf.constant([1, 0, 1])\nt = tf.constant([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(t, ind)  # [[2, 3], [0, 1], [2, 3]]\nt = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(t, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>ind = torch.tensor([1, 0, 1])\np = torch.tensor([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(p, ind)  # [[2, 3], [0, 1], [2, 3]]\np = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(p, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>A tensor to gather values from.</p> required <code>indices</code> <code>Tensor</code> <p>A tensor indicating which indices should be selected. These represent locations along the 0 axis.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the elements from <code>tensor</code> at the given <code>indices</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\gather.py</code> <pre><code>def gather(tensor: Tensor, indices: Tensor) -&gt; Tensor:\n\"\"\"Gather specific indices from a tensor.\n    The `indices` will automatically be cast to the correct type (tf, torch, np) based on the type of the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    ind = np.array([1, 0, 1])\n    n = np.array([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(n, ind)  # [[2, 3], [0, 1], [2, 3]]\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(n, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    ind = tf.constant([1, 0, 1])\n    t = tf.constant([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(t, ind)  # [[2, 3], [0, 1], [2, 3]]\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(t, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    ind = torch.tensor([1, 0, 1])\n    p = torch.tensor([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(p, ind)  # [[2, 3], [0, 1], [2, 3]]\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(p, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    Args:\n        tensor: A tensor to gather values from.\n        indices: A tensor indicating which indices should be selected. These represent locations along the 0 axis.\n    Returns:\n        A tensor containing the elements from `tensor` at the given `indices`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nindices = to_tensor(indices, 'tf')\nindices = tf.cast(indices, tf.int64)\nreturn tf.gather(tensor, indices=squeeze(indices), axis=0)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor[squeeze(indices).type(torch.int64)]\nelif isinstance(tensor, np.ndarray):\nreturn np.take(tensor, squeeze(indices).astype('int64'), axis=0)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/gather_from_batch.html", "title": "gather_from_batch", "text": ""}, {"location": "fastestimator/backend/gather_from_batch.html#fastestimator.fastestimator.backend.gather_from_batch.gather_from_batch", "title": "<code>gather_from_batch</code>", "text": "<p>Gather specific indices from a batch of data.</p> <p>This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values. The <code>indices</code> will automatically be cast to the correct type (tf, torch, np) based on the type of the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>ind = np.array([1, 0, 1])\nn = np.array([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\nn = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>ind = tf.constant([1, 0, 1])\nt = tf.constant([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\nt = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>ind = torch.tensor([1, 0, 1])\np = torch.tensor([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\np = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>A tensor of shape (batch, d1, ..., dn).</p> required <code>indices</code> <code>Tensor</code> <p>A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (batch, d2, ..., dn) containing the elements from <code>tensor</code> at the given <code>indices</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\gather_from_batch.py</code> <pre><code>def gather_from_batch(tensor: Tensor, indices: Tensor) -&gt; Tensor:\n\"\"\"Gather specific indices from a batch of data.\n    This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values.\n    The `indices` will automatically be cast to the correct type (tf, torch, np) based on the type of the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    ind = np.array([1, 0, 1])\n    n = np.array([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    ind = tf.constant([1, 0, 1])\n    t = tf.constant([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    ind = torch.tensor([1, 0, 1])\n    p = torch.tensor([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    Args:\n        tensor: A tensor of shape (batch, d1, ..., dn).\n        indices: A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.\n    Returns:\n        A tensor of shape (batch, d2, ..., dn) containing the elements from `tensor` at the given `indices`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nindices = to_tensor(indices, 'tf')\nindices = tf.cast(indices, tf.int64)\nif len(indices.shape) == 1:  # Indices not batched\nindices = expand_dims(indices, 1)\nreturn tf.gather_nd(tensor, indices=indices, batch_dims=1)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor[torch.arange(tensor.shape[0]), squeeze(indices)]\nelif isinstance(tensor, np.ndarray):\nreturn tensor[np.arange(tensor.shape[0]), squeeze(indices)]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/get_gradient.html", "title": "get_gradient", "text": ""}, {"location": "fastestimator/backend/get_gradient.html#fastestimator.fastestimator.backend.get_gradient.get_gradient", "title": "<code>get_gradient</code>", "text": "<p>Calculate gradients of a target w.r.t sources.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.Variable([1.0, 2.0, 3.0])\nwith tf.GradientTape(persistent=True) as tape:\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\nb = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target (final) tensor.</p> required <code>sources</code> <code>Union[Iterable[Tensor], Tensor]</code> <p>A sequence of source (initial) tensors.</p> required <code>higher_order</code> <code>bool</code> <p>Whether the gradient will be used for higher order gradients.</p> <code>False</code> <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>TensorFlow gradient tape. Only needed when using the TensorFlow backend.</p> <code>None</code> <code>retain_graph</code> <code>bool</code> <p>Whether to retain PyTorch graph. Only valid when using the PyTorch backend.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Iterable[Tensor], Tensor]</code> <p>Gradient(s) of the <code>target</code> with respect to the <code>sources</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\get_gradient.py</code> <pre><code>def get_gradient(target: Tensor,\nsources: Union[Iterable[Tensor], Tensor],\nhigher_order: bool = False,\ntape: Optional[tf.GradientTape] = None,\nretain_graph: bool = True) -&gt; Union[Iterable[Tensor], Tensor]:\n\"\"\"Calculate gradients of a target w.r.t sources.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.Variable([1.0, 2.0, 3.0])\n    with tf.GradientTape(persistent=True) as tape:\n        y = x * x\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n    y = x * x\n    b = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\n    b = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n    ```\n    Args:\n        target: The target (final) tensor.\n        sources: A sequence of source (initial) tensors.\n        higher_order: Whether the gradient will be used for higher order gradients.\n        tape: TensorFlow gradient tape. Only needed when using the TensorFlow backend.\n        retain_graph: Whether to retain PyTorch graph. Only valid when using the PyTorch backend.\n    Returns:\n        Gradient(s) of the `target` with respect to the `sources`.\n    Raises:\n        ValueError: If `target` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(target):\nwith NonContext() if higher_order else tape.stop_recording():\ngradients = tape.gradient(target, sources)\nelif isinstance(target, torch.Tensor):\ngradients = torch.autograd.grad(target,\nsources,\ngrad_outputs=torch.ones_like(target),\nretain_graph=retain_graph,\ncreate_graph=higher_order,\nonly_inputs=True)\nif isinstance(sources, torch.Tensor):\n#  The behavior table of tf and torch backend\n#  ---------------------------------------------------------------\n#        | case 1                     | case 2                    |\n#  ---------------------------------------------------------------|\n#  tf    | target: tf.Tensor          | target: tf.Tensor         |\n#        | sources: tf.Tensor         | sources: [tf.Tensor]      |\n#        | gradients: tf.Tensor       | gradients: [tf.Tensor]    |\n# ----------------------------------------------------------------|\n#  torch | target: torch.Tensor       | target: tf.Tensor         |\n#        | sources: torch.Tensor      | sources: [tf.Tensor]      |\n#        | gradients: (torch.Tensor,) | gradients: (torch.Tensor,)|\n# ----------------------------------------------------------------\n# In order to make the torch behavior become the same as tf in case 1, need to unwrap the gradients when\n# source is not Iterable.\ngradients = gradients[0]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(target)))\nreturn gradients\n</code></pre>"}, {"location": "fastestimator/backend/get_image_dims.html", "title": "get_image_dims", "text": ""}, {"location": "fastestimator/backend/get_image_dims.html#fastestimator.fastestimator.backend.get_image_dims.get_image_dims", "title": "<code>get_image_dims</code>", "text": "<p>Get the <code>tensor</code> height, width and channels.</p> <p>This method can be used with Numpy data: <pre><code>n = np.random.random((2, 12, 12, 3))\nb = fe.backend.get_image_dims(n)  # (3, 12, 12)\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.random.uniform((2, 12, 12, 3))\nb = fe.backend.get_image_dims(t)  # (3, 12, 12)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.rand((2, 3, 12, 12))\nb = fe.backend.get_image_dims(p)  # (3, 12, 12)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Channels, height and width of the <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\get_image_dims.py</code> <pre><code>def get_image_dims(tensor: Tensor) -&gt; Tensor:\n\"\"\"Get the `tensor` height, width and channels.\n    This method can be used with Numpy data:\n    ```python\n    n = np.random.random((2, 12, 12, 3))\n    b = fe.backend.get_image_dims(n)  # (3, 12, 12)\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.random.uniform((2, 12, 12, 3))\n    b = fe.backend.get_image_dims(t)  # (3, 12, 12)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.rand((2, 3, 12, 12))\n    b = fe.backend.get_image_dims(p)  # (3, 12, 12)\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        Channels, height and width of the `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nassert len(tensor.shape) == 3 or len(tensor.shape) == 4, \"Number of dimensions of input must be either 3 or 4\"\nshape_length = len(tensor.shape)\nif tf.is_tensor(tensor) or isinstance(tensor, np.ndarray):\nreturn tensor.shape[-1], tensor.shape[-3], tensor.shape[-2]\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.shape[-3], tensor.shape[-2], tensor.shape[-1]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/get_lr.html", "title": "get_lr", "text": ""}, {"location": "fastestimator/backend/get_lr.html#fastestimator.fastestimator.backend.get_lr.get_lr", "title": "<code>get_lr</code>", "text": "<p>Get the learning rate of a given <code>model</code> generated by <code>fe.build</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to inspect.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The learning rate of <code>model</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\get_lr.py</code> <pre><code>def get_lr(model: Union[tf.keras.Model, torch.nn.Module]) -&gt; float:\n\"\"\"Get the learning rate of a given `model` generated by `fe.build`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    Args:\n        model: A neural network instance to inspect.\n    Returns:\n        The learning rate of `model`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"get_lr only accept models from fe.build\"\nif isinstance(model, tf.keras.Model):\nlr = tf.keras.backend.get_value(model.current_optimizer.lr)\nelif isinstance(model, torch.nn.Module):\nlr = model.current_optimizer.param_groups[0]['lr']\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn lr\n</code></pre>"}, {"location": "fastestimator/backend/hinge.html", "title": "hinge", "text": ""}, {"location": "fastestimator/backend/hinge.html#fastestimator.fastestimator.backend.hinge.hinge", "title": "<code>hinge</code>", "text": "<p>Calculate the hinge loss between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\nb = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\nb = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels which should take values of 1 or -1.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The hinge loss between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes or disparate types.</p> <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\hinge.py</code> <pre><code>def hinge(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate the hinge loss between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\n    b = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\n    b = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n    ```\n    Args:\n        y_true: Ground truth class labels which should take values of 1 or -1.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n    Returns:\n        The hinge loss between `y_true` and `y_pred`\n    Raises:\n        AssertionError: If `y_true` and `y_pred` have mismatched shapes or disparate types.\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\nassert y_pred.shape == y_true.shape, \\\n        f\"Hinge loss requires y_true and y_pred to have the same shape, but found {y_true.shape} and {y_pred.shape}\"\ny_true = cast(y_true, 'float32')\nreturn reduce_mean(clip_by_value(1.0 - y_true * y_pred, min_value=0), axis=-1)\n</code></pre>"}, {"location": "fastestimator/backend/iwd.html", "title": "iwd", "text": ""}, {"location": "fastestimator/backend/iwd.html#fastestimator.fastestimator.backend.iwd.iwd", "title": "<code>iwd</code>", "text": "<p>Compute the Inverse Weighted Distance from the given input.</p> <p>This can be used as an activation function for the final layer of a neural network instead of softmax. For example, instead of: model.add(layers.Dense(classes, activation='softmax')), you could use: model.add(layers.Dense(classes, activation=lambda x: iwd(tf.nn.sigmoid(x))))</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value. Should be of shape (Batch, C) where every element in C corresponds to a (non-negative) distance to a target class.</p> required <code>power</code> <code>float</code> <p>The power to raise the inverse distances to. 1.0 results in a fairly intuitive probability output. Larger powers can widen regions of certainty, whereas values between 0 and 1 can widen regions of uncertainty.</p> <code>1.0</code> <code>max_prob</code> <code>float</code> <p>The maximum probability to assign to a class estimate when it is distance zero away from the target. For numerical stability this must be less than 1.0. We have found that using smaller values like 0.95 can lead to natural adversarial robustness.</p> <code>0.95</code> <code>pairwise_distance</code> <code>float</code> <p>The distance to any other class when the distance to a target class is zero. For example, if you have a perfect match for class 'a', what distance should be reported to class 'b'. If you have a metric where this isn't constant, just use an approximate expected distance. In that case <code>max_prob</code> will only give you approximate control over the true maximum probability.</p> <code>1.0</code> <code>eps</code> <code>Optional[Tensor]</code> <p>The numeric stability constant to be used when d approaches zero. If None then it will be computed using <code>max_prob</code> and <code>pairwise_distance</code>. If not None, then <code>max_prob</code> and <code>pairwise_distance</code> will be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A probability distribution of shape (Batch, C) where smaller distances from <code>tensor</code> correspond to larger</p> <code>Tensor</code> <p>probabilities.</p> Source code in <code>fastestimator\\fastestimator\\backend\\iwd.py</code> <pre><code>def iwd(tensor: Tensor,\npower: float = 1.0,\nmax_prob: float = 0.95,\npairwise_distance: float = 1.0,\neps: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Compute the Inverse Weighted Distance from the given input.\n    This can be used as an activation function for the final layer of a neural network instead of softmax. For example,\n    instead of: model.add(layers.Dense(classes, activation='softmax')), you could use:\n    model.add(layers.Dense(classes, activation=lambda x: iwd(tf.nn.sigmoid(x))))\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    Args:\n        tensor: The input value. Should be of shape (Batch, C) where every element in C corresponds to a (non-negative)\n            distance to a target class.\n        power: The power to raise the inverse distances to. 1.0 results in a fairly intuitive probability output. Larger\n            powers can widen regions of certainty, whereas values between 0 and 1 can widen regions of uncertainty.\n        max_prob: The maximum probability to assign to a class estimate when it is distance zero away from the target.\n            For numerical stability this must be less than 1.0. We have found that using smaller values like 0.95 can\n            lead to natural adversarial robustness.\n        pairwise_distance: The distance to any other class when the distance to a target class is zero. For example, if\n            you have a perfect match for class 'a', what distance should be reported to class 'b'. If you have a metric\n            where this isn't constant, just use an approximate expected distance. In that case `max_prob` will only give\n            you approximate control over the true maximum probability.\n        eps: The numeric stability constant to be used when d approaches zero. If None then it will be computed using\n            `max_prob` and `pairwise_distance`. If not None, then `max_prob` and `pairwise_distance` will be ignored.\n    Returns:\n        A probability distribution of shape (Batch, C) where smaller distances from `tensor` correspond to larger\n        probabilities.\n    \"\"\"\nif eps is None:\neps = np.array(pairwise_distance * math.pow((1.0 - max_prob) / (max_prob * (tensor.shape[-1] - 1)), 1 / power),\ndtype=TENSOR_TO_NP_DTYPE[tensor.dtype])\neps = to_tensor(\neps, target_type='torch' if isinstance(tensor, torch.Tensor) else 'tf' if tf.is_tensor(tensor) else 'np')\nif isinstance(eps, torch.Tensor):\neps = eps.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntensor = maximum(tensor, eps)\ntensor = tensor_pow(1.0 / tensor, power)\ntensor = tensor / reshape(reduce_sum(tensor, axis=-1), shape=[-1, 1])\nreturn tensor\n</code></pre>"}, {"location": "fastestimator/backend/load_model.html", "title": "load_model", "text": ""}, {"location": "fastestimator/backend/load_model.html#fastestimator.fastestimator.backend.load_model.load_model", "title": "<code>load_model</code>", "text": "<p>Load saved weights for a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to load.</p> required <code>weights_path</code> <code>str</code> <p>Path to the <code>model</code> weights.</p> required <code>load_optimizer</code> <code>bool</code> <p>Whether to load optimizer. If True, then it will load  file in the path. <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\load_model.py</code> <pre><code>def load_model(model: Union[tf.keras.Model, torch.nn.Module], weights_path: str, load_optimizer: bool = False):\n\"\"\"Load saved weights for a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n    ```\n    Args:\n        model: A neural network instance to load.\n        weights_path: Path to the `model` weights.\n        load_optimizer: Whether to load optimizer. If True, then it will load &lt;weights_opt&gt; file in the path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif isinstance(model, tf.keras.Model):\nmodel.load_weights(weights_path)\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pkl\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nwith open(optimizer_path, 'rb') as f:\nweight_values = pickle.load(f)\nmodel.current_optimizer.set_weights(weight_values)\nelif isinstance(model, torch.nn.Module):\nmodel.load_state_dict(torch.load(weights_path))\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pt\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nmodel.current_optimizer.load_state_dict(torch.load(optimizer_path))\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/matmul.html", "title": "matmul", "text": ""}, {"location": "fastestimator/backend/matmul.html#fastestimator.fastestimator.backend.matmul.matmul", "title": "<code>matmul</code>", "text": "<p>Perform matrix multiplication on <code>a</code> and <code>b</code>.</p> <p>This method can be used with Numpy data: <pre><code>a = np.array([[0,1,2],[3,4,5]])\nb = np.array([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>a = tf.constant([[0,1,2],[3,4,5]])\nb = tf.constant([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>a = torch.tensor([[0,1,2],[3,4,5]])\nb = torch.tensor([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Tensor</code> <p>The first matrix.</p> required <code>b</code> <code>Tensor</code> <p>The second matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The matrix multiplication result of a * b.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If either <code>a</code> or <code>b</code> are unacceptable or non-matching data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\matmul.py</code> <pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n\"\"\"Perform matrix multiplication on `a` and `b`.\n    This method can be used with Numpy data:\n    ```python\n    a = np.array([[0,1,2],[3,4,5]])\n    b = np.array([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    a = tf.constant([[0,1,2],[3,4,5]])\n    b = tf.constant([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    a = torch.tensor([[0,1,2],[3,4,5]])\n    b = torch.tensor([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    Args:\n        a: The first matrix.\n        b: The second matrix.\n    Returns:\n        The matrix multiplication result of a * b.\n    Raises:\n        ValueError: If either `a` or `b` are unacceptable or non-matching data types.\n    \"\"\"\nif tf.is_tensor(a) and tf.is_tensor(b):\nreturn tf.matmul(a, b)\nelif isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\nreturn a.matmul(b)\nelif isinstance(a, np.ndarray) and isinstance(b, np.ndarray):\nreturn np.matmul(a, b)\nelif type(a) != type(b):\nraise ValueError(f\"Tensor types do not match ({type(a)} and {type(b)})\")\nelse:\nraise ValueError(f\"Unrecognized tensor type ({type(a)} or {type(b)})\")\n</code></pre>"}, {"location": "fastestimator/backend/maximum.html", "title": "maximum", "text": ""}, {"location": "fastestimator/backend/maximum.html#fastestimator.fastestimator.backend.maximum.maximum", "title": "<code>maximum</code>", "text": "<p>Get the maximum of the given <code>tensors</code>.</p> <p>This method can be used with Numpy data: <pre><code>n1 = np.array([[2, 7, 6]])\nn2 = np.array([[2, 7, 5]])\nres = fe.backend.maximum(n1, n2) # [[2, 7, 6]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t1 = tf.constant([[2, 7, 6]])\nt2 = tf.constant([[2, 7, 5]])\nres = fe.backend.maximum(t1, t2) # [[2, 7, 6]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p1 = torch.tensor([[2, 7, 6]])\np2 = torch.tensor([[2, 7, 5]])\nres = fe.backend.maximum(p1, p2) # [[2, 7, 6]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor1</code> <code>Tensor</code> <p>First tensor.</p> required <code>tensor2</code> <code>Tensor</code> <p>Second tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of two <code>tensors</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\maximum.py</code> <pre><code>def maximum(tensor1: Tensor, tensor2: Tensor) -&gt; Tensor:\n\"\"\"Get the maximum of the given `tensors`.\n    This method can be used with Numpy data:\n    ```python\n    n1 = np.array([[2, 7, 6]])\n    n2 = np.array([[2, 7, 5]])\n    res = fe.backend.maximum(n1, n2) # [[2, 7, 6]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t1 = tf.constant([[2, 7, 6]])\n    t2 = tf.constant([[2, 7, 5]])\n    res = fe.backend.maximum(t1, t2) # [[2, 7, 6]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p1 = torch.tensor([[2, 7, 6]])\n    p2 = torch.tensor([[2, 7, 5]])\n    res = fe.backend.maximum(p1, p2) # [[2, 7, 6]]\n    ```\n    Args:\n        tensor1: First tensor.\n        tensor2: Second tensor.\n    Returns:\n        The maximum of two `tensors`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor1) and tf.is_tensor(tensor2):\nreturn tf.maximum(tensor1, tensor2)\nelif isinstance(tensor1, torch.Tensor) and isinstance(tensor2, torch.Tensor):\nreturn torch.max(tensor1, tensor2)\nelif isinstance(tensor1, np.ndarray) and isinstance(tensor2, np.ndarray):\nreturn np.maximum(tensor1, tensor2)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor1)))\n</code></pre>"}, {"location": "fastestimator/backend/mean_squared_error.html", "title": "mean_squared_error", "text": ""}, {"location": "fastestimator/backend/mean_squared_error.html#fastestimator.fastestimator.backend.mean_squared_error.mean_squared_error", "title": "<code>mean_squared_error</code>", "text": "<p>Calculate mean squared error between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MSE between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes or disparate types.</p> <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\mean_squared_error.py</code> <pre><code>def mean_squared_error(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate mean squared error between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n    Returns:\n        The MSE between `y_true` and `y_pred`\n    Raises:\n        AssertionError: If `y_true` and `y_pred` have mismatched shapes or disparate types.\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be of the same tensor type\"\nassert y_pred.shape == y_true.shape, \\\n        f\"MSE requires y_true and y_pred to have the same shape, but found {y_true.shape} and {y_pred.shape}\"\nif tf.is_tensor(y_pred):\nmse = tf.losses.MSE(y_true, y_pred)\nelif isinstance(y_pred, torch.Tensor):\nmse = reduce_mean(\ntorch.nn.MSELoss(reduction=\"none\")(y_pred, y_true), axis=[ax for ax in range(y_pred.ndim)][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn mse\n</code></pre>"}, {"location": "fastestimator/backend/percentile.html", "title": "percentile", "text": ""}, {"location": "fastestimator/backend/percentile.html#fastestimator.fastestimator.backend.percentile.percentile", "title": "<code>percentile</code>", "text": "<p>Compute the <code>percentiles</code> of a <code>tensor</code>.</p> <p>The n-th percentile of <code>tensor</code> is the value n/100 of the way from the minimum to the maximum in a sorted copy of <code>tensor</code>. If the percentile falls in between two values, the lower of the two values will be used.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor from which to extract percentiles.</p> required <code>percentiles</code> <code>Union[int, List[int]]</code> <p>One or more percentile values to be computed.</p> required <code>axis</code> <code>Union[None, int, List[int]]</code> <p>Along which axes to compute the percentile (None to compute over all axes).</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to maintain the number of dimensions from <code>tensor</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>percentiles</code> of the given <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\percentile.py</code> <pre><code>def percentile(tensor: Tensor,\npercentiles: Union[int, List[int]],\naxis: Union[None, int, List[int]] = None,\nkeepdims: bool = True) -&gt; Tensor:\n\"\"\"Compute the `percentiles` of a `tensor`.\n    The n-th percentile of `tensor` is the value n/100 of the way from the minimum to the maximum in a sorted copy of\n    `tensor`. If the percentile falls in between two values, the lower of the two values will be used.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    Args:\n        tensor: The tensor from which to extract percentiles.\n        percentiles: One or more percentile values to be computed.\n        axis: Along which axes to compute the percentile (None to compute over all axes).\n        keepdims: Whether to maintain the number of dimensions from `tensor`.\n    Returns:\n        The `percentiles` of the given `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nif isinstance(percentiles, List):\npercentiles = tf.convert_to_tensor(percentiles)\nreturn tfp.stats.percentile(tensor, percentiles, axis=axis, keep_dims=keepdims, interpolation='lower')\nelif isinstance(tensor, torch.Tensor):\nn_dims = len(tensor.shape)\nif axis is None:\n# Default behavior in tf without axis is to compress all dimensions\naxis = list(range(n_dims))\n# Convert negative axis values to their positive counterparts\nif isinstance(axis, int):\naxis = [axis]\nfor idx, elem in enumerate(axis):\naxis[idx] = elem % n_dims\n# Extract dims which are not being considered\nother_dims = sorted(set(range(n_dims)).difference(axis))\n# Flatten all of the permutation axis down for kth-value computation\npermutation = other_dims + list(axis)\npermuted = tensor.permute(*permutation)\nother_shape = [tensor.shape[i] for i in other_dims]\nother_shape.append(np.prod([tensor.shape[i] for i in axis]))\npermuted = torch.reshape(permuted, other_shape)\nresults = []\nfor tile in to_list(percentiles):\ntarget = 1 + math.floor(tile / 100.0 * (permuted.shape[-1] - 1))\nkth_val = torch.kthvalue(permuted, k=target, dim=-1, keepdim=True)[0]\nfor dim in range(n_dims - len(kth_val.shape)):\nkth_val = torch.unsqueeze(kth_val, dim=-1)\n# Undo the permutation from earlier\nkth_val = kth_val.permute(*np.argsort(permutation))\nif not keepdims:\nfor dim in reversed(axis):\nkth_val = torch.squeeze(kth_val, dim=dim)\nresults.append(kth_val)\nif isinstance(percentiles, int):\nreturn results[0]\nelse:\nreturn torch.stack(results, dim=0)\nelif isinstance(tensor, np.ndarray):\nreturn np.percentile(tensor, percentiles, axis=axis, keepdims=keepdims, interpolation='lower')\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/permute.html", "title": "permute", "text": ""}, {"location": "fastestimator/backend/permute.html#fastestimator.fastestimator.backend.permute.permute", "title": "<code>permute</code>", "text": "<p>Perform the specified <code>permutation</code> on the axes of a given <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to permute.</p> required <code>permutation</code> <code>List[int]</code> <p>The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> with axes swapped according to the <code>permutation</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\permute.py</code> <pre><code>def permute(tensor: Tensor, permutation: List[int]) -&gt; Tensor:\n\"\"\"Perform the specified `permutation` on the axes of a given `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    Args:\n        tensor: The tensor to permute.\n        permutation: The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).\n    Returns:\n        The `tensor` with axes swapped according to the `permutation`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.transpose(tensor, perm=permutation)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.permute(*permutation)\nelif isinstance(tensor, np.ndarray):\nreturn np.transpose(tensor, axes=permutation)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/random_normal_like.html", "title": "random_normal_like", "text": ""}, {"location": "fastestimator/backend/random_normal_like.html#fastestimator.fastestimator.backend.random_normal_like.random_normal_like", "title": "<code>random_normal_like</code>", "text": "<p>Generate noise shaped like <code>tensor</code> from a random normal distribution with a given <code>mean</code> and <code>std</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution to be sampled.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>The standard deviation of the normal distribution to be sampled.</p> <code>1.0</code> <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. This should be one of the floating point types.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of random normal noise with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\random_normal_like.py</code> <pre><code>def random_normal_like(tensor: Tensor, mean: float = 0.0, std: float = 1.0,\ndtype: Union[None, str] = 'float32') -&gt; Tensor:\n\"\"\"Generate noise shaped like `tensor` from a random normal distribution with a given `mean` and `std`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        mean: The mean of the normal distribution to be sampled.\n        std: The standard deviation of the normal distribution to be sampled.\n        dtype: The data type to be used when generating the resulting tensor. This should be one of the floating point\n            types.\n    Returns:\n        A tensor of random normal noise with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.random.normal(shape=tensor.shape, mean=mean, stddev=std, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.randn_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype]) * std + mean\nelif isinstance(tensor, np.ndarray):\nreturn np.random.normal(loc=mean, scale=std, size=tensor.shape).astype(dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_max.html", "title": "reduce_max", "text": ""}, {"location": "fastestimator/backend/reduce_max.html#fastestimator.fastestimator.backend.reduce_max.reduce_max", "title": "<code>reduce_max</code>", "text": "<p>Compute the maximum value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(n)  # 8\nb = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(t)  # 8\nb = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(p)  # 8\nb = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the maximum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_max.py</code> <pre><code>def reduce_max(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the maximum value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(n)  # 8\n    b = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(t)  # 8\n    b = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(p)  # 8\n    b = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the maximum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The maximum values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_max(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.max(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.max(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_mean.html", "title": "reduce_mean", "text": ""}, {"location": "fastestimator/backend/reduce_mean.html#fastestimator.fastestimator.backend.reduce_mean.reduce_mean", "title": "<code>reduce_mean</code>", "text": "<p>Compute the mean value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(n)  # 4.5\nb = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(t)  # 4.5\nb = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\nb = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(p)  # 4.5\nb = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the mean along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_mean.py</code> <pre><code>def reduce_mean(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the mean value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(n)  # 4.5\n    b = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(t)  # 4.5\n    b = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\n    b = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(p)  # 4.5\n    b = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the mean along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The mean values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_mean(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nif not keepdims:\nreturn tensor.mean()\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.mean(dim=ax, keepdim=keepdims)\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.mean(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_min.html", "title": "reduce_min", "text": ""}, {"location": "fastestimator/backend/reduce_min.html#fastestimator.fastestimator.backend.reduce_min.reduce_min", "title": "<code>reduce_min</code>", "text": "<p>Compute the min value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(n)  # 1\nb = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(t)  # 1\nb = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(p)  # 1\nb = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the min along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The min values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_min.py</code> <pre><code>def reduce_min(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the min value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(n)  # 1\n    b = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(t)  # 1\n    b = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(p)  # 1\n    b = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the min along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The min values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_min(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.min(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.min(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_sum.html", "title": "reduce_sum", "text": ""}, {"location": "fastestimator/backend/reduce_sum.html#fastestimator.fastestimator.backend.reduce_sum.reduce_sum", "title": "<code>reduce_sum</code>", "text": "<p>Compute the sum along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(n)  # 36\nb = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(t)  # 36\nb = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(p)  # 36\nb = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the sum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_sum.py</code> <pre><code>def reduce_sum(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the sum along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(n)  # 36\n    b = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(t)  # 36\n    b = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(p)  # 36\n    b = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the sum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The sum of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_sum(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\nreturn tensor.sum(dim=axis, keepdim=keepdims)\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.sum(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/backend/reshape.html#fastestimator.fastestimator.backend.reshape.reshape", "title": "<code>reshape</code>", "text": "<p>Reshape a <code>tensor</code> to conform to a given shape.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>shape</code> <code>List[int]</code> <p>The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left should be packed into that axis.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reshape.py</code> <pre><code>def reshape(tensor: Tensor, shape: List[int]) -&gt; Tensor:\n\"\"\"Reshape a `tensor` to conform to a given shape.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    Args:\n        tensor: The input value.\n        shape: The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left\n            should be packed into that axis.\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reshape(tensor, shape=shape)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.reshape(tensor, shape=shape)\nelif isinstance(tensor, np.ndarray):\nreturn np.reshape(tensor, shape)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/roll.html", "title": "roll", "text": ""}, {"location": "fastestimator/backend/roll.html#fastestimator.fastestimator.backend.roll.roll", "title": "<code>roll</code>", "text": "<p>Roll a <code>tensor</code> elements along a given axis.</p> <p>The elements are shifted forward or reverse direction by the offset of <code>shift</code>. Overflown elements beyond the last position will be re-introduced at the first position.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(n, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(n, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(n, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(n, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(t, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(t, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(t, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(t, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(p, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(p, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(p, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(p, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>shift</code> <code>Union[int, List[int]]</code> <p>The number of places by which the elements need to be shifted. If shift is a list, axis must be a list of same size.</p> required <code>axis</code> <code>Union[int, List[int]]</code> <p>axis along which elements will be rolled.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The rolled <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\roll.py</code> <pre><code>def roll(tensor: Tensor, shift: Union[int, List[int]], axis: Union[int, List[int]]) -&gt; Tensor:\n\"\"\"Roll a `tensor` elements along a given axis.\n    The elements are shifted forward or reverse direction by the offset of `shift`. Overflown elements beyond the last\n    position will be re-introduced at the first position.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(n, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(n, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(n, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(n, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(t, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(t, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(t, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(t, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(p, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(p, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(p, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(p, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    Args:\n        tensor: The input value.\n        shift: The number of places by which the elements need to be shifted. If shift is a list, axis must be a list of\n            same size.\n        axis: axis along which elements will be rolled.\n    Returns:\n        The rolled `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.roll(tensor, shift=shift, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.roll(tensor, shifts=shift, dims=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.roll(tensor, shift=shift, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/save_model.html", "title": "save_model", "text": ""}, {"location": "fastestimator/backend/save_model.html#fastestimator.fastestimator.backend.save_model.save_model", "title": "<code>save_model</code>", "text": "<p>Save <code>model</code> weights to a specific directory.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to save.</p> required <code>save_dir</code> <code>str</code> <p>Directory into which to write the <code>model</code> weights.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model (used for naming the weights file). If None, model.model_name will be used.</p> <code>None</code> <code>save_optimizer</code> <code>bool</code> <p>Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.</p> <code>False</code> <p>Returns:</p> Type Description <p>The saved model path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\save_model.py</code> <pre><code>def save_model(model: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmodel_name: Optional[str] = None,\nsave_optimizer: bool = False):\n\"\"\"Save `model` weights to a specific directory.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n    ```\n    Args:\n        model: A neural network instance to save.\n        save_dir: Directory into which to write the `model` weights.\n        model_name: The name of the model (used for naming the weights file). If None, model.model_name will be used.\n        save_optimizer: Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.\n    Returns:\n        The saved model path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif model_name is None:\nmodel_name = model.model_name\nsave_dir = os.path.normpath(save_dir)\nos.makedirs(save_dir, exist_ok=True)\nif isinstance(model, tf.keras.Model):\nmodel_path = os.path.join(save_dir, \"{}.h5\".format(model_name))\nmodel.save_weights(model_path)\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pkl\".format(model_name))\nwith open(optimizer_path, 'wb') as f:\npickle.dump(model.current_optimizer.get_weights(), f)\nreturn model_path\nelif isinstance(model, torch.nn.Module):\nmodel_path = os.path.join(save_dir, \"{}.pt\".format(model_name))\ntorch.save(model.state_dict(), model_path)\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pt\".format(model_name))\ntorch.save(model.current_optimizer.state_dict(), optimizer_path)\nreturn model_path\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/set_lr.html", "title": "set_lr", "text": ""}, {"location": "fastestimator/backend/set_lr.html#fastestimator.fastestimator.backend.set_lr.set_lr", "title": "<code>set_lr</code>", "text": "<p>Set the learning rate of a given <code>model</code> generated by <code>fe.build</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to modify.</p> required <code>lr</code> <code>float</code> <p>The learning rate to assign to the <code>model</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\set_lr.py</code> <pre><code>def set_lr(model: Union[tf.keras.Model, torch.nn.Module], lr: float):\n\"\"\"Set the learning rate of a given `model` generated by `fe.build`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n    ```\n    Args:\n        model: A neural network instance to modify.\n        lr: The learning rate to assign to the `model`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"set_lr only accept models from fe.build\"\nif isinstance(model, tf.keras.Model):\ntf.keras.backend.set_value(model.current_optimizer.lr, lr)\nelif isinstance(model, torch.nn.Module):\nfor param_group in model.current_optimizer.param_groups:\nparam_group['lr'] = lr\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/sign.html", "title": "sign", "text": ""}, {"location": "fastestimator/backend/sign.html#fastestimator.fastestimator.backend.sign.sign", "title": "<code>sign</code>", "text": "<p>Compute the sign of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.sign(n)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.sign(t)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.sign(p)  # [-1, 1, -1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sign of each value of the <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\sign.py</code> <pre><code>def sign(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the sign of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.sign(n)  # [-1, 1, -1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.sign(t)  # [-1, 1, -1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.sign(p)  # [-1, 1, -1]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The sign of each value of the `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.sign(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.sign()\nelif isinstance(tensor, np.ndarray):\nreturn np.sign(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/sparse_categorical_crossentropy.html", "title": "sparse_categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/sparse_categorical_crossentropy.html#fastestimator.fastestimator.backend.sparse_categorical_crossentropy.sparse_categorical_crossentropy", "title": "<code>sparse_categorical_crossentropy</code>", "text": "<p>Compute sparse categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [2]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [2]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a softmax will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sparse categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\sparse_categorical_crossentropy.py</code> <pre><code>def sparse_categorical_crossentropy(y_pred: Tensor,\ny_true: Tensor,\nfrom_logits: bool = False,\naverage_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute sparse categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [2]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [2]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32 or float16.\n        y_true: Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.\n        from_logits: Whether y_pred is from logits. If True, a softmax will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The sparse categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nelse:\ny_true = y_true.view(-1)\nif from_logits:\nce = torch.nn.CrossEntropyLoss(reduction=\"none\")(input=y_pred, target=y_true.long())\nelse:\nce = torch.nn.NLLLoss(reduction=\"none\")(input=torch.log(y_pred), target=y_true.long())\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/squeeze.html", "title": "squeeze", "text": ""}, {"location": "fastestimator/backend/squeeze.html#fastestimator.fastestimator.backend.squeeze.squeeze", "title": "<code>squeeze</code>", "text": "<p>Remove an <code>axis</code> from a <code>tensor</code> if that axis has length 1.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Optional[int]</code> <p>Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\squeeze.py</code> <pre><code>def squeeze(tensor: Tensor, axis: Optional[int] = None) -&gt; Tensor:\n\"\"\"Remove an `axis` from a `tensor` if that axis has length 1.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.squeeze(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nreturn torch.squeeze(tensor)\nelse:\nreturn torch.squeeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.squeeze(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/tensor_pow.html", "title": "tensor_pow", "text": ""}, {"location": "fastestimator/backend/tensor_pow.html#fastestimator.fastestimator.backend.tensor_pow.tensor_pow", "title": "<code>tensor_pow</code>", "text": "<p>Computes x^power element-wise along <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(n, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(n, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(t, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(t, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(p, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(p, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>power</code> <code>Union[int, float]</code> <p>The power to which to raise the elements in the <code>tensor</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> raised element-wise to the given <code>power</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\tensor_pow.py</code> <pre><code>def tensor_pow(tensor: Tensor, power: Union[int, float]) -&gt; Tensor:\n\"\"\"Computes x^power element-wise along `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(n, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(n, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(t, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(t, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(p, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(p, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    Args:\n        tensor: The input tensor.\n        power: The power to which to raise the elements in the `tensor`.\n    Returns:\n        The `tensor` raised element-wise to the given `power`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.pow(tensor, power)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.pow(tensor, power)\nelif isinstance(tensor, np.ndarray):\nreturn np.power(tensor, power)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/tensor_round.html", "title": "tensor_round", "text": ""}, {"location": "fastestimator/backend/tensor_round.html#fastestimator.fastestimator.backend.tensor_round.tensor_round", "title": "<code>tensor_round</code>", "text": "<p>Element-wise rounds the values of the <code>tensor</code> to nearest integer.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1.25, 4.5, 6], [4, 9.11, 16]])\nb = fe.backend.tensor_round(n)  # [[1, 4, 6], [4, 9, 16]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1.25, 4.5, 6], [4, 9.11, 16.9]])\nb = fe.backend.tensor_round(t)  # [[1, 4, 6], [4, 9, 17]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1.25, 4.5, 6], [4, 9.11, 16]])\nb = fe.backend.tensor_round(p)  # [[1, 4, 6], [4, 9, 16]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The rounded <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\tensor_round.py</code> <pre><code>def tensor_round(tensor: Tensor) -&gt; Tensor:\n\"\"\"Element-wise rounds the values of the `tensor` to nearest integer.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1.25, 4.5, 6], [4, 9.11, 16]])\n    b = fe.backend.tensor_round(n)  # [[1, 4, 6], [4, 9, 16]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1.25, 4.5, 6], [4, 9.11, 16.9]])\n    b = fe.backend.tensor_round(t)  # [[1, 4, 6], [4, 9, 17]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1.25, 4.5, 6], [4, 9.11, 16]])\n    b = fe.backend.tensor_round(p)  # [[1, 4, 6], [4, 9, 16]]\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        The rounded `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.round(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.round(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.round(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/tensor_sqrt.html", "title": "tensor_sqrt", "text": ""}, {"location": "fastestimator/backend/tensor_sqrt.html#fastestimator.fastestimator.backend.tensor_sqrt.tensor_sqrt", "title": "<code>tensor_sqrt</code>", "text": "<p>Computes element-wise square root of tensor elements.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 4, 6], [4, 9, 16]])\nb = fe.backend.tensor_sqrt(n)  # [[1.0, 2.0, 2.44948974], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 4, 6], [4, 9, 16]], dtype=tf.float32)\nb = fe.backend.tensor_sqrt(t)  # [[1.0, 2.0, 2.4494898], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1, 4, 6], [4, 9, 16]], dtype=torch.float32)\nb = fe.backend.tensor_sqrt(p)  # [[1.0, 2.0, 2.4495], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> that contains square root of input values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\tensor_sqrt.py</code> <pre><code>def tensor_sqrt(tensor: Tensor) -&gt; Tensor:\n\"\"\"Computes element-wise square root of tensor elements.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 4, 6], [4, 9, 16]])\n    b = fe.backend.tensor_sqrt(n)  # [[1.0, 2.0, 2.44948974], [2.0, 3.0, 4.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 4, 6], [4, 9, 16]], dtype=tf.float32)\n    b = fe.backend.tensor_sqrt(t)  # [[1.0, 2.0, 2.4494898], [2.0, 3.0, 4.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1, 4, 6], [4, 9, 16]], dtype=torch.float32)\n    b = fe.backend.tensor_sqrt(p)  # [[1.0, 2.0, 2.4495], [2.0, 3.0, 4.0]]\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        The `tensor` that contains square root of input values.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.sqrt(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.sqrt(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.sqrt(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/to_shape.html", "title": "to_shape", "text": ""}, {"location": "fastestimator/backend/to_shape.html#fastestimator.fastestimator.backend.to_shape.to_shape", "title": "<code>to_shape</code>", "text": "<p>Compute the shape of tensors within a collection of <code>data</code>recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>add_batch</code> <p>Whether to prepend a batch dimension to the shapes.</p> <code>False</code> <code>exact_shape</code> <p>Whether to return the exact shapes, or if False to fill the shapes with None values.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Collection, Tensor]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their shapes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_shape.py</code> <pre><code>def to_shape(data: Union[Collection, Tensor], add_batch=False, exact_shape=True) -&gt; Union[Collection, Tensor]:\n\"\"\"Compute the shape of tensors within a collection of `data`recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        add_batch: Whether to prepend a batch dimension to the shapes.\n        exact_shape: Whether to return the exact shapes, or if False to fill the shapes with None values.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their shapes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_shape(value, add_batch, exact_shape) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_shape(val, add_batch, exact_shape) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_shape(val, add_batch, exact_shape) for val in data])\nelif isinstance(data, set):\nreturn set([to_shape(val, add_batch, exact_shape) for val in data])\nelif hasattr(data, \"shape\"):\nshape = data.shape\nif not exact_shape:\nshape = [None] * len(shape)\nif add_batch:\nshape = [None] + list(shape)\nreturn shape\nelse:\nreturn to_shape(np.array(data), add_batch, exact_shape)\n</code></pre>"}, {"location": "fastestimator/backend/to_tensor.html", "title": "to_tensor", "text": ""}, {"location": "fastestimator/backend/to_tensor.html#fastestimator.fastestimator.backend.to_tensor.to_tensor", "title": "<code>to_tensor</code>", "text": "<p>Convert tensors within a collection of <code>data</code> to a given <code>target_type</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor, float, int, None]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>target_type</code> <code>str</code> <p>What kind of tensor(s) to create, either \"tf\" or \"torch\".</p> required <p>Returns:</p> Type Description <code>Union[Collection, Tensor, None]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors converted to the <code>target_type</code>.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_tensor.py</code> <pre><code>def to_tensor(data: Union[Collection, Tensor, float, int, None], target_type: str) -&gt; Union[Collection, Tensor, None]:\n\"\"\"Convert tensors within a collection of `data` to a given `target_type` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        target_type: What kind of tensor(s) to create, either \"tf\" or \"torch\".\n    Returns:\n        A collection with the same structure as `data`, but with any tensors converted to the `target_type`.\n    \"\"\"\ntarget_instance = {\"tf\": tf.Tensor, \"torch\": torch.Tensor, \"np\": np.ndarray}\nconversion_function = {\"tf\": tf.convert_to_tensor, \"torch\": torch.from_numpy, \"np\": np.array}\nif isinstance(data, target_instance[target_type]):\nreturn data\nelif data is None:\nreturn None\nelif isinstance(data, dict):\nreturn {key: to_tensor(value, target_type) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_tensor(val, target_type) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_tensor(val, target_type) for val in data])\nelif isinstance(data, set):\nreturn set([to_tensor(val, target_type) for val in data])\nelse:\nreturn conversion_function[target_type](np.array(data))\n</code></pre>"}, {"location": "fastestimator/backend/to_type.html", "title": "to_type", "text": ""}, {"location": "fastestimator/backend/to_type.html#fastestimator.fastestimator.backend.to_type.to_type", "title": "<code>to_type</code>", "text": "<p>Compute the data types of tensors within a collection of <code>data</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\ndtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\ntypes = fe.backend.to_type(data)\n# {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <p>Returns:</p> Type Description <code>Union[Collection, str]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their dtypes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_type.py</code> <pre><code>def to_type(data: Union[Collection, Tensor]) -&gt; Union[Collection, str]:\n\"\"\"Compute the data types of tensors within a collection of `data` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\n        dtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\n    types = fe.backend.to_type(data)\n    # {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their dtypes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_type(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_type(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_type(val) for val in data])\nelif isinstance(data, set):\nreturn set([to_type(val) for val in data])\nelif hasattr(data, \"dtype\"):\nreturn data.dtype\nelse:\nreturn np.array(data).dtype\n</code></pre>"}, {"location": "fastestimator/backend/transpose.html", "title": "transpose", "text": ""}, {"location": "fastestimator/backend/transpose.html#fastestimator.fastestimator.backend.transpose.transpose", "title": "<code>transpose</code>", "text": "<p>Transpose the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(n)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(t)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(p)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The transposed <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\transpose.py</code> <pre><code>def transpose(tensor: Tensor) -&gt; Tensor:\n\"\"\"Transpose the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(n)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(t)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(p)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The transposed `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.transpose(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.T\nelif isinstance(tensor, np.ndarray):\nreturn np.transpose(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/update_model.html", "title": "update_model", "text": ""}, {"location": "fastestimator/backend/update_model.html#fastestimator.fastestimator.backend.update_model.update_model", "title": "<code>update_model</code>", "text": "<p>Update <code>model</code> weights based on a given <code>loss</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nx = tf.ones((3,28,28,1))  # (batch, height, width, channels)\ny = tf.constant((1, 0, 1))\nwith tf.GradientTape(persistent=True) as tape:\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\nfe.backend.update_model(m, loss=loss, tape=tape)\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nx = torch.ones((3,1,28,28))  # (batch, channels, height, width)\ny = torch.tensor((1, 0, 1))\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\nfe.backend.update_model(m, loss=loss)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to update.</p> required <code>loss</code> <code>Union[tf.Tensor, torch.Tensor]</code> <p>A loss value to compute gradients from.</p> required <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>A TensorFlow GradientTape which was recording when the <code>loss</code> was computed (iff using TensorFlow).</p> <code>None</code> <code>retain_graph</code> <code>bool</code> <p>Whether to keep the model graph in memory (applicable only for PyTorch).</p> <code>True</code> <code>scaler</code> <code>Optional[torch.cuda.amp.GradScaler]</code> <p>A PyTorch loss scaler that scales loss when PyTorch mixed precision is used.</p> <code>None</code> <code>defer</code> <code>bool</code> <p>If True, then the model update function will be stored into the <code>deferred</code> dictionary rather than applied immediately.</p> <code>False</code> <code>deferred</code> <code>Optional[Dict[str, List[Callable[[], None]]]]</code> <p>A dictionary in which model update functions are stored.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> <code>RuntimeError</code> <p>If attempting to modify a PyTorch model which relied on gradients within a different PyTorch model which has in turn already undergone a non-deferred update.</p> Source code in <code>fastestimator\\fastestimator\\backend\\update_model.py</code> <pre><code>def update_model(model: Union[tf.keras.Model, torch.nn.Module],\nloss: Union[tf.Tensor, torch.Tensor],\ntape: Optional[tf.GradientTape] = None,\nretain_graph: bool = True,\nscaler: Optional[torch.cuda.amp.GradScaler] = None,\ndefer: bool = False,\ndeferred: Optional[Dict[str, List[Callable[[], None]]]] = None) -&gt; None:\n\"\"\"Update `model` weights based on a given `loss`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    x = tf.ones((3,28,28,1))  # (batch, height, width, channels)\n    y = tf.constant((1, 0, 1))\n    with tf.GradientTape(persistent=True) as tape:\n        pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n        loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n        fe.backend.update_model(m, loss=loss, tape=tape)\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    x = torch.ones((3,1,28,28))  # (batch, channels, height, width)\n    y = torch.tensor((1, 0, 1))\n    pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n    fe.backend.update_model(m, loss=loss)\n    ```\n    Args:\n        model: A neural network instance to update.\n        loss: A loss value to compute gradients from.\n        tape: A TensorFlow GradientTape which was recording when the `loss` was computed (iff using TensorFlow).\n        retain_graph: Whether to keep the model graph in memory (applicable only for PyTorch).\n        scaler: A PyTorch loss scaler that scales loss when PyTorch mixed precision is used.\n        defer: If True, then the model update function will be stored into the `deferred` dictionary rather than\n            applied immediately.\n        deferred: A dictionary in which model update functions are stored.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n        RuntimeError: If attempting to modify a PyTorch model which relied on gradients within a different PyTorch model\n            which has in turn already undergone a non-deferred update.\n    \"\"\"\nloss = reduce_mean(loss)\nif isinstance(model, tf.keras.Model):\n# scale up loss for mixed precision training to avoid underflow\nif isinstance(model.current_optimizer, mixed_precision.LossScaleOptimizer):\nloss = model.current_optimizer.get_scaled_loss(loss)\n# for multi-gpu training, the gradient will be combined by sum, normalize the loss\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nloss = loss / strategy.num_replicas_in_sync\ngradients = get_gradient(loss, model.trainable_variables, tape=tape)\nwith tape.stop_recording():\n# scale down gradient to balance scale-up loss\nif isinstance(model.current_optimizer, mixed_precision.LossScaleOptimizer):\ngradients = model.current_optimizer.get_unscaled_gradients(gradients)\nif defer:\ndeferred.setdefault(model.model_name, []).append(\nlambda: model.current_optimizer.apply_gradients(zip(gradients, model.trainable_variables)))\nelse:\nmodel.current_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\nelif isinstance(model, torch.nn.Module):\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\n# scale up loss for mixed precision training to avoid underflow\nif scaler is not None:\nloss = scaler.scale(loss)\ntry:\ngradients = get_gradient(loss, trainable_params, retain_graph=retain_graph)\nexcept RuntimeError as err:\nif err.args and isinstance(err.args[0], str) and err.args[0].startswith(\n'one of the variables needed for gradient computation has been modified by an inplace operation'):\nraise RuntimeError(\n\"When computing gradients for '{}', some variables it relied on during the forward pass had already\"\n\" been updated. Consider setting defer=True in earlier UpdateOps related to models which interact \"\n\"with this one.\".format(model.model_name))\nraise err\nfor gradient, parameter in zip(gradients, trainable_params):\nif parameter.grad is not None:\nparameter.grad += gradient\nelse:\nparameter.grad = gradient.clone()\nif defer:\n# Only need to call once per model since gradients are getting accumulated\ndeferred[model.model_name] = [lambda: _torch_step(model.current_optimizer, scaler)]\nelse:\n_torch_step(model.current_optimizer, scaler)\ndeferred.pop(model.model_name, None)  # Don't need those deferred steps anymore\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/watch.html", "title": "watch", "text": ""}, {"location": "fastestimator/backend/watch.html#fastestimator.fastestimator.backend.watch.watch", "title": "<code>watch</code>", "text": "<p>Monitor the given <code>tensor</code> for later gradient computations.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.ones((3,28,28,1))\nwith tf.GradientTape(persistent=True) as tape:\nx = fe.backend.watch(x, tape=tape)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.ones((3,1,28,28))  # x.requires_grad == False\nx = fe.backend.watch(x)  # x.requires_grad == True\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be monitored.</p> required <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> or a copy of the <code>tensor</code> which is being tracked for gradient computations. This value is only</p> <code>Tensor</code> <p>needed if using PyTorch as the backend.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\watch.py</code> <pre><code>def watch(tensor: Tensor, tape: Optional[tf.GradientTape] = None) -&gt; Tensor:\n\"\"\"Monitor the given `tensor` for later gradient computations.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.ones((3,28,28,1))\n    with tf.GradientTape(persistent=True) as tape:\n        x = fe.backend.watch(x, tape=tape)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.ones((3,1,28,28))  # x.requires_grad == False\n    x = fe.backend.watch(x)  # x.requires_grad == True\n    ```\n    Args:\n        tensor: The tensor to be monitored.\n        tape: A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).\n    Returns:\n        The `tensor` or a copy of the `tensor` which is being tracked for gradient computations. This value is only\n        needed if using PyTorch as the backend.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\ntape.watch(tensor)\nreturn tensor\nelif isinstance(tensor, torch.Tensor):\nif tensor.requires_grad:\nreturn tensor\n# It is tempting to just do tensor.requires_grad = True here, but that will lead to trouble\nreturn tensor.detach().requires_grad_(True)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/zeros_like.html", "title": "zeros_like", "text": ""}, {"location": "fastestimator/backend/zeros_like.html#fastestimator.fastestimator.backend.zeros_like.zeros_like", "title": "<code>zeros_like</code>", "text": "<p>Generate zeros shaped like <code>tensor</code> with a specified <code>dtype</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. If None then the <code>tensor</code> dtype is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of zeros with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\zeros_like.py</code> <pre><code>def zeros_like(tensor: Tensor, dtype: Union[None, str] = None) -&gt; Tensor:\n\"\"\"Generate zeros shaped like `tensor` with a specified `dtype`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        dtype: The data type to be used when generating the resulting tensor. If None then the `tensor` dtype is used.\n    Returns:\n        A tensor of zeros with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.zeros_like(tensor, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.zeros_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype])\nelif isinstance(tensor, np.ndarray):\nreturn np.zeros_like(tensor, dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/cli/history.html", "title": "history", "text": ""}, {"location": "fastestimator/cli/history.html#fastestimator.fastestimator.cli.history.configure_history_parser", "title": "<code>configure_history_parser</code>", "text": "<p>Add a history parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\history.py</code> <pre><code>def configure_history_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a history parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('history',\ndescription='View prior FE training histories',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('--limit', metavar='L', type=int, help=\"How many entries to return\", default=15)\nparser.add_argument('--interactive',\ndest='interactive',\nhelp=\"Whether to run an interactive session which lets you look up detailed information\",\naction='store_true',\ndefault=False)\nparser.add_argument('--args',\ndest='include_args',\nhelp=\"Whether to return a list of the args used to invoke the training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--errors',\ndest='errors',\nhelp=\"Whether to focus on failed trainings and include extra error information\",\naction='store_true',\ndefault=False)\nparser.add_argument('--features',\ndest='include_features',\nhelp=\"Whether to return a list of the FE features used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--datasets',\ndest='include_datasets',\nhelp=\"Whether to return a list of the datasets used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--pipeline',\ndest='include_pipeline',\nhelp=\"Whether to return a list of the pipeline ops used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--network',\ndest='include_network',\nhelp=\"Whether to return a list of the network ops used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--traces',\ndest='include_traces',\nhelp=\"Whether to return a list of the traces used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--pks',\ndest='include_pk',\nhelp=\"Whether to return the database primary keys of each entry\",\naction='store_true',\ndefault=False)\nparser.add_argument('--csv',\ndest='csv',\nhelp='Print the response as a csv rather than a formatted table',\naction='store_true',\ndefault=False)\nsp = parser.add_subparsers()\nsql_parser = sp.add_parser('sql',\ndescription='Perform a raw SQL query against the history database',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nsql_parser.add_argument('query',\nmetavar='&lt;Query&gt;',\ntype=str,\nhelp=\"ex: fastestimator history sql 'SELECT * FROM history'\")\nsql_parser.add_argument('--interactive',\ndest='interactive',\nhelp=\"Whether to run an interactive session which lets you look up detailed information\",\naction='store_true',\ndefault=False)\nsql_parser.add_argument('--csv',\ndest='csv',\nhelp='Print the response as a csv rather than a formatted table',\naction='store_true',\ndefault=False)\nsql_parser.set_defaults(func=history_sql)\nclear_parser = sp.add_parser('clear',\ndescription='Clear out old history entries to save space',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nclear_parser.add_argument('retain',\nmetavar='N',\nnargs='?',\ntype=int,\nhelp=\"How many of the most recent entries to keep\",\ndefault=20)\nclear_parser.set_defaults(func=clear_history)\nsettings_parser = sp.add_parser('settings',\ndescription=\"Modify history settings, such as how many logs to retain\",\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nsettings_parser.add_argument('--keep',\nmetavar='K',\ntype=int,\nhelp=\"How many of the most recent history entries to keep\")\nsettings_parser.add_argument('--keep_logs',\nmetavar='L',\ntype=int,\nhelp=\"How many of the most recent log entries to keep\")\nsettings_parser.set_defaults(func=settings)\nparser.set_defaults(func=history_basic)\n</code></pre>"}, {"location": "fastestimator/cli/history.html#fastestimator.fastestimator.cli.history.history_basic", "title": "<code>history_basic</code>", "text": "<p>A method to query FE history using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the read_sql() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the read_sql() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\history.py</code> <pre><code>def history_basic(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to query FE history using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the read_sql() method.\n        unknown: Any cli arguments not matching known inputs for the read_sql() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\nwith HistoryReader() as reader:\nreader.read_basic(limit=args['limit'],\ninteractive=args['interactive'],\ninclude_args=args['include_args'],\nerrors=args['errors'],\ninclude_pk=args['include_pk'],\ninclude_traces=args['include_traces'],\ninclude_features=args['include_features'],\ninclude_datasets=args['include_datasets'],\ninclude_pipeline=args['include_pipeline'],\ninclude_network=args['include_network'],\nas_csv=args['csv'])\n</code></pre>"}, {"location": "fastestimator/cli/logs.html", "title": "logs", "text": ""}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.configure_log_parser", "title": "<code>configure_log_parser</code>", "text": "<p>Add a logging parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def configure_log_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a logging parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('logs',\ndescription='Generates comparison graphs amongst one or more log files',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('log_dir',\nmetavar='&lt;Log Dir&gt;',\ntype=str,\nhelp=\"The path to a folder containing one or more log files\")\nparser.add_argument('--extension',\nmetavar='E',\ntype=str,\nhelp=\"The file type / extension of your logs\",\ndefault=\".txt\")\nparser.add_argument('--recursive', action='store_true', help=\"Recursively search sub-directories for log files\")\nparser.add_argument('--ignore',\nmetavar='I',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to ignore though they may be present in the log files\")\nparser.add_argument('--smooth',\nmetavar='&lt;float&gt;',\ntype=float,\nhelp=\"The amount of gaussian smoothing to apply (zero for no smoothing)\",\ndefault=1)\nparser.add_argument('--pretty_names', help=\"Clean up the metric names for display\", action='store_true')\nlegend_group = parser.add_argument_group('legend arguments')\nlegend_x_group = legend_group.add_mutually_exclusive_group(required=False)\nlegend_x_group.add_argument('--common_legend',\ndest='share_legend',\nhelp=\"Generate one legend total\",\naction='store_true',\ndefault=True)\nlegend_x_group.add_argument('--split_legend',\ndest='share_legend',\nhelp=\"Generate one legend per graph\",\naction='store_false',\ndefault=False)\nsave_group = parser.add_argument_group('output arguments')\nsave_x_group = save_group.add_mutually_exclusive_group(required=False)\nsave_x_group.add_argument(\n'--save',\nnargs='?',\nmetavar='&lt;Save Dir&gt;',\ndest='save',\naction=SaveAction,\ndefault=False,\nhelp=\"Save the output image. May be accompanied by a directory into \\\n                                              which the file is saved. If no output directory is specified, the log \\\n                                              directory will be used\")\nsave_x_group.add_argument('--display',\ndest='save',\naction='store_false',\nhelp=\"Render the image to the UI (rather than saving it)\",\ndefault=True)\nsave_x_group.set_defaults(save_dir=None)\nparser.set_defaults(func=logs)\n</code></pre>"}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.logs", "title": "<code>logs</code>", "text": "<p>A method to invoke the FE logging function using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the parse_log_dir() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the parse_log_dir() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def logs(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to invoke the FE logging function using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the parse_log_dir() method.\n        unknown: Any cli arguments not matching known inputs for the parse_log_dir() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\nparse_log_dir(args['log_dir'],\nargs['extension'],\nargs['recursive'],\nargs['smooth'],\nargs['save'],\nargs['save_dir'],\nargs['ignore'],\nargs['share_legend'],\nargs['pretty_names'])\n</code></pre>"}, {"location": "fastestimator/cli/main.html", "title": "main", "text": ""}, {"location": "fastestimator/cli/main.html#fastestimator.fastestimator.cli.main.run", "title": "<code>run</code>", "text": "<p>A function which invokes the various argument parsers and then runs the requested subroutine.</p> Source code in <code>fastestimator\\fastestimator\\cli\\main.py</code> <pre><code>def run(argv) -&gt; None:\n\"\"\"A function which invokes the various argument parsers and then runs the requested subroutine.\n    \"\"\"\nparser = argparse.ArgumentParser(allow_abbrev=False)\nsubparsers = parser.add_subparsers()\n# In python 3.7 the following 2 lines could be put into the .add_subparsers() call\nsubparsers.required = True\nsubparsers.dest = 'mode'\nconfigure_train_parser(subparsers)\nconfigure_test_parser(subparsers)\nconfigure_log_parser(subparsers)\nconfigure_history_parser(subparsers)\nargs, unknown = parser.parse_known_args(argv)\nargs.func(vars(args), unknown)\n</code></pre>"}, {"location": "fastestimator/cli/train.html", "title": "train", "text": ""}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_test_parser", "title": "<code>configure_test_parser</code>", "text": "<p>Add a testing parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_test_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a testing parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('test',\ndescription='Test a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--summary', type=str, help=\"Experiment name\", default=None)\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=test)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_train_parser", "title": "<code>configure_train_parser</code>", "text": "<p>Add a training parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_train_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a training parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('train',\ndescription='Train a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--warmup', type=str, help=\"Warmup setting, can be true, false or debug\", default=\"true\")\nparser.add_argument('--summary', type=str, help=\"Experiment name\", default=None)\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=train)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.test", "title": "<code>test</code>", "text": "<p>Load an Estimator from a file and invoke its .test() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def test(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .test() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nestimator.test(summary=args['summary'])\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.train", "title": "<code>train</code>", "text": "<p>Load an Estimator from a file and invoke its .fit() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def train(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .fit() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nwarmup_option = {\"true\": True, \"false\": False, \"debug\": \"debug\"}\nestimator.fit(warmup=warmup_option[args['warmup'].lower()], summary=args['summary'])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html", "title": "batch_dataset", "text": ""}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset", "title": "<code>BatchDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.</p> <p>This dataset helps to enable several use-cases: 1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.     <pre><code>ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\nds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\nunpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n# {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n</code></pre> 2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n</code></pre> 3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[FEDataset, Iterable[FEDataset]]</code> <p>The dataset(s) to use for batch sampling. While these should be FEDatasets, pytorch datasets will technically also work. If you use them, however, you will lose the .split() and .summary() methods.</p> required <code>num_samples</code> <code>Union[int, Iterable[int]]</code> <p>Number of samples to draw from the <code>datasets</code>. May be a single int if used in conjunction with <code>probability</code>, otherwise a list of ints of len(<code>datasets</code>) is required.</p> required <code>probability</code> <code>Optional[Iterable[float]]</code> <p>Probability to draw from each dataset. Only allowed if <code>num_samples</code> is an integer.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>@traceable()\nclass BatchDataset(FEDataset):\n\"\"\"BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.\n    This dataset helps to enable several use-cases:\n    1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.\n        ```python\n        ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\n        ds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\n        unpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n        # {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n        ```\n    2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n        ```\n    3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n        ```\n    Args:\n        datasets: The dataset(s) to use for batch sampling. While these should be FEDatasets, pytorch datasets will\n            technically also work. If you use them, however, you will lose the .split() and .summary() methods.\n        num_samples: Number of samples to draw from the `datasets`. May be a single int if used in conjunction with\n            `probability`, otherwise a list of ints of len(`datasets`) is required.\n        probability: Probability to draw from each dataset. Only allowed if `num_samples` is an integer.\n    \"\"\"\ndef __init__(self,\ndatasets: Union[FEDataset, Iterable[FEDataset]],\nnum_samples: Union[int, Iterable[int]],\nprobability: Optional[Iterable[float]] = None) -&gt; None:\nself.datasets = to_list(datasets)\nself.num_samples = to_list(num_samples)\nself.probability = to_list(probability)\nself.same_feature = False\nself.all_fe_datasets = False\nself._check_input()\nself.index_maps = []\nself.reset_index_maps()\nself.pad_value = None\ndef _check_input(self) -&gt; None:\n\"\"\"Verify that the given input values are valid.\n        Raises:\n            AssertionError: If any of the parameters are found to by unacceptable for a variety of reasons.\n        \"\"\"\nassert len(self.datasets) &gt; 1, \"must provide multiple datasets as input\"\nfor num_sample in self.num_samples:\nassert isinstance(num_sample, int) and num_sample &gt; 0, \"only accept positive integer type as num_sample\"\n# check dataset keys\ndataset_keys = [set(dataset[0].keys()) for dataset in self.datasets]\nfor key in dataset_keys:\nassert key, \"found no key in datasets\"\nis_same_key = all([dataset_keys[0] == key for key in dataset_keys])\nis_disjoint_key = sum([len(key) for key in dataset_keys]) == len(set.union(*dataset_keys))\nif len(self.datasets) &gt; 1:\nassert is_same_key != is_disjoint_key, \"dataset keys must be all same or all disjoint\"\nself.same_feature = is_same_key\nif self.probability:\nassert self.same_feature, \"keys must be exactly same among datasets when using probability distribution\"\nassert len(self.datasets) == len(self.probability), \"the length of dataset must match probability\"\nassert len(self.num_samples) == 1, \"num_sample must be scalar for probability mode\"\nassert len(self.datasets) &gt; 1, \"number of datasets must be more than one to use probability mode\"\nassert sum(self.probability) == 1, \"sum of probability must be 1\"\nfor p in self.probability:\nassert isinstance(p, float) and p &gt; 0, \"must provide positive float for probability distribution\"\nelse:\nassert len(self.datasets) == len(self.num_samples), \"the number of dataset must match num_samples\"\nif not self.same_feature:\nassert len(set(self.num_samples)) == 1, \"the number of samples must be the same for disjoint features\"\nself.all_fe_datasets = all([isinstance(dataset, FEDataset) for dataset in self.datasets])\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['BatchDataset']:\n\"\"\"This class overwrites the .split() method instead of _do_split().\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every element of the `splits` sequence.\n        Raises:\n            AssertionError: This method should never by invoked.\n        \"\"\"\nraise AssertionError(\"This method should not have been invoked. Please file a bug report\")\ndef split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['BatchDataset', List['BatchDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        Raises:\n            NotImplementedError: If the user created this dataset using one or more non-FEDataset inputs.\n        \"\"\"\nif not self.all_fe_datasets:\nraise NotImplementedError(\n\"BatchDataset.split() is not supported when BatchDataset contains non-FEDataset objects\")\nnew_datasets = [to_list(ds.split(*fractions)) for ds in self.datasets]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\n# Re-compute personal variables\nself.reset_index_maps()\nFEDataset.fix_split_traceabilty(self, results, fractions)\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'datasets': [ds.__getstate__() if hasattr(ds, '__getstate__') else {} for ds in self.datasets]}\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nif not self.all_fe_datasets:\nprint(\"FastEstimator-Warn: BatchDataset summary will be incomplete since non-FEDatasets were used.\")\nreturn DatasetSummary(num_instances=len(self), keys={})\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\ndef __len__(self) -&gt; int:\n\"\"\"Compute the length of this dataset.\n        Returns:\n            How many batches of data can this dataset serve per epoch.\n        \"\"\"\nif len(self.num_samples) &gt; 1:\nlength = max([math.ceil(len(ds) / num_sample) for ds, num_sample in zip(self.datasets, self.num_samples)])\nelse:\nnum_sample = self.num_samples[0]\nlength = max([math.ceil(len(ds) / num_sample / p) for ds, p in zip(self.datasets, self.probability)])\nreturn length\ndef __getitem__(self, batch_idx: int) -&gt; List[Dict[str, Any]]:\n\"\"\"Extract items from the underlying datasets based on the given `batch_idx`.\n        Args:\n            batch_idx: Which batch is it.\n        Returns:\n            A list of data instance dictionaries corresponding to the current `batch_idx`.\n        \"\"\"\nitems = []\nif self.same_feature:\nif self.probability:\nindex = list(np.random.choice(range(len(self.datasets)), size=self.num_samples, p=self.probability))\nnum_samples = [index.count(i) for i in range(len(self.datasets))]\nelse:\nnum_samples = self.num_samples\nfor dataset, num_sample, index_map in zip(self.datasets, num_samples, self.index_maps):\nfor idx in range(num_sample):\nitems.append(dataset[index_map[batch_idx * num_sample + idx]])\nelse:\nnum_sample = self.num_samples[0]\nfor idx in range(num_sample):\npaired_items = [\ndataset[index_map[batch_idx * num_sample + idx]] for dataset,\nindex_map in zip(self.datasets, self.index_maps)\n]\nitems.append({k: v for d in paired_items for k, v in d.items()})\nrandom.shuffle(items)\nreturn items\ndef reset_index_maps(self) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n        This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the\n        basis datasets.\n        \"\"\"\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nrandom.shuffle(mapping)\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.reset_index_maps", "title": "<code>reset_index_maps</code>", "text": "<p>Rearrange the index maps of this BatchDataset.</p> <p>This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the basis datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def reset_index_maps(self) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n    This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the\n    basis datasets.\n    \"\"\"\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nrandom.shuffle(mapping)\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\nds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>*fractions</code> <code>Union[float, int, Iterable[int]]</code> <p>Floating point values will be interpreted as percentages, integers as an absolute number of datapoints, and an iterable of integers as the exact indices of the data that should be removed in order to create the new dataset.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[BatchDataset, List[BatchDataset]]</code> <p>One or more new datasets which are created by removing elements from the current dataset. The number of</p> <code>Union[BatchDataset, List[BatchDataset]]</code> <p>datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided</p> <code>Union[BatchDataset, List[BatchDataset]]</code> <p>then the return will be a single dataset rather than a list of datasets.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the user created this dataset using one or more non-FEDataset inputs.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['BatchDataset', List['BatchDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    Raises:\n        NotImplementedError: If the user created this dataset using one or more non-FEDataset inputs.\n    \"\"\"\nif not self.all_fe_datasets:\nraise NotImplementedError(\n\"BatchDataset.split() is not supported when BatchDataset contains non-FEDataset objects\")\nnew_datasets = [to_list(ds.split(*fractions)) for ds in self.datasets]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\n# Re-compute personal variables\nself.reset_index_maps()\nFEDataset.fix_split_traceabilty(self, results, fractions)\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nif not self.all_fe_datasets:\nprint(\"FastEstimator-Warn: BatchDataset summary will be incomplete since non-FEDatasets were used.\")\nreturn DatasetSummary(num_instances=len(self), keys={})\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\n</code></pre>"}, {"location": "fastestimator/dataset/csv_dataset.html", "title": "csv_dataset", "text": ""}, {"location": "fastestimator/dataset/csv_dataset.html#fastestimator.fastestimator.dataset.csv_dataset.CSVDataset", "title": "<code>CSVDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a CSV file.</p> <p>CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the CSV file.</p> required <code>delimiter</code> <code>str</code> <p>What delimiter is used by the file.</p> <code>','</code> <code>kwargs</code> <p>Other arguments to be passed through to pandas csv reader function. See the pandas docs for details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\dataset\\csv_dataset.py</code> <pre><code>@traceable()\nclass CSVDataset(InMemoryDataset):\n\"\"\"A dataset from a CSV file.\n    CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file\n    may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information\n    that you want to feed into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the CSV file.\n        delimiter: What delimiter is used by the file.\n        kwargs: Other arguments to be passed through to pandas csv reader function. See the pandas docs for details:\n            https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.\n    \"\"\"\ndef __init__(self, file_path: str, delimiter: str = \",\", **kwargs) -&gt; None:\ndf = pd.read_csv(file_path, delimiter=delimiter, **kwargs)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html", "title": "dataset", "text": ""}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.DatasetSummary", "title": "<code>DatasetSummary</code>", "text": "<p>This class contains information summarizing a dataset object.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>num_instances</code> <code>int</code> <p>The number of data instances within the dataset (influences the size of an epoch).</p> required <code>num_classes</code> <code>Optional[int]</code> <p>How many different classes are present.</p> <code>None</code> <code>keys</code> <code>Dict[str, KeySummary]</code> <p>What keys does the dataset provide, along with summary information about each key.</p> required <code>class_key</code> <code>Optional[str]</code> <p>Which key corresponds to class information (if known).</p> <code>None</code> <code>class_key_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A mapping of the original class string values to the values which are output to the pipeline.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class DatasetSummary:\n\"\"\"This class contains information summarizing a dataset object.\n    This class is intentionally not @traceable.\n    Args:\n        num_instances: The number of data instances within the dataset (influences the size of an epoch).\n        num_classes: How many different classes are present.\n        keys: What keys does the dataset provide, along with summary information about each key.\n        class_key: Which key corresponds to class information (if known).\n        class_key_mapping: A mapping of the original class string values to the values which are output to the pipeline.\n    \"\"\"\nnum_instances: int\nnum_classes: Optional[int]\nclass_key: Optional[str]\nclass_key_mapping: Optional[Dict[str, Any]]\nkeys: Dict[str, KeySummary]\ndef __init__(self,\nnum_instances: int,\nkeys: Dict[str, KeySummary],\nnum_classes: Optional[int] = None,\nclass_key_mapping: Optional[Dict[str, Any]] = None,\nclass_key: Optional[str] = None):\nself.num_instances = num_instances\nself.class_key = class_key\nself.num_classes = num_classes\nself.class_key_mapping = class_key_mapping\nself.keys = keys\ndef __repr__(self):\nreturn \"&lt;DatasetSummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\ndef __str__(self):\nreturn jsonpickle.dumps(self, unpicklable=False)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset", "title": "<code>FEDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@traceable()\nclass FEDataset(Dataset):\ndef __len__(self) -&gt; int:\n\"\"\"Defines how many datapoints the dataset contains.\n        This is used for computing the number of datapoints available per epoch.\n        Returns:\n            The number of datapoints within the dataset.\n        \"\"\"\nraise NotImplementedError\ndef __getitem__(self, index: int) -&gt; Dict[str, Any]:\n\"\"\"Fetch a data instance at a specified index.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index.\n        \"\"\"\nraise NotImplementedError\n@classmethod\ndef fix_split_traceabilty(cls,\nparent: 'FEDataset',\nchildren: List['FEDataset'],\nfractions: Tuple[Union[float, int, Iterable[int]], ...]) -&gt; None:\n\"\"\"A method to fix traceability information after invoking the dataset .split() method.\n        Note that the default implementation of the .split() function invokes this already, so this only needs to be\n        invoked if you override the .split() method when defining a subclass (ex. BatchDataset).\n        Args:\n            parent: The parent dataset on which .split() was invoked.\n            children: The datasets generated by performing the split.\n            fractions: The fraction arguments used to generate the children (should be one-to-one with the children).\n        \"\"\"\nif hasattr(parent, '_fe_traceability_summary'):\nparent_id = FEID(id(parent))\nfractions = [\nf\"range({frac.start}, {frac.stop}, {frac.step})\" if isinstance(frac, range) else f\"{frac}\"\nfor frac in fractions\n]\nfor child, frac in zip(children, fractions):\n# noinspection PyProtectedMember\ntables = deepcopy(child._fe_traceability_summary)\n# Update the ID if necessary\nchild_id = FEID(id(child))\nif child_id not in tables:\n# The child was created without invoking its __init__ method, so its internal summary will have the\n# wrong id\ntable = tables.pop(parent_id)\ntable.fe_id = child_id\ntables[child_id] = table\nelse:\ntable = tables[child_id]\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent=parent_id, fraction=frac)\ntable.fields['split'] = split_summary\nchild._fe_traceability_summary = tables\n# noinspection PyUnresolvedReferences\ntable = parent._fe_traceability_summary.get(parent_id)\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent='self', fraction=\", \".join([f\"-{frac}\" for frac in fractions]))\ntable.fields['split'] = split_summary\n# Put the new parent summary into the child table to ensure it will always exist in the final set of tables\nfor child in children:\nchild._fe_traceability_summary[parent_id] = deepcopy(table)\ndef split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n            \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nsplits = []\nif method == 'number':\n# TODO - convert to a linear congruential generator for large datasets?\n# https://stackoverflow.com/questions/9755538/how-do-i-create-a-list-of-random-numbers-without-duplicates\nindices = random.sample(range(original_size), int_sum)\nstart = 0\nfor stop in n_samples:\nsplits.append((indices[i] for i in range(start, start + stop)))\nstart += stop\nelif method == 'indices':\nsplits = fractions\nsplits = self._do_split(splits)\nFEDataset.fix_split_traceabilty(self, splits, fractions)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        Useful if sub-classes want to split by something other than indices (see SiameseDirDataset for example).\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['FEDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nraise NotImplementedError\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nraise NotImplementedError\ndef __str__(self):\nreturn str(self.summary())\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.fix_split_traceabilty", "title": "<code>fix_split_traceabilty</code>  <code>classmethod</code>", "text": "<p>A method to fix traceability information after invoking the dataset .split() method.</p> <p>Note that the default implementation of the .split() function invokes this already, so this only needs to be invoked if you override the .split() method when defining a subclass (ex. BatchDataset).</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>FEDataset</code> <p>The parent dataset on which .split() was invoked.</p> required <code>children</code> <code>List[FEDataset]</code> <p>The datasets generated by performing the split.</p> required <code>fractions</code> <code>Tuple[Union[float, int, Iterable[int]], ...]</code> <p>The fraction arguments used to generate the children (should be one-to-one with the children).</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@classmethod\ndef fix_split_traceabilty(cls,\nparent: 'FEDataset',\nchildren: List['FEDataset'],\nfractions: Tuple[Union[float, int, Iterable[int]], ...]) -&gt; None:\n\"\"\"A method to fix traceability information after invoking the dataset .split() method.\n    Note that the default implementation of the .split() function invokes this already, so this only needs to be\n    invoked if you override the .split() method when defining a subclass (ex. BatchDataset).\n    Args:\n        parent: The parent dataset on which .split() was invoked.\n        children: The datasets generated by performing the split.\n        fractions: The fraction arguments used to generate the children (should be one-to-one with the children).\n    \"\"\"\nif hasattr(parent, '_fe_traceability_summary'):\nparent_id = FEID(id(parent))\nfractions = [\nf\"range({frac.start}, {frac.stop}, {frac.step})\" if isinstance(frac, range) else f\"{frac}\"\nfor frac in fractions\n]\nfor child, frac in zip(children, fractions):\n# noinspection PyProtectedMember\ntables = deepcopy(child._fe_traceability_summary)\n# Update the ID if necessary\nchild_id = FEID(id(child))\nif child_id not in tables:\n# The child was created without invoking its __init__ method, so its internal summary will have the\n# wrong id\ntable = tables.pop(parent_id)\ntable.fe_id = child_id\ntables[child_id] = table\nelse:\ntable = tables[child_id]\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent=parent_id, fraction=frac)\ntable.fields['split'] = split_summary\nchild._fe_traceability_summary = tables\n# noinspection PyUnresolvedReferences\ntable = parent._fe_traceability_summary.get(parent_id)\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent='self', fraction=\", \".join([f\"-{frac}\" for frac in fractions]))\ntable.fields['split'] = split_summary\n# Put the new parent summary into the child table to ensure it will always exist in the final set of tables\nfor child in children:\nchild._fe_traceability_summary[parent_id] = deepcopy(table)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\nds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>*fractions</code> <code>Union[float, int, Iterable[int]]</code> <p>Floating point values will be interpreted as percentages, integers as an absolute number of datapoints, and an iterable of integers as the exact indices of the data that should be removed in order to create the new dataset.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[FEDataset, List[FEDataset]]</code> <p>One or more new datasets which are created by removing elements from the current dataset. The number of</p> <code>Union[FEDataset, List[FEDataset]]</code> <p>datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided</p> <code>Union[FEDataset, List[FEDataset]]</code> <p>then the return will be a single dataset rather than a list of datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n        \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nsplits = []\nif method == 'number':\n# TODO - convert to a linear congruential generator for large datasets?\n# https://stackoverflow.com/questions/9755538/how-do-i-create-a-list-of-random-numbers-without-duplicates\nindices = random.sample(range(original_size), int_sum)\nstart = 0\nfor stop in n_samples:\nsplits.append((indices[i] for i in range(start, start + stop)))\nstart += stop\nelif method == 'indices':\nsplits = fractions\nsplits = self._do_split(splits)\nFEDataset.fix_split_traceabilty(self, splits, fractions)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset", "title": "<code>InMemoryDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset abstraction to simplify the implementation of datasets which hold their data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[int, Dict[str, Any]]</code> <p>A dictionary like {data_index: {}}. required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@traceable(blacklist=('data', 'summary'))\nclass InMemoryDataset(FEDataset):\n\"\"\"A dataset abstraction to simplify the implementation of datasets which hold their data in memory.\n    Args:\n        data: A dictionary like {data_index: {&lt;instance dictionary&gt;}}.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]  # Index-based data dictionary\nsummary: lru_cache\ndef __init__(self, data: Dict[int, Dict[str, Any]]) -&gt; None:\nself.data = data\n# Normally lru cache annotation is shared over all class instances, so calling cache_clear would reset all\n# caches (for example when calling .split()). Instead we make the lru cache per-instance\nself.summary = lru_cache(maxsize=1)(self.summary)\ndef __len__(self) -&gt; int:\nreturn len(self.data)\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        element = data[0]  # {\"x\": &lt;100&gt;}\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        ```\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nif isinstance(index, int):\nreturn self.data[index]\nelse:\nresult = [elem[index] for elem in self.data.values()]\nif isinstance(result[0], np.ndarray):\nreturn np.array(result)\nreturn result\ndef __setitem__(self, key: Union[int, str], value: Union[Dict[str, Any], Sequence[Any]]) -&gt; None:\n\"\"\"Modify data in the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        column = column - np.mean(column)\n        data[\"x\"] = column\n        ```\n        Args:\n            key: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be updated.\n            value: The value to be inserted for the given `key`. Must be a dictionary if `key` is an integer. Otherwise\n                must be a sequence with the same length as the current length of the dataset.\n        Raises:\n            AssertionError: If the `value` is inappropriate given the type of the `key`.\n        \"\"\"\nif isinstance(key, int):\nassert isinstance(value, Dict), \"if setting a value using an integer index, must provide a dictionary\"\nself.data[key] = value\nelse:\nassert len(value) == len(self.data), \\\n                \"input value must be of length {}, but had length {}\".format(len(self.data), len(value))\nfor i in range(len(self.data)):\nself.data[i][key] = value[i]\nself.summary.cache_clear()\ndef _skip_init(self, data: Dict[int, Dict[str, Any]], **kwargs) -&gt; 'InMemoryDataset':\n\"\"\"A helper method to create new dataset instances without invoking their __init__ methods.\n        Args:\n            data: The data dictionary to be used in the new dataset.\n            **kwargs: Any other member variables to be assigned in the new dataset.\n        Returns:\n            A new dataset based on the given inputs.\n        \"\"\"\nobj = self.__class__.__new__(self.__class__)\nobj.data = data\nfor k, v in kwargs.items():\nif k == 'summary':\ncontinue  # Ignore summary object since we're going to re-initialize it\nelse:\nobj.__setattr__(k, v)\nobj.summary = lru_cache(maxsize=1)(obj.summary)\nreturn obj\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['InMemoryDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New Datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nresults = []\nfor split in splits:\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nresults.append(self._skip_init(data, **{k: v for k, v in self.__dict__.items() if k not in {'data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself.summary.cache_clear()\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nreturn DatasetSummary(num_instances=len(self), keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nreturn DatasetSummary(num_instances=len(self), keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.KeySummary", "title": "<code>KeySummary</code>", "text": "<p>A summary of the dataset attributes corresponding to a particular key.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>num_unique_values</code> <code>Optional[int]</code> <p>The number of unique values corresponding to a particular key (if known).</p> <code>None</code> <code>shape</code> <code>List[Optional[int]]</code> <p>The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is ragged.</p> <code>()</code> <code>dtype</code> <code>str</code> <p>The data type of instances corresponding to the given key.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class KeySummary:\n\"\"\"A summary of the dataset attributes corresponding to a particular key.\n    This class is intentionally not @traceable.\n    Args:\n        num_unique_values: The number of unique values corresponding to a particular key (if known).\n        shape: The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is\n            ragged.\n        dtype: The data type of instances corresponding to the given key.\n    \"\"\"\nnum_unique_values: Optional[int]\nshape: List[Optional[int]]\ndtype: str\ndef __init__(self, dtype: str, num_unique_values: Optional[int] = None, shape: List[Optional[int]] = ()) -&gt; None:\nself.num_unique_values = num_unique_values\nself.shape = shape\nself.dtype = dtype\ndef __repr__(self):\nreturn \"&lt;KeySummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\n</code></pre>"}, {"location": "fastestimator/dataset/dir_dataset.html", "title": "dir_dataset", "text": ""}, {"location": "fastestimator/dataset/dir_dataset.html#fastestimator.fastestimator.dataset.dir_dataset.DirDataset", "title": "<code>DirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> <code>recursive_search</code> <code>bool</code> <p>Whether to search within subdirectories for files.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dir_dataset.py</code> <pre><code>@traceable()\nclass DirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/data.file.\n    Args:\n        root_dir: The path to the directory containing data.\n        data_key: What key to assign to the data values in the data dictionary.\n        file_extension: If provided then only files ending with the file_extension will be included.\n        recursive_search: Whether to search within subdirectories for files.\n    \"\"\"\ndata: Dict[int, Dict[str, str]]\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nfile_extension: Optional[str] = None,\nrecursive_search: bool = True) -&gt; None:\ndata = []\nroot_dir = os.path.normpath(root_dir)\nif not os.path.isdir(root_dir):\nraise AssertionError(\"Provided path is not a directory\")\ntry:\nfor root, dirs, files in os.walk(root_dir):\nfor file_name in files:\nif file_name.startswith(\".\") or (file_extension is not None\nand not file_name.endswith(file_extension)):\ncontinue\ndata.append((os.path.join(root, file_name), os.path.basename(root)))\nif not recursive_search:\nbreak\nexcept StopIteration:\nraise ValueError(\"Invalid directory structure for DirDataset at root: {}\".format(root_dir))\nsuper().__init__({i: {data_key: data[i][0]} for i in range(len(data))})\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html", "title": "generator_dataset", "text": ""}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset", "title": "<code>GeneratorDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset from a generator function.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Generator[Dict[str, Any], int, None]</code> <p>The generator function to invoke in order to get a data sample.</p> required <code>samples_per_epoch</code> <code>int</code> <p>How many samples should be drawn from the generator during an epoch. Note that the generator function will actually be invoke more times than the number specified here due to backend validation routines.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>@traceable()\nclass GeneratorDataset(FEDataset):\n\"\"\"A dataset from a generator function.\n    Args:\n        generator: The generator function to invoke in order to get a data sample.\n        samples_per_epoch: How many samples should be drawn from the generator during an epoch. Note that the generator\n            function will actually be invoke more times than the number specified here due to backend validation\n            routines.\n    \"\"\"\ndef __init__(self, generator: Generator[Dict[str, Any], int, None], samples_per_epoch: int) -&gt; None:\nself.generator = generator\nself.samples_per_epoch = samples_per_epoch\nnext(self.generator)  # Can't send non-none values to a new generator, so need to run a 'warm-up' first\nself.summary = lru_cache(maxsize=1)(self.summary)\ndef __len__(self):\nreturn self.samples_per_epoch\ndef __getitem__(self, index: int):\nreturn self.generator.send(index)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['GeneratorDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nwarnings.warn(\"You probably don't actually want to split a generator dataset\")\nresults = []\nfor split in splits:\nif isinstance(split, Sized):\nsize = len(split)\nelse:\n# TODO - make this efficient somehow\nsize = sum(1 for _ in split)\nresults.append(GeneratorDataset(self.generator, size))\nself.samples_per_epoch -= size\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nreturn DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nreturn DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html", "title": "labeled_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset", "title": "<code>LabeledDirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>@traceable()\nclass LabeledDirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key: What key to assign to the data values in the data dictionary.\n        label_key: What key to assign to the label values in the data dictionary.\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]\nmapping: Dict[str, Any]\nlabel_key: str\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nlabel_key: str = \"y\",\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None) -&gt; None:\n# Recursively find all the data\nroot_dir = os.path.normpath(root_dir)\ndata = {}\nkeys = deque([\"\"])\nfor _, dirs, entries in os.walk(root_dir):\nkey = keys.popleft()\ndirs = [os.path.join(key, d) for d in dirs]\ndirs.reverse()\nkeys.extendleft(dirs)\nentries = [\nos.path.join(key, e) for e in entries if not e.startswith(\".\") and e.endswith(file_extension or \"\")\n]\nif entries:\ndata[key] = entries\n# Compute label mappings\nself.mapping = label_mapping or {label: idx for idx, label in enumerate(sorted(data.keys()))}\nassert self.mapping.keys() &gt;= data.keys(), \\\n            \"Mapping provided to LabeledDirDataset is missing key(s): {}\".format(\ndata.keys() - self.mapping.keys())\n# Store the data by index\nparsed_data = {}\nidx = 0\nfor key, values in data.items():\nlabel = self.mapping[key]\nfor value in values:\nparsed_data[idx] = {data_key: os.path.join(root_dir, value), label_key: label}\nidx += 1\nself.label_key = label_key\nsuper().__init__(parsed_data)\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/numpy_dataset.html", "title": "numpy_dataset", "text": ""}, {"location": "fastestimator/dataset/numpy_dataset.html#fastestimator.fastestimator.dataset.numpy_dataset.NumpyDataset", "title": "<code>NumpyDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset constructed from a dictionary of Numpy data or list of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Union[np.ndarray, List]]</code> <p>A dictionary of data like {\"key1\": , \"key2\": [list]}. required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the Numpy arrays or lists have differing numbers of elements.</p> <code>ValueError</code> <p>If any dictionary value is not instance of Numpy array or list.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\numpy_dataset.py</code> <pre><code>@traceable()\nclass NumpyDataset(InMemoryDataset):\n\"\"\"A dataset constructed from a dictionary of Numpy data or list of data.\n    Args:\n        data: A dictionary of data like {\"key1\": &lt;numpy array&gt;, \"key2\": [list]}.\n    Raises:\n        AssertionError: If any of the Numpy arrays or lists have differing numbers of elements.\n        ValueError: If any dictionary value is not instance of Numpy array or list.\n    \"\"\"\ndef __init__(self, data: Dict[str, Union[np.ndarray, List]]) -&gt; None:\nsize = None\nfor val in data.values():\nif isinstance(val, np.ndarray):\ncurrent_size = val.shape[0]\nelif isinstance(val, list):\ncurrent_size = len(val)\nelse:\nraise ValueError(\"Please ensure you are passing numpy array or list in the data dictionary.\")\nif size is not None:\nassert size == current_size, \"All data arrays must have the same number of elements\"\nelse:\nsize = current_size\nsuper().__init__({i: {k: v[i] for k, v in data.items()} for i in range(size)})\n</code></pre>"}, {"location": "fastestimator/dataset/op_dataset.html", "title": "op_dataset", "text": ""}, {"location": "fastestimator/dataset/op_dataset.html#fastestimator.fastestimator.dataset.op_dataset.OpDataset", "title": "<code>OpDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> <p>A wrapper for datasets which allows operators to be applied to them in a pipeline.</p> <p>This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets within an Op dataset as needed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The base dataset to wrap.</p> required <code>ops</code> <code>List[NumpyOp]</code> <p>A list of ops to be applied after the base <code>dataset</code> <code>__getitem__</code> is invoked.</p> required <code>mode</code> <code>str</code> <p>What mode the system is currently running in ('train', 'eval', 'test', or 'infer').</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\op_dataset.py</code> <pre><code>@traceable()\nclass OpDataset(Dataset):\n\"\"\"A wrapper for datasets which allows operators to be applied to them in a pipeline.\n    This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets\n    within an Op dataset as needed.\n    Args:\n        dataset: The base dataset to wrap.\n        ops: A list of ops to be applied after the base `dataset` `__getitem__` is invoked.\n        mode: What mode the system is currently running in ('train', 'eval', 'test', or 'infer').\n    \"\"\"\ndef __init__(self, dataset: Dataset, ops: List[NumpyOp], mode: str) -&gt; None:\nself.dataset = dataset\nif isinstance(self.dataset, BatchDataset):\nself.dataset.reset_index_maps()\nself.ops = ops\nself.mode = mode\ndef __getitem__(self, index: int) -&gt; Mapping[str, Any]:\n\"\"\"Fetch a data instance at a specified index, and apply transformations to it.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index, with transformations applied.\n        \"\"\"\nitems = deepcopy(self.dataset[index])  # Deepcopy to prevent ops from overwriting values in datasets\nif isinstance(self.dataset, BatchDataset):\nunique_list = []\nfor item in items:\nif id(item) not in unique_list:\nforward_numpyop(self.ops, item, self.mode)\nunique_list.append(id(item))\nif self.dataset.pad_value is not None:\npad_batch(items, self.dataset.pad_value)\nitems = {key: np.array([item[key] for item in items]) for key in items[0]}\nelse:\nforward_numpyop(self.ops, items, self.mode)\nreturn items\ndef __len__(self):\nreturn len(self.dataset)\n</code></pre>"}, {"location": "fastestimator/dataset/pickle_dataset.html", "title": "pickle_dataset", "text": ""}, {"location": "fastestimator/dataset/pickle_dataset.html#fastestimator.fastestimator.dataset.pickle_dataset.PickleDataset", "title": "<code>PickleDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a pickle file.</p> <p>PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the pickle file.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\pickle_dataset.py</code> <pre><code>@traceable()\nclass PickleDataset(InMemoryDataset):\n\"\"\"A dataset from a pickle file.\n    PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed\n    using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed\n    into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the pickle file.\n    \"\"\"\ndef __init__(self, file_path: str) -&gt; None:\ndf = pd.read_pickle(file_path)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html", "title": "siamese_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset", "title": "<code>SiameseDirDataset</code>", "text": "<p>         Bases: <code>LabeledDirDataset</code></p> <p>A dataset which returns pairs of data.</p> <p>This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs, where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index rather than by data instance index.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key_left</code> <code>str</code> <p>What key to assign to the first data element in the pair.</p> <code>'x_a'</code> <code>data_key_right</code> <code>str</code> <p>What key to assign to the second data element in the pair.</p> <code>'x_b'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>percent_matching_data</code> <code>float</code> <p>What percentage of the time should data be paired by class (label value = 1).</p> <code>0.5</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>@traceable()\nclass SiameseDirDataset(LabeledDirDataset):\n\"\"\"A dataset which returns pairs of data.\n    This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs,\n    where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as\n    the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero\n    or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index\n    rather than by data instance index.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key_left: What key to assign to the first data element in the pair.\n        data_key_right: What key to assign to the second data element in the pair.\n        label_key: What key to assign to the label values in the data dictionary.\n        percent_matching_data: What percentage of the time should data be paired by class (label value = 1).\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\nclass_data: Dict[Any, Set[int]]\ndef __init__(self,\nroot_dir: str,\ndata_key_left: str = \"x_a\",\ndata_key_right: str = \"x_b\",\nlabel_key: str = \"y\",\npercent_matching_data: float = 0.5,\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None):\nsuper().__init__(root_dir, data_key_left, label_key, label_mapping, file_extension)\nself.class_data = self._data_to_class(self.data, label_key)\nself.percent_matching_data = percent_matching_data\nself.data_key_left = data_key_left\nself.data_key_right = data_key_right\nself.label_key = label_key\n@staticmethod\ndef _data_to_class(data: Dict[int, Dict[str, Any]], label_key: str) -&gt; Dict[Any, Set[int]]:\n\"\"\"A helper method to build a mapping from classes to their corresponding data indices.\n        Args:\n            data: A data dictionary like {&lt;index&gt;: {\"data_key\": &lt;data value&gt;}}.\n            label_key: Which key inside `data` corresponds to the label value for a given index entry.\n        Returns:\n            A mapping like {\"label1\": {&lt;indices with label1&gt;}, \"label2\": {&lt;indices with label2&gt;}}.\n        \"\"\"\nclass_data = {}\nfor idx, elem in data.items():\nclass_data.setdefault(elem[label_key], set()).add(idx)\nreturn class_data\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        In this case, splits are computed based on the number of classes rather than the number of instances per class.\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self.class_data)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['SiameseDirDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which classes to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing classes at the indices specified by `splits` from the current dataset.\n        \"\"\"\n# Splits in this context refer to class indices rather than the typical data indices\nresults = []\nfor split in splits:\n# Convert class indices to data indices\nint_class_keys = list(sorted(self.class_data.keys()))\nsplit = [item for i in split for item in self.class_data[int_class_keys[i]]]\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nclass_data = self._data_to_class(data, self.label_key)\nresults.append(\nself._skip_init(data,\nclass_data=class_data,\n**{k: v\nfor k, v in self.__dict__.items() if k not in {'data', 'class_data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself.class_data = self._data_to_class(self.data, self.label_key)\n# The summary function is being cached by a base class, so reset our cache here\n# noinspection PyUnresolvedReferences\nself.summary.cache_clear()\nreturn results\ndef __getitem__(self, index: int):\n\"\"\"Extract items from the dataset based on the given `batch_idx`.\n        Args:\n            index: Which data instance to use as the 'left' element.\n        Returns:\n            A datapoint for the given index.\n        \"\"\"\nbase_item = deepcopy(self.data[index])\nif np.random.uniform(0, 1) &lt; self.percent_matching_data:\n# Generate matching data\nclazz_items = self.class_data[base_item[self.label_key]]\nother = np.random.choice(list(clazz_items - {index}))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 1\nelse:\n# Generate non-matching data\nother_classes = self.class_data.keys() - {base_item[self.label_key]}\nother_class = np.random.choice(list(other_classes))\nother = np.random.choice(list(self.class_data[other_class]))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 0\nreturn base_item\ndef one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n        The similarity should be highest between the index 0 elements of the arrays.\n        Args:\n            n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n        Returns:\n            ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n            [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n        \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n            \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.one_shot_trial", "title": "<code>one_shot_trial</code>", "text": "<p>Generate one-shot trial data.</p> <p>The similarity should be highest between the index 0 elements of the arrays.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],</p> <code>List[str]</code> <p>[class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n    The similarity should be highest between the index 0 elements of the arrays.\n    Args:\n        n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n    Returns:\n        ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n        [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n    \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n        \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/data/breast_cancer.html", "title": "breast_cancer", "text": ""}, {"location": "fastestimator/dataset/data/breast_cancer.html#fastestimator.fastestimator.dataset.data.breast_cancer.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.</p> <p>For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.</p> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\breast_cancer.py</code> <pre><code>def load_data() -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.\n    For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x, y) = load_breast_cancer(return_X_y=True)\nx_train, x_eval, y_train, y_eval = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train, x_eval = np.float32(x_train), np.float32(x_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifar10.html", "title": "cifar10", "text": ""}, {"location": "fastestimator/dataset/data/cifar10.html#fastestimator.fastestimator.dataset.data.cifar10.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the CIFAR10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifar10.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the CIFAR10 dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.cifar10.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cub200.html", "title": "cub200", "text": ""}, {"location": "fastestimator/dataset/data/cub200.html#fastestimator.fastestimator.dataset.data.cub200.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.</p> <p>Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local     storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cub200.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.\n    Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local\n        storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'CUB200')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'CUB200')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, 'cub200.csv')\nimage_compressed_path = os.path.join(root_dir, 'images.tgz')\nannotation_compressed_path = os.path.join(root_dir, 'annotations.tgz')\nimage_extracted_path = os.path.join(root_dir, 'images')\nannotation_extracted_path = os.path.join(root_dir, 'annotations-mat')\nif not (os.path.exists(image_extracted_path) and os.path.exists(annotation_extracted_path)):\n# download\nif not (os.path.exists(image_compressed_path) and os.path.exists(annotation_compressed_path)):\nprint(\"Downloading data to {}\".format(root_dir))\n_download_file_from_google_drive('1GDr1OkoXdhaXWGA8S3MAq3a522Tak-nx', image_compressed_path)\n_download_file_from_google_drive('16NsbTpMs5L6hT4hUJAmpW2u7wH326WTR', annotation_compressed_path)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(image_compressed_path) as img_tar:\nimg_tar.extractall(root_dir)\nwith tarfile.open(annotation_compressed_path) as anno_tar:\nanno_tar.extractall(root_dir)\n# glob and generate csv\nif not os.path.exists(csv_path):\nimage_folder = os.path.join(root_dir, \"images\")\nclass_names = os.listdir(image_folder)\nlabel_map = {}\nimages = []\nlabels = []\nidx = 0\nfor class_name in class_names:\nif not class_name.startswith(\"._\"):\nimage_folder_class = os.path.join(image_folder, class_name)\nlabel_map[class_name] = idx\nidx += 1\nimage_names = os.listdir(image_folder_class)\nfor image_name in image_names:\nif not image_name.startswith(\"._\"):\nimages.append(os.path.join(image_folder_class, image_name))\nlabels.append(label_map[class_name])\nzipped_list = list(zip(images, labels))\nrandom.shuffle(zipped_list)\ndf = pd.DataFrame(zipped_list, columns=[\"image\", \"label\"])\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['annotation'] = df['image'].str.replace('images', 'annotations-mat').str.replace('jpg', 'mat')\ndf.to_csv(csv_path, index=False)\nprint(\"Data summary is saved at {}\".format(csv_path))\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/food101.html", "title": "food101", "text": ""}, {"location": "fastestimator/dataset/data/food101.html#fastestimator.fastestimator.dataset.data.food101.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Food-101 dataset.</p> <p>Food-101 dataset is a collection of images from 101 food categories. Sourced from http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[CSVDataset, CSVDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\food101.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[CSVDataset, CSVDataset]:\n\"\"\"Load and return the Food-101 dataset.\n    Food-101 dataset is a collection of images from 101 food categories.\n    Sourced from http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Food_101')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Food_101')\nos.makedirs(root_dir, exist_ok=True)\nimage_compressed_path = os.path.join(root_dir, 'food-101.tar.gz')\nimage_extracted_path = os.path.join(root_dir, 'food-101')\ntrain_csv_path = os.path.join(root_dir, 'train.csv')\ntest_csv_path = os.path.join(root_dir, 'test.csv')\nif not os.path.exists(image_extracted_path):\n# download\nif not os.path.exists(image_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(image_compressed_path) as img_tar:\nimg_tar.extractall(root_dir)\nlabels = open(os.path.join(root_dir, \"food-101/meta/classes.txt\"), \"r\").read().split()\nlabel_dict = {labels[i]: i for i in range(len(labels))}\nif not os.path.exists(train_csv_path):\ntrain_images = open(os.path.join(root_dir, \"food-101/meta/train.txt\"), \"r\").read().split()\nrandom.shuffle(train_images)\n_create_csv(train_images, label_dict, train_csv_path)\nif not os.path.exists(test_csv_path):\ntest_images = open(os.path.join(root_dir, \"food-101/meta/test.txt\"), \"r\").read().split()\nrandom.shuffle(test_images)\n_create_csv(test_images, label_dict, test_csv_path)\nreturn CSVDataset(train_csv_path), CSVDataset(test_csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/horse2zebra.html", "title": "horse2zebra", "text": ""}, {"location": "fastestimator/dataset/data/horse2zebra.html#fastestimator.fastestimator.dataset.data.horse2zebra.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the horse2zebra dataset.</p> <p>Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will     download the data to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The desired batch size.</p> required <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[BatchDataset, BatchDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\horse2zebra.py</code> <pre><code>def load_data(batch_size: int, root_dir: Optional[str] = None) -&gt; Tuple[BatchDataset, BatchDataset]:\n\"\"\"Load and return the horse2zebra dataset.\n    Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will\n        download the data to local storage if the data has not been previously downloaded.\n    Args:\n        batch_size: The desired batch size.\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'horse2zebra')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'horse2zebra')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'horse2zebra.zip')\ndata_folder_path = os.path.join(root_dir, 'images')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\nzip_file.extractall(root_dir)\nos.rename(os.path.join(root_dir, 'horse2zebra'), data_folder_path)\ntest_a = DirDataset(root_dir=os.path.join(data_folder_path, 'testA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntest_b = DirDataset(root_dir=os.path.join(data_folder_path, 'testB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_a = DirDataset(root_dir=os.path.join(data_folder_path, 'trainA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_b = DirDataset(root_dir=os.path.join(data_folder_path, 'trainB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\noutputs = (BatchDataset(datasets=[train_a, train_b], num_samples=[batch_size, batch_size]),\nBatchDataset(datasets=[test_a, test_b], num_samples=[batch_size, batch_size]))\nreturn outputs\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html", "title": "imdb_review", "text": ""}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the IMDB Movie review dataset.</p> <p>This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).</p> <p>Parameters:</p> Name Type Description Default <code>max_len</code> <code>int</code> <p>Maximum desired length of an input sequence.</p> required <code>vocab_size</code> <code>int</code> <p>Vocabulary size to learn word embeddings.</p> required <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def load_data(max_len: int, vocab_size: int) -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the IMDB Movie review dataset.\n    This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).\n    Args:\n        max_len: Maximum desired length of an input sequence.\n        vocab_size: Vocabulary size to learn word embeddings.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.imdb.load_data(maxlen=max_len, num_words=vocab_size)\n# pad the sequences to max length\nx_train = np.array([pad(x, max_len, 0) for x in x_train])\nx_eval = np.array([pad(x, max_len, 0) for x in x_eval])\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.pad", "title": "<code>pad</code>", "text": "<p>Pad an input_list to a given size.</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[int]</code> <p>The list to be padded.</p> required <code>padding_size</code> <code>int</code> <p>The desired length of the returned list.</p> required <code>padding_value</code> <code>int</code> <p>The value to be inserted for padding.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p><code>input_list</code> with <code>padding_value</code>s appended until the <code>padding_size</code> is reached.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def pad(input_list: List[int], padding_size: int, padding_value: int) -&gt; List[int]:\n\"\"\"Pad an input_list to a given size.\n    Args:\n        input_list: The list to be padded.\n        padding_size: The desired length of the returned list.\n        padding_value: The value to be inserted for padding.\n    Returns:\n        `input_list` with `padding_value`s appended until the `padding_size` is reached.\n    \"\"\"\nreturn input_list + [padding_value] * abs((len(input_list) - padding_size))\n</code></pre>"}, {"location": "fastestimator/dataset/data/mendeley.html", "title": "mendeley", "text": ""}, {"location": "fastestimator/dataset/data/mendeley.html#fastestimator.fastestimator.dataset.data.mendeley.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Mendeley dataset.</p> <p>Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2</p> <p>CC BY 4.0 licence: https://creativecommons.org/licenses/by/4.0/</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mendeley.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the Mendeley dataset.\n    Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray\n    Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2\n    CC BY 4.0 licence:\n    https://creativecommons.org/licenses/by/4.0/\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nurl = 'https://data.mendeley.com/datasets/rscbjbr9sj/2/files/41d542e7-7f91-47f6-9ff2-dd8e5a5a7861/' \\\n          'ChestXRay2017.zip?dl=1'\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Mendeley')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Mendeley')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'ChestXRay2017.zip')\ndata_folder_path = os.path.join(root_dir, 'chest_xray')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nstream = requests.get(url, stream=True)  # python wget does not work\ntotal_size = int(stream.headers.get('content-length', 0))\nblock_size = int(1e6)  # 1 MB\nprogress = tqdm(total=total_size, unit='B', unit_scale=True)\nwith open(data_compressed_path, 'wb') as outfile:\nfor data in stream.iter_content(block_size):\nprogress.update(len(data))\noutfile.write(data)\nprogress.close()\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"chest_xray/\"), zip_file.namelist()))\nlabel_mapping = {'NORMAL': 0, 'PNEUMONIA': 1}\nreturn LabeledDirDataset(os.path.join(data_folder_path, \"train\"), label_mapping=label_mapping,\nfile_extension=\".jpeg\"), LabeledDirDataset(os.path.join(data_folder_path, \"test\"),\nlabel_mapping=label_mapping,\nfile_extension=\".jpeg\")\n</code></pre>"}, {"location": "fastestimator/dataset/data/mitmovie_ner.html", "title": "mitmovie_ner", "text": ""}, {"location": "fastestimator/dataset/data/mitmovie_ner.html#fastestimator.fastestimator.dataset.data.mitmovie_ner.get_sentences_and_labels", "title": "<code>get_sentences_and_labels</code>", "text": "<p>Combines tokens into sentences and create vocab set for train data and labels.</p> <p>For simplicity tokens with 'O' entity are omitted.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the downloaded dataset file.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[str]], Set[str], Set[str]]</code> <p>(sentences, labels, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mitmovie_ner.py</code> <pre><code>def get_sentences_and_labels(path: str) -&gt; Tuple[List[str], List[List[str]], Set[str], Set[str]]:\n\"\"\"Combines tokens into sentences and create vocab set for train data and labels.\n    For simplicity tokens with 'O' entity are omitted.\n    Args:\n        path: Path to the downloaded dataset file.\n    Returns:\n        (sentences, labels, train_vocab, label_vocab)\n    \"\"\"\nwords, tags = [], []\nword_vocab, label_vocab = set(), set()\nsentences, labels = [], []\ndata = open(path)\nfor line in data:\nif line != '\\n':\nline = line.split()\nwords.append(line[1])\ntags.append(line[0])\nword_vocab.add(line[1])\nlabel_vocab.add(line[0])\nelse:\nsentences.append(\" \".join([s for s in words]))\nlabels.append([t for t in tags])\nwords.clear()\ntags.clear()\nsentences = list(filter(None, sentences))\nlabels = list(filter(None, labels))\nreturn sentences, labels, word_vocab, label_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/mitmovie_ner.html#fastestimator.fastestimator.dataset.data.mitmovie_ner.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the MIT Movie dataset.</p> <p>MIT Movies dataset is a semantically tagged training and test corpus in BIO format. The sentence is encoded as one token per line with information provided in tab-seprated columns. Sourced from https://groups.csail.mit.edu/sls/downloads/movie/</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]</code> <p>(train_data, eval_data, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mitmovie_ner.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]:\n\"\"\"Load and return the MIT Movie dataset.\n    MIT Movies dataset is a semantically tagged training and test corpus in BIO format. The sentence is encoded as one\n    token per line with information provided in tab-seprated columns.\n    Sourced from https://groups.csail.mit.edu/sls/downloads/movie/\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data, train_vocab, label_vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'MITMovie')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'MITMovie')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data_path = os.path.join(root_dir, 'engtrain.bio')\ntest_data_path = os.path.join(root_dir, 'engtest.bio')\nfiles = [(train_data_path, 'https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio'),\n(test_data_path, 'https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio')]\nfor data_path, download_link in files:\nif not os.path.exists(data_path):\n# Download\nprint(\"Downloading data: {}\".format(data_path))\nwget.download(download_link, data_path, bar=bar_custom)\nx_train, y_train, x_vocab, y_vocab = get_sentences_and_labels(train_data_path)\nx_eval, y_eval, x_eval_vocab, y_eval_vocab = get_sentences_and_labels(test_data_path)\nx_vocab |= x_eval_vocab\ny_vocab |= y_eval_vocab\nx_train = np.array(x_train)\nx_eval = np.array(x_eval)\ny_train = np.array(y_train)\ny_eval = np.array(y_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data, x_vocab, y_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/mnist.html", "title": "mnist", "text": ""}, {"location": "fastestimator/dataset/data/mnist.html#fastestimator.fastestimator.dataset.data.mnist.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mnist.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the MNIST dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/montgomery.html", "title": "montgomery", "text": ""}, {"location": "fastestimator/dataset/data/montgomery.html#fastestimator.fastestimator.dataset.data.montgomery.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the montgomery dataset.</p> <p>Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data     to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\montgomery.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the montgomery dataset.\n    Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data\n        to local storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Montgomery')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Montgomery')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, \"montgomery.csv\")\ndata_compressed_path = os.path.join(root_dir, 'NLM-MontgomeryCXRSet.zip')\nextract_folder_path = os.path.join(root_dir, 'MontgomerySet')\nif not os.path.exists(extract_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"MontgomerySet/\"), zip_file.namelist()))\n# glob and generate csv\nif not os.path.exists(csv_path):\nimg_list = glob(os.path.join(extract_folder_path, 'CXR_png', '*.png'))\ndf = pd.DataFrame(data={'image': img_list})\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['mask_left'] = df['image'].apply(lambda x: x.replace('CXR_png', os.path.join('ManualMask', 'leftMask')))\ndf['mask_right'] = df['image'].apply(lambda x: x.replace('CXR_png', os.path.join('ManualMask', 'rightMask')))\ndf.to_csv(csv_path, index=False)\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html", "title": "mscoco", "text": ""}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.MSCOCODataset", "title": "<code>MSCOCODataset</code>", "text": "<p>         Bases: <code>DirDataset</code></p> <p>A specialized DirDataset to handle MSCOCO data.</p> <p>This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>The path the directory containing MSOCO images.</p> required <code>annotation_file</code> <code>str</code> <p>The path to the file containing annotation data.</p> required <code>caption_file</code> <code>str</code> <p>The path the file containing caption data.</p> required <code>include_bboxes</code> <code>bool</code> <p>Whether images should be paired with their associated bounding boxes. If true, images without bounding boxes will be ignored and other images may be oversampled in order to take their place.</p> <code>True</code> <code>include_masks</code> <code>bool</code> <p>Whether images should be paired with their associated masks. If true, images without masks will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>include_captions</code> <code>bool</code> <p>Whether images should be paired with their associated captions. If true, images without captions will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>min_bbox_area</code> <p>Bounding boxes with a total area less than <code>min_bbox_area</code> will be discarded.</p> <code>1.0</code> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>@traceable()\nclass MSCOCODataset(DirDataset):\n\"\"\"A specialized DirDataset to handle MSCOCO data.\n    This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.\n    Args:\n        image_dir: The path the directory containing MSOCO images.\n        annotation_file: The path to the file containing annotation data.\n        caption_file: The path the file containing caption data.\n        include_bboxes: Whether images should be paired with their associated bounding boxes. If true, images without\n            bounding boxes will be ignored and other images may be oversampled in order to take their place.\n        include_masks: Whether images should be paired with their associated masks. If true, images without masks will\n            be ignored and other images may be oversampled in order to take their place.\n        include_captions: Whether images should be paired with their associated captions. If true, images without\n            captions will be ignored and other images may be oversampled in order to take their place.\n        min_bbox_area: Bounding boxes with a total area less than `min_bbox_area` will be discarded.\n    \"\"\"\ninstances: Optional[COCO]\ncaptions: Optional[COCO]\ndef __init__(self,\nimage_dir: str,\nannotation_file: str,\ncaption_file: str,\ninclude_bboxes: bool = True,\ninclude_masks: bool = False,\ninclude_captions: bool = False,\nmin_bbox_area=1.0) -&gt; None:\nsuper().__init__(root_dir=image_dir, data_key=\"image\", recursive_search=False)\nif include_masks:\nassert include_bboxes, \"must include bboxes with mask data\"\nself.include_bboxes = include_bboxes\nself.include_masks = include_masks\nself.min_bbox_area = min_bbox_area\nwith Suppressor():\nself.instances = COCO(annotation_file)\nself.captions = COCO(caption_file) if include_captions else None\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned. If bboxes, masks, or captions are required and the data\n                at the desired index does not have one or more of these features, then data from a random index which\n                does have all necessary features will be fetched instead.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nhas_data = False\nresponse = {}\nwhile not has_data:\nhas_box, has_mask, has_caption = True, True, True\nresponse = self._get_single_item(index)\nif isinstance(index, str):\nreturn response\nif self.include_bboxes and not response[\"bbox\"]:\nhas_box = False\nif self.include_masks and not response[\"mask\"]:\nhas_mask = False\nif self.captions and not response[\"caption\"]:\nhas_caption = False\nhas_data = has_box and has_mask and has_caption\nindex = np.random.randint(len(self))\nreturn response\ndef _get_single_item(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nresponse = super().__getitem__(index)\nif isinstance(index, str):\nreturn response\nelse:\nresponse = deepcopy(response)\nimage = response[\"image\"]\nimage_id = int(os.path.splitext(os.path.basename(image))[0])\nresponse[\"image_id\"] = image_id\nif self.include_bboxes:\nself._populate_instance_data(response, image_id)\nif self.captions:\nself._populate_caption_data(response, image_id)\nreturn response\ndef _populate_instance_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add instance data to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find data.\n        \"\"\"\ndata[\"bbox\"] = []\nif self.include_masks:\ndata[\"mask\"] = []\nannotation_ids = self.instances.getAnnIds(imgIds=image_id, iscrowd=False)\nif annotation_ids:\nannotations = self.instances.loadAnns(annotation_ids)\nfor annotation in annotations:\nif annotation[\"bbox\"][2] * annotation[\"bbox\"][3] &gt; self.min_bbox_area:\ndata[\"bbox\"].append(tuple(annotation['bbox'] + [annotation['category_id']]))\nif self.include_masks:\ndata[\"mask\"].append(self.instances.annToMask(annotation))\ndef _populate_caption_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add captions to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find captions.\n        \"\"\"\ndata[\"caption\"] = []\nannotation_ids = self.captions.getAnnIds(imgIds=image_id)\nif annotation_ids:\nannotations = self.captions.loadAnns(annotation_ids)\nfor annotation in annotations:\ndata[\"caption\"].append(annotation['caption'])\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the COCO dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>load_bboxes</code> <code>bool</code> <p>Whether to load bbox-related data.</p> <code>True</code> <code>load_masks</code> <code>bool</code> <p>Whether to load mask data (in the form of an array of 1-hot images).</p> <code>False</code> <code>load_captions</code> <code>bool</code> <p>Whether to load caption-related data.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[MSCOCODataset, MSCOCODataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\nload_bboxes: bool = True,\nload_masks: bool = False,\nload_captions: bool = False) -&gt; Tuple[MSCOCODataset, MSCOCODataset]:\n\"\"\"Load and return the COCO dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        load_bboxes: Whether to load bbox-related data.\n        load_masks: Whether to load mask data (in the form of an array of 1-hot images).\n        load_captions: Whether to load caption-related data.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'MSCOCO2017')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'MSCOCO2017')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data = os.path.join(root_dir, \"train2017\")\neval_data = os.path.join(root_dir, \"val2017\")\nannotation_data = os.path.join(root_dir, \"annotations\")\nfiles = [(train_data, \"train2017.zip\", 'http://images.cocodataset.org/zips/train2017.zip'),\n(eval_data, \"val2017.zip\", 'http://images.cocodataset.org/zips/val2017.zip'),\n(annotation_data,\n\"annotations_trainval2017.zip\",\n'http://images.cocodataset.org/annotations/annotations_trainval2017.zip')]\nfor data_dir, zip_name, download_url in files:\nif not os.path.exists(data_dir):\nzip_path = os.path.join(root_dir, zip_name)\n# Download\nif not os.path.exists(zip_path):\nprint(\"Downloading {} to {}\".format(zip_name, root_dir))\nwget.download(download_url, zip_path, bar=bar_custom)\n# Extract\nprint(\"Extracting {}\".format(zip_name))\nwith zipfile.ZipFile(zip_path, 'r') as zip_file:\nzip_file.extractall(os.path.dirname(zip_path))\ntrain_annotation = os.path.join(annotation_data, \"instances_train2017.json\")\neval_annotation = os.path.join(annotation_data, \"instances_val2017.json\")\ntrain_captions = os.path.join(annotation_data, \"captions_train2017.json\")\neval_captions = os.path.join(annotation_data, \"captions_val2017.json\")\ntrain_ds = MSCOCODataset(train_data,\ntrain_annotation,\ntrain_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\neval_ds = MSCOCODataset(eval_data,\neval_annotation,\neval_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\nreturn train_ds, eval_ds\n</code></pre>"}, {"location": "fastestimator/dataset/data/nih_chestxray.html", "title": "nih_chestxray", "text": ""}, {"location": "fastestimator/dataset/data/nih_chestxray.html#fastestimator.fastestimator.dataset.data.nih_chestxray.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\nih_chestxray.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; DirDataset:\n\"\"\"Load and return the NIH Chest X-ray dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'NIH_Chestxray')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'NIH_Chestxray')\nos.makedirs(root_dir, exist_ok=True)\nimage_extracted_path = os.path.join(root_dir, 'images')\nif not os.path.exists(image_extracted_path):\n# download data\nlinks = [\n'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n]\ndata_paths = [os.path.join(root_dir, \"images_{}.tar.gz\".format(x)) for x in range(len(links))]\nfor idx, (link, data_path) in enumerate(zip(links, data_paths)):\n_download_data(link, data_path, idx, len(links))\n# extract data\nfor idx, data_path in enumerate(data_paths):\nprint(\"Extracting {}, file {} / {}\".format(data_path, idx + 1, len(links)))\nwith tarfile.open(data_path) as img_tar:\nimg_tar.extractall(root_dir)\nreturn DirDataset(image_extracted_path, file_extension='.png', recursive_search=False)\n</code></pre>"}, {"location": "fastestimator/dataset/data/omniglot.html", "title": "omniglot", "text": ""}, {"location": "fastestimator/dataset/data/omniglot.html#fastestimator.fastestimator.dataset.data.omniglot.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Omniglot dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[SiameseDirDataset, SiameseDirDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\omniglot.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[SiameseDirDataset, SiameseDirDataset]:\n\"\"\"Load and return the Omniglot dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'Omniglot')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Omniglot')\nos.makedirs(root_dir, exist_ok=True)\ntrain_path = os.path.join(root_dir, 'images_background')\neval_path = os.path.join(root_dir, 'images_evaluation')\ntrain_zip = os.path.join(root_dir, 'images_background.zip')\neval_zip = os.path.join(root_dir, 'images_evaluation.zip')\nfiles = [(train_path, train_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip'),\n(eval_path, eval_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip')]\nfor data_path, data_zip, download_link in files:\nif not os.path.exists(data_path):\n# Download\nif not os.path.exists(data_zip):\nprint(\"Downloading data: {}\".format(data_zip))\nwget.download(download_link, data_zip, bar=bar_custom)\n# Extract\nprint(\"Extracting data: {}\".format(data_path))\nwith zipfile.ZipFile(data_zip, 'r') as zip_file:\nzip_file.extractall(root_dir)\nreturn SiameseDirDataset(train_path), SiameseDirDataset(eval_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/penn_treebank.html", "title": "penn_treebank", "text": ""}, {"location": "fastestimator/dataset/data/penn_treebank.html#fastestimator.fastestimator.dataset.data.penn_treebank.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Penn TreeBank dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>seq_length</code> <code>int</code> <p>Length of data sequence.</p> <code>64</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, NumpyDataset, List[str]]</code> <p>(train_data, eval_data, test_data, vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\penn_treebank.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\nseq_length: int = 64) -&gt; Tuple[NumpyDataset, NumpyDataset, NumpyDataset, List[str]]:\n\"\"\"Load and return the Penn TreeBank dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        seq_length: Length of data sequence.\n    Returns:\n        (train_data, eval_data, test_data, vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'PennTreeBank')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'PennTreeBank')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data_path = os.path.join(root_dir, 'ptb.train.txt')\neval_data_path = os.path.join(root_dir, 'ptb.valid.txt')\ntest_data_path = os.path.join(root_dir, 'ptb.test.txt')\nfiles = [(train_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt'),\n(eval_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt'),\n(test_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt')]\ntexts = []\nfor data_path, download_link in files:\nif not os.path.exists(data_path):\n# Download\nprint(\"Downloading data: {}\".format(data_path))\nwget.download(download_link, data_path, bar=bar_custom)\ntext = []\nwith open(data_path, 'r') as f:\nfor line in f:\ntext.extend(line.split() + ['&lt;eos&gt;'])\ntexts.append(text)\n# Build dictionary from training data\nvocab = sorted(set(texts[0]))\nword2idx = {u: i for i, u in enumerate(vocab)}\n#convert word to index and split the sequences and discard the last incomplete sequence\ndata = [[word2idx[word] for word in text[:-(len(text) % seq_length)]] for text in texts]\nx_train, x_eval, x_test = [np.array(d).reshape(-1, seq_length) for d in data]\ntrain_data = NumpyDataset(data={\"x\": x_train})\neval_data = NumpyDataset(data={\"x\": x_eval})\ntest_data = NumpyDataset(data={\"x\": x_test})\nreturn train_data, eval_data, test_data, vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/shakespeare.html", "title": "shakespeare", "text": ""}, {"location": "fastestimator/dataset/data/shakespeare.html#fastestimator.fastestimator.dataset.data.shakespeare.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Shakespeare dataset.</p> <p>Shakespeare dataset is a collection of texts written by Shakespeare. Sourced from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>seq_length</code> <code>int</code> <p>Length of data sequence.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, List[str]]</code> <p>(train_data, vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\shakespeare.py</code> <pre><code>def load_data(root_dir: Optional[str] = None, seq_length: int = 100) -&gt; Tuple[NumpyDataset, List[str]]:\n\"\"\"Load and return the Shakespeare dataset.\n    Shakespeare dataset is a collection of texts written by Shakespeare.\n    Sourced from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        seq_length: Length of data sequence.\n    Returns:\n        (train_data, vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Shakespeare')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Shakespeare')\nos.makedirs(root_dir, exist_ok=True)\nfile_path = os.path.join(root_dir, 'shakespeare.txt')\ndownload_link = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\nif not os.path.exists(file_path):\n# Download\nprint(\"Downloading data: {}\".format(file_path))\nwget.download(download_link, file_path, bar=bar_custom)\nwith open(file_path, 'rb') as f:\ntext_data = f.read().decode(encoding='utf-8')\n# Build dictionary from training data\nvocab = sorted(set(text_data))\n# Creating a mapping from unique characters to indices\nchar2idx = {u: i for i, u in enumerate(vocab)}\ntext_data = [char2idx[c] for c in text_data] + [0] * (seq_length - len(text_data) % seq_length)\ntext_data = np.array(text_data).reshape(-1, seq_length)\ntrain_data = NumpyDataset(data={\"x\": text_data})\nreturn train_data, vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/svhn.html", "title": "svhn", "text": ""}, {"location": "fastestimator/dataset/data/svhn.html#fastestimator.fastestimator.dataset.data.svhn.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Street View House Numbers (SVHN) dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[PickleDataset, PickleDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\svhn.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[PickleDataset, PickleDataset]:\n\"\"\"Load and return the Street View House Numbers (SVHN) dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'SVHN')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'SVHN')\nos.makedirs(root_dir, exist_ok=True)\ntrain_file_path = os.path.join(root_dir, 'train.pickle')\ntest_file_path = os.path.join(root_dir, 'test.pickle')\ntrain_compressed_path = os.path.join(root_dir, \"train.tar.gz\")\ntest_compressed_path = os.path.join(root_dir, \"test.tar.gz\")\ntrain_folder_path = os.path.join(root_dir, \"train\")\ntest_folder_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_folder_path):\n# download\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/train.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(train_compressed_path) as tar:\ntar.extractall(root_dir)\nif not os.path.exists(test_folder_path):\n# download\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading eval data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/test.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(test_compressed_path) as tar:\ntar.extractall(root_dir)\n# glob and generate bbox files\nif not os.path.exists(train_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(train_folder_path, \"train\", train_file_path)\nif not os.path.exists(test_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(test_folder_path, \"test\", test_file_path)\nreturn PickleDataset(train_file_path), PickleDataset(test_file_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/usps.html", "title": "usps", "text": ""}, {"location": "fastestimator/dataset/data/usps.html#fastestimator.fastestimator.dataset.data.usps.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the USPS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\usps.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the USPS dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'USPS')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'USPS')\nos.makedirs(root_dir, exist_ok=True)\n# download data to memory\ntrain_compressed_path = os.path.join(root_dir, \"zip.train.gz\")\ntest_compressed_path = os.path.join(root_dir, \"zip.test.gz\")\ntrain_base_path = os.path.join(root_dir, \"train\")\ntest_base_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_base_path):\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz',\nroot_dir,\nbar=bar_custom)\ntrain_images, train_labels = _extract_images_labels(train_compressed_path)\n_write_data(train_images, train_labels, train_base_path, \"train\")\nif not os.path.exists(test_base_path):\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading test data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz',\nroot_dir,\nbar=bar_custom)\ntest_images, test_labels = _extract_images_labels(test_compressed_path)\n_write_data(test_images, test_labels, test_base_path, \"test\")\n# make datasets\nreturn LabeledDirDataset(train_base_path, file_extension=\".png\"), LabeledDirDataset(test_base_path,\nfile_extension=\".png\")\n</code></pre>"}, {"location": "fastestimator/layers/pytorch/cropping_2d.html", "title": "cropping_2d", "text": ""}, {"location": "fastestimator/layers/pytorch/cropping_2d.html#fastestimator.fastestimator.layers.pytorch.cropping_2d.Cropping2D", "title": "<code>Cropping2D</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A layer for cropping along height and width dimensions.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <pre><code>x = torch.tensor(list(range(100))).view((1,1,10,10))\nm = fe.layers.pytorch.Cropping2D(3)\ny = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\nm = fe.layers.pytorch.Cropping2D((3, 4))\ny = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\nm = fe.layers.pytorch.Cropping2D(((1, 4), 4))\ny = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cropping</code> <code>Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int, int]]]]</code> <p>Height and width cropping parameters. If a single int 'n' is specified, then the width and height of the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w') is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h' and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then 'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right (assuming the top left corner as the 0,0 origin).</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cropping</code> has an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\layers\\pytorch\\cropping_2d.py</code> <pre><code>class Cropping2D(nn.Module):\n\"\"\"A layer for cropping along height and width dimensions.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    ```python\n    x = torch.tensor(list(range(100))).view((1,1,10,10))\n    m = fe.layers.pytorch.Cropping2D(3)\n    y = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\n    m = fe.layers.pytorch.Cropping2D((3, 4))\n    y = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\n    m = fe.layers.pytorch.Cropping2D(((1, 4), 4))\n    y = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n    ```\n    Args:\n        cropping: Height and width cropping parameters. If a single int 'n' is specified, then the width and height of\n            the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w')\n            is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h'\n            and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then\n            'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right\n            (assuming the top left corner as the 0,0 origin).\n    Raises:\n        ValueError: If `cropping` has an unacceptable data type.\n    \"\"\"\ndef __init__(self, cropping: Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int,\nint]]]] = 0) -&gt; None:\nsuper().__init__()\nif isinstance(cropping, int):\nself.cropping = ((cropping, cropping), (cropping, cropping))\nelif hasattr(cropping, '__len__'):\nif len(cropping) != 2:\nraise ValueError(f\"'cropping' should have two elements, but found {len(cropping)}\")\nif isinstance(cropping[0], int):\nheight_cropping = (cropping[0], cropping[0])\nelif hasattr(cropping[0], '__len__') and len(cropping[0]) == 2:\nheight_cropping = (cropping[0][0], cropping[0][1])\nelse:\nraise ValueError(f\"'cropping' height should be an int or tuple of ints, but found {cropping[0]}\")\nif isinstance(cropping[1], int):\nwidth_cropping = (cropping[1], cropping[1])\nelif hasattr(cropping[1], '__len__') and len(cropping[1]) == 2:\nwidth_cropping = (cropping[1][0], cropping[1][1])\nelse:\nraise ValueError(f\"'cropping' width should be an int or tuple of ints, but found {cropping[1]}\")\nself.cropping = (height_cropping, width_cropping)\nelse:\nraise ValueError(\n\"cropping` should be either an int, a tuple of 2 ints or a tuple of two tuple of 2 ints. Found: \" +\nstr(cropping))\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn x[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:-self.cropping[1][1]]\n</code></pre>"}, {"location": "fastestimator/layers/pytorch/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/layers/pytorch/hadamard.html#fastestimator.fastestimator.layers.pytorch.hadamard.HadamardCode", "title": "<code>HadamardCode</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A layer for applying an error correcting code to your outputs.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial- robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code lookup.</p> <pre><code># Use as a drop-in replacement for your softmax layer:\ndef __init__(self, classes):\nself.fc1 = nn.Linear(1024, 64)\nself.fc2 = nn.Linear(64, classes)\ndef forward(self, x):\nx = fn.relu(self.fc1(x))\nx = fn.softmax(self.fc2(x), dim=-1)\n#   ----- vs ------\ndef __init__(self, classes):\nself.fc1 = nn.Linear(1024, 64)\nself.fc2 = HadamardCode(64, classes)\ndef forward(self, x):\nx = fn.relu(self.fc1(x))\nx = self.fc2(x)\n</code></pre> <pre><code># Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\ndef __init__(self, classes):\nself.fc1 = nn.ModuleList([nn.Linear(1024, 16) for _ in range(4)])\nself.fc2 = HadamardCode([16]*4, classes)\ndef forward(self, x):\nx = [fn.relu(fc(x)) for fc in self.fc1]\nx = self.fc2(x)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>Union[int, List[int]]</code> <p>How many input features there are (inputs should be of shape (Batch, N) or [(Batch, N), ...]).</p> required <code>n_classes</code> <code>int</code> <p>How many output classes to map onto.</p> required <code>code_length</code> <code>Optional[int]</code> <p>How long of an error correcting code to use. Should be a positive multiple of 2. If not provided, the smallest power of 2 which is &gt;= <code>n_outputs</code> will be used, or 16 if the latter is larger.</p> <code>None</code> <code>max_prob</code> <code>float</code> <p>The maximum probability that can be assigned to a class. For numeric stability this must be less than 1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should be noticeably less than 1, for example 0.95 or even 0.8.</p> <code>0.95</code> <code>power</code> <code>float</code> <p>The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give slightly faster convergence at the expense of adversarial performance. Must be greater than zero.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>code_length</code> is invalid.</p> Source code in <code>fastestimator\\fastestimator\\layers\\pytorch\\hadamard.py</code> <pre><code>class HadamardCode(nn.Module):\n\"\"\"A layer for applying an error correcting code to your outputs.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial-\n    robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be\n    split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code\n    lookup.\n    ```python\n    # Use as a drop-in replacement for your softmax layer:\n    def __init__(self, classes):\n        self.fc1 = nn.Linear(1024, 64)\n        self.fc2 = nn.Linear(64, classes)\n    def forward(self, x):\n        x = fn.relu(self.fc1(x))\n        x = fn.softmax(self.fc2(x), dim=-1)\n    #   ----- vs ------\n    def __init__(self, classes):\n        self.fc1 = nn.Linear(1024, 64)\n        self.fc2 = HadamardCode(64, classes)\n    def forward(self, x):\n        x = fn.relu(self.fc1(x))\n        x = self.fc2(x)\n    ```\n    ```python\n    # Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\n    def __init__(self, classes):\n        self.fc1 = nn.ModuleList([nn.Linear(1024, 16) for _ in range(4)])\n        self.fc2 = HadamardCode([16]*4, classes)\n    def forward(self, x):\n        x = [fn.relu(fc(x)) for fc in self.fc1]\n        x = self.fc2(x)\n    ```\n    Args:\n        in_features: How many input features there are (inputs should be of shape (Batch, N) or [(Batch, N), ...]).\n        n_classes: How many output classes to map onto.\n        code_length: How long of an error correcting code to use. Should be a positive multiple of 2. If not provided,\n            the smallest power of 2 which is &gt;= `n_outputs` will be used, or 16 if the latter is larger.\n        max_prob: The maximum probability that can be assigned to a class. For numeric stability this must be less than\n            1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should\n            be noticeably less than 1, for example 0.95 or even 0.8.\n        power: The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances\n            into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small\n            values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give\n            slightly faster convergence at the expense of adversarial performance. Must be greater than zero.\n    Raises:\n        ValueError: If `code_length` is invalid.\n    \"\"\"\nheads: Union[nn.ModuleList, nn.Module]\ndef __init__(self,\nin_features: Union[int, List[int]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmax_prob: float = 0.95,\npower: float = 1.0) -&gt; None:\nsuper().__init__()\nself.n_classes = n_classes\nif code_length is None:\ncode_length = max(16, 1 &lt;&lt; (n_classes - 1).bit_length())\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nif power &lt;= 0:\nraise ValueError(f\"power must be positive, but got {power}.\")\nself.power = nn.Parameter(torch.tensor(power), requires_grad=False)\nif not 0.0 &lt; max_prob &lt; 1.0:\nraise ValueError(f\"max_prob must be in the range (0, 1), but got {max_prob}\")\nself.eps = nn.Parameter(\ntorch.tensor(self.code_length * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / power)),\nrequires_grad=False)\nlabels = hadamard(self.code_length)\n# Cut off 0th column b/c it's constant. It would also be possible to make the column sign alternate, but that\n# would break the symmetry between rows in the code.\nlabels = labels[:self.n_classes, 1:]\nself.labels = nn.Parameter(torch.tensor(labels, dtype=torch.float32), requires_grad=False)\nin_features = to_list(in_features)\nif len(in_features) &gt; code_length - 1:\nraise ValueError(f\"Too many input heads {len(in_features)} for the given code length {self.code_length}.\")\nhead_sizes = [self.code_length // len(in_features) for _ in range(len(in_features))]\nhead_sizes[0] = head_sizes[0] + self.code_length - sum(head_sizes)\nhead_sizes[0] = head_sizes[0] - 1  # We're going to cut off the 0th column from the code\nself.heads = nn.ModuleList([\nnn.Linear(in_features=in_feat, out_features=out_feat) for in_feat, out_feat in zip(in_features, head_sizes)\n])\ndef forward(self, x: List[torch.Tensor]) -&gt; torch.Tensor:\n# can't have forward function call subfunctions otherwise will fail on multi-gpu\nif isinstance(x, list):\nx = [head(tensor) for head, tensor in zip(self.heads, x)]\nx = torch.cat(x, dim=-1)\nelse:\nx = self.heads[0](x)\nx = torch.tanh(x)\n# Compute L1 distance\nx = torch.max(torch.sum(torch.abs(torch.unsqueeze(x, dim=1) - self.labels), dim=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / torch.pow(x, self.power)\nx = torch.div(x, torch.sum(x, dim=-1).view(-1, 1))\nreturn x\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/layers/tensorflow/hadamard.html#fastestimator.fastestimator.layers.tensorflow.hadamard.HadamardCode", "title": "<code>HadamardCode</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for applying an error correcting code to your outputs.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial- robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code lookup.</p> <pre><code># Use as a drop-in replacement for your softmax layer:\nmodel = Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=input_shape))\nmodel.add(layers.Dense(10, activation='softmax'))\n#   ----- vs ------\nmodel = Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=input_shape))\nmodel.add(HadamardCode(10))\n</code></pre> <pre><code># Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\ninputs = Input(input_shape)\nfeatures = Dense(1024, activation='relu')(inputs)\nheads = [Dense(20)(features) for _ in range(5)]\noutputs = HadamardCode(10)(heads)\nmodel = Model(inputs, outputs)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_classes</code> <code>int</code> <p>How many output classes to map onto.</p> required <code>code_length</code> <code>Optional[int]</code> <p>How long of an error correcting code to use. Should be a positive multiple of 2. If not provided, the smallest power of 2 which is &gt;= <code>n_outputs</code> will be used, or 16 if the latter is larger.</p> <code>None</code> <code>max_prob</code> <code>float</code> <p>The maximum probability that can be assigned to a class. For numeric stability this must be less than 1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should be noticeably less than 1, for example 0.95 or even 0.8.</p> <code>0.95</code> <code>power</code> <code>float</code> <p>The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give slightly faster convergence at the expense of adversarial performance. Must be greater than zero.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>code_length</code>, <code>max_prob</code>, or <code>power</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\hadamard.py</code> <pre><code>class HadamardCode(layers.Layer):\n\"\"\"A layer for applying an error correcting code to your outputs.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial-\n    robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be\n    split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code\n    lookup.\n    ```python\n    # Use as a drop-in replacement for your softmax layer:\n    model = Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n    model.add(layers.Dense(10, activation='softmax'))\n    #   ----- vs ------\n    model = Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n    model.add(HadamardCode(10))\n    ```\n    ```python\n    # Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\n    inputs = Input(input_shape)\n    features = Dense(1024, activation='relu')(inputs)\n    heads = [Dense(20)(features) for _ in range(5)]\n    outputs = HadamardCode(10)(heads)\n    model = Model(inputs, outputs)\n    ```\n    Args:\n        n_classes: How many output classes to map onto.\n        code_length: How long of an error correcting code to use. Should be a positive multiple of 2. If not provided,\n            the smallest power of 2 which is &gt;= `n_outputs` will be used, or 16 if the latter is larger.\n        max_prob: The maximum probability that can be assigned to a class. For numeric stability this must be less than\n            1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should\n            be noticeably less than 1, for example 0.95 or even 0.8.\n        power: The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances\n            into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small\n            values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give\n            slightly faster convergence at the expense of adversarial performance. Must be greater than zero.\n    Raises:\n        ValueError: If `code_length`, `max_prob`, or `power` are invalid.\n    \"\"\"\nheads: Union[List[layers.Dense], layers.Dense]\ndef __init__(self, n_classes: int, code_length: Optional[int] = None, max_prob: float = 0.95,\npower: float = 1.0) -&gt; None:\nsuper().__init__()\nself.n_classes = n_classes\nif code_length is None:\ncode_length = max(16, 1 &lt;&lt; (n_classes - 1).bit_length())\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nif power &lt;= 0:\nraise ValueError(f\"power must be positive, but got {power}.\")\nself.power = power\nif not 0.0 &lt; max_prob &lt; 1.0:\nraise ValueError(f\"max_prob must be in the range (0, 1), but got {max_prob}\")\nself.eps = self.code_length * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / self.power)\nself.labels = None\nself.heads = []\nself._call_fn = None\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'n_classes': self.n_classes, 'code_length': self.code_length}\ndef build(self, input_shape: Union[Tuple[int, int], List[Tuple[int, int]]]) -&gt; None:\nsingle_input = not isinstance(input_shape, list)\ninput_shape = to_list(input_shape)\nbatch_size = input_shape[0][0]\nif len(input_shape) &gt; self.code_length - 1:\nraise ValueError(f\"Too many input heads {len(input_shape)} for the given code length {self.code_length}.\")\nhead_sizes = [self.code_length // len(input_shape) for _ in range(len(input_shape))]\nhead_sizes[0] = head_sizes[0] + self.code_length - sum(head_sizes)\nhead_sizes[0] = head_sizes[0] - 1  # We're going to cut off the 0th column from the code\nfor idx, shape in enumerate(input_shape):\nif len(shape) != 2:\nraise ValueError(\"ErrorCorrectingCode layer requires input like (batch, m) or [(batch, m), ...]\")\nif shape[0] != batch_size:\nraise ValueError(\"Inputs to ErrorCorrectingCode layer must have the same batch size\")\nself.heads.append(layers.Dense(units=head_sizes[idx]))\nlabels = hadamard(self.code_length)\n# Cut off 0th column b/c it's constant. It would also be possible to make the column sign alternate, but that\n# would break the symmetry between rows in the code.\nlabels = labels[:self.n_classes, 1:]\nself.labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n# Spare extra operations when they're not needed\nif single_input:\nself.heads = self.heads[0]\nself._call_fn = self._single_head_call\nelse:\nself._call_fn = self._multi_head_call\ndef _single_head_call(self, x: tf.Tensor) -&gt; tf.Tensor:\nx = self.heads(x)\nx = tf.tanh(x)\n# Compute L1 distance\nx = tf.maximum(tf.reduce_sum(tf.abs(tf.expand_dims(x, axis=1) - self.labels), axis=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / tf.pow(x, self.power)\nx = tf.math.divide(x, tf.reshape(tf.reduce_sum(x, axis=-1), (-1, 1)))\nreturn x\ndef _multi_head_call(self, x: List[tf.Tensor]) -&gt; tf.Tensor:\nx = [head(tensor) for head, tensor in zip(self.heads, x)]\nx = tf.concat(x, axis=-1)\nx = tf.tanh(x)\n# Compute L1 distance\nx = tf.maximum(tf.reduce_sum(tf.abs(tf.expand_dims(x, axis=1) - self.labels), axis=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / tf.pow(x, self.power)\nx = tf.math.divide(x, tf.reshape(tf.reduce_sum(x, axis=-1), (-1, 1)))\nreturn x\ndef call(self, x: Union[tf.Tensor, List[tf.Tensor]], **kwargs) -&gt; tf.Tensor:\nreturn self._call_fn(x)\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/instance_norm.html", "title": "instance_norm", "text": ""}, {"location": "fastestimator/layers/tensorflow/instance_norm.html#fastestimator.fastestimator.layers.tensorflow.instance_norm.InstanceNormalization", "title": "<code>InstanceNormalization</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing instance normalization.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.</p> <pre><code>n = tfp.distributions.Normal(loc=10, scale=2)\nx = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\nm = fe.layers.tensorflow.InstanceNormalization()\ny = m(x)  # mean ~= 0, stddev ~= 0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>A numerical stability constant added to the variance.</p> <code>1e-05</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\instance_norm.py</code> <pre><code>class InstanceNormalization(layers.Layer):\n\"\"\"A layer for performing instance normalization.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See\n    https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from\n    https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.\n    ```python\n    n = tfp.distributions.Normal(loc=10, scale=2)\n    x = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\n    m = fe.layers.tensorflow.InstanceNormalization()\n    y = m(x)  # mean ~= 0, stddev ~= 0\n    ```\n    Args:\n        epsilon: A numerical stability constant added to the variance.\n    \"\"\"\ndef __init__(self, epsilon: float = 1e-5) -&gt; None:\nsuper().__init__()\nself.epsilon = epsilon\nself.scale = None\nself.offset = None\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'epsilon': self.epsilon}\ndef build(self, input_shape: Tuple[int, int, int, int]) -&gt; None:\nself.scale = self.add_weight(name='scale',\nshape=input_shape[-1:],\ninitializer=tf.random_normal_initializer(0., 0.02),\ntrainable=True)\nself.offset = self.add_weight(name='offset', shape=input_shape[-1:], initializer='zeros', trainable=True)\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nmean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\ninv = tf.math.rsqrt(variance + self.epsilon)\nnormalized = (x - mean) * inv\nreturn self.scale * normalized + self.offset\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html", "title": "reflection_padding_2d", "text": ""}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D", "title": "<code>ReflectionPadding2D</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing Reflection Padding on 2D arrays.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.</p> <pre><code>x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\ny = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\ny = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>Tuple[int, int]</code> <p>padding size (Width, Height). The padding size must be less than the size of the corresponding dimension in the input tensor.</p> <code>(1, 1)</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>class ReflectionPadding2D(layers.Layer):\n\"\"\"A layer for performing Reflection Padding on 2D arrays.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels).\n    The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.\n    ```python\n    x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\n    y = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\n    y = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n    ```\n    Args:\n        padding: padding size (Width, Height). The padding size must be less than the size of the corresponding\n            dimension in the input tensor.\n    \"\"\"\ndef __init__(self, padding: Tuple[int, int] = (1, 1)) -&gt; None:\nsuper().__init__()\nself.padding = tuple(padding)\nself.input_spec = [layers.InputSpec(ndim=4)]\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'padding': self.padding}\ndef compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nw_pad, h_pad = self.padding\nreturn tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D.compute_output_shape", "title": "<code>compute_output_shape</code>", "text": "<p>If you are using \"channels_last\" configuration</p> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>def compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n</code></pre>"}, {"location": "fastestimator/op/op.html", "title": "op", "text": ""}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.Op", "title": "<code>Op</code>", "text": "<p>A base class for FastEstimator Operators.</p> <p>Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three main variables: <code>inputs</code>, <code>outputs</code>, and <code>mode</code>. When FastEstimator executes, it holds all of its available data behind the scenes in a data dictionary. If an <code>Op</code> wants to interact with a piece of data from this dictionary, it lists the data's key as one of it's <code>inputs</code>. That data will then be passed to the <code>Op</code> when the <code>Op</code>s forward function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an <code>Op</code> wants to write data into the data dictionary, it can return values from its forward function. These values are then written into the data dictionary under the keys specified by the <code>Op</code>s <code>outputs</code>. An <code>Op</code> will only be run if its associated <code>mode</code> matches the current execution mode. For example, if an <code>Op</code> has a mode of 'eval' but FastEstimator is currently running in the 'train' mode, then the <code>Op</code>s forward function will not be called.</p> <p>Normally, if a single string \"key\" is passed as <code>inputs</code> then the value that is passed to the forward function will be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as <code>inputs</code> then the value passed to the forward function will be the element stored in the data dictionary, but wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an <code>Op</code> is anticipated to take one or more inputs and treat them all in the same way. In such cases the <code>in_list</code> member variable may be manually overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of whether <code>inputs</code> was a single string or a list of strings. For an example of when this is useful, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Similarly, by default, if an <code>Op</code> has a single <code>output</code> string \"key\" then that output R will be written into the data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as <code>outputs</code> then the return value for R from the <code>Op</code> is expected to be a list [X], where the inner value will be written to the data dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an <code>Op</code> wants to always return data in a list format without worrying about whether it had one input or more than one input. In such cases the <code>out_list</code> member variable may be manually overridden to True. This will cause the system to always assume that the response is in list format and unwrap the values before storing them into the data dictionary. For an example, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>@traceable()\nclass Op:\n\"\"\"A base class for FastEstimator Operators.\n    Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three\n    main variables: `inputs`, `outputs`, and `mode`. When FastEstimator executes, it holds all of its available data\n    behind the scenes in a data dictionary. If an `Op` wants to interact with a piece of data from this dictionary, it\n    lists the data's key as one of it's `inputs`. That data will then be passed to the `Op` when the `Op`s forward\n    function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an `Op` wants to\n    write data into the data dictionary, it can return values from its forward function. These values are then written\n    into the data dictionary under the keys specified by the `Op`s `outputs`. An `Op` will only be run if its associated\n    `mode` matches the current execution mode. For example, if an `Op` has a mode of 'eval' but FastEstimator is\n    currently running in the 'train' mode, then the `Op`s forward function will not be called.\n    Normally, if a single string \"key\" is passed as `inputs` then the value that is passed to the forward function will\n    be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as\n    `inputs` then the value passed to the forward function will be the element stored in the data dictionary, but\n    wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an `Op` is anticipated to take\n    one or more inputs and treat them all in the same way. In such cases the `in_list` member variable may be manually\n    overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of\n    whether `inputs` was a single string or a list of strings. For an example of when this is useful, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Similarly, by default, if an `Op` has a single `output` string \"key\" then that output R will be written into the\n    data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as `outputs` then the\n    return value for R from the `Op` is expected to be a list [X], where the inner value will be written to the data\n    dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an `Op` wants to always return data in a\n    list format without worrying about whether it had one input or more than one input. In such cases the `out_list`\n    member variable may be manually overridden to True. This will cause the system to always assume that the response is\n    in list format and unwrap the values before storing them into the data dictionary. For an example, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Args:\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ninputs: List[str]\noutputs: List[str]\nmode: Set[str]\nin_list: bool  # Whether inputs should be presented as a list or an individual value\nout_list: bool  # Whether outputs will be returned as a list or an individual value\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = to_list(inputs)\nself.outputs = to_list(outputs)\nself.mode = parse_modes(to_set(mode))\nself.in_list = not isinstance(inputs, str)\nself.out_list = not isinstance(outputs, str)\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.get_inputs_by_op", "title": "<code>get_inputs_by_op</code>", "text": "<p>Retrieve the necessary input data from the data dictionary in order to run an <code>op</code>.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The op to run.</p> required <code>store</code> <code>Mapping[str, Any]</code> <p>The system's data dictionary to draw inputs out of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Input data to be fed to the <code>op</code> forward function.</p> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def get_inputs_by_op(op: Op, store: Mapping[str, Any]) -&gt; Any:\n\"\"\"Retrieve the necessary input data from the data dictionary in order to run an `op`.\n    Args:\n        op: The op to run.\n        store: The system's data dictionary to draw inputs out of.\n    Returns:\n        Input data to be fed to the `op` forward function.\n    \"\"\"\nif op.in_list:\ndata = []\nelse:\ndata = None\nif op.inputs:\ndata = [store[key] for key in op.inputs]\nif not op.in_list:\ndata = data[0]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.write_outputs_by_op", "title": "<code>write_outputs_by_op</code>", "text": "<p>Write <code>outputs</code> from an <code>op</code> forward function into the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The Op which generated <code>outputs</code>.</p> required <code>store</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary into which to write the <code>outputs</code>.</p> required <code>outputs</code> <code>Any</code> <p>The value(s) generated by the <code>op</code>s forward function.</p> required Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def write_outputs_by_op(op: Op, store: MutableMapping[str, Any], outputs: Any) -&gt; None:\n\"\"\"Write `outputs` from an `op` forward function into the data dictionary.\n    Args:\n        op: The Op which generated `outputs`.\n        store: The data dictionary into which to write the `outputs`.\n        outputs: The value(s) generated by the `op`s forward function.\n    \"\"\"\nif not op.out_list:\noutputs = [outputs]\nfor key, data in zip(op.outputs, outputs):\nstore[key] = data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html", "title": "numpyop", "text": ""}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.Delete", "title": "<code>Delete</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Delete key(s) and their associated values from the data dictionary.</p> <p>The system has special logic to detect instances of this Op and delete its <code>inputs</code> from the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, List[str]]</code> <p>Existing key(s) to be deleted from the data dictionary.</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass Delete(NumpyOp):\n\"\"\"Delete key(s) and their associated values from the data dictionary.\n    The system has special logic to detect instances of this Op and delete its `inputs` from the data dictionary.\n    Args:\n        keys: Existing key(s) to be deleted from the data dictionary.\n    \"\"\"\ndef __init__(self, keys: Union[str, List[str]], mode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=keys, mode=mode)\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]], state: Dict[str, Any]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.LambdaOp", "title": "<code>LambdaOp</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Operator that performs any specified function as forward function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to be executed.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass LambdaOp(NumpyOp):\n\"\"\"An Operator that performs any specified function as forward function.\n    Args:\n        fn: The function to be executed.\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfn: Callable,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.fn = fn\nself.in_list = True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\nreturn self.fn(*data)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp", "title": "<code>NumpyOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns numpy data.</p> <p>These Operators are used in fe.Pipeline to perform data pre-processing / augmentation.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass NumpyOp(Op):\n\"\"\"An Operator class which takes and returns numpy data.\n    These Operators are used in fe.Pipeline to perform data pre-processing / augmentation.\n    \"\"\"\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n        Args:\n            data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on individual elements of data before any batching / axis expansion is performed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The arrays from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n    Args:\n        data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.forward_numpyop", "title": "<code>forward_numpyop</code>", "text": "<p>Call the forward function for list of NumpyOps, and modify the data dictionary in place.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>List[NumpyOp]</code> <p>A list of NumpyOps to execute.</p> required <code>data</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary.</p> required <code>mode</code> <code>str</code> <p>The current execution mode (\"train\", \"eval\", \"test\", or \"infer\").</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward_numpyop(ops: List[NumpyOp], data: MutableMapping[str, Any], mode: str) -&gt; None:\n\"\"\"Call the forward function for list of NumpyOps, and modify the data dictionary in place.\n    Args:\n        ops: A list of NumpyOps to execute.\n        data: The data dictionary.\n        mode: The current execution mode (\"train\", \"eval\", \"test\", or \"infer\").\n    \"\"\"\nfor op in ops:\nop_data = get_inputs_by_op(op, data)\nop_data = op.forward(op_data, {\"mode\": mode})\nif isinstance(op, Delete):\nfor key in op.inputs:\ndel data[key]\nif op.outputs:\nwrite_outputs_by_op(op, data, op_data)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/fuse.html", "title": "fuse", "text": ""}, {"location": "fastestimator/op/numpyop/meta/fuse.html#fastestimator.fastestimator.op.numpyop.meta.fuse.Fuse", "title": "<code>Fuse</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Run a sequence of NumpyOps as a single Op.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Union[NumpyOp, List[NumpyOp]]</code> <p>A sequence of NumpyOps to run. They must all share the same mode. It also doesn't support scheduled ops at the moment, though the Fuse itself may be scheduled.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code> or <code>ops</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\fuse.py</code> <pre><code>@traceable()\nclass Fuse(NumpyOp):\n\"\"\"Run a sequence of NumpyOps as a single Op.\n    Args:\n        ops: A sequence of NumpyOps to run. They must all share the same mode. It also doesn't support scheduled ops at\n            the moment, though the Fuse itself may be scheduled.\n    Raises:\n        ValueError: If `repeat` or `ops` are invalid.\n    \"\"\"\ndef __init__(self, ops: Union[NumpyOp, List[NumpyOp]]) -&gt; None:\nops = to_list(ops)\nif len(ops) &lt; 1:\nraise ValueError(\"Fuse requires at least one op\")\ninputs = []\noutputs = []\nmode = ops[0].mode\nfor op in ops:\nif op.mode != mode:\nraise ValueError(f\"All Fuse ops must share the same mode, but got {mode} and {op.mode}\")\nfor inp in op.inputs:\nif inp not in inputs and inp not in outputs:\ninputs.append(inp)\nfor out in op.outputs:\nif out not in outputs:\noutputs.append(out)\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.ops = ops\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nforward_numpyop(self.ops, data, state[\"mode\"])\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html", "title": "one_of", "text": ""}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf", "title": "<code>OneOf</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform one of several possible NumpyOps.</p> <p>Parameters:</p> Name Type Description Default <code>*numpy_ops</code> <code>NumpyOp</code> <p>A list of ops to choose between with uniform probability.</p> <code>()</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>@traceable()\nclass OneOf(NumpyOp):\n\"\"\"Perform one of several possible NumpyOps.\n    Args:\n        *numpy_ops: A list of ops to choose between with uniform probability.\n    \"\"\"\ndef __init__(self, *numpy_ops: NumpyOp) -&gt; None:\ninputs = numpy_ops[0].inputs\noutputs = numpy_ops[0].outputs\nmode = numpy_ops[0].mode\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list = numpy_ops[0].in_list\nself.out_list = numpy_ops[0].out_list\nfor op in numpy_ops[1:]:\nassert inputs == op.inputs, \"All ops within a OneOf must share the same inputs\"\nassert self.in_list == op.in_list, \"All ops within OneOf must share the same input configuration\"\nassert outputs == op.outputs, \"All ops within a OneOf must share the same outputs\"\nassert self.out_list == op.out_list, \"All ops within OneOf must share the same output configuration\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nself.ops = numpy_ops\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n        Args:\n            data: The information to be passed to one of the wrapped operators.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after application of one of the available numpyOps.\n        \"\"\"\nreturn random.choice(self.ops).forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf.forward", "title": "<code>forward</code>", "text": "<p>Execute a randomly selected op from the list of <code>numpy_ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The information to be passed to one of the wrapped operators.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after application of one of the available numpyOps.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n    Args:\n        data: The information to be passed to one of the wrapped operators.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after application of one of the available numpyOps.\n    \"\"\"\nreturn random.choice(self.ops).forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/repeat.html", "title": "repeat", "text": ""}, {"location": "fastestimator/op/numpyop/meta/repeat.html#fastestimator.fastestimator.op.numpyop.meta.repeat.Repeat", "title": "<code>Repeat</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Repeat a NumpyOp several times in a row.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>NumpyOp</code> <p>A NumpyOp to be run one or more times in a row.</p> required <code>repeat</code> <code>Union[int, Callable[..., bool]]</code> <p>How many times to repeat the <code>op</code>. This can also be a function return, in which case the function input names will be matched to keys in the data dictionary, and the <code>op</code> will be repeated until the function evaluates to False. The function evaluation will happen at the end of a forward call, so the <code>op</code> will always be evaluated at least once.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code> or <code>op</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\repeat.py</code> <pre><code>@traceable()\nclass Repeat(NumpyOp):\n\"\"\"Repeat a NumpyOp several times in a row.\n    Args:\n        op: A NumpyOp to be run one or more times in a row.\n        repeat: How many times to repeat the `op`. This can also be a function return, in which case the function input\n            names will be matched to keys in the data dictionary, and the `op` will be repeated until the function\n            evaluates to False. The function evaluation will happen at the end of a forward call, so the `op` will\n            always be evaluated at least once.\n    Raises:\n        ValueError: If `repeat` or `op` are invalid.\n    \"\"\"\ndef __init__(self, op: NumpyOp, repeat: Union[int, Callable[..., bool]] = 1) -&gt; None:\nself.repeat_inputs = []\nextra_reqs = []\nif isinstance(repeat, int):\nif repeat &lt; 1:\nraise ValueError(f\"Repeat requires repeat to be &gt;= 1, but got {repeat}\")\nelse:\nself.repeat_inputs.extend(inspect.signature(repeat).parameters.keys())\nextra_reqs = list(set(self.repeat_inputs) - set(op.outputs))\nself.repeat = repeat\nsuper().__init__(inputs=op.inputs + extra_reqs, outputs=op.outputs, mode=op.mode)\nself.ops = [op]\n@property\ndef op(self) -&gt; NumpyOp:\nreturn self.ops[0]\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nif isinstance(self.repeat, int):\nfor i in range(self.repeat):\nforward_numpyop(self.ops, data, state[\"mode\"])\nelse:\nforward_numpyop(self.ops, data, state[\"mode\"])\nwhile self.repeat(*[data[var_name] for var_name in self.repeat_inputs]):\nforward_numpyop(self.ops, data, state[\"mode\"])\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html", "title": "sometimes", "text": ""}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes", "title": "<code>Sometimes</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform a NumpyOp with a given probability.</p> <p>Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking the Sometimes.</p> <p>Parameters:</p> Name Type Description Default <code>numpy_op</code> <code>NumpyOp</code> <p>The operator to be performed.</p> required <code>prob</code> <code>float</code> <p>The probability of execution, which should be in the range: [0-1).</p> <code>0.5</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>@traceable()\nclass Sometimes(NumpyOp):\n\"\"\"Perform a NumpyOp with a given probability.\n    Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data\n    dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but\n    Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before\n    invoking the Sometimes.\n    Args:\n        numpy_op: The operator to be performed.\n        prob: The probability of execution, which should be in the range: [0-1).\n    \"\"\"\ndef __init__(self, numpy_op: NumpyOp, prob: float = 0.5) -&gt; None:\n# We're going to try to collect any missing output keys from the data dictionary so that they don't get\n# overridden when Sometimes chooses not to execute.\ninps = set(numpy_op.inputs)\nouts = set(numpy_op.outputs)\nself.extra_inputs = list(outs - inps)  # Used by traceability\nself.inp_idx = len(numpy_op.inputs)\nsuper().__init__(inputs=numpy_op.inputs + self.extra_inputs, outputs=numpy_op.outputs, mode=numpy_op.mode)\n# Note that in_list and out_list will always be true\nself.op = numpy_op\nself.prob = prob\ndef __getstate__(self) -&gt; Dict[str, Dict[Any, Any]]:\nreturn {'op': self.op.__getstate__() if hasattr(self.op, '__getstate__') else {}}\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n        Args:\n            data: The information to be passed to the wrapped operator.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The original `data`, or the `data` after running it through the wrapped operator.\n        \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes.forward", "title": "<code>forward</code>", "text": "<p>Execute the wrapped operator a certain fraction of the time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[np.ndarray]</code> <p>The information to be passed to the wrapped operator.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>List[np.ndarray]</code> <p>The original <code>data</code>, or the <code>data</code> after running it through the wrapped operator.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n    Args:\n        data: The information to be passed to the wrapped operator.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The original `data`, or the `data` after running it through the wrapped operator.\n    \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/affine.html", "title": "affine", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/affine.html#fastestimator.fastestimator.op.numpyop.multivariate.affine.Affine", "title": "<code>Affine</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Perform affine transformations on an image.</p> <p>Parameters:</p> Name Type Description Default <code>rotate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to rotate an image (in degrees). If a single value is given then images will be rotated by     a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated     by a value sampled from the range [a, b].</p> <code>0</code> <code>scale</code> <code>Union[float, Tuple[float, float]]</code> <p>How much to scale an image (in percentage). If a single value is given then all images will be scaled     by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled     based on a value drawn from the range [a,b].</p> <code>1.0</code> <code>shear</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to shear an image (in degrees). If a single value is given then all images will be sheared     on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will     be sheared on X and Y by two values randomly sampled from the range [a, b].</p> <code>0</code> <code>translate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to translate an image. If a single value is given then the translation extent will be     sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from     the range [a,b]. If integers are given then the translation will be in pixels. If a float then     it will be as a fraction of the image size.</p> <code>0</code> <code>border_handling</code> <code>Union[str, List[str]]</code> <p>What to do in order to fill newly created pixels. Options are 'constant', 'edge',     'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly     selected from the options in the list.</p> <code>'reflect'</code> <code>fill_value</code> <code>Number</code> <p>What pixel value to insert when border_handling is 'constant'.</p> <code>0</code> <code>interpolation</code> <code>str</code> <p>What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',     'bilinear', 'bicubic', 'biquartic', and 'biquintic'.</p> <code>'bilinear'</code> <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\affine.py</code> <pre><code>@traceable()\nclass Affine(MultiVariateAlbumentation):\n\"\"\"Perform affine transformations on an image.\n    Args:\n        rotate: How much to rotate an image (in degrees). If a single value is given then images will be rotated by\n                a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated\n                by a value sampled from the range [a, b].\n        scale: How much to scale an image (in percentage). If a single value is given then all images will be scaled\n                by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled\n                based on a value drawn from the range [a,b].\n        shear: How much to shear an image (in degrees). If a single value is given then all images will be sheared\n                on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will\n                be sheared on X and Y by two values randomly sampled from the range [a, b].\n        translate: How much to translate an image. If a single value is given then the translation extent will be\n                sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from\n                the range [a,b]. If integers are given then the translation will be in pixels. If a float then\n                it will be as a fraction of the image size.\n        border_handling: What to do in order to fill newly created pixels. Options are 'constant', 'edge',\n                'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly\n                selected from the options in the list.\n        fill_value: What pixel value to insert when border_handling is 'constant'.\n        interpolation: What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',\n                'bilinear', 'bicubic', 'biquartic', and 'biquintic'.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nrotate: Union[Number, Tuple[Number, Number]] = 0,\nscale: Union[float, Tuple[float, float]] = 1.0,\nshear: Union[Number, Tuple[Number, Number]] = 0,\ntranslate: Union[Number, Tuple[Number, Number]] = 0,\nborder_handling: Union[str, List[str]] = \"reflect\",\nfill_value: Number = 0,\ninterpolation: str = \"bilinear\",\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\norder = {'nearest_neighbor': 0, 'bilinear': 1, 'bicubic': 3, 'biquartic': 4, 'biquintic': 5}[interpolation]\nif isinstance(translate, int) or (isinstance(translate, Tuple) and isinstance(translate[0], int)):\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_px=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nelse:\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_percent=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nsuper().__init__(func,\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html", "title": "center_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.center_crop.CenterCrop", "title": "<code>CenterCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop the center of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32 (but uint8 is more efficient)</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\center_crop.py</code> <pre><code>@traceable()\nclass CenterCrop(MultiVariateAlbumentation):\n\"\"\"Crop the center of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32 (but uint8 is more efficient)\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CenterCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop.html", "title": "crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop.html#fastestimator.fastestimator.op.numpyop.multivariate.crop.Crop", "title": "<code>Crop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a region from the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>x_min</code> <code>int</code> <p>Minimum upper left x coordinate.</p> <code>0</code> <code>y_min</code> <code>int</code> <p>Minimum upper left y coordinate.</p> <code>0</code> <code>x_max</code> <code>int</code> <p>Maximum lower right x coordinate.</p> <code>1024</code> <code>y_max</code> <code>int</code> <p>Maximum lower right y coordinate.</p> <code>1024</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop.py</code> <pre><code>@traceable()\nclass Crop(MultiVariateAlbumentation):\n\"\"\"Crop a region from the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        x_min: Minimum upper left x coordinate.\n        y_min: Minimum upper left y coordinate.\n        x_max: Maximum lower right x coordinate.\n        y_max: Maximum lower right y coordinate.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nx_min: int = 0,\ny_min: int = 0,\nx_max: int = 1024,\ny_max: int = 1024,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CropAlb(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html", "title": "crop_non_empty_mask_if_exists", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html#fastestimator.fastestimator.op.numpyop.multivariate.crop_non_empty_mask_if_exists.CropNonEmptyMaskIfExists", "title": "<code>CropNonEmptyMaskIfExists</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop an area with mask if mask is non-empty, otherwise crop randomly.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Vertical size of crop in pixels.</p> required <code>width</code> <code>int</code> <p>Horizontal size of crop in pixels.</p> required <code>ignore_values</code> <code>Optional[List[int]]</code> <p>Values to ignore in mask, <code>0</code> values are always ignored (e.g. if background value is 5 set  <code>ignore_values=[5]</code> to ignore).</p> <code>None</code> <code>ignore_channels</code> <code>Optional[List[int]]</code> <p>Channels to ignore in mask (e.g. if background is a first channel set <code>ignore_channels=[0]</code> to ignore).</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop_non_empty_mask_if_exists.py</code> <pre><code>@traceable()\nclass CropNonEmptyMaskIfExists(MultiVariateAlbumentation):\n\"\"\"Crop an area with mask if mask is non-empty, otherwise crop randomly.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Vertical size of crop in pixels.\n        width: Horizontal size of crop in pixels.\n        ignore_values: Values to ignore in mask, `0` values are always ignored (e.g. if background value is 5 set \n            `ignore_values=[5]` to ignore).\n        ignore_channels: Channels to ignore in mask (e.g. if background is a first channel set `ignore_channels=[0]` to\n            ignore).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nignore_values: Optional[List[int]] = None,\nignore_channels: Optional[List[int]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nCropNonEmptyMaskIfExistsAlb(height=height,\nwidth=width,\nignore_values=ignore_values,\nignore_channels=ignore_channels,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html", "title": "elastic_transform", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html#fastestimator.fastestimator.op.numpyop.multivariate.elastic_transform.ElasticTransform", "title": "<code>ElasticTransform</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Elastic deformation of images.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Scaling factor during point translation.</p> <code>34.0</code> <code>sigma</code> <code>float</code> <p>Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.</p> <code>4.0</code> <code>alpha_affine</code> <code>float</code> <p>The range will be (-alpha_affine, alpha_affine).</p> <code>50.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large (512x512) images.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\elastic_transform.py</code> <pre><code>@traceable()\nclass ElasticTransform(MultiVariateAlbumentation):\n\"\"\"Elastic deformation of images.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        alpha: Scaling factor during point translation.\n        sigma: Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.\n        alpha_affine: The range will be (-alpha_affine, alpha_affine).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        approximate: Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X\n            speedup on large (512x512) images.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nalpha: float = 34.0,\nsigma: float = 4.0,\nalpha_affine: float = 50.0,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\napproximate: bool = False,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nElasticTransformAlb(alpha=alpha,\nsigma=sigma,\nalpha_affine=alpha_affine,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\napproximate=approximate,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/flip.html", "title": "flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/flip.html#fastestimator.fastestimator.op.numpyop.multivariate.flip.Flip", "title": "<code>Flip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image either horizontally, vertically, or both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\flip.py</code> <pre><code>@traceable()\nclass Flip(MultiVariateAlbumentation):\n\"\"\"Flip an image either horizontally, vertically, or both.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html", "title": "grid_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.grid_distortion.GridDistortion", "title": "<code>GridDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Distort an image within a grid sub-division</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>count of grid cells on each side.</p> <code>5</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.3</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\grid_distortion.py</code> <pre><code>@traceable()\nclass GridDistortion(MultiVariateAlbumentation):\n\"\"\"Distort an image within a grid sub-division\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        num_steps: count of grid cells on each side.\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nnum_steps: int = 5,\ndistort_limit: Union[float, Tuple[float, float]] = 0.3,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nGridDistortionAlb(num_steps=num_steps,\ndistort_limit=distort_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html", "title": "horizontal_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.horizontal_flip.HorizontalFlip", "title": "<code>HorizontalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image horizontally.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\horizontal_flip.py</code> <pre><code>@traceable()\nclass HorizontalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image horizontally.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html", "title": "longest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.longest_max_size.LongestMaxSize", "title": "<code>LongestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\longest_max_size.py</code> <pre><code>@traceable()\nclass LongestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(LongestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html", "title": "mask_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html#fastestimator.fastestimator.op.numpyop.multivariate.mask_dropout.MaskDropout", "title": "<code>MaskDropout</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Zero out objects from an image + mask pair.</p> <p>An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask. The mask must be single-channel image, with zero values treated as background. The image can be any number of channels.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).     image_out: The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>max_objects</code> <code>Union[int, Tuple[int, int]]</code> <p>Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]</p> <code>1</code> <code>image_fill_value</code> <code>Union[int, float, str]</code> <p>Fill value to use when filling image. Can be 'inpaint' to apply in-painting (works only  for 3-channel images)</p> <code>0</code> <code>mask_fill_value</code> <code>Union[int, float]</code> <p>Fill value to use when filling mask.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\mask_dropout.py</code> <pre><code>@traceable()\nclass MaskDropout(MultiVariateAlbumentation):\n\"\"\"Zero out objects from an image + mask pair.\n    An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance\n    from mask. The mask must be single-channel image, with zero values treated as background. The image can be any\n    number of channels.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n                image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        max_objects: Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]\n        image_fill_value: Fill value to use when filling image.\n            Can be 'inpaint' to apply in-painting (works only  for 3-channel images)\n        mask_fill_value: Fill value to use when filling mask.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_objects: Union[int, Tuple[int, int]] = 1,\nimage_fill_value: Union[int, float, str] = 0,\nmask_fill_value: Union[int, float] = 0,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nMaskDropoutAlb(max_objects=max_objects,\nimage_fill_value=image_fill_value,\nmask_fill_value=mask_fill_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html", "title": "multivariate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html#fastestimator.fastestimator.op.numpyop.multivariate.multivariate.MultiVariateAlbumentation", "title": "<code>MultiVariateAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A base class for the DualTransform albumentation functions.</p> <p>DualTransforms are functions which apply simultaneously to images and corresponding information such as masks  and/or bounding boxes.</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>DualTransform</code> <p>An Albumentation function to be invoked.</p> required <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If none of the various inputs such as <code>image_in</code> or <code>mask_in</code> are provided.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\multivariate.py</code> <pre><code>@traceable()\nclass MultiVariateAlbumentation(NumpyOp):\n\"\"\"A base class for the DualTransform albumentation functions.\n     DualTransforms are functions which apply simultaneously to images and corresponding information such as masks\n     and/or bounding boxes.\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Raises:\n        AssertionError: If none of the various inputs such as `image_in` or `mask_in` are provided.\n    \"\"\"\ndef __init__(self,\nfunc: DualTransform,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None,\nextra_in_keys: Optional[Dict[str, str]] = None,\nextra_out_keys: Optional[Dict[str, str]] = None):\nassert any((image_in, mask_in, masks_in, bbox_in, keypoints_in)), \"At least one input must be non-None\"\nimage_out = image_out or image_in\nmask_out = mask_out or mask_in\nmasks_out = masks_out or masks_in\nbbox_out = bbox_out or bbox_in\nkeypoints_out = keypoints_out or keypoints_in\nkeys = OrderedDict([(\"image\", image_in), (\"mask\", mask_in), (\"masks\", masks_in), (\"bboxes\", bbox_in),\n(\"keypoints\", keypoints_in)])\nif extra_in_keys:\nkeys.update(extra_in_keys)\nself.keys_in = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nkeys = OrderedDict([(\"image\", image_out), (\"mask\", mask_out), (\"masks\", masks_out), (\"bboxes\", bbox_out),\n(\"keypoints\", keypoints_out)])\nif extra_out_keys:\nkeys.update(extra_out_keys)\nself.keys_out = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nsuper().__init__(inputs=list(self.keys_in.values()), outputs=list(self.keys_out.values()), mode=mode)\nif isinstance(bbox_params, str):\nbbox_params = BboxParams(bbox_params)\nif isinstance(keypoint_params, str):\nkeypoint_params = KeypointParams(keypoint_params)\nself.func = Compose(transforms=[func], bbox_params=bbox_params, keypoint_params=keypoint_params)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresult = self.func(**{k: v for k, v in zip(self.keys_in.keys(), data)})\nreturn [result[k] for k in self.keys_out.keys()]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html", "title": "optical_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.optical_distortion.OpticalDistortion", "title": "<code>OpticalDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Apply optical distortion to an image / mask.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.05</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit). </p> <code>0.05</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\optical_distortion.py</code> <pre><code>@traceable()\nclass OpticalDistortion(MultiVariateAlbumentation):\n\"\"\"Apply optical distortion to an image / mask.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        shift_limit: If shift_limit is a single float, the range will be (-shift_limit, shift_limit). \n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ndistort_limit: Union[float, Tuple[float, float]] = 0.05,\nshift_limit: Union[float, Tuple[float, float]] = 0.05,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nOpticalDistortionAlb(distort_limit=distort_limit,\nshift_limit=shift_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html", "title": "pad_if_needed", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html#fastestimator.fastestimator.op.numpyop.multivariate.pad_if_needed.PadIfNeeded", "title": "<code>PadIfNeeded</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Pad the sides of an image / mask if size is less than a desired number.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_height</code> <code>int</code> <p>Minimal result image height.</p> <code>1024</code> <code>min_width</code> <code>int</code> <p>Minimal result image width.</p> <code>1024</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>cv2.BORDER_REFLECT_101</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value for mask if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\pad_if_needed.py</code> <pre><code>@traceable()\nclass PadIfNeeded(MultiVariateAlbumentation):\n\"\"\"Pad the sides of an image / mask if size is less than a desired number.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_height: Minimal result image height.\n        min_width: Minimal result image width.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_height: int = 1024,\nmin_width: int = 1024,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nPadIfNeededAlb(min_height=min_height,\nmin_width=min_width,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html", "title": "random_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop.RandomCrop", "title": "<code>RandomCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop.py</code> <pre><code>@traceable()\nclass RandomCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html", "title": "random_crop_near_bbox", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop_near_bbox.RandomCropNearBBox", "title": "<code>RandomCropNearBBox</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop bbox from an image with random shift by x,y coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>cropping_bbox_in</code> <code>str</code> <p>The key of the cropping box, in [x1, y1, x2, y2] format.</p> required <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_part_shift</code> <code>float</code> <p>Float value in the range (0.0, 1.0).</p> <code>0.3</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop_near_bbox.py</code> <pre><code>@traceable()\nclass RandomCropNearBBox(MultiVariateAlbumentation):\n\"\"\"Crop bbox from an image with random shift by x,y coordinates.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        cropping_bbox_in: The key of the cropping box, in [x1, y1, x2, y2] format.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_part_shift: Float value in the range (0.0, 1.0).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ncropping_bbox_in: str,\nmax_part_shift: float = 0.3,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropNearBBoxAlb(max_part_shift=max_part_shift, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nextra_in_keys={\"cropping_bbox\": cropping_bbox_in})\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html", "title": "random_grid_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html#fastestimator.fastestimator.op.numpyop.multivariate.random_grid_shuffle.RandomGridShuffle", "title": "<code>RandomGridShuffle</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Divide an image into a grid and randomly shuffle the grid's cells.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>grid</code> <code>Tuple[int, int]</code> <p>size of grid for splitting image (height, width).</p> <code>(3, 3)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_grid_shuffle.py</code> <pre><code>@traceable()\nclass RandomGridShuffle(MultiVariateAlbumentation):\n\"\"\"Divide an image into a grid and randomly shuffle the grid's cells.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        grid: size of grid for splitting image (height, width).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ngrid: Tuple[int, int] = (3, 3),\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(RandomGridShuffleAlb(grid=grid, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html", "title": "random_resized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_resized_crop.RandomResizedCrop", "title": "<code>RandomResizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>scale</code> <code>Tuple[float, float]</code> <p>Range of size of the origin size cropped.</p> <code>(0.08, 1.0)</code> <code>ratio</code> <code>Tuple[float, float]</code> <p>Range of aspect ratio of the origin aspect ratio cropped.</p> <code>(0.75, 4 / 3)</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_resized_crop.py</code> <pre><code>@traceable()\nclass RandomResizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        scale: Range of size of the origin size cropped.\n        ratio: Range of aspect ratio of the origin aspect ratio cropped.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nscale: Tuple[float, float] = (0.08, 1.0),\nratio: Tuple[float, float] = (0.75, 4 / 3),\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomResizedCropAlb(height=height,\nwidth=width,\nscale=scale,\nratio=ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html", "title": "random_rotate_90", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html#fastestimator.fastestimator.op.numpyop.multivariate.random_rotate_90.RandomRotate90", "title": "<code>RandomRotate90</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate a given input randomly by 90 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_rotate_90.py</code> <pre><code>@traceable()\nclass RandomRotate90(MultiVariateAlbumentation):\n\"\"\"Rotate a given input randomly by 90 degrees.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RotateAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html", "title": "random_scale", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html#fastestimator.fastestimator.op.numpyop.multivariate.random_scale.RandomScale", "title": "<code>RandomScale</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit).</p> <code>0.1</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_scale.py</code> <pre><code>@traceable()\nclass RandomScale(MultiVariateAlbumentation):\n\"\"\"Randomly resize the input. Output image size is different from the input image size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (1 - scale_limit, 1 + scale_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomScaleAlb(scale_limit=scale_limit, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html", "title": "random_sized_bbox_safe_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_bbox_safe_crop.RandomSizedBBoxSafeCrop", "title": "<code>RandomSizedBBoxSafeCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size without loss of bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>erosion_rate</code> <code>float</code> <p>Erosion rate applied on input image height before crop.</p> <code>0.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_bbox_safe_crop.py</code> <pre><code>@traceable()\nclass RandomSizedBBoxSafeCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size without loss of bboxes.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        erosion_rate: Erosion rate applied on input image height before crop.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nerosion_rate: float = 0.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None):\nsuper().__init__(\nRandomSizedBBoxSafeCropAlb(height=height,\nwidth=width,\nerosion_rate=erosion_rate,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=None,\nbbox_params=bbox_params,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html", "title": "random_sized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_crop.RandomSizedCrop", "title": "<code>RandomSizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_max_height</code> <code>Tuple[int, int]</code> <p>Crop size limits.</p> required <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>w2h_ratio</code> <code>float</code> <p>Aspect ratio of crop.</p> <code>1.0</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_crop.py</code> <pre><code>@traceable()\nclass RandomSizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_max_height: Crop size limits.\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        w2h_ratio: Aspect ratio of crop.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_max_height: Tuple[int, int],\nheight: int,\nwidth: int,\nw2h_ratio: float = 1.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomSizedCropAlb(min_max_height=min_max_height,\nheight=height,\nwidth=width,\nw2h_ratio=w2h_ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html", "title": "read_mat", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html#fastestimator.fastestimator.op.numpyop.multivariate.read_mat.ReadMat", "title": "<code>ReadMat</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading .mat files from disk.</p> <p>This expects every sample to have a separate .mat file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Dictionary key that contains the .mat path.</p> required <code>keys</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) to read from the .mat file.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given filepath.</p> <code>''</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\read_mat.py</code> <pre><code>@traceable()\nclass ReadMat(NumpyOp):\n\"\"\"A class for reading .mat files from disk.\n    This expects every sample to have a separate .mat file.\n    Args:\n        file: Dictionary key that contains the .mat path.\n        keys: Key(s) to read from the .mat file.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        parent_path: Parent path that will be prepended to a given filepath.\n    \"\"\"\ndef __init__(self,\nfile: str,\nkeys: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\"):\nsuper().__init__(inputs=file, outputs=keys, mode=mode)\nself.parent_path = parent_path\nself.out_list = True\ndef forward(self, data: str, state: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\ndata = loadmat(os.path.normpath(os.path.join(self.parent_path, data)))\nresults = [data[key] for key in self.outputs]\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/resize.html", "title": "resize", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/resize.html#fastestimator.fastestimator.op.numpyop.multivariate.resize.Resize", "title": "<code>Resize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Desired height of the output.</p> required <code>width</code> <code>int</code> <p>Desired width of the output.</p> required <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\resize.py</code> <pre><code>@traceable()\nclass Resize(MultiVariateAlbumentation):\n\"\"\"Resize the input to the given height and width.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Desired height of the output.\n        width: Desired width of the output.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(ResizeAlb(height=height, width=width, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html", "title": "rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.rotate.Rotate", "title": "<code>Rotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit).</p> <code>90</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\rotate.py</code> <pre><code>@traceable()\nclass Rotate(MultiVariateAlbumentation):\n\"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        limit: Range from which a random angle is picked. If limit is a single int an angle is picked from\n            (-limit, limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nlimit: Union[int, Tuple[int, int]] = 90,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRotateAlb(limit=limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html", "title": "shift_scale_rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.shift_scale_rotate.ShiftScaleRotate", "title": "<code>ShiftScaleRotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].</p> <code>0.0625</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit).</p> <code>0.1</code> <code>rotate_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit).</p> <code>45</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\shift_scale_rotate.py</code> <pre><code>@traceable()\nclass ShiftScaleRotate(MultiVariateAlbumentation):\n\"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        shift_limit: Shift factor range for both height and width. If shift_limit is a single float value, the range\n            will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (-scale_limit, scale_limit).\n        rotate_limit: Rotation range. If rotate_limit is a single int value, the range will be\n            (-rotate_limit, rotate_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nshift_limit: Union[float, Tuple[float, float]] = 0.0625,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\nrotate_limit: Union[int, Tuple[int, int]] = 45,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nShiftScaleRotateAlb(shift_limit=shift_limit,\nscale_limit=scale_limit,\nrotate_limit=rotate_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html", "title": "smallest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.smallest_max_size.SmallestMaxSize", "title": "<code>SmallestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of smallest side of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\smallest_max_size.py</code> <pre><code>@traceable()\nclass SmallestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of smallest side of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(SmallestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html", "title": "transpose", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html#fastestimator.fastestimator.op.numpyop.multivariate.transpose.Transpose", "title": "<code>Transpose</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\transpose.py</code> <pre><code>@traceable()\nclass Transpose(MultiVariateAlbumentation):\n\"\"\"Transpose the input by swapping rows and columns.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(TransposeAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html", "title": "vertical_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.vertical_flip.VerticalFlip", "title": "<code>VerticalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image vertically (over x-axis).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\vertical_flip.py</code> <pre><code>@traceable()\nclass VerticalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image vertically (over x-axis).\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(VerticalFlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/binarize.html", "title": "binarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/binarize.html#fastestimator.fastestimator.op.numpyop.univariate.binarize.Binarize", "title": "<code>Binarize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Binarization threshold.</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\binarize.py</code> <pre><code>@traceable()\nclass Binarize(NumpyOp):\n\"\"\"Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.\n    Args:\n        threshold: Binarization threshold.\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nthreshold: float,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.threshold = threshold\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [(dat &gt;= self.threshold).astype(np.float32) for dat in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/blur.html", "title": "blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/blur.html#fastestimator.fastestimator.op.numpyop.univariate.blur.Blur", "title": "<code>Blur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a randomly-sized kernel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\blur.py</code> <pre><code>@traceable()\nclass Blur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a randomly-sized kernel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(BlurAlb(blur_limit=blur_limit, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html", "title": "channel_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.channel_dropout.ChannelDropout", "title": "<code>ChannelDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly drop channels from the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>channel_drop_range</code> <code>Tuple[int, int]</code> <p>Range from which we choose the number of channels to drop.</p> <code>(1, 1)</code> <code>fill_value</code> <code>Union[int, float]</code> <p>Pixel values for the dropped channel.</p> <code>0</code> Image types <p>int8, uint16, unit32, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_dropout.py</code> <pre><code>@traceable()\nclass ChannelDropout(ImageOnlyAlbumentation):\n\"\"\"Randomly drop channels from the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        channel_drop_range: Range from which we choose the number of channels to drop.\n        fill_value: Pixel values for the dropped channel.\n    Image types:\n        int8, uint16, unit32, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nchannel_drop_range: Tuple[int, int] = (1, 1),\nfill_value: Union[int, float] = 0):\nsuper().__init__(\nChannelDropoutAlb(channel_drop_range=channel_drop_range, fill_value=fill_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html", "title": "channel_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html#fastestimator.fastestimator.op.numpyop.univariate.channel_shuffle.ChannelShuffle", "title": "<code>ChannelShuffle</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly rearrange channels of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>int8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_shuffle.py</code> <pre><code>@traceable()\nclass ChannelShuffle(ImageOnlyAlbumentation):\n\"\"\"Randomly rearrange channels of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        int8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ChannelShuffleAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html", "title": "channel_transpose", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html#fastestimator.fastestimator.op.numpyop.univariate.channel_transpose.ChannelTranspose", "title": "<code>ChannelTranspose</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transpose the data (for example to make it channel-width-height instead of width-height-channel).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>axes</code> <code>List[int]</code> <p>The permutation order.</p> <code>(2, 0, 1)</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_transpose.py</code> <pre><code>@traceable()\nclass ChannelTranspose(NumpyOp):\n\"\"\"Transpose the data (for example to make it channel-width-height instead of width-height-channel).\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        axes: The permutation order.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\naxes: List[int] = (2, 0, 1)):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axes = axes\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.transpose(elem, self.axes) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/clahe.html", "title": "clahe", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/clahe.html#fastestimator.fastestimator.op.numpyop.univariate.clahe.CLAHE", "title": "<code>CLAHE</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply contrast limited adaptive histogram equalization to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>clip_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit).</p> <code>4.0</code> <code>tile_grid_size</code> <code>Tuple[int, int]</code> <p>size of grid for histogram equalization.</p> <code>(8, 8)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\clahe.py</code> <pre><code>@traceable()\nclass CLAHE(ImageOnlyAlbumentation):\n\"\"\"Apply contrast limited adaptive histogram equalization to the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        clip_limit: upper threshold value for contrast limiting. If clip_limit is a single float value, the range will\n            be (1, clip_limit).\n        tile_grid_size: size of grid for histogram equalization.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nclip_limit: Union[float, Tuple[float, float]] = 4.0,\ntile_grid_size: Tuple[int, int] = (8, 8)):\nsuper().__init__(CLAHEAlb(clip_limit=clip_limit, tile_grid_size=tile_grid_size, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html", "title": "coarse_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.coarse_dropout.CoarseDropout", "title": "<code>CoarseDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Drop rectangular regions from an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_holes</code> <code>int</code> <p>Maximum number of regions to zero out.</p> <code>8</code> <code>max_height</code> <code>int</code> <p>Maximum height of the hole.</p> <code>8</code> <code>max_width</code> <code>int</code> <p>Maximum width of the hole.</p> <code>8</code> <code>min_holes</code> <code>Optional[int]</code> <p>Minimum number of regions to zero out. If <code>None</code>, <code>min_holes</code> is set to <code>max_holes</code>.</p> <code>None</code> <code>min_height</code> <code>Optional[int]</code> <p>Minimum height of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_height</code>.</p> <code>None</code> <code>min_width</code> <code>Optional[int]</code> <p>Minimum width of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_width</code>.</p> <code>None</code> <code>fill_value</code> <code>Union[int, float, List[int], List[float]]</code> <p>value for dropped pixels.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\coarse_dropout.py</code> <pre><code>@traceable()\nclass CoarseDropout(ImageOnlyAlbumentation):\n\"\"\"Drop rectangular regions from an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_holes: Maximum number of regions to zero out.\n        max_height: Maximum height of the hole.\n        max_width: Maximum width of the hole.\n        min_holes: Minimum number of regions to zero out. If `None`, `min_holes` is set to `max_holes`.\n        min_height: Minimum height of the hole. If `None`, `min_height` is set to `max_height`.\n        min_width: Minimum width of the hole. If `None`, `min_height` is set to `max_width`.\n        fill_value: value for dropped pixels.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_holes: int = 8,\nmax_height: int = 8,\nmax_width: int = 8,\nmin_holes: Optional[int] = None,\nmin_height: Optional[int] = None,\nmin_width: Optional[int] = None,\nfill_value: Union[int, float, List[int], List[float]] = 0):\nsuper().__init__(\nCoarseDropoutAlb(max_holes=max_holes,\nmax_height=max_height,\nmax_width=max_width,\nmin_holes=min_holes,\nmin_height=min_height,\nmin_width=min_width,\nfill_value=fill_value,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/downscale.html", "title": "downscale", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/downscale.html#fastestimator.fastestimator.op.numpyop.univariate.downscale.Downscale", "title": "<code>Downscale</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease image quality by downscaling and then upscaling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>scale_min</code> <code>float</code> <p>Lower bound on the image scale. Should be &lt; 1.</p> <code>0.25</code> <code>scale_max</code> <code>float</code> <p>Upper bound on the image scale. Should be &gt;= scale_min.</p> <code>0.25</code> <code>interpolation</code> <code>int</code> <p>cv2 interpolation method.</p> <code>cv2.INTER_NEAREST</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\downscale.py</code> <pre><code>@traceable()\nclass Downscale(ImageOnlyAlbumentation):\n\"\"\"Decrease image quality by downscaling and then upscaling.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        scale_min: Lower bound on the image scale. Should be &lt; 1.\n        scale_max:  Upper bound on the image scale. Should be &gt;= scale_min.\n        interpolation: cv2 interpolation method.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nscale_min: float = 0.25,\nscale_max: float = 0.25,\ninterpolation: int = cv2.INTER_NEAREST):\nsuper().__init__(\nDownscaleAlb(scale_min=scale_min, scale_max=scale_max, interpolation=interpolation, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/equalize.html", "title": "equalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/equalize.html#fastestimator.fastestimator.op.numpyop.univariate.equalize.Equalize", "title": "<code>Equalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Equalize the image histogram.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>eq_mode</code> <code>str</code> <p>{'cv', 'pil'}. Use OpenCV or Pillow equalization method.</p> <code>'cv'</code> <code>by_channels</code> <code>bool</code> <p>If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by <code>Y</code> channel.</p> <code>True</code> <code>mask</code> <code>Union[None, np.ndarray]</code> <p>If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel array. Function signature must include <code>image</code> argument.</p> <code>None</code> <code>mask_params</code> <code>List[str]</code> <p>Params for mask function.</p> <code>()</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\equalize.py</code> <pre><code>@traceable()\nclass Equalize(ImageOnlyAlbumentation):\n\"\"\"Equalize the image histogram.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        eq_mode: {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels: If True, use equalization by channels separately, else convert image to YCbCr representation and\n            use equalization by `Y` channel.\n        mask: If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel\n            array. Function signature must include `image` argument.\n        mask_params: Params for mask function.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\neq_mode: str = \"cv\",\nby_channels: bool = True,\nmask: Union[None, np.ndarray] = None,\nmask_params: List[str] = ()):\nsuper().__init__(\nEqualizeAlb(mode=eq_mode, by_channels=by_channels, mask=mask, mask_params=mask_params, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html", "title": "expand_dims", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html#fastestimator.fastestimator.op.numpyop.univariate.expand_dims.ExpandDims", "title": "<code>ExpandDims</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transpose the data (for example to make it channel-width-height instead of width-height-channel)</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of inputs to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified inputs.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis to expand.</p> <code>-1</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\expand_dims.py</code> <pre><code>@traceable()\nclass ExpandDims(NumpyOp):\n\"\"\"Transpose the data (for example to make it channel-width-height instead of width-height-channel)\n    Args:\n        inputs: Key(s) of inputs to be modified.\n        outputs: Key(s) into which to write the modified inputs.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        axis: The axis to expand.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\naxis: int = -1):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.expand_dims(elem, self.axis) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/from_float.html", "title": "from_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/from_float.html#fastestimator.fastestimator.op.numpyop.univariate.from_float.FromFloat", "title": "<code>FromFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Takes an input float image in range [0, 1.0] and then multiplies by <code>max_value</code> to get an int image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the multiplier. If None it will be inferred by dtype.</p> <code>None</code> <code>dtype</code> <code>Union[str, np.dtype]</code> <p>The data type to cast the output as.</p> <code>'uint16'</code> Image types <p>float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\from_float.py</code> <pre><code>@traceable()\nclass FromFloat(ImageOnlyAlbumentation):\n\"\"\"Takes an input float image in range [0, 1.0] and then multiplies by `max_value` to get an int image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_value: The maximum value to serve as the multiplier. If None it will be inferred by dtype.\n        dtype: The data type to cast the output as.\n    Image types:\n        float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None,\ndtype: Union[str, np.dtype] = \"uint16\"):\nsuper().__init__(FromFloatAlb(max_value=max_value, dtype=dtype, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html", "title": "gaussian_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_blur.GaussianBlur", "title": "<code>GaussianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a Gaussian filter of random kernel size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_blur.py</code> <pre><code>@traceable()\nclass GaussianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a Gaussian filter of random kernel size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(GaussianBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html", "title": "gaussian_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_noise.GaussianNoise", "title": "<code>GaussianNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply gaussian noise to the image.</p> <p>WARNING: This assumes that floating point images are in the range [0,1] and will trim the output to that range. If your image is in a range like [-0.5, 0.5] then you do not want to use this Op.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>var_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).</p> <code>(10.0, 50.0)</code> <code>mean</code> <code>float</code> <p>Mean of the noise.</p> <code>0.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_noise.py</code> <pre><code>@traceable()\nclass GaussianNoise(ImageOnlyAlbumentation):\n\"\"\"Apply gaussian noise to the image.\n    WARNING: This assumes that floating point images are in the range [0,1] and will trim the output to that range. If\n    your image is in a range like [-0.5, 0.5] then you do not want to use this Op.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        var_limit: Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).\n        mean: Mean of the noise.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nvar_limit: Union[float, Tuple[float, float]] = (10.0, 50.0),\nmean: float = 0.0):\nsuper().__init__(GaussNoiseAlb(var_limit=var_limit, mean=mean, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/hadamard.html#fastestimator.fastestimator.op.numpyop.univariate.hadamard.Hadamard", "title": "<code>Hadamard</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert integer labels into hadamard code representations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor(s) to be converted.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor(s) in hadamard code representation.</p> required <code>n_classes</code> <code>int</code> <p>How many classes are there in the inputs.</p> required <code>code_length</code> <code>Optional[int]</code> <p>What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unequal number of <code>inputs</code> and <code>outputs</code> are provided, or if <code>code_length</code> is invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\hadamard.py</code> <pre><code>@traceable()\nclass Hadamard(NumpyOp):\n\"\"\"Convert integer labels into hadamard code representations.\n    Args:\n        inputs: Key of the input tensor(s) to be converted.\n        outputs: Key of the output tensor(s) in hadamard code representation.\n        n_classes: How many classes are there in the inputs.\n        code_length: What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Raises:\n        ValueError: If an unequal number of `inputs` and `outputs` are provided, or if `code_length` is invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nif len(self.inputs) != len(self.outputs):\nraise ValueError(\"Hadamard requires the same number of input and output keys.\")\nself.n_classes = n_classes\nif code_length is None:\ncode_length = 1 &lt;&lt; (n_classes - 1).bit_length()\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nlabels = hadamard(self.code_length).astype(np.float32)\nlabels[np.arange(0, self.code_length, 2), 0] = -1  # Make first column alternate\nlabels = labels[:self.n_classes]\nself.labels = labels\ndef forward(self, data: List[Union[int, np.ndarray]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n# TODO - also support one hot with smoothed labels?\nreturn [gather(tensor=self.labels, indices=np.array(inp)) for inp in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html", "title": "hue_saturation_value", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html#fastestimator.fastestimator.op.numpyop.univariate.hue_saturation_value.HueSaturationValue", "title": "<code>HueSaturationValue</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly modify the hue, saturation, and value of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>hue_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit).</p> <code>20</code> <code>sat_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit).</p> <code>30</code> <code>val_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\hue_saturation_value.py</code> <pre><code>@traceable()\nclass HueSaturationValue(ImageOnlyAlbumentation):\n\"\"\"Randomly modify the hue, saturation, and value of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        hue_shift_limit: Range for changing hue. If hue_shift_limit is a single int, the range will be\n            (-hue_shift_limit, hue_shift_limit).\n        sat_shift_limit: Range for changing saturation. If sat_shift_limit is a single int, the range will be\n            (-sat_shift_limit, sat_shift_limit).\n        val_shift_limit: range for changing value. If val_shift_limit is a single int, the range will be\n            (-val_shift_limit, val_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nhue_shift_limit: Union[int, Tuple[int, int]] = 20,\nsat_shift_limit: Union[int, Tuple[int, int]] = 30,\nval_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nHueSaturationValueAlb(hue_shift_limit=hue_shift_limit,\nsat_shift_limit=sat_shift_limit,\nval_shift_limit=val_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html", "title": "image_compression", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html#fastestimator.fastestimator.op.numpyop.univariate.image_compression.ImageCompression", "title": "<code>ImageCompression</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease compression of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>quality_lower</code> <code>float</code> <p>Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>99</code> <code>quality_upper</code> <code>float</code> <p>Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>100</code> <code>compression_type</code> <code>ImgCmpAlb.ImageCompressionType</code> <p>should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.</p> <code>ImgCmpAlb.ImageCompressionType.JPEG</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\image_compression.py</code> <pre><code>@traceable()\nclass ImageCompression(ImageOnlyAlbumentation):\n\"\"\"Decrease compression of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        quality_lower: Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        quality_upper: Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        compression_type: should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nquality_lower: float = 99,\nquality_upper: float = 100,\ncompression_type: ImgCmpAlb.ImageCompressionType = ImgCmpAlb.ImageCompressionType.JPEG):\nsuper().__init__(\nImgCmpAlb(quality_lower=quality_lower,\nquality_upper=quality_upper,\ncompression_type=compression_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html", "title": "invert_img", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html#fastestimator.fastestimator.op.numpyop.univariate.invert_img.InvertImg", "title": "<code>InvertImg</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert an image by subtracting its pixel values from 255.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>int8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\invert_img.py</code> <pre><code>@traceable()\nclass InvertImg(ImageOnlyAlbumentation):\n\"\"\"Invert an image by subtracting its pixel values from 255.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        int8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(InvertImgAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html", "title": "iso_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html#fastestimator.fastestimator.op.numpyop.univariate.iso_noise.ISONoise", "title": "<code>ISONoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply camera sensor noise.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>color_shift</code> <code>Tuple[float, float]</code> <p>Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS colorspace.</p> <code>(0.01, 0.05)</code> <code>intensity</code> <code>Tuple[float, float]</code> <p>Multiplicative factor that controls the strength of color and luminace noise.</p> <code>(0.1, 0.5)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\iso_noise.py</code> <pre><code>@traceable()\nclass ISONoise(ImageOnlyAlbumentation):\n\"\"\"Apply camera sensor noise.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        color_shift: Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS\n            colorspace.\n        intensity: Multiplicative factor that controls the strength of color and luminace noise.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ncolor_shift: Tuple[float, float] = (0.01, 0.05),\nintensity: Tuple[float, float] = (0.1, 0.5)):\nsuper().__init__(ISONoiseAlb(color_shift=color_shift, intensity=intensity, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html", "title": "median_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html#fastestimator.fastestimator.op.numpyop.univariate.median_blur.MedianBlur", "title": "<code>MedianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with median filter of random aperture size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf). If image is a float type then only 3 and 5 are valid sizes.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\median_blur.py</code> <pre><code>@traceable()\nclass MedianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with median filter of random aperture size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf).\n            If image is a float type then only 3 and 5 are valid sizes.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 5):\nsuper().__init__(MedianBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/minmax.html", "title": "minmax", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/minmax.html#fastestimator.fastestimator.op.numpyop.univariate.minmax.Minmax", "title": "<code>Minmax</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Normalize data using the minmax method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>A small value to prevent numeric instability in the division.</p> <code>1e-07</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\minmax.py</code> <pre><code>@traceable()\nclass Minmax(NumpyOp):\n\"\"\"Normalize data using the minmax method.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        epsilon: A small value to prevent numeric instability in the division.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nepsilon: float = 1e-7):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.epsilon = epsilon\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_minmax(elem) for elem in data]\ndef _apply_minmax(self, data: np.ndarray) -&gt; np.ndarray:\ndata_max = np.max(data)\ndata_min = np.min(data)\ndata = (data - data_min) / max((data_max - data_min), self.epsilon)\nreturn data.astype(np.float32)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html", "title": "motion_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html#fastestimator.fastestimator.op.numpyop.univariate.motion_blur.MotionBlur", "title": "<code>MotionBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Motion Blur the image with a randomly-sized kernel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in the range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\motion_blur.py</code> <pre><code>@traceable()\nclass MotionBlur(ImageOnlyAlbumentation):\n\"\"\"Motion Blur the image with a randomly-sized kernel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in the range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(MotionBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html", "title": "multiplicative_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html#fastestimator.fastestimator.op.numpyop.univariate.multiplicative_noise.MultiplicativeNoise", "title": "<code>MultiplicativeNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Multiply an image with random perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>multiplier</code> <code>Union[float, Tuple[float, float]]</code> <p>If a single float, the image will be multiplied by this number. If tuple of floats then <code>multiplier</code> will be in the range [multiplier[0], multiplier[1]).</p> <code>(0.9, 1.1)</code> <code>per_channel</code> <code>bool</code> <p>Whether to sample different multipliers for each channel of the image.</p> <code>False</code> <code>elementwise</code> <code>bool</code> <p>If <code>False</code> multiply multiply all pixels in an image with a random value sampled once. If <code>True</code> Multiply image pixels with values that are pixelwise randomly sampled.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\multiplicative_noise.py</code> <pre><code>@traceable()\nclass MultiplicativeNoise(ImageOnlyAlbumentation):\n\"\"\"Multiply an image with random perturbations.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        multiplier: If a single float, the image will be multiplied by this number. If tuple of floats then `multiplier`\n            will be in the range [multiplier[0], multiplier[1]).\n        per_channel: Whether to sample different multipliers for each channel of the image.\n        elementwise: If `False` multiply multiply all pixels in an image with a random value sampled once.\n            If `True` Multiply image pixels with values that are pixelwise randomly sampled.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmultiplier: Union[float, Tuple[float, float]] = (0.9, 1.1),\nper_channel: bool = False,\nelementwise: bool = False):\nsuper().__init__(\nMultiplicativeNoiseAlb(multiplier=multiplier,\nper_channel=per_channel,\nelementwise=elementwise,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/normalize.html", "title": "normalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/normalize.html#fastestimator.fastestimator.op.numpyop.univariate.normalize.Normalize", "title": "<code>Normalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>mean</code> <code>Union[float, Tuple[float, ...]]</code> <p>Mean values to subtract.</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Union[float, Tuple[float, ...]]</code> <p>The divisor.</p> <code>(0.229, 0.224, 0.225)</code> <code>max_pixel_value</code> <code>float</code> <p>Maximum possible pixel value.</p> <code>255.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\normalize.py</code> <pre><code>@traceable()\nclass Normalize(ImageOnlyAlbumentation):\n\"\"\"Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        mean: Mean values to subtract.\n        std: The divisor.\n        max_pixel_value: Maximum possible pixel value.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmean: Union[float, Tuple[float, ...]] = (0.485, 0.456, 0.406),\nstd: Union[float, Tuple[float, ...]] = (0.229, 0.224, 0.225),\nmax_pixel_value: float = 255.0):\nsuper().__init__(NormalizeAlb(mean=mean, std=std, max_pixel_value=max_pixel_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/onehot.html", "title": "onehot", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/onehot.html#fastestimator.fastestimator.op.numpyop.univariate.onehot.Onehot", "title": "<code>Onehot</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transform an integer label to one-hot-encoding.</p> <p>This can be desirable for increasing robustness against incorrect labels: https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Input key(s) of labels to be onehot encoded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Output key(s) of labels.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes.</p> required <code>label_smoothing</code> <code>float</code> <p>Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing / num_classes, the other class output is: label_smoothing / num_classes.</p> <code>0.0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\onehot.py</code> <pre><code>@traceable()\nclass Onehot(NumpyOp):\n\"\"\"Transform an integer label to one-hot-encoding.\n    This can be desirable for increasing robustness against incorrect labels:\n    https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n    Args:\n        inputs: Input key(s) of labels to be onehot encoded.\n        outputs: Output key(s) of labels.\n        num_classes: Total number of classes.\n        label_smoothing: Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing\n            / num_classes, the other class output is: label_smoothing / num_classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nnum_classes: int,\nlabel_smoothing: float = 0.0,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.num_classes = num_classes\nself.label_smoothing = label_smoothing\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Union[int, np.ndarray]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_onehot(elem) for elem in data]\ndef _apply_onehot(self, data: Union[int, np.ndarray]) -&gt; np.ndarray:\nclass_index = np.array(data)\nassert \"int\" in str(class_index.dtype)\nassert class_index.size == 1, \"data must have only one item\"\nclass_index = class_index.item()\nassert class_index &lt; self.num_classes, \"label value should be smaller than num_classes\"\noutput = np.full((self.num_classes), fill_value=self.label_smoothing / self.num_classes)\noutput[class_index] = 1.0 - self.label_smoothing + self.label_smoothing / self.num_classes\nreturn output\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html", "title": "pad_sequence", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html#fastestimator.fastestimator.op.numpyop.univariate.pad_sequence.PadSequence", "title": "<code>PadSequence</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Pad sequences to the same length with provided value.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be padded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are padded.</p> required <code>max_len</code> <code>int</code> <p>Maximum length of all sequences.</p> required <code>value</code> <code>Union[str, int]</code> <p>Padding value.</p> <code>0</code> <code>append</code> <code>bool</code> <p>Pad before or after the sequences. True for padding the values after the sequence, False otherwise.</p> <code>True</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\pad_sequence.py</code> <pre><code>@traceable()\nclass PadSequence(NumpyOp):\n\"\"\"Pad sequences to the same length with provided value.\n    Args:\n        inputs: Key(s) of sequences to be padded.\n        outputs: Key(s) of sequences that are padded.\n        max_len: Maximum length of all sequences.\n        value: Padding value.\n        append: Pad before or after the sequences. True for padding the values after the sequence, False otherwise.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmax_len: int,\nvalue: Union[str, int] = 0,\nappend: bool = True,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.max_len = max_len\nself.value = value\nself.append = append\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._pad_sequence(elem) for elem in data]\ndef _pad_sequence(self, data: np.ndarray) -&gt; np.ndarray:\n\"\"\"Pad the input sequence to the maximum length. Sequences longer than `max_len` are truncated.\n        Args:\n            data: input sequence in the data.\n        Returns:\n            Padded sequence\n        \"\"\"\nif len(data) &lt; self.max_len:\npad_len = self.max_len - len(data)\npad_arr = np.full(pad_len, self.value)\nif self.append:\ndata = np.append(data, pad_arr)\nelse:\ndata = np.append(pad_arr, data)\nelse:\ndata = data[:self.max_len]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/posterize.html", "title": "posterize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/posterize.html#fastestimator.fastestimator.op.numpyop.univariate.posterize.Posterize", "title": "<code>Posterize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Reduce the number of bits for each color channel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>num_bits</code> <code>Union[int, Tuple[int, int], Tuple[int, int, int], Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]]</code> <p>Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be in the range [0, 8].</p> <code>4</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\posterize.py</code> <pre><code>@traceable()\nclass Posterize(ImageOnlyAlbumentation):\n\"\"\"Reduce the number of bits for each color channel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        num_bits: Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet\n            of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be\n            in the range [0, 8].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nnum_bits: Union[int,\nTuple[int, int],\nTuple[int, int, int],\nTuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]] = 4):\nsuper().__init__(PosterizeAlb(num_bits=num_bits, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html", "title": "random_brightness_contrast", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html#fastestimator.fastestimator.op.numpyop.univariate.random_brightness_contrast.RandomBrightnessContrast", "title": "<code>RandomBrightnessContrast</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly change the brightness and contrast of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>brightness_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing brightness. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>contrast_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing contrast. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>brightness_by_max</code> <code>bool</code> <p>If True adjust contrast by image dtype maximum, else adjust contrast by image mean.</p> <code>True</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_brightness_contrast.py</code> <pre><code>@traceable()\nclass RandomBrightnessContrast(ImageOnlyAlbumentation):\n\"\"\"Randomly change the brightness and contrast of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        brightness_limit: Factor range for changing brightness.\n            If limit is a single float, the range will be (-limit, limit).\n        contrast_limit: Factor range for changing contrast.\n            If limit is a single float, the range will be (-limit, limit).\n        brightness_by_max: If True adjust contrast by image dtype maximum, else adjust contrast by image mean.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nbrightness_limit: Union[float, Tuple[float, float]] = 0.2,\ncontrast_limit: Union[float, Tuple[float, float]] = 0.2,\nbrightness_by_max: bool = True):\nsuper().__init__(\nRandomBrightnessContrastAlb(brightness_limit=brightness_limit,\ncontrast_limit=contrast_limit,\nbrightness_by_max=brightness_by_max,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html", "title": "random_fog", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html#fastestimator.fastestimator.op.numpyop.univariate.random_fog.RandomFog", "title": "<code>RandomFog</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add fog to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>fog_coef_lower</code> <code>float</code> <p>Lower limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>0.3</code> <code>fog_coef_upper</code> <code>float</code> <p>Upper limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>1.0</code> <code>alpha_coef</code> <code>float</code> <p>Transparency of the fog circles. Should be in the range [0, 1].</p> <code>0.08</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_fog.py</code> <pre><code>@traceable()\nclass RandomFog(ImageOnlyAlbumentation):\n\"\"\"Add fog to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        fog_coef_lower: Lower limit for fog intensity coefficient. Should be in the range [0, 1].\n        fog_coef_upper: Upper limit for fog intensity coefficient. Should be in the range [0, 1].\n        alpha_coef: Transparency of the fog circles. Should be in the range [0, 1].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nfog_coef_lower: float = 0.3,\nfog_coef_upper: float = 1.0,\nalpha_coef: float = 0.08):\nsuper().__init__(\nRandomFogAlb(fog_coef_lower=fog_coef_lower,\nfog_coef_upper=fog_coef_upper,\nalpha_coef=alpha_coef,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html", "title": "random_gamma", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html#fastestimator.fastestimator.op.numpyop.univariate.random_gamma.RandomGamma", "title": "<code>RandomGamma</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply a gamma transform to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>gamma_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).</p> <code>(80, 120)</code> <code>eps</code> <code>float</code> <p>A numerical stability constant to avoid division by zero.</p> <code>1e-07</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_gamma.py</code> <pre><code>@traceable()\nclass RandomGamma(ImageOnlyAlbumentation):\n\"\"\"Apply a gamma transform to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        gamma_limit: If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).\n        eps: A numerical stability constant to avoid division by zero.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ngamma_limit: Union[float, Tuple[float, float]] = (80, 120),\neps: float = 1e-7):\nsuper().__init__(RandomGammaAlb(gamma_limit=gamma_limit, eps=eps, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html", "title": "random_rain", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html#fastestimator.fastestimator.op.numpyop.univariate.random_rain.RandomRain", "title": "<code>RandomRain</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add rain to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>slant_lower</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>-10</code> <code>slant_upper</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>10</code> <code>drop_length</code> <code>int</code> <p>Should be in range [0, 100].</p> <code>20</code> <code>drop_width</code> <code>int</code> <p>Should be in range [1, 5].</p> <code>1</code> <code>drop_color</code> <code>Tuple[int, int, int]</code> <p>Rain lines color (r, g, b).</p> <code>(200, 200, 200)</code> <code>blur_value</code> <code>int</code> <p>How blurry to make the rain.</p> <code>7</code> <code>brightness_coefficient</code> <code>float</code> <p>Rainy days are usually shady. Should be in range [0, 1].</p> <code>0.7</code> <code>rain_type</code> <code>Optional[str]</code> <p>One of [None, \"drizzle\", \"heavy\", \"torrential\"].</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_rain.py</code> <pre><code>@traceable()\nclass RandomRain(ImageOnlyAlbumentation):\n\"\"\"Add rain to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        slant_lower: Should be in range [-20, 20].\n        slant_upper: Should be in range [-20, 20].\n        drop_length: Should be in range [0, 100].\n        drop_width: Should be in range [1, 5].\n        drop_color: Rain lines color (r, g, b).\n        blur_value: How blurry to make the rain.\n        brightness_coefficient: Rainy days are usually shady. Should be in range [0, 1].\n        rain_type: One of [None, \"drizzle\", \"heavy\", \"torrential\"].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nslant_lower: int = -10,\nslant_upper: int = 10,\ndrop_length: int = 20,\ndrop_width: int = 1,\ndrop_color: Tuple[int, int, int] = (200, 200, 200),\nblur_value: int = 7,\nbrightness_coefficient: float = 0.7,\nrain_type: Optional[str] = None):\nsuper().__init__(\nRandomRainAlb(slant_lower=slant_lower,\nslant_upper=slant_upper,\ndrop_length=drop_length,\ndrop_width=drop_width,\ndrop_color=drop_color,  # Their docstring type hint doesn't match the real code\nblur_value=blur_value,\nbrightness_coefficient=brightness_coefficient,\nrain_type=rain_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html", "title": "random_shadow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html#fastestimator.fastestimator.op.numpyop.univariate.random_shadow.RandomShadow", "title": "<code>RandomShadow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add shadows to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>shadow_roi</code> <code>Tuple[float, float, float, float]</code> <p>Region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0.0, 0.5, 1.0, 1.0)</code> <code>num_shadows_lower</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [0, <code>num_shadows_upper</code>].</p> <code>1</code> <code>num_shadows_upper</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [<code>num_shadows_lower</code>, inf].</p> <code>2</code> <code>shadow_dimension</code> <code>int</code> <p>Number of edges in the shadow polygons.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_shadow.py</code> <pre><code>@traceable()\nclass RandomShadow(ImageOnlyAlbumentation):\n\"\"\"Add shadows to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        shadow_roi: Region of the image where shadows will appear (x_min, y_min, x_max, y_max).\n            All values should be in range [0, 1].\n        num_shadows_lower: Lower limit for the possible number of shadows. Should be in range [0, `num_shadows_upper`].\n        num_shadows_upper: Lower limit for the possible number of shadows.\n            Should be in range [`num_shadows_lower`, inf].\n        shadow_dimension: Number of edges in the shadow polygons.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nshadow_roi: Tuple[float, float, float, float] = (0.0, 0.5, 1.0, 1.0),\nnum_shadows_lower: int = 1,\nnum_shadows_upper: int = 2,\nshadow_dimension: int = 5):\nprint(\"\\033[93m {}\\033[00m\".format(\n\"Warning! RandomShadow does not work with multi-threaded Pipelines. Either do not use this Op or else \" +\n\"set your Pipeline num_process=0\"))\n# TODO - Have pipeline look for bad ops and auto-magically set num_process correctly\nsuper().__init__(\nRandomShadowAlb(shadow_roi=shadow_roi,\nnum_shadows_lower=num_shadows_lower,\nnum_shadows_upper=num_shadows_upper,\nshadow_dimension=shadow_dimension,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html", "title": "random_snow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html#fastestimator.fastestimator.op.numpyop.univariate.random_snow.RandomSnow", "title": "<code>RandomSnow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Bleach out some pixels to simulate snow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>snow_point_lower</code> <code>float</code> <p>Lower bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.1</code> <code>snow_point_upper</code> <code>float</code> <p>Upper bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.3</code> <code>brightness_coeff</code> <code>float</code> <p>A larger number will lead to a more snow on the image. Should be &gt;= 0.</p> <code>2.5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_snow.py</code> <pre><code>@traceable()\nclass RandomSnow(ImageOnlyAlbumentation):\n\"\"\"Bleach out some pixels to simulate snow.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        snow_point_lower: Lower bound of the amount of snow. Should be in the range [0, 1].\n        snow_point_upper: Upper bound of the amount of snow. Should be in the range [0, 1].\n        brightness_coeff: A larger number will lead to a more snow on the image. Should be &gt;= 0.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nsnow_point_lower: float = 0.1,\nsnow_point_upper: float = 0.3,\nbrightness_coeff: float = 2.5):\nsuper().__init__(\nRandomSnowAlb(snow_point_lower=snow_point_lower,\nsnow_point_upper=snow_point_upper,\nbrightness_coeff=brightness_coeff,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html", "title": "random_sun_flare", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html#fastestimator.fastestimator.op.numpyop.univariate.random_sun_flare.RandomSunFlare", "title": "<code>RandomSunFlare</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add a sun flare to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>flare_roi</code> <code>Tuple[float, float, float, float]</code> <p>region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0, 0, 1, 0.5)</code> <code>angle_lower</code> <code>float</code> <p>should be in range [0, <code>angle_upper</code>].</p> <code>0.0</code> <code>angle_upper</code> <code>float</code> <p>should be in range [<code>angle_lower</code>, 1].</p> <code>1.0</code> <code>num_flare_circles_lower</code> <code>int</code> <p>lower limit for the number of flare circles. Should be in range [0, <code>num_flare_circles_upper</code>].</p> <code>6</code> <code>num_flare_circles_upper</code> <code>int</code> <p>upper limit for the number of flare circles. Should be in range [<code>num_flare_circles_lower</code>, inf].</p> <code>10</code> <code>src_radius</code> <code>int</code> <p>Radius of the flare.</p> <code>400</code> <code>src_color</code> <code>Tuple[int, int, int]</code> <p>Color of the flare (R,G,B).</p> <code>(255, 255, 255)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_sun_flare.py</code> <pre><code>@traceable()\nclass RandomSunFlare(ImageOnlyAlbumentation):\n\"\"\"Add a sun flare to the image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        flare_roi: region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be\n            in range [0, 1].\n        angle_lower: should be in range [0, `angle_upper`].\n        angle_upper: should be in range [`angle_lower`, 1].\n        num_flare_circles_lower: lower limit for the number of flare circles.\n            Should be in range [0, `num_flare_circles_upper`].\n        num_flare_circles_upper: upper limit for the number of flare circles.\n            Should be in range [`num_flare_circles_lower`, inf].\n        src_radius: Radius of the flare.\n        src_color: Color of the flare (R,G,B).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nflare_roi: Tuple[float, float, float, float] = (0, 0, 1, 0.5),\nangle_lower: float = 0.0,\nangle_upper: float = 1.0,\nnum_flare_circles_lower: int = 6,\nnum_flare_circles_upper: int = 10,\nsrc_radius: int = 400,\nsrc_color: Tuple[int, int, int] = (255, 255, 255)):\nsuper().__init__(\nRandomSunFlareAlb(flare_roi=flare_roi,\nangle_lower=angle_lower,\nangle_upper=angle_upper,\nnum_flare_circles_lower=num_flare_circles_lower,\nnum_flare_circles_upper=num_flare_circles_upper,\nsrc_radius=src_radius,\nsrc_color=src_color,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/read_image.html", "title": "read_image", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/read_image.html#fastestimator.fastestimator.op.numpyop.univariate.read_image.ReadImage", "title": "<code>ReadImage</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading png or jpg images from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of paths to images to be loaded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given path.</p> <code>''</code> <code>color_flag</code> <code>Union[str, int]</code> <p>Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.</p> <code>cv2.IMREAD_COLOR</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>inputs</code> and <code>outputs</code> have mismatched lengths, or the <code>color_flag</code> is unacceptable.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\read_image.py</code> <pre><code>@traceable()\nclass ReadImage(NumpyOp):\n\"\"\"A class for reading png or jpg images from disk.\n    Args:\n        inputs: Key(s) of paths to images to be loaded.\n        outputs: Key(s) of images to be output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        parent_path: Parent path that will be prepended to a given path.\n        color_flag: Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.\n    Raises:\n        AssertionError: If `inputs` and `outputs` have mismatched lengths, or the `color_flag` is unacceptable.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\",\ncolor_flag: Union[str, int] = cv2.IMREAD_COLOR):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nif isinstance(self.inputs, List) and isinstance(self.outputs, List):\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.parent_path = parent_path\nassert isinstance(color_flag, int) or color_flag in {'color', 'gray', 'grey'}, \\\n            f\"Unacceptable color_flag value: {color_flag}\"\nself.color_flag = color_flag\nif self.color_flag == \"color\":\nself.color_flag = cv2.IMREAD_COLOR\nelif self.color_flag in {\"gray\", \"grey\"}:\nself.color_flag = cv2.IMREAD_GRAYSCALE\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._read(elem) for elem in data]\ndef _read(self, path: str) -&gt; np.ndarray:\npath = os.path.normpath(os.path.join(self.parent_path, path))\nimg = cv2.imread(path, self.color_flag)\nif self.color_flag in {\ncv2.IMREAD_COLOR, cv2.IMREAD_REDUCED_COLOR_2, cv2.IMREAD_REDUCED_COLOR_4, cv2.IMREAD_REDUCED_COLOR_8\n}:\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nif not isinstance(img, np.ndarray):\nraise ValueError('cv2 did not read correctly for file \"{}\"'.format(path))\nif img.ndim == 2:\nimg = np.expand_dims(img, -1)\nreturn img\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/reshape.html#fastestimator.fastestimator.op.numpyop.univariate.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Op which re-shapes data to a target shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of the data to be reshaped.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\reshape.py</code> <pre><code>@traceable()\nclass Reshape(NumpyOp):\n\"\"\"An Op which re-shapes data to a target shape.\n    Args:\n        shape: The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.\n        inputs: Key(s) of the data to be reshaped.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nshape: Union[int, Tuple[int, ...]],\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.shape = shape\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_reshape(elem) for elem in data]\ndef _apply_reshape(self, data):\ndata = np.reshape(data, self.shape)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html", "title": "rgb_shift", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html#fastestimator.fastestimator.op.numpyop.univariate.rgb_shift.RGBShift", "title": "<code>RGBShift</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly shift the channel values for an input RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the normalized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>r_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit).</p> <code>20</code> <code>g_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the green channel. If g_shift_limit is a single int, the range will be (-g_shift_limit, g_shift_limit).</p> <code>20</code> <code>b_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rgb_shift.py</code> <pre><code>@traceable()\nclass RGBShift(ImageOnlyAlbumentation):\n\"\"\"Randomly shift the channel values for an input RGB image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the normalized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        r_shift_limit: range for changing values for the red channel. If r_shift_limit is a single int, the range\n            will be (-r_shift_limit, r_shift_limit).\n        g_shift_limit: range for changing values for the green channel. If g_shift_limit is a single int, the range\n            will be (-g_shift_limit, g_shift_limit).\n        b_shift_limit: range for changing values for the blue channel. If b_shift_limit is a single int, the range\n            will be (-b_shift_limit, b_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nr_shift_limit: Union[int, Tuple[int, int]] = 20,\ng_shift_limit: Union[int, Tuple[int, int]] = 20,\nb_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nRGBShiftAlb(r_shift_limit=r_shift_limit,\ng_shift_limit=g_shift_limit,\nb_shift_limit=b_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/solarize.html", "title": "solarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/solarize.html#fastestimator.fastestimator.op.numpyop.univariate.solarize.Solarize", "title": "<code>Solarize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be solarized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the solarized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>threshold</code> <code>Union[int, Tuple[int, int], float, Tuple[float, float]]</code> <p>Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].</p> <code>128</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\solarize.py</code> <pre><code>@traceable()\nclass Solarize(ImageOnlyAlbumentation):\n\"\"\"Invert all pixel values above a threshold.\n    Args:\n        inputs: Key(s) of images to be solarized.\n        outputs: Key(s) into which to write the solarized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        threshold: Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nthreshold: Union[int, Tuple[int, int], float, Tuple[float, float]] = 128):\nsuper().__init__(SolarizeAlb(threshold=threshold, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_array.html", "title": "to_array", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_array.html#fastestimator.fastestimator.op.numpyop.univariate.to_array.ToArray", "title": "<code>ToArray</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert data to a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of the data to be converted.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype to apply to the output array, or None to infer the type.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_array.py</code> <pre><code>@traceable()\nclass ToArray(NumpyOp):\n\"\"\"Convert data to a numpy array.\n    Args:\n        inputs: Key(s) of the data to be converted.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        dtype: The dtype to apply to the output array, or None to infer the type.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ndtype: Optional[str] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.dtype = dtype\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Any], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_transform(elem) for elem in data]\ndef _apply_transform(self, data: Any) -&gt; np.ndarray:\nreturn np.array(data, dtype=self.dtype)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_float.html", "title": "to_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_float.html#fastestimator.fastestimator.op.numpyop.univariate.to_float.ToFloat", "title": "<code>ToFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divides an input by max_value to give a float image in range [0,1].</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to floating point representation.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the divisor. If None it will be inferred by dtype.</p> <code>None</code> Image types <p>Any</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_float.py</code> <pre><code>@traceable()\nclass ToFloat(ImageOnlyAlbumentation):\n\"\"\"Divides an input by max_value to give a float image in range [0,1].\n    Args:\n        inputs: Key(s) of images to be converted to floating point representation.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_value: The maximum value to serve as the divisor. If None it will be inferred by dtype.\n    Image types:\n        Any\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None):\nsuper().__init__(ToFloatAlb(max_value=max_value, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html", "title": "to_gray", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html#fastestimator.fastestimator.op.numpyop.univariate.to_gray.ToGray", "title": "<code>ToGray</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to grayscale.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_gray.py</code> <pre><code>@traceable()\nclass ToGray(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.\n    Args:\n        inputs: Key(s) of images to be converted to grayscale.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToGrayAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html", "title": "to_sepia", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html#fastestimator.fastestimator.op.numpyop.univariate.to_sepia.ToSepia", "title": "<code>ToSepia</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to sepia.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to sepia.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the sepia images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_sepia.py</code> <pre><code>@traceable()\nclass ToSepia(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to sepia.\n    Args:\n        inputs: Key(s) of images to be converted to sepia.\n        outputs: Key(s) into which to write the sepia images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToSepiaAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html", "title": "tokenize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html#fastestimator.fastestimator.op.numpyop.univariate.tokenize.Tokenize", "title": "<code>Tokenize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Split the sequences into tokens.</p> <p>Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if defined in the passed function object. By default, tokenize only splits the sequences into tokens.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be tokenized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are tokenized.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>tokenize_fn</code> <code>Union[None, Callable[[str], List[str]]]</code> <p>Tokenization function object.</p> <code>None</code> <code>to_lower_case</code> <code>bool</code> <p>Whether to convert tokens to lowercase.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\tokenize.py</code> <pre><code>@traceable()\nclass Tokenize(NumpyOp):\n\"\"\"Split the sequences into tokens.\n    Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if\n    defined in the passed function object. By default, tokenize only splits the sequences into tokens.\n    Args:\n        inputs: Key(s) of sequences to be tokenized.\n        outputs: Key(s) of sequences that are tokenized.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        tokenize_fn: Tokenization function object.\n        to_lower_case: Whether to convert tokens to lowercase.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ntokenize_fn: Union[None, Callable[[str], List[str]]] = None,\nto_lower_case: bool = False) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.tokenize_fn = tokenize_fn\nself.to_lower_case = to_lower_case\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[List[str]]:\nreturn [self._apply_tokenization(seq) for seq in data]\ndef _apply_tokenization(self, data: str) -&gt; List[str]:\n\"\"\"Split the sequence into tokens and apply lowercase if `do_lower_case` is set.\n        Args:\n            data: Input sequence.\n        Returns:\n            A list of tokens.\n        \"\"\"\nif self.tokenize_fn:\ndata = self.tokenize_fn(data)\nelse:\ndata = data.split()\nif self.to_lower_case:\ndata = list(map(lambda x: x.lower(), data))\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/univariate.html", "title": "univariate", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/univariate.html#fastestimator.fastestimator.op.numpyop.univariate.univariate.ImageOnlyAlbumentation", "title": "<code>ImageOnlyAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Operators which apply to single images (as opposed to images + masks or images + bounding boxes).</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ImageOnlyTransform</code> <p>An Albumentation function to be invoked.</p> required <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the <code>func</code> will be run in replay mode so that the exact same augmentation is applied to each value.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\univariate.py</code> <pre><code>@traceable()\nclass ImageOnlyAlbumentation(NumpyOp):\n\"\"\"Operators which apply to single images (as opposed to images + masks or images + bounding boxes).\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        inputs: Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the\n            `func` will be run in replay mode so that the exact same augmentation is applied to each value.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfunc: ImageOnlyTransform,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.func = Compose(transforms=[func])\nself.replay_func = ReplayCompose(transforms=[deepcopy(func)])\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresults = [self.replay_func(image=data[0]) if len(data) &gt; 1 else self.func(image=data[0])]\nfor i in range(1, len(data)):\nresults.append(self.replay_func.replay(results[0]['replay'], image=data[i]))\nreturn [result[\"image\"] for result in results]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html", "title": "word_to_id", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html#fastestimator.fastestimator.op.numpyop.univariate.word_to_id.WordtoId", "title": "<code>WordtoId</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Converts words to their corresponding id using mapper function or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Union[Dict[str, int], Callable[[List[str]], List[int]]]</code> <p>Mapper function or dictionary</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be converted to ids.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences are converted to ids.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\word_to_id.py</code> <pre><code>@traceable()\nclass WordtoId(NumpyOp):\n\"\"\"Converts words to their corresponding id using mapper function or dictionary.\n    Args:\n        mapping: Mapper function or dictionary\n        inputs: Key(s) of sequences to be converted to ids.\n        outputs: Key(s) of sequences are converted to ids.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(\nself,\nmapping: Union[Dict[str, int], Callable[[List[str]], List[int]]],\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\n) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nassert callable(mapping) or isinstance(mapping, dict), \\\n            \"Incorrect data type provided for `mapping`. Please provide a function or a dictionary.\"\nself.mapping = mapping\ndef forward(self, data: List[List[str]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._convert_to_id(elem) for elem in data]\ndef _convert_to_id(self, data: List[str]) -&gt; np.ndarray:\n\"\"\"Flatten the input list and map the token to ids using mapper function or lookup table.\n        Args:\n            data: Input array of tokens\n        Raises:\n            Exception: If neither of the mapper function or dictionary object is passed\n        Returns:\n            Array of token ids\n        \"\"\"\nif callable(self.mapping):\ndata = self.mapping(data)\nelse:\ndata = [self.mapping.get(token) for token in data]\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/argmax.html", "title": "argmax", "text": ""}, {"location": "fastestimator/op/tensorop/argmax.html#fastestimator.fastestimator.op.tensorop.argmax.Argmax", "title": "<code>Argmax</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Get the argmax from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>axis</code> <code>int</code> <p>The axis along which to collect the argmax.</p> <code>0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\argmax.py</code> <pre><code>@traceable()\nclass Argmax(TensorOp):\n\"\"\"Get the argmax from a tensor.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        outputs: The key(s) under which to save the output.\n        axis: The axis along which to collect the argmax.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\naxis: int = 0,\nmode: Union[None, str, Iterable[str]] = \"eval\"):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nreturn [argmax(tensor=tensor, axis=self.axis) for tensor in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/average.html", "title": "average", "text": ""}, {"location": "fastestimator/op/tensorop/average.html#fastestimator.fastestimator.op.tensorop.average.Average", "title": "<code>Average</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Compute the average across tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Keys of tensors to be averaged.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\average.py</code> <pre><code>@traceable()\nclass Average(TensorOp):\n\"\"\"Compute the average across tensors.\n    Args:\n        inputs: Keys of tensors to be averaged.\n        outputs: The key under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self, inputs: Union[str, Iterable[str]], outputs: str, mode: Union[None, str,\nIterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, False\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\nresult = zeros_like(data[0])\nfor tensor in data:\nresult += tensor\nreturn result / len(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gather.html", "title": "gather", "text": ""}, {"location": "fastestimator/op/tensorop/gather.html#fastestimator.fastestimator.op.tensorop.gather.Gather", "title": "<code>Gather</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Gather values from an input tensor.</p> <p>If indices are not provided, the maximum values along the batch dimension will be collected.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>indices</code> <code>Union[None, str, List[str]]</code> <p>A tensor containing target indices to gather.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gather.py</code> <pre><code>@traceable()\nclass Gather(TensorOp):\n\"\"\"Gather values from an input tensor.\n    If indices are not provided, the maximum values along the batch dimension will be collected.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        indices: A tensor containing target indices to gather.\n        outputs: The key(s) under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nindices: Union[None, str, List[str]] = None,\nmode: Union[None, str, Iterable[str]] = \"eval\"):\nindices = to_list(indices)\nself.num_indices = len(indices)\ncombined_inputs = indices\ncombined_inputs.extend(to_list(inputs))\nsuper().__init__(inputs=combined_inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nindices = data[:self.num_indices]\ninputs = data[self.num_indices:]\nresults = []\nfor idx, tensor in enumerate(inputs):\n# Check len(indices[0]) since an empty indices element is used to trigger the else\nif tf.is_tensor(indices[0]) or isinstance(indices[0], torch.Tensor):\nelem_len = indices[0].shape[0]\nelse:\nelem_len = len(indices[0])\nif len(indices) &gt; idx and elem_len &gt; 0:\nresults.append(gather_from_batch(tensor, indices=indices[idx]))\nelif len(indices) == 1 and elem_len &gt; 0:\n# One set of indices for all outputs\nresults.append(gather_from_batch(tensor, indices=indices[0]))\nelse:\nresults.append(reduce_max(tensor, 1))  # The maximum value within each batch element\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/tensorop/reshape.html#fastestimator.fastestimator.op.tensorop.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Reshape a input tensor to conform to a given shape.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor that is to be reshaped.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor that has been reshaped.</p> required <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>Target shape.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\reshape.py</code> <pre><code>@traceable()\nclass Reshape(TensorOp):\n\"\"\"Reshape a input tensor to conform to a given shape.\n    Args:\n        inputs: Key of the input tensor that is to be reshaped.\n        outputs: Key of the output tensor that has been reshaped.\n        shape: Target shape.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nshape: Union[int, Tuple[int, ...]],\nmode: Union[None, str, Iterable[str]] = \"!infer\") -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.shape = list(shape)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nreturn [reshape(elem, self.shape) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html", "title": "tensorop", "text": ""}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.LambdaOp", "title": "<code>LambdaOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>An Operator that performs any specified function as forward function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to be executed.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>@traceable()\nclass LambdaOp(TensorOp):\n\"\"\"An Operator that performs any specified function as forward function.\n    Args:\n        fn: The function to be executed.\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfn: Callable,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.fn = fn\nself.in_list = True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nreturn self.fn(*data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp", "title": "<code>TensorOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns tensor data.</p> <p>These Operators are used in fe.Network to perform graph-based operations like neural network training.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>@traceable()\nclass TensorOp(Op):\n\"\"\"An Operator class which takes and returns tensor data.\n    These Operators are used in fe.Network to perform graph-based operations like neural network training.\n    \"\"\"\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on batches of data.\n        Args:\n            data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\ndef build(self, framework: str) -&gt; None:\n\"\"\"A method which will be invoked during Network instantiation.\n        This method can be used to augment the natural __init__ method of the TensorOp once the desired backend\n        framework is known.\n        Args:\n            framework: Which framework this Op will be executing in. One of 'tf' or 'torch'.\n        \"\"\"\npass\n# ###########################################################################\n# The methods below this point can be ignored by most non-FE developers\n# ###########################################################################\n# noinspection PyMethodMayBeStatic\ndef get_fe_models(self) -&gt; Set[Model]:\n\"\"\"A method to get any models held by this Op.\n        All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate\n        models, for example by the Network during load_epoch().\n        Returns:\n            Any models held by this Op.\n        \"\"\"\nreturn set()\n# noinspection PyMethodMayBeStatic\ndef get_fe_loss_keys(self) -&gt; Set[str]:\n\"\"\"A method to get any loss keys held by this Op.\n        All users and most developers can safely ignore this method. This method may be invoked to gather information\n        about losses, for example by the Network in get_loss_keys().\n        Returns:\n            Any loss keys held by this Op.\n        \"\"\"\nreturn set()\n# noinspection PyMethodMayBeStatic\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\n\"\"\"A method to get / set whether this Op should retain network gradients after computing them.\n        All users and most developers can safely ignore this method. Ops which do not compute gradients should leave\n        this method alone. If this method is invoked with `retain` as True or False, then the gradient computations\n        performed by this Op should retain or discard the graph respectively afterwards.\n        Args:\n            retain: If None, then return the current retain_graph status of the Op. If True or False, then set the\n                retain_graph status of the op to the new status and return the new status.\n        Returns:\n            Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not\n            compute backward gradients.\n        \"\"\"\nreturn None\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.build", "title": "<code>build</code>", "text": "<p>A method which will be invoked during Network instantiation.</p> <p>This method can be used to augment the natural init method of the TensorOp once the desired backend framework is known.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Which framework this Op will be executing in. One of 'tf' or 'torch'.</p> required Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def build(self, framework: str) -&gt; None:\n\"\"\"A method which will be invoked during Network instantiation.\n    This method can be used to augment the natural __init__ method of the TensorOp once the desired backend\n    framework is known.\n    Args:\n        framework: Which framework this Op will be executing in. One of 'tf' or 'torch'.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.fe_retain_graph", "title": "<code>fe_retain_graph</code>", "text": "<p>A method to get / set whether this Op should retain network gradients after computing them.</p> <p>All users and most developers can safely ignore this method. Ops which do not compute gradients should leave this method alone. If this method is invoked with <code>retain</code> as True or False, then the gradient computations performed by this Op should retain or discard the graph respectively afterwards.</p> <p>Parameters:</p> Name Type Description Default <code>retain</code> <code>Optional[bool]</code> <p>If None, then return the current retain_graph status of the Op. If True or False, then set the retain_graph status of the op to the new status and return the new status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[bool]</code> <p>Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not</p> <code>Optional[bool]</code> <p>compute backward gradients.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\n\"\"\"A method to get / set whether this Op should retain network gradients after computing them.\n    All users and most developers can safely ignore this method. Ops which do not compute gradients should leave\n    this method alone. If this method is invoked with `retain` as True or False, then the gradient computations\n    performed by this Op should retain or discard the graph respectively afterwards.\n    Args:\n        retain: If None, then return the current retain_graph status of the Op. If True or False, then set the\n            retain_graph status of the op to the new status and return the new status.\n    Returns:\n        Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not\n        compute backward gradients.\n    \"\"\"\nreturn None\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[Tensor]]</code> <p>The batch from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[Tensor, List[Tensor]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on batches of data.\n    Args:\n        data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.get_fe_loss_keys", "title": "<code>get_fe_loss_keys</code>", "text": "<p>A method to get any loss keys held by this Op.</p> <p>All users and most developers can safely ignore this method. This method may be invoked to gather information about losses, for example by the Network in get_loss_keys().</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Any loss keys held by this Op.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def get_fe_loss_keys(self) -&gt; Set[str]:\n\"\"\"A method to get any loss keys held by this Op.\n    All users and most developers can safely ignore this method. This method may be invoked to gather information\n    about losses, for example by the Network in get_loss_keys().\n    Returns:\n        Any loss keys held by this Op.\n    \"\"\"\nreturn set()\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.get_fe_models", "title": "<code>get_fe_models</code>", "text": "<p>A method to get any models held by this Op.</p> <p>All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate models, for example by the Network during load_epoch().</p> <p>Returns:</p> Type Description <code>Set[Model]</code> <p>Any models held by this Op.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def get_fe_models(self) -&gt; Set[Model]:\n\"\"\"A method to get any models held by this Op.\n    All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate\n    models, for example by the Network during load_epoch().\n    Returns:\n        Any models held by this Op.\n    \"\"\"\nreturn set()\n</code></pre>"}, {"location": "fastestimator/op/tensorop/un_hadamard.html", "title": "un_hadamard", "text": ""}, {"location": "fastestimator/op/tensorop/un_hadamard.html#fastestimator.fastestimator.op.tensorop.un_hadamard.UnHadamard", "title": "<code>UnHadamard</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Convert hadamard encoded class representations into onehot probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor(s) to be converted.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor(s) as class probabilities.</p> required <code>n_classes</code> <code>int</code> <p>How many classes are there in the inputs.</p> required <code>code_length</code> <code>Optional[int]</code> <p>What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\un_hadamard.py</code> <pre><code>@traceable()\nclass UnHadamard(TensorOp):\n\"\"\"Convert hadamard encoded class representations into onehot probabilities.\n    Args:\n        inputs: Key of the input tensor(s) to be converted.\n        outputs: Key of the output tensor(s) as class probabilities.\n        n_classes: How many classes are there in the inputs.\n        code_length: What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.n_classes = n_classes\nif code_length is None:\ncode_length = 1 &lt;&lt; (n_classes - 1).bit_length()\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}\")\nself.code_length = code_length\nself.labels = None\nself.eps = None\ndef build(self, framework: str) -&gt; None:\nlabels = hadamard(self.code_length).astype(np.float32)\nlabels[np.arange(0, self.code_length, 2), 0] = -1  # Make first column alternate\nlabels = labels[:self.n_classes]\nself.labels = to_tensor(labels, target_type=framework)\nmax_prob = 0.99999  # This will only be approximate since the first column is alternating\npower = 1.0\nself.eps = to_tensor(\nnp.array((self.code_length + 1) * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / power),\ndtype=np.float32),\ntarget_type=framework)\nif framework == \"torch\":\nself.labels = self.labels.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nself.eps = self.eps.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nresults = []\nfor elem in data:\n# L1 Distance\nx = reduce_sum(abs(expand_dims(elem, axis=1) - self.labels), axis=-1)\nx = iwd(x, power=1.0, eps=self.eps)\nresults.append(x)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/augmentation/cutmix_batch.html", "title": "cutmix_batch", "text": ""}, {"location": "fastestimator/op/tensorop/augmentation/cutmix_batch.html#fastestimator.fastestimator.op.tensorop.augmentation.cutmix_batch.CutMixBatch", "title": "<code>CutMixBatch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs cutmix augmentation on a batch of tensors.</p> <p>In this augmentation technique patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. This class should be used in conjunction with MixLoss to perform CutMix training, which helps to reduce over-fitting, perform object detection, and against adversarial attacks (https://arxiv.org/pdf/1905.04899.pdf).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>str</code> <p>Key of the image batch to be cut-mixed.</p> required <code>outputs</code> <code>Iterable[str]</code> <p>Keys under which to store the cut-mixed images and lambda value.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'train'</code> <code>alpha</code> <code>Union[float, Tensor]</code> <p>The alpha value defining the beta distribution to be drawn from during training which controls the combination ratio between image pairs.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided inputs are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\augmentation\\cutmix_batch.py</code> <pre><code>class CutMixBatch(TensorOp):\n\"\"\"This class performs cutmix augmentation on a batch of tensors.\n    In this augmentation technique patches are cut and pasted among training images where the ground truth labels are\n    also mixed proportionally to the area of the patches. This class should be used in conjunction with MixLoss to\n    perform CutMix training, which helps to reduce over-fitting, perform object detection, and against adversarial\n    attacks (https://arxiv.org/pdf/1905.04899.pdf).\n    Args:\n        inputs: Key of the image batch to be cut-mixed.\n        outputs: Keys under which to store the cut-mixed images and lambda value.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        alpha: The alpha value defining the beta distribution to be drawn from during training which controls the\n            combination ratio between image pairs.\n    Raises:\n        AssertionError: If the provided inputs are invalid.\n    \"\"\"\ndef __init__(self,\ninputs: str,\noutputs: Iterable[str],\nmode: Union[None, str, Iterable[str]] = 'train',\nalpha: Union[float, Tensor] = 1.0) -&gt; None:\nassert alpha &gt; 0, \"Alpha value must be greater than zero\"\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert len(self.outputs) == len(self.inputs) + 1, \"CutMixBatch should generate 1 more output than it has inputs\"\nself.alpha = alpha\nself.beta = None\nself.uniform = None\ndef build(self, framework: str) -&gt; None:\nif framework == 'tf':\nself.beta = tfp.distributions.Beta(self.alpha, self.alpha)\nself.uniform = tfp.distributions.Uniform()\nelif framework == 'torch':\nself.beta = torch.distributions.beta.Beta(self.alpha, self.alpha)\nself.uniform = torch.distributions.uniform.Uniform(low=0, high=1)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\n@staticmethod\ndef _get_patch_coordinates(tensor: Tensor, x: Tensor, y: Tensor,\nlam: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n\"\"\"Randomly cut the patches from input images.\n        If patches are going to be pasted in other image, combination ratio between two images is defined by `lam`.\n        Cropping region indicates where to drop out from the image and `cut_x` &amp; `cut_y` are used to calculate cropping\n        region whose aspect ratio is proportional to the original image.\n        Args:\n            tensor: The input value.\n            lam: Combination ratio between two images. Larger the lambda value is smaller the patch would be. A\n                scalar tensor containing value between 0 and 1.\n            x: X-coordinate in image from which patch needs to be cropped. A scalar tensor containing value between 0\n                and 1 which in turn is transformed in the range of image width.\n            y: Y-coordinate in image from which patch needs to be cropped. A scalar tensor containing value between 0\n                and 1 which in turn is transformed in the range of image height.\n        Returns:\n            The X and Y coordinates of the cropped patch along with width and height.\n        \"\"\"\n_, img_height, img_width = get_image_dims(tensor)\ncut_x = img_width * x\ncut_y = img_height * y\ncut_w = img_width * tensor_sqrt(1 - lam)\ncut_h = img_height * tensor_sqrt(1 - lam)\nbbox_x1 = cast(tensor_round(clip_by_value(cut_x - cut_w / 2, min_value=0)), \"int32\")\nbbox_x2 = cast(tensor_round(clip_by_value(cut_x + cut_w / 2, max_value=img_width)), \"int32\")\nbbox_y1 = cast(tensor_round(clip_by_value(cut_y - cut_h / 2, min_value=0)), \"int32\")\nbbox_y2 = cast(tensor_round(clip_by_value(cut_y + cut_h / 2, max_value=img_height)), \"int32\")\nreturn bbox_x1, bbox_x2, bbox_y1, bbox_y2, img_width, img_height\ndef forward(self, data: Tensor, state: Dict[str, Any]) -&gt; Tuple[Tensor, Tensor]:\nlam = self.beta.sample()\nlam = maximum(lam, (1 - lam))\ncut_x = self.uniform.sample()\ncut_y = self.uniform.sample()\nbbox_x1, bbox_x2, bbox_y1, bbox_y2, width, height = self._get_patch_coordinates(data, cut_x, cut_y, lam=lam)\nif tf.is_tensor(data):\npatches = roll(data, shift=1, axis=0)[:, bbox_y1:bbox_y2,\nbbox_x1:bbox_x2, :] - data[:, bbox_y1:bbox_y2, bbox_x1:bbox_x2, :]\npatches = tf.pad(patches, [[0, 0], [bbox_y1, height - bbox_y2], [bbox_x1, width - bbox_x2], [0, 0]],\nmode=\"CONSTANT\",\nconstant_values=0)\ndata = data + patches\nelse:\ndata[:, :, bbox_y1:bbox_y2, bbox_x1:bbox_x2] = roll(data, shift=1,\naxis=0)[:, :, bbox_y1:bbox_y2, bbox_x1:bbox_x2]\n# adjust lambda to match pixel ratio\nlam = 1 - cast(((bbox_x2 - bbox_x1) * (bbox_y2 - bbox_y1)), dtype=\"float32\") / (width * height)\nreturn data, lam\n</code></pre>"}, {"location": "fastestimator/op/tensorop/augmentation/mixup_batch.html", "title": "mixup_batch", "text": ""}, {"location": "fastestimator/op/tensorop/augmentation/mixup_batch.html#fastestimator.fastestimator.op.tensorop.augmentation.mixup_batch.MixUpBatch", "title": "<code>MixUpBatch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>MixUp augmentation for tensors.</p> <p>This class should be used in conjunction with MixLoss to perform mix-up training, which helps to reduce over-fitting, stabilize GAN training, and against adversarial attacks (https://arxiv.org/abs/1710.09412).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the input to be mixed up.</p> required <code>outputs</code> <code>Iterable[str]</code> <p>Key to store the mixed-up outputs.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode to execute in. Probably 'train'.</p> <code>'train'</code> <code>alpha</code> <code>float</code> <p>The alpha value defining the beta distribution to be drawn from during training.</p> <code>1.0</code> <code>shared_beta</code> <code>bool</code> <p>Sample a single beta for a batch or element wise beta for each image.</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input arguments are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\augmentation\\mixup_batch.py</code> <pre><code>class MixUpBatch(TensorOp):\n\"\"\"MixUp augmentation for tensors.\n    This class should be used in conjunction with MixLoss to perform mix-up training, which helps to reduce\n    over-fitting, stabilize GAN training, and against adversarial attacks (https://arxiv.org/abs/1710.09412).\n    Args:\n        inputs: Key of the input to be mixed up.\n        outputs: Key to store the mixed-up outputs.\n        mode: What mode to execute in. Probably 'train'.\n        alpha: The alpha value defining the beta distribution to be drawn from during training.\n        shared_beta: Sample a single beta for a batch or element wise beta for each image.\n    Raises:\n        AssertionError: If input arguments are invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Iterable[str],\nmode: Union[None, str, Iterable[str]] = 'train',\nalpha: float = 1.0,\nshared_beta: bool = True):\nassert alpha &gt; 0, \"MixUp alpha value must be greater than zero\"\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert len(self.outputs) == len(self.inputs) + 1, \"MixUpBatch requires 1 more output than inputs\"\nself.alpha = alpha\nself.beta = None\nself.shared_beta = shared_beta\nself.in_list, self.out_list = True, True\ndef build(self, framework: str) -&gt; None:\nif framework == 'tf':\nself.beta = tfp.distributions.Beta(self.alpha, self.alpha)\nelif framework == 'torch':\nself.beta = torch.distributions.beta.Beta(self.alpha, self.alpha)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nif self.shared_beta:\nlam = self.beta.sample()\nelse:\nlam = self.beta.sample(sample_shape=(data[0].shape[0], ))\nshape = [-1] + [1] * (len(data[0].shape) - 1)\nlam = reshape(lam, shape)\nlam = maximum(lam, (1 - lam))\nmix = [lam * elem + (1.0 - lam) * roll(elem, shift=1, axis=0) for elem in data]\nreturn mix + [lam]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html", "title": "fgsm", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html#fastestimator.fastestimator.op.tensorop.gradient.fgsm.FGSM", "title": "<code>FGSM</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Create an adversarial sample from input data using the Fast Gradient Sign Method.</p> <p>See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Key of the input to be attacked.</p> required <code>loss</code> <code>str</code> <p>Key of the loss value to use for gradient computation.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>epsilon</code> <code>float</code> <p>The strength of the perturbation to use in the attack.</p> <code>0.01</code> <code>clip_low</code> <code>Optional[float]</code> <p>a minimum value to clip the output by (defaults to min value of data when set to None).</p> <code>None</code> <code>clip_high</code> <code>Optional[float]</code> <p>a maximum value to clip the output by (defaults to max value of data when set to None).</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\fgsm.py</code> <pre><code>@traceable()\nclass FGSM(TensorOp):\n\"\"\"Create an adversarial sample from input data using the Fast Gradient Sign Method.\n    See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.\n    Args:\n        data: Key of the input to be attacked.\n        loss: Key of the loss value to use for gradient computation.\n        outputs: The key under which to save the output.\n        epsilon: The strength of the perturbation to use in the attack.\n        clip_low: a minimum value to clip the output by (defaults to min value of data when set to None).\n        clip_high: a maximum value to clip the output by (defaults to max value of data when set to None).\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ndata: str,\nloss: str,\noutputs: str,\nepsilon: float = 0.01,\nclip_low: Optional[float] = None,\nclip_high: Optional[float] = None,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=[data, loss], outputs=outputs, mode=mode)\nself.epsilon = epsilon\nself.clip_low = clip_low\nself.clip_high = clip_high\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ndata, loss = data\ngrad = get_gradient(target=loss, sources=data, tape=state['tape'], retain_graph=self.retain_graph)\nadverse_data = clip_by_value(data + self.epsilon * sign(grad),\nmin_value=self.clip_low or reduce_min(data),\nmax_value=self.clip_high or reduce_max(data))\nreturn adverse_data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/gradient.html", "title": "gradient", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/gradient.html#fastestimator.fastestimator.op.tensorop.gradient.gradient.GradientOp", "title": "<code>GradientOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Return the gradients of finals w.r.t. inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to compute gradients with respect to.</p> required <code>finals</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to compute gradients from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the gradients.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\gradient.py</code> <pre><code>@traceable()\nclass GradientOp(TensorOp):\n\"\"\"Return the gradients of finals w.r.t. inputs.\n    Args:\n        inputs: The tensor(s) to compute gradients with respect to.\n        finals: The tensor(s) to compute gradients from.\n        outputs: The key(s) under which to save the gradients.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\nfinals: Union[str, List[str]],\noutputs: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = None):\ninputs = to_list(inputs)\nfinals = to_list(finals)\noutputs = to_list(outputs)\nassert len(inputs) == len(finals) == len(outputs), \\\n            \"GradientOp requires the same number of inputs, finals, and outputs\"\ninputs.extend(finals)\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\ninitials = data[:len(data) // 2]\nfinals = data[len(data) // 2:]\nresults = []\nfor initial, final in zip(initials, finals):\nresults.append(get_gradient(final, initial, tape=state['tape'], retain_graph=self.retain_graph))\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/watch.html", "title": "watch", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/watch.html#fastestimator.fastestimator.op.tensorop.gradient.watch.Watch", "title": "<code>Watch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Watch one or more tensors for later gradient computation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>which tensors to watch during future computation.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\watch.py</code> <pre><code>@traceable()\nclass Watch(TensorOp):\n\"\"\"Watch one or more tensors for later gradient computation.\n    Args:\n        inputs: which tensors to watch during future computation.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self, inputs: Union[None, str, Iterable[str]], mode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=inputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nfor idx, tensor in enumerate(data):\ndata[idx] = watch(tensor=tensor, tape=state['tape'])\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html", "title": "cross_entropy", "text": ""}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html#fastestimator.fastestimator.op.tensorop.loss.cross_entropy.CrossEntropy", "title": "<code>CrossEntropy</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss value.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without softmax).</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> <code>form</code> <code>Optional[str]</code> <p>What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will automatically infer the correct form based on tensor shape.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\cross_entropy.py</code> <pre><code>@traceable()\nclass CrossEntropy(LossOp):\n\"\"\"Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss value.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        from_logits: Whether y_pred is logits (without softmax).\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n        form: What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will\n            automatically infer the correct form based on tensor shape.\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nfrom_logits: bool = False,\naverage_loss: bool = True,\nform: Optional[str] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, average_loss=average_loss)\nself.from_logits = from_logits\nself.form = form\nself.cross_entropy_fn = {\n\"binary\": binary_crossentropy,\n\"categorical\": categorical_crossentropy,\n\"sparse\": sparse_categorical_crossentropy\n}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nform = self.form\nif form is None:\nif len(y_pred.shape) == 2 and y_pred.shape[-1] &gt; 1:\nif len(y_true.shape) == 2 and y_true.shape[-1] &gt; 1:\nform = \"categorical\"\nelse:\nform = \"sparse\"\nelse:\nform = \"binary\"\nloss = self.cross_entropy_fn[form](y_pred, y_true, from_logits=self.from_logits, average_loss=self.average_loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/hinge.html", "title": "hinge", "text": ""}, {"location": "fastestimator/op/tensorop/loss/hinge.html#fastestimator.fastestimator.op.tensorop.loss.hinge.Hinge", "title": "<code>Hinge</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate the hinge loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\hinge.py</code> <pre><code>@traceable()\nclass Hinge(LossOp):\n\"\"\"Calculate the hinge loss between two tensors.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nself.average_loss = average_loss\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nloss = hinge(y_true=y_true, y_pred=y_pred)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/loss.html", "title": "loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/loss.html#fastestimator.fastestimator.op.tensorop.loss.loss.LossOp", "title": "<code>LossOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Abstract base LossOp class.</p> <p>A base class for loss operations. It can be used directly to perform value pass-through (see the adversarial training showcase for an example of when this is useful).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>List[str]</code> <p>String key under which to store the computed loss.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\loss.py</code> <pre><code>class LossOp(TensorOp):\n\"\"\"Abstract base LossOp class.\n    A base class for loss operations. It can be used directly to perform value pass-through (see the adversarial\n    training showcase for an example of when this is useful).\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]] = None,\noutputs: List[str] = None,\nmode: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.average_loss = average_loss\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[self.true_key_idx]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[self.pred_key_idx]\n@property\ndef true_key_idx(self) -&gt; int:\nreturn 1\n@property\ndef pred_key_idx(self) -&gt; int:\nreturn 0\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html", "title": "mean_squared_error", "text": ""}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html#fastestimator.fastestimator.op.tensorop.loss.mean_squared_error.MeanSquaredError", "title": "<code>MeanSquaredError</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate the mean squared error loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\mean_squared_error.py</code> <pre><code>@traceable()\nclass MeanSquaredError(LossOp):\n\"\"\"Calculate the mean squared error loss between two tensors.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nself.average_loss = average_loss\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nloss = mean_squared_error(y_true=y_true, y_pred=y_pred)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/mix_loss.html", "title": "mix_loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/mix_loss.html#fastestimator.fastestimator.op.tensorop.loss.mix_loss.MixLoss", "title": "<code>MixLoss</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Loss class to compute mixiup and cutmix losses.</p> <p>This class should be used in conjunction with MixUpBatch and CutMixBatch to perform mix-up training, which helps to reduce over-fitting, stabilize GAN training, and harden against adversarial attacks. See https://arxiv.org/abs/1710.09412 for details.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>LossOp</code> <p>A loss object which we use to calculate the underlying loss of MixLoss. This should be an object of type fe.op.tensorop.loss.loss.LossOp.</p> required <code>lam</code> <code>str</code> <p>The key of the lambda value generated by MixUpBatch or CutMixBatch.</p> required <code>average_loss</code> <code>bool</code> <p>Whether the final loss should be averaged or not.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>loss</code> has multiple outputs.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\mix_loss.py</code> <pre><code>class MixLoss(LossOp):\n\"\"\"Loss class to compute mixiup and cutmix losses.\n    This class should be used in conjunction with MixUpBatch and CutMixBatch to perform mix-up training, which helps to\n    reduce over-fitting, stabilize GAN training, and harden against adversarial attacks. See\n    https://arxiv.org/abs/1710.09412 for details.\n    Args:\n        loss: A loss object which we use to calculate the underlying loss of MixLoss. This should be an object of type\n            fe.op.tensorop.loss.loss.LossOp.\n        lam: The key of the lambda value generated by MixUpBatch or CutMixBatch.\n        average_loss: Whether the final loss should be averaged or not.\n    Raises:\n        ValueError: If the provided `loss` has multiple outputs.\n    \"\"\"\ndef __init__(self, loss: LossOp, lam: str, average_loss: bool = True):\nself.loss = loss\nself.loss.average_loss = False\nif len(loss.outputs) != 1:\nraise ValueError(\"MixLoss only supports lossOps which have a single output.\")\nsuper().__init__(inputs=[lam] + loss.inputs, outputs=loss.outputs, mode=loss.mode, average_loss=average_loss)\nself.out_list = False\n@property\ndef pred_key_idx(self) -&gt; int:\nreturn self.loss.pred_key_idx + 1\n@property\ndef true_key_idx(self) -&gt; int:\nreturn self.loss.true_key_idx + 1\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\nlam, *args = data\nloss1 = self.loss.forward(args, state)\nargs[self.loss.true_key_idx] = roll(args[self.loss.true_key_idx], shift=1, axis=0)\nloss2 = self.loss.forward(args, state)\nloss = lam * loss1 + (1.0 - lam) * loss2\nif self.average_loss:\nloss = fe.backend.reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/fuse.html", "title": "fuse", "text": ""}, {"location": "fastestimator/op/tensorop/meta/fuse.html#fastestimator.fastestimator.op.tensorop.meta.fuse.Fuse", "title": "<code>Fuse</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Run a sequence of TensorOps as a single Op.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Union[TensorOp, List[TensorOp]]</code> <p>A sequence of TensorOps to run. They must all share the same mode. It also doesn't support scheduled ops at the moment, though the subnet itself may be scheduled.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>ops</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\fuse.py</code> <pre><code>@traceable()\nclass Fuse(TensorOp):\n\"\"\"Run a sequence of TensorOps as a single Op.\n    Args:\n        ops: A sequence of TensorOps to run. They must all share the same mode. It also doesn't support scheduled ops at\n            the moment, though the subnet itself may be scheduled.\n    Raises:\n        ValueError: If `ops` are invalid.\n    \"\"\"\ndef __init__(self, ops: Union[TensorOp, List[TensorOp]]) -&gt; None:\nops = to_list(ops)\nif len(ops) &lt; 1:\nraise ValueError(\"Fuse requires at least one op\")\ninputs = []\noutputs = []\nmode = ops[0].mode\nself.last_retain_idx = 0\nself.models = set()\nself.loss_keys = set()\nfor idx, op in enumerate(ops):\nif op.mode != mode:\nraise ValueError(f\"All Fuse ops must share the same mode, but got {mode} and {op.mode}\")\nfor inp in op.inputs:\nif inp not in inputs and inp not in outputs:\ninputs.append(inp)\nfor out in op.outputs:\nif out not in outputs:\noutputs.append(out)\nif op.fe_retain_graph(True) is not None:  # Set all of the internal ops to retain\nself.last_retain_idx = idx  # Keep tabs on the last one since it might be set to False\nself.models |= op.get_fe_models()\nself.loss_keys |= op.get_fe_loss_keys()\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.ops = ops\ndef build(self, framework: str) -&gt; None:\nfor op in self.ops:\nop.build(framework)\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.models\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.loss_keys\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nreturn self.ops[self.last_retain_idx].fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nBaseNetwork._forward_batch(data, state, self.ops)\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/one_of.html", "title": "one_of", "text": ""}, {"location": "fastestimator/op/tensorop/meta/one_of.html#fastestimator.fastestimator.op.tensorop.meta.one_of.OneOf", "title": "<code>OneOf</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Perform one of several possible TensorOps.</p> <p>Parameters:</p> Name Type Description Default <code>*tensor_ops</code> <code>TensorOp</code> <p>A list of ops to choose between with uniform probability.</p> <code>()</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\one_of.py</code> <pre><code>@traceable()\nclass OneOf(TensorOp):\n\"\"\"Perform one of several possible TensorOps.\n    Args:\n        *tensor_ops: A list of ops to choose between with uniform probability.\n    \"\"\"\ndef __init__(self, *tensor_ops: TensorOp) -&gt; None:\ninputs = tensor_ops[0].inputs\noutputs = tensor_ops[0].outputs\nmode = tensor_ops[0].mode\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list = tensor_ops[0].in_list\nself.out_list = tensor_ops[0].out_list\nfor op in tensor_ops[1:]:\nassert inputs == op.inputs, \"All ops within a OneOf must share the same inputs\"\nassert self.in_list == op.in_list, \"All ops within OneOf must share the same input configuration\"\nassert outputs == op.outputs, \"All ops within a OneOf must share the same outputs\"\nassert self.out_list == op.out_list, \"All ops within OneOf must share the same output configuration\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nself.ops = tensor_ops\nself.prob_fn = None\nself.invoke_fn = None\ndef build(self, framework: str) -&gt; None:\nif framework == 'tf':\nself.prob_fn = tfp.distributions.Uniform(low=0, high=len(self.ops))\nself.invoke_fn = lambda idx, data, state: tf.switch_case(idx, [lambda: op.forward(data, state) for op in\nself.ops])\nelif framework == 'torch':\nself.prob_fn = torch.distributions.uniform.Uniform(low=0, high=len(self.ops))\nself.invoke_fn = lambda idx, data, state: self.ops[idx].forward(data, state)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn set.union(*[op.get_fe_loss_keys() for op in self.ops])\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn set.union(*[op.get_fe_models() for op in self.ops])\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nresp = None\nfor op in self.ops:\nresp = resp or op.fe_retain_graph(retain)\nreturn resp\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n        Args:\n            data: The information to be passed to one of the wrapped operators.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after application of one of the available numpyOps.\n        \"\"\"\nidx = cast(self.prob_fn.sample(), dtype='int32')\nreturn self.invoke_fn(idx, data, state)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/one_of.html#fastestimator.fastestimator.op.tensorop.meta.one_of.OneOf.forward", "title": "<code>forward</code>", "text": "<p>Execute a randomly selected op from the list of <code>numpy_ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[Tensor]]</code> <p>The information to be passed to one of the wrapped operators.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>The <code>data</code> after application of one of the available numpyOps.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\one_of.py</code> <pre><code>def forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n    Args:\n        data: The information to be passed to one of the wrapped operators.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after application of one of the available numpyOps.\n    \"\"\"\nidx = cast(self.prob_fn.sample(), dtype='int32')\nreturn self.invoke_fn(idx, data, state)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/repeat.html", "title": "repeat", "text": ""}, {"location": "fastestimator/op/tensorop/meta/repeat.html#fastestimator.fastestimator.op.tensorop.meta.repeat.Repeat", "title": "<code>Repeat</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Repeat a TensorOp several times in a row.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>TensorOp</code> <p>A TensorOp to be run one or more times in a row.</p> required <code>repeat</code> <code>Union[int, Callable[..., bool]]</code> <p>How many times to repeat the <code>op</code>. This can also be a function return, in which case the function input names will be matched to keys in the data dictionary, and the <code>op</code> will be repeated until the function evaluates to False. The function evaluation will happen at the end of a forward call, so the <code>op</code> will always be evaluated at least once.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code> or <code>op</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\repeat.py</code> <pre><code>@traceable()\nclass Repeat(TensorOp):\n\"\"\"Repeat a TensorOp several times in a row.\n    Args:\n        op: A TensorOp to be run one or more times in a row.\n        repeat: How many times to repeat the `op`. This can also be a function return, in which case the function input\n            names will be matched to keys in the data dictionary, and the `op` will be repeated until the function\n            evaluates to False. The function evaluation will happen at the end of a forward call, so the `op` will\n            always be evaluated at least once.\n    Raises:\n        ValueError: If `repeat` or `op` are invalid.\n    \"\"\"\ndef __init__(self, op: TensorOp, repeat: Union[int, Callable[..., bool]] = 1) -&gt; None:\nself.repeat_inputs = []\nextra_reqs = []\nif isinstance(repeat, int):\nif repeat &lt; 1:\nraise ValueError(f\"Repeat requires repeat to be &gt;= 1, but got {repeat}\")\nelse:\nself.repeat_inputs.extend(inspect.signature(repeat).parameters.keys())\nextra_reqs = list(set(self.repeat_inputs) - set(op.outputs))\nself.repeat = repeat\nsuper().__init__(inputs=op.inputs + extra_reqs, outputs=op.outputs, mode=op.mode)\nself.ops = [op]\nself.retain_graph = None\nself.while_fn = None\n@property\ndef op(self) -&gt; TensorOp:\nreturn self.ops[0]\ndef build(self, framework: str) -&gt; None:\nself.op.build(framework)\nif framework == 'tf':\nself.while_fn = self._tf_while\nelse:\nself.while_fn = self._torch_while\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.op.get_fe_models()\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.op.get_fe_loss_keys()\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.op.fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n# Set retain to true since might loop over a gradient aware op\nself.op.fe_retain_graph(True)\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nif isinstance(self.repeat, int):\nfor i in range(self.repeat - 1):\n# Perform n-1 rounds with all ops having retain_graph == True\nBaseNetwork._forward_batch(data, state, self.ops)\n# Let retain be whatever it was meant to be for the final sequence\nself.op.fe_retain_graph(self.retain_graph)\n# Final round of ops\nBaseNetwork._forward_batch(data, state, self.ops)\nelse:\nBaseNetwork._forward_batch(data, state, self.ops)\ndata = self.while_fn(data, state)\n# TODO - Find some magic way to invoke this at the right moment\nself.op.fe_retain_graph(self.retain_graph)\nreturn [data[key] for key in self.outputs]\ndef _torch_while(self, data: Dict[str, Tensor], state: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"A helper function to invoke a while loop.\n        Args:\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            A reference to the updated data dictionary.\n        \"\"\"\nwhile self.repeat(*[data[var_name] for var_name in self.repeat_inputs]):\nBaseNetwork._forward_batch(data, state, self.ops)\nreturn data\ndef _tf_while(self, data: Dict[str, Tensor], state: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"A helper function to invoke a while loop.\n        Args:\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            A reference to the updated data dictionary.\n        \"\"\"\nargs = ([data[var_name] for var_name in self.repeat_inputs], data, state)\nargs = tf.while_loop(self._tf_cond, self._tf_body, args)\nreturn args[1]\ndef _tf_cond(self, cnd: List[Tensor], data: Dict[str, Tensor], state: Dict[str, Any]) -&gt; bool:\n\"\"\"A helper function determine whether to keep invoking the while method.\n        Note that `data` and `state` are unused here, but required since tf.while_loop needs the cond and body to have\n        the same input argument signatures.\n        Args:\n            cnd: A list of arguments to be passed to the condition function.\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            Whether to continue looping.\n        \"\"\"\nreturn self.repeat(*cnd)\ndef _tf_body(self, cnd: List[Tensor], data: Dict[str, Tensor],\nstate: Dict[str, Any]) -&gt; Tuple[List[Tensor], Dict[str, Tensor], Dict[str, Any]]:\n\"\"\"A helper function to execute the body of a while method.\n        Note that `cnd` is unused here, but required since tf.while_loop needs the cond and body to have the same input\n        argument signatures.\n        Args:\n            cnd: A list of arguments to be passed to the condition function.\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            The updated `cnd` values, along with the modified data and state dictionaries.\n        \"\"\"\nBaseNetwork._forward_batch(data, state, self.ops)\nreturn [data[var_name] for var_name in self.repeat_inputs], data, state\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/sometimes.html", "title": "sometimes", "text": ""}, {"location": "fastestimator/op/tensorop/meta/sometimes.html#fastestimator.fastestimator.op.tensorop.meta.sometimes.Sometimes", "title": "<code>Sometimes</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Perform a NumpyOp with a given probability.</p> <p>Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking the Sometimes.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_op</code> <code>TensorOp</code> <p>The operator to be performed.</p> required <code>prob</code> <code>float</code> <p>The probability of execution, which should be in the range: [0-1).</p> <code>0.5</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\sometimes.py</code> <pre><code>@traceable()\nclass Sometimes(TensorOp):\n\"\"\"Perform a NumpyOp with a given probability.\n    Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data\n    dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but\n    Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before\n    invoking the Sometimes.\n    Args:\n        tensor_op: The operator to be performed.\n        prob: The probability of execution, which should be in the range: [0-1).\n    \"\"\"\ndef __init__(self, tensor_op: TensorOp, prob: float = 0.5) -&gt; None:\n# We're going to try to collect any missing output keys from the data dictionary so that they don't get\n# overridden when Sometimes chooses not to execute.\ninps = set(tensor_op.inputs)\nouts = set(tensor_op.outputs)\nself.extra_inputs = list(outs - inps)  # Used by traceability\nself.inp_idx = len(tensor_op.inputs)\nsuper().__init__(inputs=tensor_op.inputs + self.extra_inputs, outputs=tensor_op.outputs, mode=tensor_op.mode)\n# Note that in_list and out_list will always be true\nself.op = tensor_op\nself.prob = prob\nself.prob_fn = None\ndef build(self, framework: str) -&gt; None:\nif framework == 'tf':\nself.prob_fn = tfp.distributions.Uniform()\nelif framework == 'torch':\nself.prob_fn = torch.distributions.uniform.Uniform(low=0, high=1)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.op.get_fe_loss_keys()\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.op.get_fe_models()\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nreturn self.op.fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, Dict[Any, Any]]:\nreturn {'op': self.op.__getstate__() if hasattr(self.op, '__getstate__') else {}}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n        Args:\n            data: The information to be passed to the wrapped operator.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The original `data`, or the `data` after running it through the wrapped operator.\n        \"\"\"\nif self.prob &gt; self.prob_fn.sample():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/sometimes.html#fastestimator.fastestimator.op.tensorop.meta.sometimes.Sometimes.forward", "title": "<code>forward</code>", "text": "<p>Execute the wrapped operator a certain fraction of the time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Tensor]</code> <p>The information to be passed to the wrapped operator.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>The original <code>data</code>, or the <code>data</code> after running it through the wrapped operator.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\sometimes.py</code> <pre><code>def forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n    Args:\n        data: The information to be passed to the wrapped operator.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The original `data`, or the `data` after running it through the wrapped operator.\n    \"\"\"\nif self.prob &gt; self.prob_fn.sample():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/model.html", "title": "model", "text": ""}, {"location": "fastestimator/op/tensorop/model/model.html#fastestimator.fastestimator.op.tensorop.model.model.ModelOp", "title": "<code>ModelOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs forward passes of a neural network over batch data to generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model compiled by fe.build.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key of input training data.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store predictions.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Indicates whether the model should have its weights tracked for update.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\model.py</code> <pre><code>@traceable()\nclass ModelOp(TensorOp):\n\"\"\"This class performs forward passes of a neural network over batch data to generate predictions.\n    Args:\n        model: A model compiled by fe.build.\n        inputs: String key of input training data.\n        outputs: String key under which to store predictions.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        trainable: Indicates whether the model should have its weights tracked for update.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\ntrainable: bool = True):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert hasattr(model, \"fe_compiled\"), \"must use fe.build to compile the model before use\"\nself.model = model\nself.trainable = trainable\nself.epoch_spec = None\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn {self.model}\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\ntraining = state['mode'] == \"train\" and self.trainable\nif isinstance(self.model, torch.nn.Module) and self.epoch_spec != state['epoch']:\n# Gather model input specs for the sake of TensorBoard and Traceability\nself.model.fe_input_spec = FeInputSpec(data, self.model)\nself.epoch_spec = state['epoch']\ndata = feed_forward(self.model, data, training=training)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/update.html", "title": "update", "text": ""}, {"location": "fastestimator/op/tensorop/model/update.html#fastestimator.fastestimator.op.tensorop.model.update.UpdateOp", "title": "<code>UpdateOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs updates to a model's weights based on the loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>Model instance compiled by fe.build.</p> required <code>loss_name</code> <code>str</code> <p>The name of loss.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'train'</code> <code>defer</code> <code>bool</code> <p>Whether to defer the actual application of the update until the end of the step. This can be necessary in PyTorch when trying to update multiple models which depend on one another (ex. certain GANs). By default, all UpdateOps which appear contiguously as the last ops of a Network will be deferred. We hope that you will never need to worry about this flag, but it's here for you if you need it.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\update.py</code> <pre><code>@traceable()\nclass UpdateOp(TensorOp):\n\"\"\"This class performs updates to a model's weights based on the loss.\n    Args:\n        model: Model instance compiled by fe.build.\n        loss_name: The name of loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        defer: Whether to defer the actual application of the update until the end of the step. This can be necessary\n            in PyTorch when trying to update multiple models which depend on one another (ex. certain GANs). By default,\n            all UpdateOps which appear contiguously as the last ops of a Network will be deferred. We hope that you will\n            never need to worry about this flag, but it's here for you if you need it.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nloss_name: str,\nmode: Union[None, str, Iterable[str]] = \"train\",\ndefer: bool = False):\nsuper().__init__(inputs=loss_name, outputs=None, mode=mode)\nself.model = model\nself.retain_graph = False\nself.weight_decay = isinstance(self.model, tf.keras.Model) and self.model.losses\nself.defer = defer\nif not hasattr(self.model, \"loss_name\"):\nself.model.loss_name = {loss_name}\nelse:\nself.model.loss_name.add(loss_name)\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn {self.model}\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn set(self.inputs)\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; None:\nif not state[\"warmup\"]:\nif self.weight_decay:\ndata = data + tf.reduce_sum(self.model.losses)\nupdate_model(self.model,\ndata,\ntape=state['tape'],\nretain_graph=self.retain_graph,\nscaler=state[\"scaler\"],\ndefer=self.defer,\ndeferred=state[\"deferred\"])\n</code></pre>"}, {"location": "fastestimator/schedule/lr_shedule.html", "title": "lr_shedule", "text": ""}, {"location": "fastestimator/schedule/lr_shedule.html#fastestimator.fastestimator.schedule.lr_shedule.cosine_decay", "title": "<code>cosine_decay</code>", "text": "<p>Learning rate cosine decay function (using half of cosine curve).</p> <p>This method is useful for scheduling learning rates which oscillate over time: <pre><code>s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])\n</code></pre></p> <p>For more information, check out SGDR: https://arxiv.org/pdf/1608.03983.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>The current step or epoch during training starting from 1.</p> required <code>cycle_length</code> <code>int</code> <p>The decay cycle length.</p> required <code>init_lr</code> <code>float</code> <p>Initial learning rate to decay from.</p> required <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <code>start</code> <code>int</code> <p>The step or epoch to start the decay schedule.</p> <code>1</code> <code>cycle_multiplier</code> <code>int</code> <p>The factor by which next cycle length will be multiplied.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>lr</code> <p>learning rate given current step or epoch.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\lr_shedule.py</code> <pre><code>def cosine_decay(time: int,\ncycle_length: int,\ninit_lr: float,\nmin_lr: float = 1e-6,\nstart: int = 1,\ncycle_multiplier: int = 1):\n\"\"\"Learning rate cosine decay function (using half of cosine curve).\n    This method is useful for scheduling learning rates which oscillate over time:\n    ```python\n    s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])\n    ```\n    For more information, check out SGDR: https://arxiv.org/pdf/1608.03983.pdf.\n    Args:\n        time: The current step or epoch during training starting from 1.\n        cycle_length: The decay cycle length.\n        init_lr: Initial learning rate to decay from.\n        min_lr: Minimum learning rate.\n        start: The step or epoch to start the decay schedule.\n        cycle_multiplier: The factor by which next cycle length will be multiplied.\n    Returns:\n        lr: learning rate given current step or epoch.\n    \"\"\"\nif time &lt; start:\nlr = init_lr\nelse:\ntime = time - start + 1\nif cycle_multiplier &gt; 1:\ncurrent_cycle_idx = math.ceil(\nmath.log(time * (cycle_multiplier - 1) / cycle_length + 1) / math.log(cycle_multiplier)) - 1\ncumulative = cycle_length * (cycle_multiplier**current_cycle_idx - 1) / (cycle_multiplier - 1)\nelif cycle_multiplier == 1:\ncurrent_cycle_idx = math.ceil(time / cycle_length) - 1\ncumulative = current_cycle_idx * cycle_length\nelse:\nraise ValueError(\"multiplier must be at least 1\")\ncurrent_cycle_length = cycle_length * cycle_multiplier**current_cycle_idx\ntime_in_cycle = (time - cumulative) / current_cycle_length\nlr = (init_lr - min_lr) / 2 * math.cos(time_in_cycle * math.pi) + (init_lr + min_lr) / 2\nreturn lr\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html", "title": "schedule", "text": ""}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.EpochScheduler", "title": "<code>EpochScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which selects entries based on a specified epoch mapping.</p> <p>This can be useful for making networks grow over time, or to use more challenging data augmentation as training progresses.</p> <pre><code>s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"a\"\ns.get_current_value(epoch=3)  # \"b\"\ns.get_current_value(epoch=4)  # None\ns.get_current_value(epoch=99)  # None\ns.get_current_value(epoch=100)  # \"c\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epoch_dict</code> <code>Dict[int, T]</code> <p>A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key will be used to determine which element to return. None values may be used to cause nothing to happen for a particular epoch.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>epoch_dict</code> is of the wrong type, or contains invalid keys.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass EpochScheduler(Scheduler[T]):\n\"\"\"A scheduler which selects entries based on a specified epoch mapping.\n    This can be useful for making networks grow over time, or to use more challenging data augmentation as training\n    progresses.\n    ```python\n    s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"a\"\n    s.get_current_value(epoch=3)  # \"b\"\n    s.get_current_value(epoch=4)  # None\n    s.get_current_value(epoch=99)  # None\n    s.get_current_value(epoch=100)  # \"c\"\n    ```\n    Args:\n        epoch_dict: A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key\n            will be used to determine which element to return. None values may be used to cause nothing to happen for a\n            particular epoch.\n    Raises:\n        AssertionError: If the `epoch_dict` is of the wrong type, or contains invalid keys.\n    \"\"\"\ndef __init__(self, epoch_dict: Dict[int, T]) -&gt; None:\nassert isinstance(epoch_dict, dict), \"must provide dictionary as epoch_dict\"\nself.epoch_dict = epoch_dict\nself.keys = sorted(self.epoch_dict)\nfor key in self.keys:\nassert isinstance(key, int), \"found non-integer key: {}\".format(key)\nassert key &gt;= 1, \"found non-positive key: {}\".format(key)\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\nif epoch in self.keys:\nvalue = self.epoch_dict[epoch]\nelse:\nlast_key = self._get_last_key(epoch)\nif last_key is None:\nvalue = None\nelse:\nvalue = self.epoch_dict[last_key]\nreturn value\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn list(self.epoch_dict.values())\ndef _get_last_key(self, epoch: int) -&gt; Union[int, None]:\n\"\"\"Find the nearest prior key to the given epoch.\n        Args:\n            epoch: The current target epoch.\n        Returns:\n            The largest epoch number &lt;= the given `epoch` that is in the `epoch_dict`.\n        \"\"\"\nlast_key = None\nfor key in self.keys:\nif key &gt; epoch:\nbreak\nlast_key = key\nreturn last_key\ndef __getstate__(self) -&gt; Dict[str, Dict[int, Dict[Any, Any]]]:\nreturn {\n'epoch_dict':\n{key: elem.__getstate__()\nfor key, elem in self.epoch_dict.items() if hasattr(elem, '__getstate__')}\n}\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.RepeatScheduler", "title": "<code>RepeatScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which repeats a collection of entries one after another every epoch.</p> <p>One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.</p> <pre><code>s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"b\"\ns.get_current_value(epoch=3)  # \"c\"\ns.get_current_value(epoch=4)  # \"a\"\ns.get_current_value(epoch=5)  # \"b\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repeat_list</code> <code>List[Optional[T]]</code> <p>What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>repeat_list</code> is not a List.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass RepeatScheduler(Scheduler[T]):\n\"\"\"A scheduler which repeats a collection of entries one after another every epoch.\n    One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a\n    different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.\n    ```python\n    s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"b\"\n    s.get_current_value(epoch=3)  # \"c\"\n    s.get_current_value(epoch=4)  # \"a\"\n    s.get_current_value(epoch=5)  # \"b\"\n    ```\n    Args:\n        repeat_list: What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing\n        happen for a particular epoch, None values may be used.\n    Raises:\n        AssertionError: If `repeat_list` is not a List.\n    \"\"\"\ndef __init__(self, repeat_list: List[Optional[T]]) -&gt; None:\nassert isinstance(repeat_list, List), \"must provide a list as input of RepeatSchedule\"\nself.repeat_list = repeat_list\nself.cycle_length = len(repeat_list)\nassert self.cycle_length &gt; 1, \"list length must be greater than 1\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n# epoch-1 since the training epoch is 1-indexed rather than 0-indexed.\nreturn self.repeat_list[(epoch - 1) % self.cycle_length]\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn self.repeat_list\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {\n'repeat_list': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.repeat_list]\n}\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler", "title": "<code>Scheduler</code>", "text": "<p>         Bases: <code>Generic[T]</code></p> <p>A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass Scheduler(Generic[T]):\n\"\"\"A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.\n    \"\"\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n        Args:\n            epoch: The current epoch.\n        Returns:\n            The element from the Scheduler to be used at the given `epoch`. This value might be None.\n        \"\"\"\nraise NotImplementedError\ndef get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n        Returns:\n            A list of all the values stored in the `Scheduler`. This may contain None values.\n        \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_all_values", "title": "<code>get_all_values</code>", "text": "<p>Get a list of all the possible values stored in the <code>Scheduler</code>.</p> <p>Returns:</p> Type Description <code>List[Optional[T]]</code> <p>A list of all the values stored in the <code>Scheduler</code>. This may contain None values.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n    Returns:\n        A list of all the values stored in the `Scheduler`. This may contain None values.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_current_value", "title": "<code>get_current_value</code>", "text": "<p>Fetch whichever of the <code>Scheduler</code>s elements is appropriate based on the current epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch.</p> required <p>Returns:</p> Type Description <code>Optional[T]</code> <p>The element from the Scheduler to be used at the given <code>epoch</code>. This value might be None.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n    Args:\n        epoch: The current epoch.\n    Returns:\n        The element from the Scheduler to be used at the given `epoch`. This value might be None.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_current_items", "title": "<code>get_current_items</code>", "text": "<p>Select items which should be executed for given mode and epoch.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[Union[T, Scheduler[T]]]</code> <p>A list of possible items or Schedulers of items to choose from.</p> required <code>run_modes</code> <code>Optional[Union[str, Iterable[str]]]</code> <p>The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of all modes will be returned.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>The desired execution epoch. If None, items across all epochs will be returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[T]</code> <p>The items which should be executed.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_items(items: Iterable[Union[T, Scheduler[T]]],\nrun_modes: Optional[Union[str, Iterable[str]]] = None,\nepoch: Optional[int] = None) -&gt; List[T]:\n\"\"\"Select items which should be executed for given mode and epoch.\n    Args:\n        items: A list of possible items or Schedulers of items to choose from.\n        run_modes: The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of\n            all modes will be returned.\n        epoch: The desired execution epoch. If None, items across all epochs will be returned.\n    Returns:\n        The items which should be executed.\n    \"\"\"\nselected_items = []\nrun_modes = to_set(run_modes)\nfor item in items:\nif isinstance(item, Scheduler):\nif epoch is None:\nitem = item.get_all_values()\nelse:\nitem = [item.get_current_value(epoch)]\nelse:\nitem = [item]\nfor item_ in item:\nif item_ and (not run_modes or not hasattr(item_, \"mode\") or not item_.mode\nor item_.mode.intersection(run_modes)):\nselected_items.append(item_)\nreturn selected_items\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_signature_epochs", "title": "<code>get_signature_epochs</code>", "text": "<p>Find all epochs of changes due to schedulers.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>List of items to scan from.</p> required <code>total_epochs</code> <code>int</code> <p>The maximum epoch number to consider when searching for signature epochs.</p> required <code>mode</code> <code>Optional[str]</code> <p>Current execution mode. If None, all execution modes will be considered.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>The epoch numbers of changes.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_signature_epochs(items: List[Any], total_epochs: int, mode: Optional[str] = None) -&gt; List[int]:\n\"\"\"Find all epochs of changes due to schedulers.\n    Args:\n        items: List of items to scan from.\n        total_epochs: The maximum epoch number to consider when searching for signature epochs.\n        mode: Current execution mode. If None, all execution modes will be considered.\n    Returns:\n        The epoch numbers of changes.\n    \"\"\"\nunique_configs = []\nsignature_epochs = []\nfor epoch in range(1, total_epochs + 1):\nepoch_config = get_current_items(items, run_modes=mode, epoch=epoch)\nif epoch_config not in unique_configs:\nunique_configs.append(epoch_config)\nsignature_epochs.append(epoch)\nreturn signature_epochs\n</code></pre>"}, {"location": "fastestimator/summary/history.html", "title": "history", "text": ""}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader", "title": "<code>HistoryReader</code>", "text": "<p>A class to read history information from the database.</p> <p>This class is intentionally not @traceable.</p> <p>This class should be used as as a context manager, for example:</p> <pre><code>with HistoryReader() as reader:\nreader.read_basic()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>class HistoryReader:\n\"\"\"A class to read history information from the database.\n    This class is intentionally not @traceable.\n    This class should be used as as a context manager, for example:\n    ```python\n    with HistoryReader() as reader:\n        reader.read_basic()\n    ```\n    Args:\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndef __init__(self, db_path: Optional[str] = None):\nself.db_path = db_path\nself.db = None  # sql.Connection\nself.response = None  # List[sql.Row]\ndef __enter__(self) -&gt; 'HistoryReader':\nself.db = connect(self.db_path)\nself.db.set_trace_callback(print)  # Show the query in case user wants to adapt it later\nreturn self\ndef __exit__(self, exc_type: Optional[Type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -&gt; None:\nself.db.close()\ndef read_basic(self,\nlimit: int = 10,\ninteractive: bool = False,\ninclude_args: bool = False,\nerrors: bool = False,\ninclude_pk: bool = False,\ninclude_features: bool = False,\ninclude_traces: bool = False,\ninclude_datasets: bool = False,\ninclude_pipeline: bool = False,\ninclude_network: bool = False,\nas_csv: bool = False) -&gt; None:\n\"\"\"Perform a pre-defined (and possibly interactive) set of sql selects against the history database.\n        Outputs will be printed to stdout.\n        Args:\n            limit: The maximum number of responses to look up.\n            interactive: Whether to run this function interactively, prompting the user for additional input along the\n                way. This enables things like error and log retrieval for individual experiments.\n            include_args: Whether to output the arguments used to run each experiment.\n            errors: Whether to filter the output to only include failed experiments, as well as including more\n                information about the specific errors that occurred.\n            include_pk: Whether to include the primary keys (experiment ids) of each history entry.\n            include_features: Whether to include the FE features that were employed by each training.\n            include_traces: Whether to include the traces that were used in each training.\n            include_datasets: Whether to include the dataset (classes) that were used in each training.\n            include_pipeline: Whether to include the pipeline ops that were used in each training.\n            include_network: Whether to include the network (post)processing ops that were used in each training.\n            as_csv: Whether to print the output as a csv rather than in a formatted table.\n        \"\"\"\n# Build the query string\nerror_select = \", errors.exc_type error\" if errors else ''\nerror_join = \"LEFT JOIN errors ON errors.fk = h.pk \" if errors else ''\nerror_where = \" WHERE h.status &lt;&gt; 'Completed' \" if errors else ''\nfeature_select = \", fg.features\" if include_features else ''\nfeature_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(feature, ', ') features FROM features f GROUP BY f.fk\" \\\n                       \") fg ON fg.fk = h.pk \" if include_features else ''\ndataset_select = \", dsg.datasets \" if include_datasets else ''\ndataset_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(dataset || ' (' || mode || ')', ', ') datasets \" \\\n                       \"FROM datasets ds GROUP BY ds.fk\" \\\n                       \") dsg ON dsg.fk = h.pk \" if include_datasets else ''\npipeline_select = \", pg.pipeline_ops\" if include_pipeline else ''\npipeline_join = \"LEFT JOIN (\" \\\n                        \"SELECT fk, GROUP_CONCAT(pipe_op, ', ') pipeline_ops FROM pipeline p GROUP BY p.fk\" \\\n                        \") pg ON pg.fk = h.pk \" if include_pipeline else ''\nnetwork_select = \", ng.network_ops, ppg.postprocessing_ops\" if include_network else ''\nnetwork_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(net_op, ', ') network_ops FROM network n GROUP BY n.fk\" \\\n                       \") ng ON ng.fk = h.pk \" \\\n                       \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(pp_op, ', ') postprocessing_ops FROM postprocess pp GROUP BY pp.fk\" \\\n                       \") ppg ON ppg.fk = h.pk \" if include_network else ''\ntrace_select = \", tg.traces \" if include_traces else ''\ntrace_join = \"LEFT JOIN (\" \\\n                     \"SELECT fk, GROUP_CONCAT(trace, ', ') traces FROM traces t GROUP BY t.fk\" \\\n                     \") tg ON tg.fk = h.pk \" if include_traces else ''\nquery = f\"SELECT h.*{error_select}{feature_select}{dataset_select}{pipeline_select}{network_select}\" \\\n                f\"{trace_select} FROM history h {error_join}{feature_join}{dataset_join}{pipeline_join}{network_join}\" \\\n                f\"{trace_join}{error_where}ORDER BY h.train_start DESC LIMIT (?)\"\n# We have to hide these after-the-fact since later process may require pk behind the scenes\nhide = []\nif not include_pk:\nhide.append('pk')\nif not include_args:\nhide.append('args')\nself.read_sql(query, args=[limit], hide_cols=hide, as_csv=as_csv, interactive=interactive)\ndef read_sql(self,\nquery: str,\nargs: Iterable[Any] = (),\nhide_cols: Iterable[str] = (),\nas_csv: bool = False,\ninteractive: bool = False) -&gt; None:\n\"\"\"Perform a (possibly interactive) sql query against the database.\n        Args:\n            query: The sql query to execute.\n            args: Any parameterized arguments to be inserted into the `query`.\n            hide_cols: Any columns to hide from the printed output.\n            as_csv: Whether to print the output in csv format or in table format.\n            interactive: Whether to run this function interactively, prompting the user for additional input along the\n                way. This enables things like error and log retrieval for individual experiments.\n        \"\"\"\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(query, args)\nself.response = cursor.fetchall()\nnames = [col[0] for col in cursor.description]\n# Build nice output table\ntable = PrettyTable()\ntable.field_names = names\nfor row in self.response:\ntable.add_row(row)\nfor col in hide_cols:\nif col in table.field_names:\ntable.del_column(col)\nif interactive:\ntable.add_autoindex()\nif as_csv:\nprint(table.get_csv_string())\nelse:\nprint(table)\nif interactive:\nwhile True:\ninp = input(\"\\033[93m{}\\033[00m\".format(\"Enter --help for available command details. Enter without an \"\n\"argument to re-print the current response. X to exit.\\n\"))\nif inp in ('X', 'x'):\nbreak\nif inp == \"\":\nprint(query)\nprint(table)\ncontinue\nnew_query = self._parse_input(inp)\nif new_query:\nreturn self.read_sql(new_query, hide_cols=hide_cols, as_csv=as_csv, interactive=interactive)\ndef _parse_input(self, inp: str) -&gt; Optional[str]:\n\"\"\"Take cli input and run it through command parsers to execute an appropriate subroutine.\n        Args:\n            inp: The cli input provided by an end user.\n        Returns:\n            The output (if any) of the appropriate sub-command after executing on the given input.\n        \"\"\"\nparser = argparse.ArgumentParser(allow_abbrev=False)\nsubparsers = parser.add_subparsers()\nsubparsers.required = True\nsubparsers.dest = 'cmd'\nself._configure_sql_parser(subparsers)\nself._configure_log_parser(subparsers)\nself._configure_err_parser(subparsers)\ntry:\nargs, unknown = parser.parse_known_args(inp.split())\nexcept SystemExit:\nreturn\nreturn args.func(vars(args), unknown)\ndef _configure_sql_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a sql parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_sql = subparsers.add_parser('sql',\ndescription='Provide a new sql query to be executed',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_sql.add_argument('query', metavar='&lt;Query&gt;', type=str, nargs='+', help=\"ex: sql SELECT * FROM history\")\np_sql.set_defaults(func=self._echo_sql)\ndef _configure_log_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a log parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_log = subparsers.add_parser(\n'log',\ndescription='Retrieve one or more output logs. This command requires '\n'that you currently have experiments selected.',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_log.add_argument('indices',\nmetavar='I',\ntype=int,\nnargs='+',\nhelp=\"Indices of experiments for which to print logs\")\np_log.add_argument(\n'--file',\nmetavar='F',\naction=SaveAction,\ndefault=False,\ndest='file',\nnargs='?',\nhelp='Whether to write the logs to disk. May be accompanied by a directory or filename into which to save \\\n                 the log(s). If none is specified then the ~/fastestimator_data directory will be used.')\np_log.set_defaults(func=self._fetch_logs)\ndef _configure_err_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add an error parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_err = subparsers.add_parser(\n'err',\ndescription='Retrieve one or more error tracebacks. This command requires '\n'that you currently have experiments selected.',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_err.add_argument('indices',\nmetavar='I',\ntype=int,\nnargs='+',\nhelp=\"Indices of experiments for which to print error tracebacks\")\np_err.set_defaults(func=self._fetch_errs)\n@staticmethod\ndef _echo_sql(args: Dict[str, Any], unknown: List[str]) -&gt; Optional[str]:\n\"\"\"A method to compile parsed user input back into a single sql query.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        Returns:\n            A single string containing the user sql query.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn None\nreturn \" \".join(args['query'])\ndef _fetch_logs(self, args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to collect and return a given set of logs from the database.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn\nsave = args['file']\nsave_path = None\nif save:\nsave_path = args['file_dir']\nif save_path is None:\nsave_path = os.path.join(str(Path.home()), 'fastestimator_data')\nsave = 'dir'\nprint(f\"Writing log(s) to {save_path}\")\nelse:\nsave = 'file'\nprint(f'Writing log to {save_path}')\nlogs = {}\nfor idx in args['indices']:\nselection = self.response[idx - 1]  # Auto index starts at 1\npk = selection['pk']\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(\"SELECT log FROM logs WHERE logs.fk = (?)\", [pk])\nlogs[idx] = cursor.fetchall()\nwith open(save_path, 'w') if save == 'file' else NonContext() as f:\nf = sys.stdout if f is None else f\nfor idx, log in logs.items():\nwith open(os.path.join(save_path, f\"{idx}.txt\"), 'w') if save == 'dir' else NonContext() as f1:\nf1 = f if f1 is None else f1\nif log:\nf1.write(f'\\n@@@@@@@@@@@ Log for Index {idx} @@@@@@@@@@@\\n\\n')\nf1.write(log[0]['log'])\nf1.write('\\n')\nelse:\nf1.write(f\"No logs found for Index {idx}\\n\")\ndef _fetch_errs(self, args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to collect and return a given set of error logs from the database.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn\nfor idx in args['indices']:\nselection = self.response[idx - 1]  # Auto index starts at 1\npk = selection['pk']\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(\"SELECT exc_tb FROM errors WHERE errors.fk = (?)\", [pk])\nerr = cursor.fetchall()\nif err:\nprint(f'@@@@@@@@@@@ Traceback for Index {idx} @@@@@@@@@@@')\nprint(err[0]['exc_tb'])\nelse:\nprint(f\"No error traceback found for Index {idx}\")\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader.read_basic", "title": "<code>read_basic</code>", "text": "<p>Perform a pre-defined (and possibly interactive) set of sql selects against the history database.</p> <p>Outputs will be printed to stdout.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of responses to look up.</p> <code>10</code> <code>interactive</code> <code>bool</code> <p>Whether to run this function interactively, prompting the user for additional input along the way. This enables things like error and log retrieval for individual experiments.</p> <code>False</code> <code>include_args</code> <code>bool</code> <p>Whether to output the arguments used to run each experiment.</p> <code>False</code> <code>errors</code> <code>bool</code> <p>Whether to filter the output to only include failed experiments, as well as including more information about the specific errors that occurred.</p> <code>False</code> <code>include_pk</code> <code>bool</code> <p>Whether to include the primary keys (experiment ids) of each history entry.</p> <code>False</code> <code>include_features</code> <code>bool</code> <p>Whether to include the FE features that were employed by each training.</p> <code>False</code> <code>include_traces</code> <code>bool</code> <p>Whether to include the traces that were used in each training.</p> <code>False</code> <code>include_datasets</code> <code>bool</code> <p>Whether to include the dataset (classes) that were used in each training.</p> <code>False</code> <code>include_pipeline</code> <code>bool</code> <p>Whether to include the pipeline ops that were used in each training.</p> <code>False</code> <code>include_network</code> <code>bool</code> <p>Whether to include the network (post)processing ops that were used in each training.</p> <code>False</code> <code>as_csv</code> <code>bool</code> <p>Whether to print the output as a csv rather than in a formatted table.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def read_basic(self,\nlimit: int = 10,\ninteractive: bool = False,\ninclude_args: bool = False,\nerrors: bool = False,\ninclude_pk: bool = False,\ninclude_features: bool = False,\ninclude_traces: bool = False,\ninclude_datasets: bool = False,\ninclude_pipeline: bool = False,\ninclude_network: bool = False,\nas_csv: bool = False) -&gt; None:\n\"\"\"Perform a pre-defined (and possibly interactive) set of sql selects against the history database.\n    Outputs will be printed to stdout.\n    Args:\n        limit: The maximum number of responses to look up.\n        interactive: Whether to run this function interactively, prompting the user for additional input along the\n            way. This enables things like error and log retrieval for individual experiments.\n        include_args: Whether to output the arguments used to run each experiment.\n        errors: Whether to filter the output to only include failed experiments, as well as including more\n            information about the specific errors that occurred.\n        include_pk: Whether to include the primary keys (experiment ids) of each history entry.\n        include_features: Whether to include the FE features that were employed by each training.\n        include_traces: Whether to include the traces that were used in each training.\n        include_datasets: Whether to include the dataset (classes) that were used in each training.\n        include_pipeline: Whether to include the pipeline ops that were used in each training.\n        include_network: Whether to include the network (post)processing ops that were used in each training.\n        as_csv: Whether to print the output as a csv rather than in a formatted table.\n    \"\"\"\n# Build the query string\nerror_select = \", errors.exc_type error\" if errors else ''\nerror_join = \"LEFT JOIN errors ON errors.fk = h.pk \" if errors else ''\nerror_where = \" WHERE h.status &lt;&gt; 'Completed' \" if errors else ''\nfeature_select = \", fg.features\" if include_features else ''\nfeature_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(feature, ', ') features FROM features f GROUP BY f.fk\" \\\n                   \") fg ON fg.fk = h.pk \" if include_features else ''\ndataset_select = \", dsg.datasets \" if include_datasets else ''\ndataset_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(dataset || ' (' || mode || ')', ', ') datasets \" \\\n                   \"FROM datasets ds GROUP BY ds.fk\" \\\n                   \") dsg ON dsg.fk = h.pk \" if include_datasets else ''\npipeline_select = \", pg.pipeline_ops\" if include_pipeline else ''\npipeline_join = \"LEFT JOIN (\" \\\n                    \"SELECT fk, GROUP_CONCAT(pipe_op, ', ') pipeline_ops FROM pipeline p GROUP BY p.fk\" \\\n                    \") pg ON pg.fk = h.pk \" if include_pipeline else ''\nnetwork_select = \", ng.network_ops, ppg.postprocessing_ops\" if include_network else ''\nnetwork_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(net_op, ', ') network_ops FROM network n GROUP BY n.fk\" \\\n                   \") ng ON ng.fk = h.pk \" \\\n                   \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(pp_op, ', ') postprocessing_ops FROM postprocess pp GROUP BY pp.fk\" \\\n                   \") ppg ON ppg.fk = h.pk \" if include_network else ''\ntrace_select = \", tg.traces \" if include_traces else ''\ntrace_join = \"LEFT JOIN (\" \\\n                 \"SELECT fk, GROUP_CONCAT(trace, ', ') traces FROM traces t GROUP BY t.fk\" \\\n                 \") tg ON tg.fk = h.pk \" if include_traces else ''\nquery = f\"SELECT h.*{error_select}{feature_select}{dataset_select}{pipeline_select}{network_select}\" \\\n            f\"{trace_select} FROM history h {error_join}{feature_join}{dataset_join}{pipeline_join}{network_join}\" \\\n            f\"{trace_join}{error_where}ORDER BY h.train_start DESC LIMIT (?)\"\n# We have to hide these after-the-fact since later process may require pk behind the scenes\nhide = []\nif not include_pk:\nhide.append('pk')\nif not include_args:\nhide.append('args')\nself.read_sql(query, args=[limit], hide_cols=hide, as_csv=as_csv, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader.read_sql", "title": "<code>read_sql</code>", "text": "<p>Perform a (possibly interactive) sql query against the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The sql query to execute.</p> required <code>args</code> <code>Iterable[Any]</code> <p>Any parameterized arguments to be inserted into the <code>query</code>.</p> <code>()</code> <code>hide_cols</code> <code>Iterable[str]</code> <p>Any columns to hide from the printed output.</p> <code>()</code> <code>as_csv</code> <code>bool</code> <p>Whether to print the output in csv format or in table format.</p> <code>False</code> <code>interactive</code> <code>bool</code> <p>Whether to run this function interactively, prompting the user for additional input along the way. This enables things like error and log retrieval for individual experiments.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def read_sql(self,\nquery: str,\nargs: Iterable[Any] = (),\nhide_cols: Iterable[str] = (),\nas_csv: bool = False,\ninteractive: bool = False) -&gt; None:\n\"\"\"Perform a (possibly interactive) sql query against the database.\n    Args:\n        query: The sql query to execute.\n        args: Any parameterized arguments to be inserted into the `query`.\n        hide_cols: Any columns to hide from the printed output.\n        as_csv: Whether to print the output in csv format or in table format.\n        interactive: Whether to run this function interactively, prompting the user for additional input along the\n            way. This enables things like error and log retrieval for individual experiments.\n    \"\"\"\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(query, args)\nself.response = cursor.fetchall()\nnames = [col[0] for col in cursor.description]\n# Build nice output table\ntable = PrettyTable()\ntable.field_names = names\nfor row in self.response:\ntable.add_row(row)\nfor col in hide_cols:\nif col in table.field_names:\ntable.del_column(col)\nif interactive:\ntable.add_autoindex()\nif as_csv:\nprint(table.get_csv_string())\nelse:\nprint(table)\nif interactive:\nwhile True:\ninp = input(\"\\033[93m{}\\033[00m\".format(\"Enter --help for available command details. Enter without an \"\n\"argument to re-print the current response. X to exit.\\n\"))\nif inp in ('X', 'x'):\nbreak\nif inp == \"\":\nprint(query)\nprint(table)\ncontinue\nnew_query = self._parse_input(inp)\nif new_query:\nreturn self.read_sql(new_query, hide_cols=hide_cols, as_csv=as_csv, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryRecorder", "title": "<code>HistoryRecorder</code>", "text": "<p>A class to record what you're doing.</p> <p>This class is intentionally not @traceable.</p> <p>It will capture output logs, exceptions, and general information about the training / environment. This class should be used as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>System</code> <p>The system object corresponding to the current training.</p> required <code>est_path</code> <code>str</code> <p>The path to the file responsible for creating the current estimator (this is for bookkeeping, it can technically be any string).</p> required <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>class HistoryRecorder:\n\"\"\"A class to record what you're doing.\n    This class is intentionally not @traceable.\n    It will capture output logs, exceptions, and general information about the training / environment. This class should\n    be used as a context manager.\n    Args:\n        system: The system object corresponding to the current training.\n        est_path: The path to the file responsible for creating the current estimator (this is for bookkeeping, it can\n            technically be any string).\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndef __init__(self, system: System, est_path: str, db_path: Optional[str] = None):\n# Prepare db adapters\nsql.register_adapter(bool, int)\nsql.register_converter(\"BOOL\", lambda v: bool(int(v)))\nsql.register_adapter(list, str)\nsql.register_converter(\"LIST[STR]\", lambda v: parse_string_to_python(v))\n# Prepare variables\nself.filename = os.path.basename(est_path)\nself.db_path = db_path if db_path else os.path.join(str(Path.home()), 'fastestimator_data', 'history.db')\nself.system = system\nself.db = None\nself.ident = (multiprocessing.current_process().pid, threading.get_ident())\nself.pk = None\nself.stdout = None\ndef __enter__(self) -&gt; None:\nself.db = connect(self.db_path)\nself.ident = (multiprocessing.current_process().pid, threading.get_ident())\nself.pk = self.system.exp_id  # This might be changed later by RestoreWizard. See the _check_for_restart method\n# Check whether an entry for this pk already exists, for example if a user ran .fit() and is now running .test()\nwith closing(self.db.cursor()) as cursor:\nexists = cursor.execute(\"SELECT pk FROM history WHERE pk = (?)\", [self.pk])\nexists = exists.fetchall()\nif not exists:\nself.db.execute(\n_MAKE_HIST_ENTRY,\n{\n'pk': self.pk,\n'fname': self.filename,\n'status': 'Launched',\n'exp': self.system.summary.name,\n'args': sys.argv[1:],\n'version': sys.modules['fastestimator'].__version__,\n'start': datetime.now(),\n'gpus': torch.cuda.device_count(),\n'cpus': os.cpu_count(),\n'workers': self.system.pipeline.num_process\n})\nself.db.executemany(_MAKE_FEAT_ENTRY, self._get_features_in_use())\nself.db.executemany(_MAKE_DS_ENTRY, self._get_datasets_in_use())\nself.db.executemany(_MAKE_PIPE_ENTRY, self._get_items_in_use(self.system.pipeline.ops))\nself.db.executemany(_MAKE_NET_ENTRY, self._get_items_in_use(self.system.network.ops))\nself.db.executemany(_MAKE_TRACE_ENTRY, self._get_items_in_use(self.system.traces))\nself.db.execute(_MAKE_LOG_ENTRY, {'log': '', 'fk': self.pk})\nself.db.commit()\n# Take over the output logging\nself.stdout = sys.stdout\nsys.stdout = self\ndef __exit__(self, exc_type: Optional[Type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -&gt; None:\nself._check_for_restart()\nself.flush()\nsys.stdout = self.stdout\n# In test mode only overwrite the train_end time if it hasn't already been set\ntrain_end_query = \"(?)\" if self.system.mode in ('train', 'eval') else \"IFNULL(train_end, (?))\"\nquery = f\"UPDATE history set train_end = {train_end_query}, status = (?) WHERE pk = (?)\"\nself.db.execute(\nquery,\n[\ndatetime.now(),\n\"Completed\" if exc_type is None else \"Aborted\" if exc_type == KeyboardInterrupt else \"Failed\",\nself.pk\n])\nif exc_type is not None:\nargs = {\n'type': exc_type.__name__,\n'tb': \"\\n\".join(traceback.format_exception(exc_type, exc_val, exc_tb)),\n'fk': self.pk\n}\nself.db.execute(_MAKE_ERR_ENTRY_P1, args)\nself.db.execute(_MAKE_ERR_ENTRY_P2, args)\nself.db.commit()\nself._apply_limits()\nself.db.close()\ndef _check_for_restart(self) -&gt; None:\n\"\"\"Determine whether a training has been restarted via RestoreWizard. If so, update the history accordingly.\n        If RestoreWizard has been invoked, then the system exp_id will have changed since self.pk was initialized. This\n        method will do related bookkeeping, and then swap self.pk for the restored id.\n        \"\"\"\nif self.pk == self.system.exp_id:\nreturn\n# RestoreWizard reset the system, we are continuing an old training rather than starting a new one\n# First make sure the old entry is still available to edit\nwith closing(self.db.cursor()) as cursor:\nexists = cursor.execute(\"SELECT pk FROM history WHERE pk = (?)\", [self.system.exp_id])\nexists = exists.fetchall()\nif exists:\n# If we still have the original entry, we will delete our new one and update the old instead\nself.db.execute(\"DELETE FROM history WHERE pk = (?)\", [self.pk])\nelse:\n# The old record doesn't exist, so we will use the new record instead\nself.db.execute(\"UPDATE history SET pk = (?) WHERE pk = (?)\", [self.system.exp_id, self.pk])\nself.pk = self.system.exp_id\nself.db.execute(\"UPDATE history SET n_restarts = n_restarts + 1 WHERE pk = (?)\", [self.pk])\nself.db.commit()\ndef _get_features_in_use(self) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which interesting FE features are being used by the current training.\n        Returns:\n            A list of entries which can be written into the 'features' db table.\n        \"\"\"\nfeatures = []\nif sys.modules['fastestimator'].fe_deterministic_seed is not None:\nfeatures.append({'feature': 'Deterministic', 'fk': self.pk})\nif mixed_precision.global_policy().compute_dtype == 'float16':\nfeatures.append({'feature': 'MixedPrecision', 'fk': self.pk})\nreturn features\ndef _get_datasets_in_use(self) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which datasets are being used by the current training.\n        Returns:\n            A list of entries which can be written into the 'datasets' db table.\n        \"\"\"\ndatasets = []\nfor mode, ds in self.system.pipeline.data.items():\ndatasets.append({'mode': mode, 'dataset': type(ds).__name__, 'fk': self.pk})\nreturn datasets\ndef _get_items_in_use(self, items: List[Any]) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which objects are being used by the current training.\n        Args:\n            items: A list of Schedulers, Ops, and/or traces which are being used by the system.\n        Returns:\n            The elements from `items` converted into database-ready entries.\n        \"\"\"\nops = []\nfor op in items:\nop_list = [op]\nif isinstance(op, Scheduler):\nop_list = list(filter(lambda x: x is not None, op.get_all_values()))\nop_list.append(op)  # Put scheduler in too so that usage can be tracked too\nops.extend([{'op': type(elem).__name__, 'fk': self.pk} for elem in op_list])\nreturn ops\ndef _apply_limits(self) -&gt; None:\n\"\"\"Remove old history and/or log entries if they exceed the limits defined in the settings table.\n        \"\"\"\nself.db.execute(\"DELETE FROM history WHERE train_start &lt;= (\"\n\"SELECT train_start FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (\"\n\"SELECT n_keep FROM settings WHERE pk = 0))\")\nself.db.execute(\"DELETE FROM logs WHERE fk IN (\"\n\"SELECT pk FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (\"\n\"SELECT n_keep_logs FROM settings WHERE pk = 0))\")\nself.db.commit()  # Have to commit before vacuuming\nif sum(int(digit) for digit in str(abs(self.pk))) % 10 == 0:\n# 10% of time do a vacuum (expensive). We don't use random.randint here due to deterministic training. Also,\n# don't use pk directly because last digit is not uniformly distributed.\nself.db.execute(\"PRAGMA VACUUM;\")\nself.db.commit()\nelse:\n# Otherwise do a less costly optimize\nself.db.execute(\"PRAGMA optimize;\")\nself.db.commit()\nself.db.close()\ndef write(self, output: str) -&gt; None:\nself.stdout.write(output)\nif multiprocessing.current_process().pid == self.ident[0] and threading.get_ident() == self.ident[1]:\n# Flush can also get invoked by pipeline multi-processing, but db should only be accessed by main thread.\n# This can happen, for example, when pipeline prints a warning that a certain key is unused and will be\n# dropped.\nself._check_for_restart()  # Check here instead of just waiting for __exit__ in case system powers off later\nself.db.execute('UPDATE logs SET log = log || (?) WHERE fk = (?)', [output, self.pk])\nself.db.commit()\ndef flush(self) -&gt; None:\nself.stdout.flush()\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.connect", "title": "<code>connect</code>", "text": "<p>Open a connection to a sqlite database, creating one if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>Optional[str]</code> <p>The path to the database file. Or None to default to ~/fastestimator_data/history.db</p> <code>None</code> <p>Returns:</p> Type Description <code>sql.Connection</code> <p>An open connection to the database, with schema instantiated and foreign keys enabled.</p> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def connect(db_path: Optional[str] = None) -&gt; sql.Connection:\n\"\"\"Open a connection to a sqlite database, creating one if it does not already exist.\n    Args:\n        db_path: The path to the database file. Or None to default to ~/fastestimator_data/history.db\n    Returns:\n        An open connection to the database, with schema instantiated and foreign keys enabled.\n    \"\"\"\nif db_path is None:\ndb_path = os.path.join(str(Path.home()), 'fastestimator_data', 'history.db')\nif db_path != ':memory:':  # This is a reserved keyword to create an in-memory database\nos.makedirs(os.path.dirname(db_path), exist_ok=True)  # Make sure folders exist before creating disk file\nconnection = sql.connect(db_path, detect_types=sql.PARSE_DECLTYPES | sql.PARSE_COLNAMES)\nconnection.execute(\"PRAGMA foreign_keys = 1\")  # Enable FK constraints\nconnection.row_factory = sql.Row  # Get nice query return objects\n# Build the schema if it doesn't exist\nconnection.execute(_MAKE_HIST_TABLE)\nconnection.execute(_MAKE_FEAT_TABLE)\nconnection.execute(_MAKE_DS_TABLE)\nconnection.execute(_MAKE_PIPELINE_TABLE)\nconnection.execute(_MAKE_NETWORK_TABLE)\nconnection.execute(_MAKE_POST_PROCESS_TABLE)\nconnection.execute(_MAKE_TRACE_TABLE)\nconnection.execute(_MAKE_ERR_TABLE)\nconnection.execute(_MAKE_LOG_TABLE)\nconnection.execute(_MAKE_SETTINGS_TABLE)\nconnection.execute(_MAKE_SETTINGS_ENTRY)\nconnection.commit()\nreturn connection\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.delete", "title": "<code>delete</code>", "text": "<p>Remove history entries from a database.</p> <p>This will also remove associated data such as logs due to foreign key constraints.</p> <p>Parameters:</p> Name Type Description Default <code>n_keep</code> <code>int</code> <p>How many history entries to keep.</p> <code>20</code> <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def delete(n_keep: int = 20, db_path: Optional[str] = None) -&gt; None:\n\"\"\"Remove history entries from a database.\n    This will also remove associated data such as logs due to foreign key constraints.\n    Args:\n        n_keep: How many history entries to keep.\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndb = connect(db_path)\ndb.execute(\n\"DELETE FROM history WHERE train_start &lt;= (\"\n\"SELECT train_start FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (?))\", [n_keep])\ndb.commit()  # Can't vacuum while there are uncommitted changes\ndb.execute(\"VACUUM\")  # Free the memory\ndb.commit()\ndb.close()\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.update_settings", "title": "<code>update_settings</code>", "text": "<p>Update the history database settings.</p> <p>Updated settings will be enforced the next time a training or delete operation is called.</p> <p>Parameters:</p> Name Type Description Default <code>n_keep</code> <code>Optional[int]</code> <p>How many history entries should be retained.</p> <code>None</code> <code>n_keep_logs</code> <code>Optional[int]</code> <p>How many logs should be retained. This value should be &lt;= <code>n_keep</code>.</p> <code>None</code> <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def update_settings(n_keep: Optional[int] = None, n_keep_logs: Optional[int] = None,\ndb_path: Optional[str] = None) -&gt; None:\n\"\"\"Update the history database settings.\n    Updated settings will be enforced the next time a training or delete operation is called.\n    Args:\n        n_keep: How many history entries should be retained.\n        n_keep_logs: How many logs should be retained. This value should be &lt;= `n_keep`.\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndb = connect(db_path)\n# Ensure limits are non-negative\nif n_keep:\nn_keep = max(n_keep, 0)\nif n_keep_logs:\nn_keep_logs = max(n_keep_logs, 0)\n# Perform the update\nif n_keep is not None and n_keep_logs is not None:\ndb.execute(\"UPDATE settings SET n_keep = :keep, n_keep_logs = MIN(:keep, :logs) WHERE pk = 0\", {\n'keep': n_keep, 'logs': n_keep_logs\n})\nelif n_keep is not None:\ndb.execute(\"UPDATE settings SET n_keep = :keep, n_keep_logs = MIN(n_keep_logs, :keep) WHERE pk = 0\",\n{'keep': n_keep})\nelif n_keep_logs is not None:\ndb.execute(\"UPDATE settings SET n_keep_logs = MIN(n_keep, (?)) WHERE pk = 0\", [n_keep_logs])\ndb.commit()\nwith closing(db.cursor()) as cursor:\ncursor.execute(\"SELECT * FROM settings\")\nresponse = from_db_cursor(cursor)\n# Hide implementation details from end user\nresponse.del_column('pk')\nresponse.del_column('schema_version')\nprint(response)\ndb.close()\n</code></pre>"}, {"location": "fastestimator/summary/summary.html", "title": "summary", "text": ""}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary", "title": "<code>Summary</code>", "text": "<p>A summary object that records training history.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the experiment. If None then experiment results will be ignored.</p> required <code>system_config</code> <code>Optional[List[FeSummaryTable]]</code> <p>A description of the initialization parameters defining the estimator associated with this experiment.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>class Summary:\n\"\"\"A summary object that records training history.\n    This class is intentionally not @traceable.\n    Args:\n        name: Name of the experiment. If None then experiment results will be ignored.\n        system_config: A description of the initialization parameters defining the estimator associated with this\n            experiment.\n    \"\"\"\ndef __init__(self, name: Optional[str], system_config: Optional[List[FeSummaryTable]] = None) -&gt; None:\nself.name = name\nself.system_config = system_config\nself.history = defaultdict(lambda: defaultdict(dict))  # {mode: {key: {step: value}}}\ndef merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n        Args:\n            other: Other `summary` object to be merged.\n        \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\ndef __bool__(self) -&gt; bool:\n\"\"\"Whether training history should be recorded.\n        Returns:\n            True iff this `Summary` has a non-None name.\n        \"\"\"\nreturn bool(self.name)\ndef __getstate__(self):\n\"\"\"Get a representation of the state of this object.\n        This method is invoked by pickle.\n        Returns:\n            The information to be recorded by a pickle summary of this object.\n        \"\"\"\nstate = self.__dict__.copy()\ndel state['system_config']\nstate['history'] = dict(state['history'])\nreturn state\ndef __setstate__(self, state: Dict[str, Any]) -&gt; None:\n\"\"\"Set this objects internal state from a dictionary of variables.\n        This method is invoked by pickle.\n        Args:\n            state: The saved state to be used by this object.\n        \"\"\"\nhistory = defaultdict(lambda: defaultdict(dict))\nhistory.update(state.get('history', {}))\nstate['history'] = history\nself.__dict__.update(state)\n</code></pre>"}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary.merge", "title": "<code>merge</code>", "text": "<p>Merge another <code>Summary</code> into this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Summary</code> <p>Other <code>summary</code> object to be merged.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>def merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n    Args:\n        other: Other `summary` object to be merged.\n    \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\n</code></pre>"}, {"location": "fastestimator/summary/system.html", "title": "system", "text": ""}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System", "title": "<code>System</code>", "text": "<p>A class which tracks state information while the fe.Estimator is running.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>BaseNetwork</code> <p>The network instance being used by the current fe.Estimator.</p> required <code>pipeline</code> <code>Pipeline</code> <p>The pipeline instance being used by the current fe.Estimator.</p> required <code>traces</code> <code>List[Union[Trace, Scheduler[Trace]]]</code> <p>The traces provided to the current fe.Estimator.</p> required <code>mode</code> <code>Optional[str]</code> <p>The current execution mode (or None for warmup).</p> <code>None</code> <code>num_devices</code> <code>int</code> <p>How many GPUs are available for training.</p> <code>torch.cuda.device_count()</code> <code>log_steps</code> <code>Optional[int]</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>None</code> <code>total_epochs</code> <code>int</code> <p>How many epochs training is expected to run for.</p> <code>0</code> <code>max_train_steps_per_epoch</code> <code>Optional[int]</code> <p>Whether training epochs will be cut short after N steps (or use None if they will run to completion)</p> <code>None</code> <code>system_config</code> <code>Optional[List[FeSummaryTable]]</code> <p>A description of the initialization parameters defining the associated estimator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>mode</code> <p>What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.</p> <code>exp_id</code> <code>int</code> <p>A unique identifier for current training experiment.</p> <code>global_step</code> <code>Optional[int]</code> <p>How many training steps have elapsed.</p> <code>num_devices</code> <p>How many GPUs are available for training.</p> <code>log_steps</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>total_epochs</code> <p>How many epochs training is expected to run for.</p> <code>epoch_idx</code> <code>Optional[int]</code> <p>The current epoch index for the training (starting from 1).</p> <code>batch_idx</code> <p>The current batch index within an epoch (starting from 1).</p> <code>stop_training</code> <p>A flag to signal that training should abort.</p> <code>network</code> <p>A reference to the network being used.</p> <code>pipeline</code> <p>A reference to the pipeline being used.</p> <code>traces</code> <p>The traces being used.</p> <code>max_train_steps_per_epoch</code> <p>Training will complete after n steps even if loader is not yet exhausted.</p> <code>max_eval_steps_per_epoch</code> <p>Evaluation will complete after n steps even if loader is not yet exhausted.</p> <code>summary</code> <p>An object to write experiment results to.</p> <code>experiment_time</code> <p>A timestamp indicating when this model was trained.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>class System:\n\"\"\"A class which tracks state information while the fe.Estimator is running.\n    This class is intentionally not @traceable.\n    Args:\n        network: The network instance being used by the current fe.Estimator.\n        pipeline: The pipeline instance being used by the current fe.Estimator.\n        traces: The traces provided to the current fe.Estimator.\n        mode: The current execution mode (or None for warmup).\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        max_train_steps_per_epoch: Whether training epochs will be cut short after N steps (or use None if they will run\n            to completion)\n        system_config: A description of the initialization parameters defining the associated estimator.\n    Attributes:\n        mode: What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.\n        exp_id: A unique identifier for current training experiment.\n        global_step: How many training steps have elapsed.\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        epoch_idx: The current epoch index for the training (starting from 1).\n        batch_idx: The current batch index within an epoch (starting from 1).\n        stop_training: A flag to signal that training should abort.\n        network: A reference to the network being used.\n        pipeline: A reference to the pipeline being used.\n        traces: The traces being used.\n        max_train_steps_per_epoch: Training will complete after n steps even if loader is not yet exhausted.\n        max_eval_steps_per_epoch: Evaluation will complete after n steps even if loader is not yet exhausted.\n        summary: An object to write experiment results to.\n        experiment_time: A timestamp indicating when this model was trained.\n    \"\"\"\nmode: Optional[str]\nexp_id: int\nglobal_step: Optional[int]\nnum_devices: int\nlog_steps: Optional[int]\ntotal_epochs: int\nepoch_idx: Optional[int]\nbatch_idx: Optional[int]\nstop_training: bool\nnetwork: BaseNetwork\npipeline: Pipeline\ntraces: List[Union['Trace', Scheduler['Trace']]]\nmax_train_steps_per_epoch: Optional[int]\nmax_eval_steps_per_epoch: Optional[int]\nsummary: Summary\nexperiment_time: str\ndef __init__(self,\nnetwork: BaseNetwork,\npipeline: Pipeline,\ntraces: List[Union['Trace', Scheduler['Trace']]],\nmode: Optional[str] = None,\nnum_devices: int = torch.cuda.device_count(),\nlog_steps: Optional[int] = None,\ntotal_epochs: int = 0,\nmax_train_steps_per_epoch: Optional[int] = None,\nmax_eval_steps_per_epoch: Optional[int] = None,\nsystem_config: Optional[List[FeSummaryTable]] = None) -&gt; None:\nself.network = network\nself.pipeline = pipeline\nself.traces = traces\nself.mode = mode\nself.num_devices = num_devices\nself.log_steps = log_steps\nself.total_epochs = total_epochs\nself.batch_idx = None\nself.max_train_steps_per_epoch = max_train_steps_per_epoch\nself.max_eval_steps_per_epoch = max_eval_steps_per_epoch\nself.stop_training = False\nself.summary = Summary(None, system_config)\nself.experiment_time = \"\"\nself._initialize_state()\ndef _initialize_state(self) -&gt; None:\n\"\"\"Initialize the training state.\n        \"\"\"\nself.global_step = None\nself.epoch_idx = 0\n# Get a 64 bit random id related to current time\nself.exp_id = int.from_bytes(uuid.uuid1().bytes, byteorder='big', signed=True) &gt;&gt; 64\ndef update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n        \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\ndef update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n        \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\ndef reset(self, summary_name: Optional[str] = None, system_config: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n        Args:\n            summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n            system_config: A description of the initialization parameters defining the associated estimator.\n        \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name, system_config)\ndef reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n        Args:\n            summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n        \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\ndef write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n        Args:\n            key: The key to write into the summary object.\n            value: The value to write into the summary object.\n        \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\ndef save_state(self, save_dir: str) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            save_dir: The directory into which to save the state\n        \"\"\"\nos.makedirs(save_dir, exist_ok=True)\n# Start with the high-level info. We could use pickle for this but having it human readable is nice.\nstate = {key: value for key, value in self.__dict__.items() if is_restorable(value)[0]}\nwith open(os.path.join(save_dir, 'system.json'), 'w') as fp:\njson.dump(state, fp, indent=4)\n# Save all of the models / optimizer states\nfor model in self.network.models:\nsave_model(model, save_dir=save_dir, save_optimizer=True)\n# Save everything else\nobjects = {\n'summary': self.summary,\n'traces': [trace.__getstate__() if hasattr(trace, '__getstate__') else {} for trace in self.traces],\n'tops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.ops],\n'nops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.pipeline.ops],\n'ds':\n{key: value.__getstate__()\nfor key, value in self.pipeline.data.items() if hasattr(value, '__getstate__')}\n}\nwith open(os.path.join(save_dir, 'objects.pkl'), 'wb') as file:\npickle.dump(objects, file)\ndef load_state(self, load_dir: str) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            load_dir: The directory from which to reload the state.\n        Raises:\n            FileNotFoundError: If necessary files can not be found.\n        \"\"\"\n# Reload the high-level system information\nsystem_path = os.path.join(load_dir, 'system.json')\nif not os.path.exists(system_path):\nraise FileNotFoundError(f\"Could not find system summary file at {system_path}\")\nwith open(system_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# Reload the models\nfor model in self.network.models:\nself._load_model(model, load_dir)\n# Reload everything else\nobjects_path = os.path.join(load_dir, 'objects.pkl')\nif not os.path.exists(objects_path):\nraise FileNotFoundError(f\"Could not find the objects summary file at {objects_path}\")\nwith open(objects_path, 'rb') as file:\nobjects = pickle.load(file)\nself.summary.__dict__.update(objects['summary'].__dict__)\nself._load_list(objects, 'traces', self.traces)\nself._load_list(objects, 'tops', self.network.ops)\nself._load_list(objects, 'nops', self.pipeline.ops)\nself._load_dict(objects, 'ds', self.pipeline.data)\n@staticmethod\ndef _load_model(model: Model, base_path: str) -&gt; None:\n\"\"\"Load model and optimizer weights from disk.\n        Args:\n            model: The model to be loaded.\n            base_path: The folder where the model should be located.\n        Raises:\n            ValueError: If the model is of an unknown type.\n            FileNotFoundError: If the model weights or optimizer state is missing.\n        \"\"\"\nif isinstance(model, tf.keras.Model):\nmodel_ext, optimizer_ext = 'h5', 'pkl'\nelif isinstance(model, torch.nn.Module):\nmodel_ext, optimizer_ext = 'pt', 'pt'\nelse:\nraise ValueError(f\"Unknown model type: {type(model)}\")\nweights_path = os.path.join(base_path, f\"{model.model_name}.{model_ext}\")\nif not os.path.exists(weights_path):\nraise FileNotFoundError(f\"Cannot find model weights file at {weights_path}\")\noptimizer_path = os.path.join(base_path, f\"{model.model_name}_opt.{optimizer_ext}\")\nif not os.path.exists(optimizer_path):\nraise FileNotFoundError(f\"Cannot find model optimizer file at {optimizer_path}\")\nload_model(model, weights_path=weights_path, load_optimizer=True)\n@staticmethod\ndef _load_list(states: Dict[str, Any], state_key: str, in_memory_objects: List[Any]) -&gt; None:\n\"\"\"Load a list of pickled states from the disk.\n        Args:\n            states: The states to be restored.\n            state_key: Which state to select from the dictionary.\n            in_memory_objects: The existing in memory objects to be updated.\n        Raises:\n            ValueError: If the number of saved states does not match the number of in-memory objects.\n        \"\"\"\nstates = states[state_key]\nif not isinstance(states, list):\nraise ValueError(f\"Expected {state_key} to contain a list, but found a {type(states)}\")\nif len(states) != len(in_memory_objects):\nraise ValueError(\"Expected saved {} to contain {} objects, but found {} instead\".format(\nstate_key, len(in_memory_objects), len(states)))\nfor obj, state in zip(in_memory_objects, states):\nif hasattr(obj, '__setstate__'):\nobj.__setstate__(state)\nelif hasattr(obj, '__dict__'):\nobj.__dict__.update(state)\nelse:\n# Might be a None or something else that can't be updated\npass\n@staticmethod\ndef _load_dict(states: Dict[str, Any], state_key: str, in_memory_objects: Dict[Any, Any]) -&gt; None:\n\"\"\"Load a dictionary of pickled states from the disk.\n        Args:\n            states: The states to be restored.\n            state_key: Which state to select from the dictionary.\n            in_memory_objects: The existing in memory objects to be updated.\n        Raises:\n            ValueError: If the configuration of saved states does not match the number of in-memory objects.\n            FileNotFoundError: If the desired state file cannot be found.\n        \"\"\"\nstates = states[state_key]\nif not isinstance(states, dict):\nraise ValueError(f\"Expected {state_key} to contain a dict, but found a {type(states)}\")\n# Note that not being a subset is different from being a superset\nif not states.keys() &lt;= in_memory_objects.keys():\nraise ValueError(\"Saved {} contained unexpected keys: {}\".format(state_key,\nstates.keys() - in_memory_objects.keys()))\nfor key, state in states.items():\nobj = in_memory_objects[key]\nif hasattr(obj, '__setstate__'):\nobj.__setstate__(state)\nelif hasattr(obj, '__dict__'):\nobj.__dict__.update(state)\nelse:\n# Might be a None or something else that can't be updated\npass\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.load_state", "title": "<code>load_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory from which to reload the state.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If necessary files can not be found.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def load_state(self, load_dir: str) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        load_dir: The directory from which to reload the state.\n    Raises:\n        FileNotFoundError: If necessary files can not be found.\n    \"\"\"\n# Reload the high-level system information\nsystem_path = os.path.join(load_dir, 'system.json')\nif not os.path.exists(system_path):\nraise FileNotFoundError(f\"Could not find system summary file at {system_path}\")\nwith open(system_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# Reload the models\nfor model in self.network.models:\nself._load_model(model, load_dir)\n# Reload everything else\nobjects_path = os.path.join(load_dir, 'objects.pkl')\nif not os.path.exists(objects_path):\nraise FileNotFoundError(f\"Could not find the objects summary file at {objects_path}\")\nwith open(objects_path, 'rb') as file:\nobjects = pickle.load(file)\nself.summary.__dict__.update(objects['summary'].__dict__)\nself._load_list(objects, 'traces', self.traces)\nself._load_list(objects, 'tops', self.network.ops)\nself._load_list(objects, 'nops', self.pipeline.ops)\nself._load_dict(objects, 'ds', self.pipeline.data)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset", "title": "<code>reset</code>", "text": "<p>Reset the current <code>System</code> for a new round of training, including a new <code>Summary</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. The <code>Summary</code> object will store information iff name is not None.</p> <code>None</code> <code>system_config</code> <code>Optional[str]</code> <p>A description of the initialization parameters defining the associated estimator.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset(self, summary_name: Optional[str] = None, system_config: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n    Args:\n        summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n        system_config: A description of the initialization parameters defining the associated estimator.\n    \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name, system_config)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset_for_test", "title": "<code>reset_for_test</code>", "text": "<p>Partially reset the current <code>System</code> object for a new round of testing.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. If not provided, the system will re-use the previous summary name.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n    Args:\n        summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n    \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.save_state", "title": "<code>save_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory into which to save the state</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def save_state(self, save_dir: str) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        save_dir: The directory into which to save the state\n    \"\"\"\nos.makedirs(save_dir, exist_ok=True)\n# Start with the high-level info. We could use pickle for this but having it human readable is nice.\nstate = {key: value for key, value in self.__dict__.items() if is_restorable(value)[0]}\nwith open(os.path.join(save_dir, 'system.json'), 'w') as fp:\njson.dump(state, fp, indent=4)\n# Save all of the models / optimizer states\nfor model in self.network.models:\nsave_model(model, save_dir=save_dir, save_optimizer=True)\n# Save everything else\nobjects = {\n'summary': self.summary,\n'traces': [trace.__getstate__() if hasattr(trace, '__getstate__') else {} for trace in self.traces],\n'tops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.ops],\n'nops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.pipeline.ops],\n'ds':\n{key: value.__getstate__()\nfor key, value in self.pipeline.data.items() if hasattr(value, '__getstate__')}\n}\nwith open(os.path.join(save_dir, 'objects.pkl'), 'wb') as file:\npickle.dump(objects, file)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_batch_idx", "title": "<code>update_batch_idx</code>", "text": "<p>Increment the current <code>batch_idx</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n    \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_global_step", "title": "<code>update_global_step</code>", "text": "<p>Increment the current <code>global_step</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n    \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.write_summary", "title": "<code>write_summary</code>", "text": "<p>Write an entry into the <code>Summary</code> object (iff the experiment was named).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to write into the summary object.</p> required <code>value</code> <code>Any</code> <p>The value to write into the summary object.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n    Args:\n        key: The key to write into the summary object.\n        value: The value to write into the summary object.\n    \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html", "title": "log_parse", "text": ""}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_dir", "title": "<code>parse_log_dir</code>", "text": "<p>A function which will gather all log files within a given folder and pass them along for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>The path to a directory containing log files.</p> required <code>log_extension</code> <code>str</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>recursive_search</code> <code>bool</code> <p>Whether to recursively search sub-directories for log files.</p> <code>False</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>1</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_dir(dir_path: str,\nlog_extension: str = '.txt',\nrecursive_search: bool = False,\nsmooth_factor: float = 1,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\nshare_legend: bool = True,\npretty_names: bool = False) -&gt; None:\n\"\"\"A function which will gather all log files within a given folder and pass them along for visualization.\n    Args:\n        dir_path: The path to a directory containing log files.\n        log_extension: The extension of the log files.\n        recursive_search: Whether to recursively search sub-directories for log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n    \"\"\"\nloader = DirDataset(root_dir=dir_path, file_extension=log_extension, recursive_search=recursive_search)\nfile_paths = list(map(lambda d: d['x'], loader.data.values()))\nparse_log_files(file_paths,\nlog_extension,\nsmooth_factor,\nsave,\nsave_path,\nignore_metrics,\nshare_legend,\npretty_names)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_file", "title": "<code>parse_log_file</code>", "text": "<p>A function which will parse log files into a dictionary of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to a log file.</p> required <code>file_extension</code> <code>str</code> <p>The extension of the log file.</p> required <p>Returns:</p> Type Description <code>Summary</code> <p>An experiment summarizing the given log file.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_file(file_path: str, file_extension: str) -&gt; Summary:\n\"\"\"A function which will parse log files into a dictionary of metrics.\n    Args:\n        file_path: The path to a log file.\n        file_extension: The extension of the log file.\n    Returns:\n        An experiment summarizing the given log file.\n    \"\"\"\n# TODO: need to handle multi-line output like confusion matrix\nexperiment = Summary(strip_suffix(os.path.split(file_path)[1].strip(), file_extension))\nwith open(file_path) as file:\nfor line in file:\nmode = None\nif line.startswith(\"FastEstimator-Train\") or line.startswith(\"FastEstimator-Finish\"):\nmode = \"train\"\nelif line.startswith(\"FastEstimator-Eval\"):\nmode = \"eval\"\nelif line.startswith(\"FastEstimator-Test\"):\nmode = \"test\"\nif mode is None:\ncontinue\nparsed_line = re.findall(r\"([^:;]+):[\\s]*([-]?[0-9]+[.]?[0-9]*(e[-]?[0-9]+[.]?[0-9]*)?);\", line)\nstep = parsed_line[0]\nassert step[0].strip() == \"step\", \\\n                \"Log file (%s) seems to be missing step information, or step is not listed first\" % file\nfor metric in parsed_line[1:]:\nexperiment.history[mode][metric[0].strip()].update({int(step[1]): float(metric[1])})\nreturn experiment\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_files", "title": "<code>parse_log_files</code>", "text": "<p>Parse one or more log files for graphing.</p> <p>This function which will iterate through the given log file paths, parse them to extract metrics, remove any metrics which are blacklisted, and then pass the necessary information on the graphing function.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>A list of paths to various log files.</p> required <code>log_extension</code> <code>Optional[str]</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no log files are provided.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_files(file_paths: List[str],\nlog_extension: Optional[str] = '.txt',\nsmooth_factor: float = 0,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\nshare_legend: bool = True,\npretty_names: bool = False) -&gt; None:\n\"\"\"Parse one or more log files for graphing.\n    This function which will iterate through the given log file paths, parse them to extract metrics, remove any\n    metrics which are blacklisted, and then pass the necessary information on the graphing function.\n    Args:\n        file_paths: A list of paths to various log files.\n        log_extension: The extension of the log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n    Raises:\n        AssertionError: If no log files are provided.\n    \"\"\"\nif file_paths is None or len(file_paths) &lt; 1:\nraise AssertionError(\"must provide at least one log file\")\nif save and save_path is None:\nsave_path = file_paths[0]\nexperiments = []\nfor file_path in file_paths:\nexperiments.append(parse_log_file(file_path, log_extension))\nvisualize_logs(experiments,\nsave_path=save_path,\nsmooth_factor=smooth_factor,\nshare_legend=share_legend,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html", "title": "log_plot", "text": ""}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.plot_logs", "title": "<code>plot_logs</code>", "text": "<p>A function which will plot experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any keys to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of keys to include during plotting. If None then all will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>The handle of the pyplot figure.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def plot_logs(experiments: List[Summary],\nsmooth_factor: float = 0,\nshare_legend: bool = True,\nignore_metrics: Optional[Set[str]] = None,\npretty_names: bool = False,\ninclude_metrics: Optional[Set[str]] = None) -&gt; plt.Figure:\n\"\"\"A function which will plot experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any keys to ignore during plotting.\n        include_metrics: A whitelist of keys to include during plotting. If None then all will be included.\n    Returns:\n        The handle of the pyplot figure.\n    \"\"\"\nexperiments = to_list(experiments)\nn_experiments = len(experiments)\nif n_experiments == 0:\nreturn plt.subplots(111)[0]\nignore_keys = ignore_metrics or set()\nignore_keys = to_set(ignore_keys)\nignore_keys |= {'epoch'}\ninclude_keys = to_set(include_metrics)\n# TODO: epoch should be indicated on the axis (top x axis?). Problem - different epochs per experiment.\n# TODO: figure out how ignore_metrics should interact with mode\nmetric_histories = defaultdict(_MetricGroup)  # metric: MetricGroup\nfor idx, experiment in enumerate(experiments):\nhistory = experiment.history\n# Since python dicts remember insertion order, sort the history so that train mode is always plotted on bottom\nfor mode, metrics in sorted(history.items(),\nkey=lambda x: 0 if x[0] == 'train' else 1 if x[0] == 'eval' else 2 if x[0] == 'test'\nelse 3 if x[0] == 'infer' else 4):\nfor metric, step_val in metrics.items():\nif len(step_val) == 0:\ncontinue  # Ignore empty metrics\nif metric in ignore_keys:\ncontinue\nif include_keys and metric not in include_keys:\ncontinue\nmetric_histories[metric].add(idx, mode, step_val)\nmetric_list = list(sorted(metric_histories.keys()))\nif len(metric_list) == 0:\nreturn plt.subplots(111)[0]\n# If sharing legend and there is more than 1 plot, then dedicate 1 subplot for the legend\nshare_legend = share_legend and (len(metric_list) &gt; 1)\nn_legends = math.ceil(n_experiments / 4)\nn_plots = len(metric_list) + (share_legend * n_legends)\n# map the metrics into an n x n grid, then remove any extra columns. Final grid will be n x m with m &lt;= n\nn_rows = math.ceil(math.sqrt(n_plots))\nn_cols = math.ceil(n_plots / n_rows)\nmetric_grid_location = {}\nnd1_metrics = []\nidx = 0\nfor metric in metric_list:\nif metric_histories[metric].ndim() == 1:\n# Delay placement of the 1D plots until the end\nnd1_metrics.append(metric)\nelse:\nmetric_grid_location[metric] = (idx // n_cols, idx % n_cols)\nidx += 1\nfor metric in nd1_metrics:\nmetric_grid_location[metric] = (idx // n_cols, idx % n_cols)\nidx += 1\nsns.set_context('paper')\nfig, axs = plt.subplots(n_rows, n_cols, sharex='all', figsize=(4 * n_cols, 2.8 * n_rows))\n# If only one row, need to re-format the axs object for consistency. Likewise for columns\nif n_rows == 1:\naxs = [axs]\nif n_cols == 1:\naxs = [axs]\nfor metric in metric_grid_location.keys():\naxis = axs[metric_grid_location[metric][0]][metric_grid_location[metric][1]]\nif metric_histories[metric].ndim() == 1:\naxis.grid(linestyle='')\nelse:\naxis.grid(linestyle='--')\naxis.ticklabel_format(axis='y', style='sci', scilimits=(-2, 3))\naxis.set_title(metric if not pretty_names else prettify_metric_name(metric), fontweight='bold')\naxis.spines['top'].set_visible(False)\naxis.spines['right'].set_visible(False)\naxis.spines['bottom'].set_visible(False)\naxis.spines['left'].set_visible(False)\naxis.tick_params(bottom=False, left=False)\n# some of the later rows/columns might be unused or reserved for legends, so disable them\nlast_row_idx = math.ceil(len(metric_list) / n_cols) - 1\nlast_column_idx = len(metric_list) - last_row_idx * n_cols - 1\nfor c in range(n_cols):\nif c &lt;= last_column_idx:\naxs[last_row_idx][c].set_xlabel('Steps')\naxs[last_row_idx][c].xaxis.set_tick_params(which='both', labelbottom=True)\nelse:\naxs[last_row_idx][c].axis('off')\naxs[last_row_idx - 1][c].set_xlabel('Steps')\naxs[last_row_idx - 1][c].xaxis.set_tick_params(which='both', labelbottom=True)\nfor r in range(last_row_idx + 1, n_rows):\naxs[r][c].axis('off')\n# the 1D metrics don't need x axis, so move them up, starting with the last in case multiple rows of them\nfor metric in reversed(nd1_metrics):\nrow = metric_grid_location[metric][0]\ncol = metric_grid_location[metric][1]\naxs[row][col].axis('off')\nif row &gt; 0:\naxs[row - 1][col].set_xlabel('Steps')\naxs[row - 1][col].xaxis.set_tick_params(which='both', labelbottom=True)\ncolors = sns.hls_palette(n_colors=n_experiments, s=0.95) if n_experiments &gt; 10 else sns.color_palette(\"colorblind\")\ncolor_offset = defaultdict(lambda: 0)\n# If there is only 1 experiment, we will use alternate colors based on mode\nif n_experiments == 1:\ncolor_offset['eval'] = 1\ncolor_offset['test'] = 2\ncolor_offset['infer'] = 3\nhandles = []\nlabels = []\nhas_label = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: False)))  # exp_id : {mode: {type: True}}\nax_text = defaultdict(lambda: (0.0, 0.9))  # Where to put the text on a given axis\nfor exp_idx, experiment in enumerate(experiments):\nfor metric, group in metric_histories.items():\naxis = axs[metric_grid_location[metric][0]][metric_grid_location[metric][1]]\nif group.ndim() == 1:\n# Single value\nfor mode in group.modes(exp_idx):\nax_id = id(axis)\nprefix = f\"{experiment.name} ({mode})\" if n_experiments &gt; 1 else f\"{mode}\"\naxis.text(ax_text[ax_id][0],\nax_text[ax_id][1],\nf\"{prefix}: {group.get_val(exp_idx, mode)}\",\ncolor=colors[exp_idx + color_offset[mode]],\ntransform=axis.transAxes)\nax_text[ax_id] = (ax_text[ax_id][0], ax_text[ax_id][1] - 0.1)\nif ax_text[ax_id][1] &lt; 0:\nax_text[ax_id] = (ax_text[ax_id][0] + 0.5, 0.9)\nelif group.ndim() == 2:\nfor mode, data in group[exp_idx].items():\ntitle = f\"{experiment.name} ({mode})\" if n_experiments &gt; 1 else f\"{mode}\"\nif data.shape[0] &lt; 2:\n# This particular mode only has a single data point, so need to draw a shape instead of a line\nxy = (data[0][0], data[0][1])\nif mode == 'train':\nstyle = MarkerStyle(marker='o', fillstyle='full')\nelif mode == 'eval':\nstyle = MarkerStyle(marker='v', fillstyle='full')\nelif mode == 'test':\nstyle = MarkerStyle(marker='*', fillstyle='full')\nelse:\nstyle = MarkerStyle(marker='s', fillstyle='full')\ns = axis.scatter(xy[0],\nxy[1],\ns=40,\nc=[colors[exp_idx + color_offset[mode]]],\nmarker=style,\nlinewidth=1.0,\nedgecolors='black',\nzorder=3)  # zorder to put markers on top of line segments\nif not has_label[exp_idx][mode]['patch']:\nlabels.append(title)\nhandles.append(s)\nhas_label[exp_idx][mode]['patch'] = True\nelse:\n# We can draw a line\ny = data[:, 1] if smooth_factor == 0 else gaussian_filter1d(data[:, 1], sigma=smooth_factor)\nln = axis.plot(\ndata[:, 0],\ny,\ncolor=colors[exp_idx + color_offset[mode]],\nlabel=title,\nlinewidth=1.5,\nlinestyle='solid' if mode == 'train' else\n'dashed' if mode == 'eval' else 'dotted' if mode == 'test' else 'dashdot')\nif not has_label[exp_idx][mode]['line']:\nlabels.append(title)\nhandles.append(ln[0])\nhas_label[exp_idx][mode]['line'] = True\nelse:\n# Some kind of image or matrix. Not implemented yet.\npass\nplt.tight_layout()\nif labels:\nif share_legend:\n# Sort the labels\nhandles = [h for _, h in sorted(zip(labels, handles), key=lambda pair: pair[0])]\nlabels = sorted(labels)\n# Split the labels over multiple legends if there are too many to fit in one axis\nelems_per_legend = math.ceil(len(labels) / n_legends)\ni = 0\nfor r in range(last_row_idx, n_rows):\nfor c in range(last_column_idx + 1 if r == last_row_idx else 0, n_cols):\nif len(handles) &lt;= i:\nbreak\naxs[r][c].legend(\nhandles[i:i + elems_per_legend],\nlabels[i:i + elems_per_legend],\nloc='center',\nfontsize='large' if elems_per_legend &lt;= 6 else 'medium' if elems_per_legend &lt;= 8 else 'small')\ni += elems_per_legend\nelse:\nfor i in range(n_rows):\nfor j in range(n_cols):\nif i == last_row_idx and j &gt; last_column_idx:\nbreak\naxs[i][j].legend(loc='best', fontsize='small')\nreturn fig\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.visualize_logs", "title": "<code>visualize_logs</code>", "text": "<p>A function which will save or display experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>save_path</code> <code>str</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of metric keys (None whitelists all keys).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def visualize_logs(experiments: List[Summary],\nsave_path: str = None,\nsmooth_factor: float = 0,\nshare_legend: bool = True,\npretty_names: bool = False,\nignore_metrics: Optional[Set[str]] = None,\ninclude_metrics: Optional[Set[str]] = None,\nverbose: bool = True):\n\"\"\"A function which will save or display experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any metrics to ignore during plotting.\n        include_metrics: A whitelist of metric keys (None whitelists all keys).\n        verbose: Whether to print out the save location.\n    \"\"\"\nplot_logs(experiments,\nsmooth_factor=smooth_factor,\nshare_legend=share_legend,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics,\ninclude_metrics=include_metrics)\nif save_path is None:\nplt.show()\nelse:\nsave_path = os.path.normpath(save_path)\nroot_dir = os.path.dirname(save_path)\nif root_dir == \"\":\nroot_dir = \".\"\nos.makedirs(root_dir, exist_ok=True)\nsave_file = os.path.join(root_dir, os.path.basename(save_path) or 'parse_logs.png')\nif verbose:\nprint(\"Saving to {}\".format(save_file))\nplt.savefig(save_file, dpi=300, bbox_inches=\"tight\")\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html", "title": "nightly_util", "text": ""}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_apphub_source_dir_path", "title": "<code>get_apphub_source_dir_path</code>", "text": "<p>Get the absolute path to the apphub folder containing the files to be tested by the <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the corresponding apphub directory.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_apphub_source_dir_path(working_file: str) -&gt; str:\n\"\"\"Get the absolute path to the apphub folder containing the files to be tested by the `working_file`.\n    Args:\n        working_file: The absolute path to a test file.\n    Returns:\n        The absolute path to the corresponding apphub directory.\n    \"\"\"\napphub_path = get_uncle_path(\"apphub\", working_file)\nrelative_dir_path = get_relative_path(\"apphub_scripts\", working_file)\nsource_dir_path = os.path.join(apphub_path, relative_dir_path)\nreturn source_dir_path\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_relative_path", "title": "<code>get_relative_path</code>", "text": "<p>Convert an absolute path into a relative path within the parent folder.</p> <p>Parameters:</p> Name Type Description Default <code>parent_dir</code> <code>str</code> <p>A parent folder</p> required <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The relative path to the test file within the parent_dir folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> is not located within the parent_dir folder.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_relative_path(parent_dir: str, working_file: str) -&gt; str:\n\"\"\"Convert an absolute path into a relative path within the parent folder.\n    Args:\n        parent_dir: A parent folder\n        working_file: The absolute path to a test file.\n    Returns:\n        The relative path to the test file within the parent_dir folder.\n    Raises:\n        OSError: If the `working_file` is not located within the parent_dir folder.\n    \"\"\"\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nsplit = current_dir.split(\"{}/\".format(parent_dir))\nif len(split) == 1:\nraise OSError(\"This file need to be put inside {} directory\".format(parent_dir))\nreturn split[-1]\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_uncle_path", "title": "<code>get_uncle_path</code>", "text": "<p>Find the path to the uncle folder of <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uncle_dir</code> <code>str</code> <p>A target uncle folder</p> required <code>working_file</code> <code>str</code> <p>A file within the same FastEstimator repository as apphub examples.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The root path to the apphub folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> does not correspond to any of the uncle paths.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_uncle_path(uncle_dir: str, working_file: str) -&gt; str:\n\"\"\"Find the path to the uncle folder of `working_file`.\n    Args:\n        uncle_dir: A target uncle folder\n        working_file: A file within the same FastEstimator repository as apphub examples.\n    Returns:\n        The root path to the apphub folder.\n    Raises:\n        OSError: If the `working_file` does not correspond to any of the uncle paths.\n    \"\"\"\nuncle_path = None\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nwhile current_dir != \"/\":\ncurrent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\nif uncle_dir in os.listdir(current_dir):\nuncle_path = os.path.abspath(os.path.join(current_dir, uncle_dir))\nbreak\nif uncle_path is None:\nraise OSError(\"Could not find the {} directory\".format(uncle_dir))\nreturn uncle_path\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html", "title": "unittest_util", "text": ""}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.OneLayerTorchModel", "title": "<code>OneLayerTorchModel</code>", "text": "<p>         Bases: <code>torch.nn.Module</code></p> <p>Torch Model with one dense layer without activation function. * Model input shape: (3,) * Model output: (1,) * dense layer weight: [1.0, 2.0, 3.0]</p> <p>How to feed_forward this model <pre><code>model = OneLayerTorchModel()\nx = torch.tensor([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\nb = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n</code></pre></p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>class OneLayerTorchModel(torch.nn.Module):\n\"\"\"Torch Model with one dense layer without activation function.\n    * Model input shape: (3,)\n    * Model output: (1,)\n    * dense layer weight: [1.0, 2.0, 3.0]\n    How to feed_forward this model\n    ```python\n    model = OneLayerTorchModel()\n    x = torch.tensor([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\n    b = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n    ```\n    \"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__()\nself.fc1 = torch.nn.Linear(3, 1, bias=False)\nself.fc1.weight.data = torch.tensor([[1, 2, 3]], dtype=torch.float32)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = self.fc1(x)\nreturn x\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.check_img_similar", "title": "<code>check_img_similar</code>", "text": "<p>Check whether img1 and img2 array are similar based on pixel to pixel comparision</p> <p>Parameters:</p> Name Type Description Default <code>img1</code> <code>np.ndarray</code> <p>Image 1</p> required <code>img2</code> <code>np.ndarray</code> <p>Image 2</p> required <code>ptol</code> <code>int</code> <p>Pixel value tolerance</p> <code>3</code> <code>ntol</code> <code>float</code> <p>Number of pixel difference tolerace rate</p> <code>0.01</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean of whether the images are similar</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def check_img_similar(img1: np.ndarray, img2: np.ndarray, ptol: int = 3, ntol: float = 0.01) -&gt; bool:\n\"\"\"Check whether img1 and img2 array are similar based on pixel to pixel comparision\n    Args:\n        img1: Image 1\n        img2: Image 2\n        ptol: Pixel value tolerance\n        ntol: Number of pixel difference tolerace rate\n    Returns:\n        Boolean of whether the images are similar\n    \"\"\"\nif img1.shape == img2.shape:\ndiff = np.abs(img1.astype(np.float32) - img2.astype(np.float32))\nn_pixel_diff = diff[diff &gt; ptol].size\nif n_pixel_diff &lt; img1.size * ntol:\nreturn True\nelse:\nreturn False\nreturn False\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.fig_to_rgb_array", "title": "<code>fig_to_rgb_array</code>", "text": "<p>Convert image in plt.Figure to numpy array</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>plt.Figure</code> <p>Input figure object</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image array</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def fig_to_rgb_array(fig: plt.Figure) -&gt; np.ndarray:\n\"\"\"Convert image in plt.Figure to numpy array\n    Args:\n        fig: Input figure object\n    Returns:\n        Image array\n    \"\"\"\nfig.canvas.draw()\nbuf = fig.canvas.tostring_rgb()\nncols, nrows = fig.canvas.get_width_height()\nreturn np.frombuffer(buf, dtype=np.uint8).reshape(nrows, ncols, 3)\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.img_to_rgb_array", "title": "<code>img_to_rgb_array</code>", "text": "<p>Read png file to numpy array (RGB)</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Image path</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image numpy array</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def img_to_rgb_array(path: str) -&gt; np.ndarray:\n\"\"\"Read png file to numpy array (RGB)\n    Args:\n        path: Image path\n    Returns:\n        Image numpy array\n    \"\"\"\nreturn np.asarray(Image.open(path).convert('RGB'))\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.is_equal", "title": "<code>is_equal</code>", "text": "<p>Check whether input objects are equal. The object type can be nested iterable (list, tuple, set, dict) and with elements such as int, float, np.ndarray, tf.Tensor, tf.Varaible, torch.Tensor</p> <p>Parameters:</p> Name Type Description Default <code>obj1</code> <code>Any</code> <p>Input object 1</p> required <code>obj2</code> <code>Any</code> <p>Input object 2</p> required <code>assert_type</code> <code>bool</code> <p>Whether to assert the same data type</p> <code>True</code> <code>assert_dtype</code> <code>bool</code> <p>Whether to assert the same dtype in case of nd.array, tf.Tensor, torch.Tensor</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean of whether those two object are equal</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def is_equal(obj1: Any, obj2: Any, assert_type: bool = True, assert_dtype: bool = False) -&gt; bool:\n\"\"\"Check whether input objects are equal. The object type can be nested iterable (list, tuple, set, dict) and\n    with elements such as int, float, np.ndarray, tf.Tensor, tf.Varaible, torch.Tensor\n    Args:\n        obj1: Input object 1\n        obj2: Input object 2\n        assert_type: Whether to assert the same data type\n        assert_dtype: Whether to assert the same dtype in case of nd.array, tf.Tensor, torch.Tensor\n    Returns:\n        Boolean of whether those two object are equal\n    \"\"\"\nif assert_type and type(obj1) != type(obj2):\nreturn False\nif type(obj1) in [list, set, tuple]:\nif len(obj1) != len(obj2):\nreturn False\nfor iter1, iter2 in zip(obj1, obj2):\nif not is_equal(iter1, iter2):\nreturn False\nreturn True\nelif type(obj1) == dict:\nif len(obj1) != len(obj2):\nreturn False\nif obj1.keys() != obj2.keys():\nreturn False\nfor value1, value2 in zip(obj1.values(), obj2.values()):\nif not is_equal(value1, value2):\nreturn False\nreturn True\nelif type(obj1) == np.ndarray:\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nreturn np.array_equal(obj1, obj2)\nelif tf.is_tensor(obj1):\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nobj1 = obj1.numpy()\nobj2 = obj2.numpy()\nreturn np.array_equal(obj1, obj2)\nelif isinstance(obj1, torch.Tensor):\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nreturn torch.equal(obj1, obj2)\nelse:\nreturn obj1 == obj2\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.one_layer_tf_model", "title": "<code>one_layer_tf_model</code>", "text": "<p>Tensorflow Model with one dense layer without activation function. * Model input shape: (3,) * Model output: (1,) * dense layer weight: [1.0, 2.0, 3.0]</p> <p>How to feed_forward this model <pre><code>model = one_layer_tf_model()\nx = tf.constant([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\nb = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n</code></pre></p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>tf.keras.Model: The model</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def one_layer_tf_model() -&gt; tf.keras.Model:\n\"\"\"Tensorflow Model with one dense layer without activation function.\n    * Model input shape: (3,)\n    * Model output: (1,)\n    * dense layer weight: [1.0, 2.0, 3.0]\n    How to feed_forward this model\n    ```python\n    model = one_layer_tf_model()\n    x = tf.constant([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\n    b = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n    ```\n    Returns:\n        tf.keras.Model: The model\n    \"\"\"\ninp = tf.keras.layers.Input([3])\nx = tf.keras.layers.Dense(units=1, use_bias=False)(inp)\nmodel = tf.keras.models.Model(inputs=inp, outputs=x)\nmodel.layers[1].set_weights([np.array([[1.0], [2.0], [3.0]])])\nreturn model\n</code></pre>"}, {"location": "fastestimator/trace/trace.html", "title": "trace", "text": ""}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.EvalEssential", "title": "<code>EvalEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during evaluation.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Any keys which should be collected over the course of an eval epoch.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass EvalEssential(Trace):\n\"\"\"A trace to collect important information during evaluation.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Any keys which should be collected over the course of an eval epoch.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(mode=\"eval\", inputs=monitor_names)\nself.eval_results = None\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.eval_results = None\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.eval_results is None:\nself.eval_results = {key: [data[key]] for key in self.inputs if key in data}\nelse:\nfor key in self.inputs:\nif key in data:\nself.eval_results[key].append(data[key])\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key, value_list in self.eval_results.items():\ndata.write_with_log(key, np.mean(np.array(value_list), axis=0))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Logger", "title": "<code>Logger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace that prints log messages.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass Logger(Trace):\n\"\"\"A Trace that prints log messages.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    \"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__(inputs=\"*\")\ndef on_begin(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nstart_step = 1 if not self.system.global_step else self.system.global_step\nself._print_message(\"FastEstimator-Start: step: {}; \".format(start_step), data)\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.system.log_steps and (self.system.global_step % self.system.log_steps\n== 0 or self.system.global_step == 1):\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data)\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.system.log_steps:\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == \"eval\":\nself._print_message(\"FastEstimator-Eval: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Test: step: {}; \".format(self.system.global_step), data, True)\ndef on_end(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Finish: step: {}; \".format(self.system.global_step), data)\ndef _print_message(self, header: str, data: Data, log_epoch: bool = False) -&gt; None:\n\"\"\"Print a log message to the screen, and record the `data` into the `system` summary.\n        Args:\n            header: The prefix for the log message.\n            data: A collection of data to be recorded.\n            log_epoch: Whether epoch information should be included in the log message.\n        \"\"\"\nlog_message = header\nif log_epoch:\nlog_message += \"epoch: {}; \".format(self.system.epoch_idx)\nself.system.write_summary('epoch', self.system.epoch_idx)\nfor key, val in data.read_logs().items():\nval = to_number(val)\nself.system.write_summary(key, val)\nif val.size &gt; 1:\nlog_message += \"\\n{}:\\n{};\".format(key, np.array2string(val, separator=','))\nelse:\nlog_message += \"{}: {}; \".format(key, str(val))\nprint(log_message)\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace", "title": "<code>Trace</code>", "text": "<p>Trace controls the training loop. Users can use the <code>Trace</code> base class to customize their own functionality.</p> <p>Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are also given a pointer to the current <code>System</code> instance which allows access to more information as well as giving the ability to modify or even cancel training. The order of function invocations is as follows:</p> <pre><code>        Training:                                       Testing:\n\n    on_begin                                            on_begin\n        |                                                   |\n    on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n        |                          |                        |                         |\n    on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n        |                       |  |                        |                      |  |\n    on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n        |                          ^                        |                         |\n    on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n        |                          |                        |\n    on_epoch_begin (eval)          |                    on_end\n        |                          ^\n    on_batch_begin (eval) &lt;----&lt;   |\n        |                      |   |\n    on_batch_end (eval) &gt;-----^    |\n        |                          |\n    on_epoch_end (eval) &gt;----------^\n        |\n    on_end\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to read from the state dictionary as inputs.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to write into the system buffer.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass Trace:\n\"\"\"Trace controls the training loop. Users can use the `Trace` base class to customize their own functionality.\n    Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are\n    also given a pointer to the current `System` instance which allows access to more information as well as giving the\n    ability to modify or even cancel training. The order of function invocations is as follows:\n    ``` plot\n            Training:                                       Testing:\n        on_begin                                            on_begin\n            |                                                   |\n        on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n            |                          |                        |                         |\n        on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n            |                       |  |                        |                      |  |\n        on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n            |                          ^                        |                         |\n        on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n            |                          |                        |\n        on_epoch_begin (eval)          |                    on_end\n            |                          ^\n        on_batch_begin (eval) &lt;----&lt;   |\n            |                      |   |\n        on_batch_end (eval) &gt;-----^    |\n            |                          |\n        on_epoch_end (eval) &gt;----------^\n            |\n        on_end\n    ```\n    Args:\n        inputs: A set of keys that this trace intends to read from the state dictionary as inputs.\n        outputs: A set of keys that this trace intends to write into the system buffer.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\nsystem: System\ninputs: List[str]\noutputs: List[str]\nmode: Set[str]\n# You can put keys in here to have them automatically added to EvalEssential without the user having to manually add\n# them to the Estimator monitor_names. See BestModelSaver for an example.\nfe_monitor_names: Set[str]\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = to_list(inputs)\nself.outputs = to_list(outputs)\nself.mode = parse_modes(to_set(mode))\nself.fe_monitor_names = set()  # The use-case here is rare enough that we don't want to add this to the init sig\ndef on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n        Args:\n            data: The current batch and prediction data, as well as any information written by prior `Traces`.\n        \"\"\"\npass\ndef on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Runs at the beginning of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_end", "title": "<code>on_batch_end</code>", "text": "<p>Runs at the end of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>The current batch and prediction data, as well as any information written by prior <code>Traces</code>.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n    Args:\n        data: The current batch and prediction data, as well as any information written by prior `Traces`.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_begin", "title": "<code>on_begin</code>", "text": "<p>Runs once at the beginning of training or testing.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_end", "title": "<code>on_end</code>", "text": "<p>Runs once at the end training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Runs at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_end", "title": "<code>on_epoch_end</code>", "text": "<p>Runs at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.TrainEssential", "title": "<code>TrainEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during training.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Which keys from the data dictionary to monitor during training.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass TrainEssential(Trace):\n\"\"\"A trace to collect important information during training.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Which keys from the data dictionary to monitor during training.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(inputs=monitor_names, mode=\"train\", outputs=[\"steps/sec\", \"epoch_time\", \"total_time\"])\nself.elapse_times = []\nself.train_start = None\nself.epoch_start = None\nself.step_start = None\ndef on_begin(self, data: Data) -&gt; None:\nself.train_start = time.perf_counter()\ndata.write_with_log(\"num_device\", self.system.num_devices)\ndata.write_with_log(\"logging_interval\", self.system.log_steps)\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.epoch_start = time.perf_counter()\nself.step_start = time.perf_counter()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.log_steps and (self.system.global_step % self.system.log_steps == 0\nor self.system.global_step == 1):\nfor key in self.inputs:\nif key in data:\ndata.write_with_log(key, data[key])\nif self.system.global_step &gt; 1:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"steps/sec\", round(self.system.log_steps / np.sum(self.elapse_times), 2))\nself.elapse_times = []\nself.step_start = time.perf_counter()\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"epoch_time\", \"{} sec\".format(round(time.perf_counter() - self.epoch_start, 2)))\ndef on_end(self, data: Data) -&gt; None:\nself.system.mode = 'train'  # Set mode to 'train' for better log visualization\ndata.write_with_log(\"total_time\", \"{} sec\".format(round(time.perf_counter() - self.train_start, 2)))\nfor model in self.system.network.models:\nif hasattr(model, \"current_optimizer\"):\ndata.write_with_log(model.model_name + \"_lr\", get_lr(model))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.sort_traces", "title": "<code>sort_traces</code>", "text": "<p>Sort traces to attempt to resolve any dependency issues.</p> <p>This is essentially a topological sort, but it doesn't seem worthwhile to convert the data into a graph representation in order to get the slightly better asymptotic runtime complexity.</p> <p>Parameters:</p> Name Type Description Default <code>traces</code> <code>List[Trace]</code> <p>A list of traces (not inside schedulers) to be sorted.</p> required <code>available_outputs</code> <code>Optional[Set[str]]</code> <p>What output keys are already available for the traces to use. If None are provided, the sorting algorithm will assume that any keys not generated by traces are being provided by the system. This results in a less rigorous sorting.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Trace]</code> <p>The sorted list of <code>traces</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If Traces have circular dependencies or require input keys which are not available.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def sort_traces(traces: List[Trace], available_outputs: Optional[Set[str]] = None) -&gt; List[Trace]:\n\"\"\"Sort traces to attempt to resolve any dependency issues.\n    This is essentially a topological sort, but it doesn't seem worthwhile to convert the data into a graph\n    representation in order to get the slightly better asymptotic runtime complexity.\n    Args:\n        traces: A list of traces (not inside schedulers) to be sorted.\n        available_outputs: What output keys are already available for the traces to use. If None are provided, the\n            sorting algorithm will assume that any keys not generated by traces are being provided by the system.\n            This results in a less rigorous sorting.\n    Returns:\n        The sorted list of `traces`.\n    Raises:\n        AssertionError: If Traces have circular dependencies or require input keys which are not available.\n    \"\"\"\nsorted_traces = []\ntrace_outputs = {output for trace in traces for output in trace.outputs}\nif available_outputs is None:\n# Assume that anything not generated by a Trace is provided by the system\navailable_outputs = {inp for trace in traces for inp in trace.inputs} - trace_outputs\nweak_sort = True\nelse:\navailable_outputs = to_set(available_outputs)\nweak_sort = False\nend_traces = deque()\nintermediate_traces = deque()\nintermediate_outputs = set()\ntrace_deque = deque(traces)\nwhile trace_deque:\ntrace = trace_deque.popleft()\nins = set(trace.inputs)\nouts = set(trace.outputs)\nif not ins or isinstance(trace, (TrainEssential, EvalEssential)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelif \"*\" in ins:\nif outs:\nend_traces.appendleft(trace)\nelse:\nend_traces.append(trace)\nelif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelse:\nintermediate_traces.append(trace)\nintermediate_outputs |= outs\nalready_seen = set()\nwhile intermediate_traces:\ntrace = intermediate_traces.popleft()\nins = set(trace.inputs)\nouts = set(trace.outputs)\nalready_seen.add(trace)\nif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nalready_seen.clear()\nelif ins &lt;= (available_outputs | intermediate_outputs):\nintermediate_traces.append(trace)\nelse:\nraise AssertionError(\"The {} trace has unsatisfiable inputs: {}\".format(\ntype(trace).__name__, \", \".join(ins - (available_outputs | intermediate_outputs))))\nif intermediate_traces and len(already_seen) == len(intermediate_traces):\nraise AssertionError(\"Dependency cycle detected amongst traces: {}\".format(\", \".join(\n[type(tr).__name__ for tr in already_seen])))\nsorted_traces.extend(list(end_traces))\nreturn sorted_traces\n</code></pre>"}, {"location": "fastestimator/trace/adapt/early_stopping.html", "title": "early_stopping", "text": ""}, {"location": "fastestimator/trace/adapt/early_stopping.html#fastestimator.fastestimator.trace.adapt.early_stopping.EarlyStopping", "title": "<code>EarlyStopping</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Stop training when a monitored quantity has stopped improving.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>str</code> <p>Quantity to be monitored.</p> <code>'loss'</code> <code>min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement.</p> <code>0.0</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped.</p> <code>0</code> <code>compare</code> <code>str</code> <p>One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in <code>max</code> mode it will stop when the quantity monitored has stopped increasing.</p> <code>'min'</code> <code>baseline</code> <code>Optional[float]</code> <p>Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.</p> <code>None</code> <code>mode</code> <code>str</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>compare</code> is an invalid value or more than one <code>monitor</code> is provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\early_stopping.py</code> <pre><code>@traceable()\nclass EarlyStopping(Trace):\n\"\"\"Stop training when a monitored quantity has stopped improving.\n    Args:\n        monitor: Quantity to be monitored.\n        min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an\n            absolute change of less than min_delta will count as no improvement.\n        patience: Number of epochs with no improvement after which training will be stopped.\n        compare: One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored\n            has stopped decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing.\n        baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't\n            show improvement over the baseline.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Raises:\n        ValueError: If `compare` is an invalid value or more than one `monitor` is provided.\n    \"\"\"\ndef __init__(self,\nmonitor: str = \"loss\",\nmin_delta: float = 0.0,\npatience: int = 0,\ncompare: str = 'min',\nbaseline: Optional[float] = None,\nmode: str = 'eval') -&gt; None:\nsuper().__init__(inputs=monitor, mode=mode)\nif len(self.inputs) != 1:\nraise ValueError(\"EarlyStopping supports only one monitor key\")\nif compare not in ['min', 'max']:\nraise ValueError(\"compare_mode can only be `min` or `max`\")\nself.monitored_key = monitor\nself.fe_monitor_names.add(monitor)\nself.min_delta = abs(min_delta)\nself.wait = 0\nself.best = 0\nself.patience = patience\nself.baseline = baseline\nif compare == 'min':\nself.monitor_op = np.less\nself.min_delta *= -1\nelse:\nself.monitor_op = np.greater\ndef on_begin(self, data: Data) -&gt; None:\nself.wait = 0\nif self.baseline is not None:\nself.best = self.baseline\nelse:\nself.best = np.Inf if self.monitor_op == np.less else -np.Inf\ndef on_epoch_end(self, data: Data) -&gt; None:\ncurrent = data[self.monitored_key]\nif current is None:\nreturn\nif self.monitor_op(current - self.min_delta, self.best):\nself.best = current\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nself.system.stop_training = True\nprint(\"FastEstimator-EarlyStopping: '{}' triggered an early stop. Its best value was {} at epoch {}\".\nformat(self.monitored_key, self.best, self.system.epoch_idx - self.wait))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/lr_scheduler.html", "title": "lr_scheduler", "text": ""}, {"location": "fastestimator/trace/adapt/lr_scheduler.html#fastestimator.fastestimator.trace.adapt.lr_scheduler.LRScheduler", "title": "<code>LRScheduler</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Learning rate scheduler trace that changes the learning rate while training.</p> <p>This class requires an input function which takes either 'epoch' or 'step' as input: <pre><code>s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on step\ns = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>lr_fn</code> <code>Callable[[int], float]</code> <p>A lr scheduling function that takes either 'epoch' or 'step' as input.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>lr_fn</code> is not configured properly.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\lr_scheduler.py</code> <pre><code>@traceable()\nclass LRScheduler(Trace):\n\"\"\"Learning rate scheduler trace that changes the learning rate while training.\n    This class requires an input function which takes either 'epoch' or 'step' as input:\n    ```python\n    s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on step\n    s = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n    ```\n    Args:\n        model: A model instance compiled with fe.build.\n        lr_fn: A lr scheduling function that takes either 'epoch' or 'step' as input.\n    Raises:\n        AssertionError: If the `lr_fn` is not configured properly.\n    \"\"\"\nsystem: System\ndef __init__(self, model: Union[tf.keras.Model, torch.nn.Module], lr_fn: Callable[[int], float]) -&gt; None:\nself.model = model\nself.lr_fn = lr_fn\nassert hasattr(lr_fn, \"__call__\"), \"lr_fn must be a function\"\narg = list(inspect.signature(lr_fn).parameters.keys())\nassert len(arg) == 1 and arg[0] in {\"step\", \"epoch\"}, \"the lr_fn input arg must be either 'step' or 'epoch'\"\nself.schedule_mode = arg[0]\nsuper().__init__(mode=\"train\", outputs=self.model.model_name + \"_lr\")\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.schedule_mode == \"epoch\":\nnew_lr = np.float32(self.lr_fn(self.system.epoch_idx))\nset_lr(self.model, new_lr)\ndef on_batch_begin(self, data: Data) -&gt; None:\nif self.schedule_mode == \"step\":\nnew_lr = np.float32(self.lr_fn(self.system.global_step))\nset_lr(self.model, new_lr)\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.log_steps and (self.system.global_step % self.system.log_steps == 0\nor self.system.global_step == 1):\ncurrent_lr = np.float32(get_lr(self.model))\ndata.write_with_log(self.outputs[0], current_lr)\n</code></pre>"}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html", "title": "reduce_lr_on_plateau", "text": ""}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html#fastestimator.fastestimator.trace.adapt.reduce_lr_on_plateau.ReduceLROnPlateau", "title": "<code>ReduceLROnPlateau</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Reduce learning rate based on evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>metric</code> <code>Optional[str]</code> <p>The metric name to be monitored. If None, the model's validation loss will be used as the metric.</p> <code>None</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait before reducing LR again.</p> <code>10</code> <code>factor</code> <code>float</code> <p>Reduce factor for the learning rate.</p> <code>0.1</code> <code>best_mode</code> <code>str</code> <p>Higher is better (\"max\") or lower is better (\"min\").</p> <code>'min'</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the loss cannot be inferred from the <code>model</code> and a <code>metric</code> was not provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\reduce_lr_on_plateau.py</code> <pre><code>@traceable()\nclass ReduceLROnPlateau(Trace):\n\"\"\"Reduce learning rate based on evaluation results.\n    Args:\n        model: A model instance compiled with fe.build.\n        metric: The metric name to be monitored. If None, the model's validation loss will be used as the metric.\n        patience: Number of epochs to wait before reducing LR again.\n        factor: Reduce factor for the learning rate.\n        best_mode: Higher is better (\"max\") or lower is better (\"min\").\n        min_lr: Minimum learning rate.\n    Raises:\n        AssertionError: If the loss cannot be inferred from the `model` and a `metric` was not provided.\n    \"\"\"\nsystem: System\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nmetric: Optional[str] = None,\npatience: int = 10,\nfactor: float = 0.1,\nbest_mode: str = \"min\",\nmin_lr: float = 1e-6) -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \"cannot infer model loss name, please put the model to UpdateOp first\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\", inputs=metric, outputs=model.model_name + \"_lr\")\nself.fe_monitor_names.add(metric)\nself.model = model\nself.patience = patience\nself.factor = factor\nself.best_mode = best_mode\nself.min_lr = min_lr\nself.wait = 0\nif self.best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"best_mode must be either 'min' or 'max'\")\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.inputs[0]], self.best):\nself.best = data[self.inputs[0]]\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nnew_lr = max(self.min_lr, np.float32(self.factor * get_lr(self.model)))\nset_lr(self.model, new_lr)\nself.wait = 0\ndata.write_with_log(self.outputs[0], new_lr)\nprint(\"FastEstimator-ReduceLROnPlateau: learning rate reduced to {}\".format(new_lr))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/terminate_on_nan.html", "title": "terminate_on_nan", "text": ""}, {"location": "fastestimator/trace/adapt/terminate_on_nan.html#fastestimator.fastestimator.trace.adapt.terminate_on_nan.TerminateOnNaN", "title": "<code>TerminateOnNaN</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>End Training if a NaN value is detected.</p> <p>By default (monitor_names=None) it will monitor all loss values at the end of each batch. If one or more inputs are specified, it will only monitor those values. Inputs may be loss keys and/or the keys corresponding to the outputs of other traces (ex. accuracy).</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Union[None, str, Iterable[str]]</code> <p>key(s) to monitor for NaN values. If None, all loss values will be monitored. \"*\" will monitor all trace output keys and losses.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\terminate_on_nan.py</code> <pre><code>@traceable()\nclass TerminateOnNaN(Trace):\n\"\"\"End Training if a NaN value is detected.\n    By default (monitor_names=None) it will monitor all loss values at the end of each batch. If one or more inputs are\n    specified, it will only monitor those values. Inputs may be loss keys and/or the keys corresponding to the outputs\n    of other traces (ex. accuracy).\n    Args:\n        monitor_names: key(s) to monitor for NaN values. If None, all loss values will be monitored. \"*\" will monitor\n            all trace output keys and losses.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self, monitor_names: Union[None, str, Iterable[str]] = None, mode: Union[None, str,\nSet[str]] = None) -&gt; None:\nsuper().__init__(inputs=monitor_names, mode=mode)\nself.monitor_keys = {}\nself.in_list = True\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif not self.inputs:\nself.monitor_keys = self.system.network.get_loss_keys()\nelif \"*\" in self.inputs:\nself.monitor_keys = self.system.network.get_loss_keys()\nfor trace in get_current_items(self.system.traces, run_modes=self.system.mode, epoch=self.system.epoch_idx):\nself.monitor_keys.update(trace.outputs)\nelse:\nself.monitor_keys = self.inputs\ndef on_batch_end(self, data: Data) -&gt; None:\nfor key in self.monitor_keys:\nif key in data:\nif check_nan(data[key]):\nself.system.stop_training = True\nprint(\"FastEstimator-TerminateOnNaN: NaN Detected in: {}\".format(key))\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.monitor_keys:\nif key in data:\nif check_nan(data[key]):\nself.system.stop_training = True\nprint(\"FastEstimator-TerminateOnNaN: NaN Detected in: {}\".format(key))\n</code></pre>"}, {"location": "fastestimator/trace/io/best_model_saver.html", "title": "best_model_saver", "text": ""}, {"location": "fastestimator/trace/io/best_model_saver.html#fastestimator.fastestimator.trace.io.best_model_saver.BestModelSaver", "title": "<code>BestModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save the weights of best model based on a given evaluation metric.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the model.</p> required <code>metric</code> <code>Optional[str]</code> <p>Eval metric name to monitor. If None, the model's loss will be used.</p> <code>None</code> <code>save_best_mode</code> <code>str</code> <p>Can be 'min' or 'max'.</p> <code>'min'</code> <code>load_best_final</code> <code>bool</code> <p>Whether to automatically reload the best model (if available) after training.</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a <code>metric</code> is not provided and it cannot be inferred from the <code>model</code>.</p> <code>ValueError</code> <p>If <code>save_best_mode</code> is an unacceptable string.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\best_model_saver.py</code> <pre><code>@traceable()\nclass BestModelSaver(Trace):\n\"\"\"Save the weights of best model based on a given evaluation metric.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the model.\n        metric: Eval metric name to monitor. If None, the model's loss will be used.\n        save_best_mode: Can be 'min' or 'max'.\n        load_best_final: Whether to automatically reload the best model (if available) after training.\n    Raises:\n        AssertionError: If a `metric` is not provided and it cannot be inferred from the `model`.\n        ValueError: If `save_best_mode` is an unacceptable string.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmetric: Optional[str] = None,\nsave_best_mode: str = \"min\",\nload_best_final: bool = False) -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \"cannot infer model loss name, please put the model to UpdateOp first\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\",\ninputs=metric,\noutputs=[\"since_best_{}\".format(metric), \"{}_{}\".format(save_best_mode, metric)])\nself.fe_monitor_names.add(metric)\nself.model = model\nself.model_name = \"{}_best_{}\".format(self.model.model_name, self.metric)\nself.save_dir = save_dir\nself.save_best_mode = save_best_mode\nself.load_best_final = load_best_final\nself.model_path = None\nself.since_best = 0\nif self.save_best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.save_best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"save_best_mode must be either 'min' or 'max'\")\n@property\ndef metric(self) -&gt; str:\nreturn self.inputs[0]\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.metric], self.best):\nself.best = data[self.metric]\nself.since_best = 0\nif self.save_dir:\nself.model_path = save_model(self.model, self.save_dir, self.model_name)\nprint(\"FastEstimator-BestModelSaver: Saved model to {}\".format(self.model_path))\nelse:\nself.since_best += 1\ndata.write_with_log(self.outputs[0], self.since_best)\ndata.write_with_log(self.outputs[1], self.best)\ndef on_end(self, data: Data) -&gt; None:\nif self.load_best_final and self.model_path:\nprint(\"FastEstimator-BestModelSaver: Restoring model from {}\".format(self.model_path))\nload_model(self.model, self.model_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/csv_logger.html", "title": "csv_logger", "text": ""}, {"location": "fastestimator/trace/io/csv_logger.html#fastestimator.fastestimator.trace.io.csv_logger.CSVLogger", "title": "<code>CSVLogger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Log monitored quantities in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>monitor_names</code> <code>Optional[Union[List[str], str]]</code> <p>List of keys to monitor. If None then all metrics will be recorded.</p> <code>None</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\csv_logger.py</code> <pre><code>@traceable()\nclass CSVLogger(Trace):\n\"\"\"Log monitored quantities in a CSV file.\n    Args:\n        filename: Output filename.\n        monitor_names: List of keys to monitor. If None then all metrics will be recorded.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfilename: str,\nmonitor_names: Optional[Union[List[str], str]] = None,\nmode: Union[str, Set[str]] = (\"eval\", \"test\")) -&gt; None:\nsuper().__init__(inputs=\"*\" if monitor_names is None else monitor_names, mode=mode)\nself.filename = filename\nself.data = None\ndef on_begin(self, data: Data) -&gt; None:\nself.data = defaultdict(list)\ndef on_epoch_end(self, data: Data) -&gt; None:\nself.data[\"mode\"].append(self.system.mode)\nself.data[\"epoch\"].append(self.system.epoch_idx)\nif \"*\" in self.inputs:\nfor key, value in data.read_logs().items():\nself.data[key].append(value)\nelse:\nfor key in self.inputs:\nself.data[key].append(data[key])\ndef on_end(self, data: Data) -&gt; None:\ndf = pd.DataFrame(data=self.data)\nif os.path.exists(self.filename):\ndf.to_csv(self.filename, mode='a', index=False)\nelse:\ndf.to_csv(self.filename, index=False)\n</code></pre>"}, {"location": "fastestimator/trace/io/image_saver.html", "title": "image_saver", "text": ""}, {"location": "fastestimator/trace/io/image_saver.html#fastestimator.fastestimator.trace.io.image_saver.ImageSaver", "title": "<code>ImageSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that saves images to the disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be saved.</p> required <code>save_dir</code> <code>str</code> <p>The directory into which to write the images.</p> <code>os.getcwd()</code> <code>dpi</code> <code>int</code> <p>How many dots per inch to save.</p> <code>300</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_saver.py</code> <pre><code>@traceable()\nclass ImageSaver(Trace):\n\"\"\"A trace that saves images to the disk.\n    Args:\n        inputs: Key(s) of images to be saved.\n        save_dir: The directory into which to write the images.\n        dpi: How many dots per inch to save.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nsave_dir: str = os.getcwd(),\ndpi: int = 300,\nmode: Union[str, Set[str]] = (\"eval\", \"test\")) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nself.save_dir = save_dir\nself.dpi = dpi\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nif isinstance(imgs, ImgData):\nf = imgs.paint_figure()\nim_path = os.path.join(self.save_dir,\n\"{}_{}_epoch_{}.png\".format(key, self.system.mode, self.system.epoch_idx))\nplt.savefig(im_path, dpi=self.dpi, bbox_inches=\"tight\")\nplt.close(f)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\nelse:\nfor idx, img in enumerate(imgs):\nf = show_image(img, title=key)\nim_path = os.path.join(\nself.save_dir,\n\"{}_{}_epoch_{}_elem_{}.png\".format(key, self.system.mode, self.system.epoch_idx, idx))\nplt.savefig(im_path, dpi=self.dpi, bbox_inches=\"tight\")\nplt.close(f)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\n</code></pre>"}, {"location": "fastestimator/trace/io/image_viewer.html", "title": "image_viewer", "text": ""}, {"location": "fastestimator/trace/io/image_viewer.html#fastestimator.fastestimator.trace.io.image_viewer.ImageViewer", "title": "<code>ImageViewer</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that interrupts your training in order to display images on the screen.</p> <p>This class is useful primarily for Jupyter Notebook, or for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be displayed.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>width</code> <code>int</code> <p>The width in inches of the figure.</p> <code>12</code> <code>height</code> <code>int</code> <p>The height in inches of the figure.</p> <code>6</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_viewer.py</code> <pre><code>@traceable()\nclass ImageViewer(Trace):\n\"\"\"A trace that interrupts your training in order to display images on the screen.\n    This class is useful primarily for Jupyter Notebook, or for debugging purposes.\n    Args:\n        inputs: Key(s) of images to be displayed.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        width: The width in inches of the figure.\n        height: The height in inches of the figure.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\nwidth: int = 12,\nheight: int = 6) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nplt.rcParams['figure.figsize'] = [width, height]\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nif isinstance(imgs, ImgData):\nfig = imgs.paint_numpy(dpi=96)\nplt.imshow(fig[0])\nplt.axis('off')\nplt.tight_layout()\nplt.show()\nelse:\nfor idx, img in enumerate(imgs):\nshow_image(img, title=\"{}_{}\".format(key, idx))\nplt.show()\n</code></pre>"}, {"location": "fastestimator/trace/io/model_saver.html", "title": "model_saver", "text": ""}, {"location": "fastestimator/trace/io/model_saver.html#fastestimator.fastestimator.trace.io.model_saver.ModelSaver", "title": "<code>ModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save model weights based on epoch frequency during training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the <code>model</code>.</p> required <code>frequency</code> <code>int</code> <p>Model saving frequency in epoch(s).</p> <code>1</code> <code>max_to_keep</code> <code>Optional[int]</code> <p>Maximum number of latest saved files to keep. If 0 or None, all models will be saved.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\model_saver.py</code> <pre><code>@traceable()\nclass ModelSaver(Trace):\n\"\"\"Save model weights based on epoch frequency during training.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the `model`.\n        frequency: Model saving frequency in epoch(s).\n        max_to_keep: Maximum number of latest saved files to keep. If 0 or None, all models will be saved.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nfrequency: int = 1,\nmax_to_keep: Optional[int] = None) -&gt; None:\nsuper().__init__(mode=\"train\")\nself.model = model\nself.save_dir = save_dir\nself.frequency = frequency\nif max_to_keep is not None and max_to_keep &lt; 0:\nraise ValueError(f\"max_to_keep should be a non-negative integer, but got {max_to_keep}\")\nself.file_queue = deque([None] * (max_to_keep or 0), maxlen=max_to_keep or 0)\ndef on_epoch_end(self, data: Data) -&gt; None:\n# No model will be saved when save_dir is None, which makes smoke test easier.\nif self.save_dir and self.system.epoch_idx % self.frequency == 0:\nmodel_name = \"{}_epoch_{}\".format(self.model.model_name, self.system.epoch_idx)\nmodel_path = save_model(self.model, self.save_dir, model_name)\nprint(\"FastEstimator-ModelSaver: Saved model to {}\".format(model_path))\nrm_path = self.file_queue[self.file_queue.maxlen - 1] if self.file_queue.maxlen else None\nif rm_path:\nos.remove(rm_path)\nprint(\"FastEstimator-ModelSaver: Removed model {} due to file number exceeding max_to_keep\".format(\nrm_path))\nself.file_queue.appendleft(model_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/restore_wizard.html", "title": "restore_wizard", "text": ""}, {"location": "fastestimator/trace/io/restore_wizard.html#fastestimator.fastestimator.trace.io.restore_wizard.RestoreWizard", "title": "<code>RestoreWizard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that can backup and load your entire training status.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory to save and load the training status.</p> required <code>frequency</code> <code>int</code> <p>Saving frequency in epoch(s).</p> <code>1</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\restore_wizard.py</code> <pre><code>@traceable()\nclass RestoreWizard(Trace):\n\"\"\"A trace that can backup and load your entire training status.\n    Args:\n        directory: Directory to save and load the training status.\n        frequency: Saving frequency in epoch(s).\n    \"\"\"\ndef __init__(self, directory: str, frequency: int = 1) -&gt; None:\nsuper().__init__(inputs=\"*\", mode=\"train\")  # inputs to cause this trace to sort to the end of the list\nself.directory = os.path.abspath(os.path.normpath(directory))\nself.frequency = frequency\n# For robust saving, we need to create 2 different directories and have a key file to switch between them\nself.dirs = [os.path.join(self.directory, 'A'), os.path.join(self.directory, 'B')]\nself.key_path = os.path.join(self.directory, 'key.txt')\nself.dir_idx = 0\ndef on_begin(self, data: Data) -&gt; None:\nif fe.fe_deterministic_seed is not None:\nraise RuntimeError(\"You cannot use RestoreWizard while in deterministic training mode since a restored\" +\n\" training can't guarantee that all prngs will be reset to exactly the same position\")\nif not self.should_restore():\nself._cleanup(self.dirs)  # Remove any partially completed checkpoints\nprint(\"FastEstimator-RestoreWizard: Backing up to {}\".format(self.directory))\nelse:\nself._load_key()\ndirectory = self.dirs[self.dir_idx]\nself.system.load_state(directory)\ndata.write_with_log(\"epoch\", self.system.epoch_idx)\nprint(\"FastEstimator-RestoreWizard: Restoring from {}, resume training\".format(directory))\nself.dir_idx = int(not self.dir_idx)  # Flip the idx so that next save goes to other dir\nself._cleanup(self.dirs[self.dir_idx])  # Clean out the other dir in case it had a partial save\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.epoch_idx % self.frequency == 0:\ndirectory = self.dirs[self.dir_idx]\nself.system.save_state(directory)\nself._write_key()\n# Everything after this is free to die without causing problems with restore\nself.dir_idx = int(not self.dir_idx)\nself._cleanup(self.dirs[self.dir_idx])\nprint(\"FastEstimator-RestoreWizard: Saved milestones to {}\".format(directory))\ndef should_restore(self) -&gt; bool:\n\"\"\"Whether a restore will be performed.\n        Returns:\n            True iff the wizard will perform a restore.\n        \"\"\"\nreturn os.path.exists(self.directory) and os.path.exists(self.key_path)\ndef _load_key(self) -&gt; None:\n\"\"\"Set the dir_idx based on the key last saved by the restore wizard.\n        Raises:\n            ValueError: If the key file has been modified.\n        \"\"\"\nwith open(self.key_path, 'r') as key_file:\nkey = key_file.readline()\nif key not in ('A', 'B'):\nraise ValueError(\"RestoreWizard encountered an invalid key file at {}. Either delete it to restart, or undo\"\n\" whatever manual changes were made to the file.\".format(self.key_path))\nself.dir_idx = 0 if key == 'A' else 1\ndef _write_key(self) -&gt; None:\n\"\"\"Generate a new key file and then atomically replace the old key file.\n        \"\"\"\nsub_dir = self.dirs[self.dir_idx]\nnew_key_path = os.path.join(sub_dir, 'key.txt')\nwith open(new_key_path, 'w') as new_key_file:\nnew_key_file.write(\"B\" if self.dir_idx else \"A\")\nos.replace(new_key_path, self.key_path)  # This operation is atomic per POSIX requirements\n@staticmethod\ndef _cleanup(paths: Union[str, List[str]]) -&gt; None:\n\"\"\"Delete stale directories if they exist.\n        Args:\n            paths: Which directories to delete.\n        \"\"\"\npaths = to_list(paths)\nfor path in paths:\nif os.path.exists(path):\nshutil.rmtree(path)\n</code></pre>"}, {"location": "fastestimator/trace/io/restore_wizard.html#fastestimator.fastestimator.trace.io.restore_wizard.RestoreWizard.should_restore", "title": "<code>should_restore</code>", "text": "<p>Whether a restore will be performed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True iff the wizard will perform a restore.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\restore_wizard.py</code> <pre><code>def should_restore(self) -&gt; bool:\n\"\"\"Whether a restore will be performed.\n    Returns:\n        True iff the wizard will perform a restore.\n    \"\"\"\nreturn os.path.exists(self.directory) and os.path.exists(self.key_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/tensorboard.html", "title": "tensorboard", "text": ""}, {"location": "fastestimator/trace/io/tensorboard.html#fastestimator.fastestimator.trace.io.tensorboard.TensorBoard", "title": "<code>TensorBoard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Output data for use in TensorBoard.</p> <p>Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the --reload_multifile=true flag until their multi-writer use case is finished: https://github.com/tensorflow/tensorboard/issues/1063</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Path of the directory where the log files to be parsed by TensorBoard should be saved.</p> <code>'logs'</code> <code>update_freq</code> <code>Union[None, int, str]</code> <p>'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000, the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace mostly useless.</p> <code>100</code> <code>write_graph</code> <code>bool</code> <p>Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.</p> <code>True</code> <code>write_images</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard images.</p> <code>None</code> <code>weight_histogram_freq</code> <code>Union[None, int, str]</code> <p>Frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. Same argument format as <code>update_freq</code>.</p> <code>None</code> <code>paint_weights</code> <code>bool</code> <p>If True the system will attempt to visualize model weights as an image.</p> <code>False</code> <code>write_embeddings</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard embeddings.</p> <code>None</code> <code>embedding_labels</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to label information for the <code>write_embeddings</code>.</p> <code>None</code> <code>embedding_images</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to raw images to be associated with the <code>write_embeddings</code>.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\tensorboard.py</code> <pre><code>@traceable()\nclass TensorBoard(Trace):\n\"\"\"Output data for use in TensorBoard.\n    Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the\n    --reload_multifile=true flag until their multi-writer use case is finished:\n    https://github.com/tensorflow/tensorboard/issues/1063\n    Args:\n        log_dir: Path of the directory where the log files to be parsed by TensorBoard should be saved.\n        update_freq: 'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and\n            metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000,\n            the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings\n            like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to\n            TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace\n            mostly useless.\n        write_graph: Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph\n            is set to True.\n        write_images: If a string or list of strings is provided, the corresponding keys will be written to TensorBoard\n            images.\n        weight_histogram_freq: Frequency (in epochs) at which to compute activation and weight histograms for the layers\n            of the model. Same argument format as `update_freq`.\n        paint_weights: If True the system will attempt to visualize model weights as an image.\n        write_embeddings: If a string or list of strings is provided, the corresponding keys will be written to\n            TensorBoard embeddings.\n        embedding_labels: Keys corresponding to label information for the `write_embeddings`.\n        embedding_images: Keys corresponding to raw images to be associated with the `write_embeddings`.\n    \"\"\"\nFreq = namedtuple('Freq', ['is_step', 'freq'])\nwriter: _BaseWriter\ndef __init__(self,\nlog_dir: str = 'logs',\nupdate_freq: Union[None, int, str] = 100,\nwrite_graph: bool = True,\nwrite_images: Union[None, str, List[str]] = None,\nweight_histogram_freq: Union[None, int, str] = None,\npaint_weights: bool = False,\nwrite_embeddings: Union[None, str, List[str]] = None,\nembedding_labels: Union[None, str, List[str]] = None,\nembedding_images: Union[None, str, List[str]] = None) -&gt; None:\nsuper().__init__(inputs=\"*\")\nself.root_log_dir = log_dir\nself.update_freq = self._parse_freq(update_freq)\nself.write_graph = write_graph\nself.painted_graphs = set()\nself.write_images = to_set(write_images)\nself.histogram_freq = self._parse_freq(weight_histogram_freq)\nif paint_weights and self.histogram_freq.freq == 0:\nself.histogram_freq.is_step = False\nself.histogram_freq.freq = 1\nself.paint_weights = paint_weights\nwrite_embeddings = to_list(write_embeddings)\nembedding_labels = to_list(embedding_labels)\nif embedding_labels:\nassert len(embedding_labels) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_labels keys, but recieved {len(embedding_labels)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_labels = [None for _ in range(len(write_embeddings))]\nembedding_images = to_list(embedding_images)\nif embedding_images:\nassert len(embedding_images) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_images keys, but recieved {len(embedding_images)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_images = [None for _ in range(len(write_embeddings))]\nself.write_embeddings = [(feature, label, img_label) for feature,\nlabel,\nimg_label in zip(write_embeddings, embedding_labels, embedding_images)]\ndef _parse_freq(self, freq: Union[None, str, int]) -&gt; Freq:\n\"\"\"A helper function to convert string based frequency inputs into epochs or steps\n        Args:\n            freq: One of either None, \"step\", \"epoch\", \"#s\", \"#e\", or #, where # is an integer.\n        Returns:\n            A `Freq` object recording whether the trace should run on an epoch basis or a step basis, as well as the\n            frequency with which it should run.\n        \"\"\"\nif freq is None:\nreturn self.Freq(False, 0)\nif isinstance(freq, int):\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(True, freq)\nif isinstance(freq, str):\nif freq in {'step', 's'}:\nreturn self.Freq(True, 1)\nif freq in {'epoch', 'e'}:\nreturn self.Freq(False, 1)\nparts = re.match(r\"^([0-9]+)([se])$\", freq)\nif parts is None:\nraise ValueError(f\"Tensorboard frequency argument must be formatted like &lt;int&gt;&lt;s|e&gt; but got {freq}\")\nfreq = int(parts[1])\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(parts[2] == 's', freq)\nelse:\nraise ValueError(f\"Unrecognized type passed as Tensorboard frequency: {type(freq)}\")\ndef on_begin(self, data: Data) -&gt; None:\nprint(\"FastEstimator-Tensorboard: writing logs to {}\".format(\nos.path.abspath(os.path.join(self.root_log_dir, self.system.experiment_time))))\nself.writer = _TfWriter(self.root_log_dir, self.system.experiment_time, self.system.network) if isinstance(\nself.system.network, TFNetwork) else _TorchWriter(\nself.root_log_dir, self.system.experiment_time, self.system.network)\nif self.write_graph and self.system.global_step == 1:\nself.painted_graphs = set()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.write_graph and self.system.network.epoch_models.symmetric_difference(self.painted_graphs):\nself.writer.write_epoch_models(mode=self.system.mode)\nself.painted_graphs = self.system.network.epoch_models\nif self.system.mode != 'train':\nreturn\nif self.histogram_freq.freq and self.histogram_freq.is_step and \\\n                self.system.global_step % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\nif self.update_freq.freq and self.update_freq.is_step and self.system.global_step % self.update_freq.freq == 0:\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == 'train' and self.histogram_freq.freq and not self.histogram_freq.is_step and \\\n                self.system.epoch_idx % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\nif self.update_freq.freq and (self.update_freq.is_step or self.system.epoch_idx % self.update_freq.freq == 0):\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\ndef on_end(self, data: Data) -&gt; None:\nself.writer.close()\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html", "title": "test_report", "text": ""}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestCase", "title": "<code>TestCase</code>", "text": "<p>This class defines the test case that the TestReport trace will take to perform auto-testing.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A test description.</p> required <code>criteria</code> <code>Callable[..., Union[bool, np.ndarray]]</code> <p>A function to perform the test. For an aggregate test, <code>criteria</code> needs to return True when the test passes and False when it fails. For a per-instance test, <code>criteria</code> needs to return a boolean np.ndarray, where entries show corresponding test results (True if the test of that data instance passes; False if it fails).</p> required <code>aggregate</code> <code>bool</code> <p>If True, this test is aggregate type and its <code>criteria</code> function will be examined at epoch_end. If False, this test is per-instance type and its <code>criteria</code> function will be examined at batch_end.</p> <code>True</code> <code>fail_threshold</code> <code>int</code> <p>Threshold of failure instance number to judge the per-instance test as failed or passed. If the failure number is above this value, then the test fails; otherwise it passes. It can only be set when <code>aggregate</code> is equal to False.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If user set <code>fail_threshold</code> for an aggregate test.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@traceable()\nclass TestCase:\n\"\"\"This class defines the test case that the TestReport trace will take to perform auto-testing.\n    Args:\n        description: A test description.\n        criteria: A function to perform the test. For an aggregate test, `criteria` needs to return True when the test\n            passes and False when it fails. For a per-instance test, `criteria` needs to return a boolean np.ndarray,\n            where entries show corresponding test results (True if the test of that data instance passes; False if it\n            fails).\n        aggregate: If True, this test is aggregate type and its `criteria` function will be examined at epoch_end. If\n            False, this test is per-instance type and its `criteria` function will be examined at batch_end.\n        fail_threshold: Threshold of failure instance number to judge the per-instance test as failed or passed. If\n            the failure number is above this value, then the test fails; otherwise it passes. It can only be set when\n            `aggregate` is equal to False.\n    Raises:\n        ValueError: If user set `fail_threshold` for an aggregate test.\n    \"\"\"\ndef __init__(self,\ndescription: str,\ncriteria: Callable[..., Union[bool, np.ndarray]],\naggregate: bool = True,\nfail_threshold: int = 0) -&gt; None:\nself.description = description\nself.criteria = criteria\nself.criteria_inputs = inspect.signature(criteria).parameters.keys()\nself.aggregate = aggregate\nif self.aggregate:\nif fail_threshold:\nraise ValueError(\"fail_threshold cannot be set in a aggregate test\")\nelse:\nself.fail_threshold = fail_threshold\nself.result = None\nself.input_val = None\nself.fail_id = []\nself.init_result()\ndef init_result(self) -&gt; None:\n\"\"\"Reset the test result.\n        \"\"\"\nif self.aggregate:\nself.result = None\nself.input_val = None\nelse:\nself.result = []\nself.fail_id = []\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestCase.init_result", "title": "<code>init_result</code>", "text": "<p>Reset the test result.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>def init_result(self) -&gt; None:\n\"\"\"Reset the test result.\n    \"\"\"\nif self.aggregate:\nself.result = None\nself.input_val = None\nelse:\nself.result = []\nself.fail_id = []\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport", "title": "<code>TestReport</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Automate testing and report generation.</p> <p>This trace will evaluate all its <code>test_cases</code> during test mode and generate a PDF report and a JSON test result.</p> <p>Parameters:</p> Name Type Description Default <code>test_cases</code> <code>Union[TestCase, List[TestCase]]</code> <p>The test(s) to be run.</p> required <code>save_path</code> <code>str</code> <p>Where to save the outputs.</p> required <code>test_title</code> <code>Optional[str]</code> <p>The title of the test, or None to use the experiment name.</p> <code>None</code> <code>data_id</code> <code>str</code> <p>Data instance ID key. If provided, then per-instances test will include failing instance IDs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@traceable()\nclass TestReport(Trace):\n\"\"\"Automate testing and report generation.\n    This trace will evaluate all its `test_cases` during test mode and generate a PDF report and a JSON test result.\n    Args:\n        test_cases: The test(s) to be run.\n        save_path: Where to save the outputs.\n        test_title: The title of the test, or None to use the experiment name.\n        data_id: Data instance ID key. If provided, then per-instances test will include failing instance IDs.\n    \"\"\"\ndef __init__(self,\ntest_cases: Union[TestCase, List[TestCase]],\nsave_path: str,\ntest_title: Optional[str] = None,\ndata_id: str = None) -&gt; None:\nself.check_pdf_dependency()\nself.test_title = test_title\nself.report_name = None\nself.instance_cases = []\nself.aggregate_cases = []\nself.data_id = data_id\nall_inputs = to_set(self.data_id)\nfor case in to_list(test_cases):\nall_inputs.update(case.criteria_inputs)\nif case.aggregate:\nself.aggregate_cases.append(case)\nelse:\nself.instance_cases.append(case)\npath = os.path.normpath(save_path)\npath = os.path.abspath(path)\nroot_dir = os.path.dirname(path)\nreport = os.path.basename(path) or 'report'\nreport = report.split('.')[0]\nself.save_dir = os.path.join(root_dir, report)\nself.resource_dir = os.path.join(self.save_dir, \"resources\")\nos.makedirs(self.save_dir, exist_ok=True)\nos.makedirs(self.resource_dir, exist_ok=True)\nself.json_summary = {}\n# PDF document related\nself.doc = None\nself.test_id = None\nsuper().__init__(inputs=all_inputs, mode=\"test\")\ndef on_begin(self, data: Data) -&gt; None:\nself._sanitize_report_name()\nself._initialize_json_summary()\nfor case in self.instance_cases + self.aggregate_cases:\ncase.init_result()\ndef on_batch_end(self, data: Data) -&gt; None:\nfor case in self.instance_cases:\nresult = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\nif not isinstance(result, np.ndarray):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of per-instance test needs to be ndarray with dtype bool.\")\nelif result.dtype != np.dtype(\"bool\"):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of per-instance test needs to be ndarray with dtype bool.\")\nresult = result.reshape(-1)\ncase.result.append(result)\nif self.data_id:\ndata_id = to_number(data[self.data_id]).reshape((-1, ))\nif data_id.size != result.size:\nraise ValueError(f\"In test with description '{case.description}': \"\n\"Array size of criteria return doesn't match ID array size. Size of criteria\"\n\"return should be equal to the batch_size such that each entry represents the test\"\n\"result of its corresponding data instance.\")\ncase.fail_id.append(data_id[result == False])\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor case in self.aggregate_cases:\nresult = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\nif not isinstance(result, (bool, np.bool_)):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of aggregate-case test needs to be a bool.\")\ncase.result = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\ncase.input_val = {var_name: self._to_serializable(data[var_name]) for var_name in case.criteria_inputs}\ndef on_end(self, data: Data) -&gt; None:\nfor case in self.instance_cases:\ncase_dict = {\"test_type\": \"per-instance\", \"description\": case.description}\nresult = np.hstack(case.result)\nfail_num = np.sum(result == False)\ncase_dict[\"passed\"] = self._to_serializable(fail_num &lt;= case.fail_threshold)\ncase_dict[\"fail_threshold\"] = case.fail_threshold\ncase_dict[\"fail_number\"] = self._to_serializable(fail_num)\nif self.data_id:\nfail_id = np.hstack(case.fail_id)\ncase_dict[\"fail_id\"] = self._to_serializable(fail_id)\nself.json_summary[\"tests\"].append(case_dict)\nfor case in self.aggregate_cases:\ncase_dict = {\n\"test_type\": \"aggregate\",\n\"description\": case.description,\n\"passed\": self._to_serializable(case.result),\n\"inputs\": case.input_val\n}\nself.json_summary[\"tests\"].append(case_dict)\nself.json_summary[\"execution_time(s)\"] = time() - self.json_summary[\"execution_time(s)\"]\nself._dump_json()\nself._init_document()\nself._write_body_content()\nself._dump_pdf()\ndef _initialize_json_summary(self) -&gt; None:\n\"\"\"Initialize json summary.\n        \"\"\"\nself.json_summary = {\n\"title\": self.test_title, \"timestamp\": str(datetime.now()), \"execution_time(s)\": time(), \"tests\": []\n}\ndef _sanitize_report_name(self) -&gt; None:\n\"\"\"Sanitize report name and make it class attribute.\n        Raises:\n            RuntimeError: If a test title was not provided and the user did not set an experiment name.\n        \"\"\"\nexp_name = self.system.summary.name or self.test_title\nif not exp_name:\nraise RuntimeError(\"TestReport requires an experiment name to be provided in estimator.fit(), or a title\")\n# Convert the experiment name to a report name (useful for saving multiple experiments into same directory)\nreport_name = \"\".join('_' if c == ' ' else c for c in exp_name\nif c.isalnum() or c in (' ', '_')).rstrip(\"_\").lower()\nself.report_name = re.sub('_{2,}', '_', report_name) + \"_TestReport\"\nif self.test_title is None:\nself.test_title = exp_name\ndef _init_document(self) -&gt; None:\n\"\"\"Initialize latex document.\n        \"\"\"\nself.doc = self._init_document_geometry()\nself.doc.packages.append(Package(name='placeins', options=['section']))\nself.doc.packages.append(Package(name='float'))\nself.doc.packages.append(Package(name='hyperref', options='hidelinks'))\nself.doc.preamble.append(NoEscape(r'\\aboverulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\belowrulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\renewcommand{\\arraystretch}{1.2}'))\n# new column type for tabularx\nself.doc.preamble.append(NoEscape(r'\\newcolumntype{Y}{&gt;{\\centering\\arraybackslash}X}'))\nself._write_title()\nself._write_toc()\ndef _write_title(self) -&gt; None:\n\"\"\"Write the title content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.preamble.append(Command('title', self.json_summary[\"title\"]))\nself.doc.preamble.append(Command('author', f\"FastEstimator {fe.__version__}\"))\nself.doc.preamble.append(Command('date', NoEscape(r'\\today')))\nself.doc.append(NoEscape(r'\\maketitle'))\ndef _write_toc(self) -&gt; None:\n\"\"\"Write the table of contents. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.append(NoEscape(r'\\tableofcontents'))\nself.doc.append(NoEscape(r'\\newpage'))\ndef _write_body_content(self) -&gt; None:\n\"\"\"Write the main content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself._document_test_result()\ndef _document_test_result(self) -&gt; None:\n\"\"\"Document test results including test summary, passed tests, and failed tests.\n        \"\"\"\nself.test_id = 1\ninstance_pass_tests, aggregate_pass_tests, instance_fail_tests, aggregate_fail_tests = [], [], [], []\nfor test in self.json_summary[\"tests\"]:\nif test[\"test_type\"] == \"per-instance\" and test[\"passed\"]:\ninstance_pass_tests.append(test)\nelif test[\"test_type\"] == \"per-instance\" and not test[\"passed\"]:\ninstance_fail_tests.append(test)\nelif test[\"test_type\"] == \"aggregate\" and test[\"passed\"]:\naggregate_pass_tests.append(test)\nelif test[\"test_type\"] == \"aggregate\" and not test[\"passed\"]:\naggregate_fail_tests.append(test)\nwith self.doc.create(Section(\"Test Summary\")):\nwith self.doc.create(Itemize()) as itemize:\nitemize.add_item(\nescape_latex(\"Execution time: {:.2f} seconds\".format(self.json_summary['execution_time(s)'])))\nwith self.doc.create(Table(position='H')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\nself._document_summary_table(pass_num=len(instance_pass_tests) + len(aggregate_pass_tests),\nfail_num=len(instance_fail_tests) + len(aggregate_fail_tests))\nif instance_fail_tests or aggregate_fail_tests:\nwith self.doc.create(Section(\"Failed Tests\")):\nif len(aggregate_fail_tests) &gt; 0:\nwith self.doc.create(Subsection(\"Failed Aggregate Tests\")):\nself._document_aggregate_table(tests=aggregate_fail_tests)\nif len(instance_fail_tests) &gt; 0:\nwith self.doc.create(Subsection(\"Failed Per-Instance Tests\")):\nself._document_instance_table(tests=instance_fail_tests, with_id=bool(self.data_id))\nif instance_pass_tests or aggregate_pass_tests:\nwith self.doc.create(Section(\"Passed Tests\")):\nif aggregate_pass_tests:\nwith self.doc.create(Subsection(\"Passed Aggregate Tests\")):\nself._document_aggregate_table(tests=aggregate_pass_tests)\nif instance_pass_tests:\nwith self.doc.create(Subsection(\"Passed Per-Instance Tests\")):\nself._document_instance_table(tests=instance_pass_tests, with_id=bool(self.data_id))\nself.doc.append(NoEscape(r'\\newpage'))  # For QMS report\ndef _document_summary_table(self, pass_num: int, fail_num: int) -&gt; None:\n\"\"\"Document a summary table.\n        Args:\n            pass_num: Total number of passed tests.\n            fail_num: Total number of failed tests.\n        \"\"\"\nwith self.doc.create(Tabularx('|Y|Y|Y|', booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\ntabular.add_row((\"Total Tests\", \"Total Passed \", \"Total Failed\"), strict=False)\ntabular.add_hline()\ntabular.add_row((pass_num + fail_num, pass_num, fail_num), strict=False)\ndef _document_instance_table(self, tests: List[Dict[str, Any]], with_id: bool):\n\"\"\"Document a result table of per-instance tests.\n        Args:\n            tests: List of corresponding test dictionary to make a table.\n            with_id: Whether the test information includes data ID.\n        \"\"\"\nif with_id:\ntable_spec = '|c|p{5cm}|c|c|p{5cm}|'\ncolumn_num = 5\nelse:\ntable_spec = '|c|p{10cm}|c|c|'\ncolumn_num = 4\nwith self.doc.create(LongTable(table_spec, pos=['h!'], booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\nrow_cells = [\nMultiColumn(size=1, align='|c|', data=\"Test ID\"),\nMultiColumn(size=1, align='c|', data=\"Test Description\"),\nMultiColumn(size=1, align='c|', data=\"Pass Threshold\"),\nMultiColumn(size=1, align='c|', data=\"Failure Count\")\n]\nif with_id:\nrow_cells.append(MultiColumn(size=1, align='c|', data=\"Failure Data Instance ID\"))\ntabular.add_row(row_cells)\n# add table header and footer\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(column_num, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\nfor idx, test in enumerate(tests):\nif idx &gt; 0:\ntabular.add_hline()\ndes_data = [WrapText(data=x, threshold=27) for x in test[\"description\"].split(\" \")]\nrow_cells = [\nself.test_id,\nIterJoin(data=des_data, token=\" \"),\nNoEscape(r'$\\le $' + str(test[\"fail_threshold\"])),\ntest[\"fail_number\"]\n]\nif with_id:\nid_data = [WrapText(data=x, threshold=27) for x in test[\"fail_id\"]]\nrow_cells.append(IterJoin(data=id_data, token=\", \"))\ntabular.add_row(row_cells)\nself.test_id += 1\ndef _document_aggregate_table(self, tests: List[Dict[str, Any]]) -&gt; None:\n\"\"\"Document a result table of aggregate tests.\n        Args:\n            tests: List of corresponding test dictionary to make a table.\n        \"\"\"\nwith self.doc.create(LongTable('|c|p{8cm}|p{7.3cm}|', booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\ntabular.add_row((MultiColumn(size=1, align='|c|', data=\"Test ID\"),\nMultiColumn(size=1, align='c|', data=\"Test Description\"),\nMultiColumn(size=1, align='c|', data=\"Input Value\")))\n# add table header and footer\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(3, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\nfor idx, test in enumerate(tests):\nif idx &gt; 0:\ntabular.add_hline()\ninp_data = [f\"{arg}={self.sanitize_value(value)}\" for arg, value in test[\"inputs\"].items()]\ninp_data = [WrapText(data=x, threshold=27) for x in inp_data]\ndes_data = [WrapText(data=x, threshold=27) for x in test[\"description\"].split(\" \")]\nrow_cells = [\nself.test_id,\nIterJoin(data=des_data, token=\" \"),\nIterJoin(data=inp_data, token=escape_latex(\", \\n\")),\n]\ntabular.add_row(row_cells)\nself.test_id += 1\ndef _dump_pdf(self) -&gt; None:\n\"\"\"Dump PDF summary report.\n        \"\"\"\nif shutil.which(\"latexmk\") is None and shutil.which(\"pdflatex\") is None:\n# No LaTeX Compiler is available\nself.doc.generate_tex(os.path.join(self.save_dir, self.report_name))\nsuffix = '.tex'\nelse:\n# Force a double-compile since some compilers will struggle with TOC generation\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False, clean=False)\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False)\nsuffix = '.pdf'\nprint(\"FastEstimator-TestReport: Report written to {}{}\".format(os.path.join(self.save_dir, self.report_name),\nsuffix))\ndef _dump_json(self) -&gt; None:\n\"\"\"Dump JSON file.\n        \"\"\"\njson_path = os.path.join(self.resource_dir, self.report_name + \".json\")\nwith open(json_path, 'w') as fp:\njson.dump(self.json_summary, fp, indent=4)\n@staticmethod\ndef _to_serializable(obj: Any) -&gt; Union[float, int, list]:\n\"\"\"Convert to JSON serializable type.\n        Args:\n            obj: Any object that needs to be converted.\n        Return:\n            JSON serializable object that essentially is equivalent to input obj.\n        \"\"\"\nif isinstance(obj, np.ndarray):\nif obj.size &gt; 0:\nshape = obj.shape\nobj = obj.reshape((-1, ))\nobj = np.vectorize(TestReport._element_to_serializable)(obj)\nobj = obj.reshape(shape)\nobj = obj.tolist()\nelse:\nobj = TestReport._element_to_serializable(obj)\nreturn obj\n@staticmethod\ndef _element_to_serializable(obj: Any) -&gt; Any:\n\"\"\"Convert to JSON serializable type.\n        This function can handle any object type except ndarray.\n        Args:\n            obj: Any object except ndarray that needs to be converted.\n        Return:\n            JSON serializable object that essentially is equivalent to input obj.\n        \"\"\"\nif isinstance(obj, bytes):\nobj = obj.decode('utf-8')\nelif isinstance(obj, np.generic):\nobj = obj.item()\nreturn obj\n@staticmethod\ndef check_pdf_dependency() -&gt; None:\n\"\"\"Check dependency of PDF-generating packages.\n        Raises:\n            OSError: Some required package has not been installed.\n        \"\"\"\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n                'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\n@staticmethod\ndef sanitize_value(value: Union[int, float]) -&gt; str:\n\"\"\"Sanitize input value for a better report display.\n        Args:\n            value: Value to be sanitized.\n        Returns:\n            Sanitized string of `value`.\n        \"\"\"\nif 1000 &gt; value &gt;= 0.001:\nreturn f\"{value:.3f}\"\nelse:\nreturn f\"{value:.3e}\"\n@staticmethod\ndef _init_document_geometry() -&gt; Document:\n\"\"\"Init geometry setting of the document.\n        Return:\n            Initialized Document object.\n        \"\"\"\nreturn Document(geometry_options=['lmargin=2cm', 'rmargin=2cm', 'bmargin=2cm'])\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport.check_pdf_dependency", "title": "<code>check_pdf_dependency</code>  <code>staticmethod</code>", "text": "<p>Check dependency of PDF-generating packages.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>Some required package has not been installed.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@staticmethod\ndef check_pdf_dependency() -&gt; None:\n\"\"\"Check dependency of PDF-generating packages.\n    Raises:\n        OSError: Some required package has not been installed.\n    \"\"\"\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n            'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport.sanitize_value", "title": "<code>sanitize_value</code>  <code>staticmethod</code>", "text": "<p>Sanitize input value for a better report display.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[int, float]</code> <p>Value to be sanitized.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sanitized string of <code>value</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@staticmethod\ndef sanitize_value(value: Union[int, float]) -&gt; str:\n\"\"\"Sanitize input value for a better report display.\n    Args:\n        value: Value to be sanitized.\n    Returns:\n        Sanitized string of `value`.\n    \"\"\"\nif 1000 &gt; value &gt;= 0.001:\nreturn f\"{value:.3f}\"\nelse:\nreturn f\"{value:.3e}\"\n</code></pre>"}, {"location": "fastestimator/trace/io/traceability.html", "title": "traceability", "text": ""}, {"location": "fastestimator/trace/io/traceability.html#fastestimator.fastestimator.trace.io.traceability.Traceability", "title": "<code>Traceability</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Automatically generate summary reports of the training.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Where to save the output files. Note that this will generate a new folder with the given name, into which the report and corresponding graphics assets will be written.</p> required <code>extra_objects</code> <code>Any</code> <p>Any extra objects which are not part of the Estimator, but which you want to capture in the summary report. One example could be an extra pipeline which performs pre-processing.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If graphviz is not installed.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\traceability.py</code> <pre><code>@traceable()\nclass Traceability(Trace):\n\"\"\"Automatically generate summary reports of the training.\n    Args:\n        save_path: Where to save the output files. Note that this will generate a new folder with the given name, into\n            which the report and corresponding graphics assets will be written.\n        extra_objects: Any extra objects which are not part of the Estimator, but which you want to capture in the\n            summary report. One example could be an extra pipeline which performs pre-processing.\n    Raises:\n        OSError: If graphviz is not installed.\n    \"\"\"\ndef __init__(self, save_path: str, extra_objects: Any = None):\n# Verify that graphviz is available on this machine\ntry:\npydot.Dot.create(pydot.Dot())\nexcept OSError:\nraise OSError(\n\"Traceability requires that graphviz be installed. See www.graphviz.org/download for more information.\")\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n                'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\nsuper().__init__(inputs=\"*\", mode=\"!infer\")  # Claim wildcard inputs to get this trace sorted last\n# Report assets will get saved into a folder for portability\npath = os.path.normpath(save_path)\npath = os.path.abspath(path)\nroot_dir = os.path.dirname(path)\nreport = os.path.basename(path) or 'report'\nreport = report.split('.')[0]\nself.save_dir = os.path.join(root_dir, report)\nself.resource_dir = os.path.join(self.save_dir, 'resources')\nself.report_name = None  # This will be set later by the experiment name\nos.makedirs(self.save_dir, exist_ok=True)\nos.makedirs(self.resource_dir, exist_ok=True)\n# Other member variables\nself.config_tables = []\n# Extra objects will automatically get included in the report since this Trace is @traceable, so we don't need\n# to do anything with them. Referencing here to stop IDEs from flagging the argument as unused and removing it.\nto_list(extra_objects)\nself.doc = Document()\nself.log_splicer = None\ndef on_begin(self, data: Data) -&gt; None:\nexp_name = self.system.summary.name\nif not exp_name:\nraise RuntimeError(\"Traceability reports require an experiment name to be provided in estimator.fit()\")\n# Convert the experiment name to a report name (useful for saving multiple experiments into same directory)\nreport_name = \"\".join('_' if c == ' ' else c for c in exp_name\nif c.isalnum() or c in (' ', '_')).rstrip().lower()\nreport_name = re.sub('_{2,}', '_', report_name)\nself.report_name = report_name or 'report'\n# Send experiment logs into a file\nlog_path = os.path.join(self.resource_dir, f\"{report_name}.txt\")\nif self.system.mode != 'test':\n# See if there's a RestoreWizard\nrestore = False\nfor trace in self.system.traces:\nif isinstance(trace, RestoreWizard):\nrestore = trace.should_restore()\nif not restore:\n# If not running in test mode, we need to remove any old log file since it would get appended to\nwith contextlib.suppress(FileNotFoundError):\nos.remove(log_path)\nself.log_splicer = LogSplicer(log_path)\nself.log_splicer.__enter__()\n# Get the initialization summary information for the experiment\nself.config_tables = self.system.summary.system_config\nmodels = self.system.network.models\nn_floats = len(self.config_tables) + len(models)\nself.doc = self._init_document_geometry()\n# Keep tables/figures in their sections\nself.doc.packages.append(Package(name='placeins', options=['section']))\nself.doc.preamble.append(NoEscape(r'\\usetikzlibrary{positioning}'))\n# Fix an issue with too many tables for LaTeX to render\nself.doc.preamble.append(NoEscape(r'\\maxdeadcycles=' + str(2 * n_floats + 10) + ''))\nself.doc.preamble.append(NoEscape(r'\\extrafloats{' + str(n_floats + 10) + '}'))\n# Manipulate booktab tables so that their horizontal lines don't break\nself.doc.preamble.append(NoEscape(r'\\aboverulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\belowrulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\renewcommand{\\arraystretch}{1.2}'))\nself._write_title()\nself._write_toc()\ndef on_end(self, data: Data) -&gt; None:\nself._write_body_content()\n# Need to move the tikz dependency after the xcolor package\nself.doc.dumps_packages()\npackages = self.doc.packages\ntikz = Package(name='tikz')\npackages.discard(tikz)\npackages.add(tikz)\nif shutil.which(\"latexmk\") is None and shutil.which(\"pdflatex\") is None:\n# No LaTeX Compiler is available\nself.doc.generate_tex(os.path.join(self.save_dir, self.report_name))\nsuffix = '.tex'\nelse:\n# Force a double-compile since some compilers will struggle with TOC generation\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False, clean=False)\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False)\nsuffix = '.pdf'\nprint(\"FastEstimator-Traceability: Report written to {}{}\".format(os.path.join(self.save_dir, self.report_name),\nsuffix))\nself.log_splicer.__exit__()\ndef _write_title(self) -&gt; None:\n\"\"\"Write the title content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.preamble.append(Command('title', self.system.summary.name))\nself.doc.preamble.append(Command('author', f\"FastEstimator {fe.__version__}\"))\nself.doc.preamble.append(Command('date', NoEscape(r'\\today')))\nself.doc.append(NoEscape(r'\\maketitle'))\ndef _write_toc(self) -&gt; None:\n\"\"\"Write the table of contents. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.append(NoEscape(r'\\tableofcontents'))\nself.doc.append(NoEscape(r'\\newpage'))\ndef _write_body_content(self) -&gt; None:\n\"\"\"Write the main content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself._document_training_graphs()\nself.doc.append(NoEscape(r'\\newpage'))\nself._document_fe_graph()\nself.doc.append(NoEscape(r'\\newpage'))\nself._document_init_params()\nself._document_models()\nself._document_sys_config()\nself.doc.append(NoEscape(r'\\newpage'))\ndef _document_training_graphs(self) -&gt; None:\n\"\"\"Add training graphs to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"Training Graphs\")):\nlog_path = os.path.join(self.resource_dir, f'{self.report_name}_logs.png')\nvisualize_logs(experiments=[self.system.summary],\nsave_path=log_path,\nverbose=False,\nignore_metrics={'num_device', 'logging_interval'})\nwith self.doc.create(Figure(position='h!')) as plot:\nplot.add_image(os.path.relpath(log_path, start=self.save_dir),\nwidth=NoEscape(r'1.0\\textwidth,height=0.95\\textheight,keepaspectratio'))\ndef _document_fe_graph(self) -&gt; None:\n\"\"\"Add FE execution graphs into the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"FastEstimator Architecture\")):\nfor mode in self.system.pipeline.data.keys():\nscheduled_items = self.system.pipeline.get_scheduled_items(\nmode) + self.system.network.get_scheduled_items(mode) + self.system.traces\nsignature_epochs = get_signature_epochs(scheduled_items, total_epochs=self.system.epoch_idx, mode=mode)\nepochs_with_data = self.system.pipeline.get_epochs_with_data(total_epochs=self.system.epoch_idx,\nmode=mode)\nif set(signature_epochs) &amp; epochs_with_data:\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(mode.capitalize())):\nfor epoch in signature_epochs:\nif epoch not in epochs_with_data:\ncontinue\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(\nSubsubsection(f\"Epoch {epoch}\",\nlabel=Label(Marker(name=f\"{mode}{epoch}\", prefix=\"ssubsec\")))):\ndiagram = self._draw_diagram(mode, epoch)\nltx = d2t.dot2tex(diagram.to_string(), figonly=True)\nargs = Arguments(**{'max width': r'\\textwidth, max height=0.9\\textheight'})\nargs.escape = False\nwith self.doc.create(Center()):\nwith self.doc.create(AdjustBox(arguments=args)) as box:\nbox.append(NoEscape(ltx))\ndef _document_init_params(self) -&gt; None:\n\"\"\"Add initialization parameters to the traceability document.\n        \"\"\"\nfrom fastestimator.estimator import Estimator  # Avoid circular import\nwith self.doc.create(Section(\"Parameters\")):\nmodel_ids = {\nFEID(id(model))\nfor model in self.system.network.models if isinstance(model, (tf.keras.Model, torch.nn.Module))\n}\n# Locate the datasets in order to provide extra details about them later in the summary\ndatasets = {}\nfor mode in ['train', 'eval', 'test']:\nobjs = to_list(self.system.pipeline.data.get(mode, None))\nidx = 0\nwhile idx &lt; len(objs):\nobj = objs[idx]\nif obj:\nfeid = FEID(id(obj))\nif feid not in datasets:\ndatasets[feid] = ({mode}, obj)\nelse:\ndatasets[feid][0].add(mode)\nif isinstance(obj, Scheduler):\nobjs.extend(obj.get_all_values())\nidx += 1\n# Parse the config tables\nstart = 0\nstart = self._loop_tables(start,\nclasses=(Estimator, BaseNetwork, Pipeline),\nname=\"Base Classes\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=Scheduler,\nname=\"Schedulers\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start, classes=Trace, name=\"Traces\", model_ids=model_ids, datasets=datasets)\nstart = self._loop_tables(start, classes=Op, name=\"Operators\", model_ids=model_ids, datasets=datasets)\nstart = self._loop_tables(start,\nclasses=(Dataset, tf.data.Dataset),\nname=\"Datasets\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=(tf.keras.Model, torch.nn.Module),\nname=\"Models\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=types.FunctionType,\nname=\"Functions\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=(np.ndarray, tf.Tensor, tf.Variable, torch.Tensor),\nname=\"Tensors\",\nmodel_ids=model_ids,\ndatasets=datasets)\nself._loop_tables(start, classes=Any, name=\"Miscellaneous\", model_ids=model_ids, datasets=datasets)\ndef _loop_tables(self,\nstart: int,\nclasses: Union[type, Tuple[type, ...]],\nname: str,\nmodel_ids: Set[FEID],\ndatasets: Dict[FEID, Tuple[Set[str], Any]]) -&gt; int:\n\"\"\"Iterate through tables grouping them into subsections.\n        Args:\n            start: What index to start searching from.\n            classes: What classes are acceptable for this subsection.\n            name: What to call this subsection.\n            model_ids: The ids of any known models.\n            datasets: A mapping like {ID: ({modes}, dataset)}. Useful for augmenting the displayed information.\n        Returns:\n            The new start index after traversing as many spaces as possible along the list of tables.\n        \"\"\"\nstop = start\nwhile stop &lt; len(self.config_tables):\nif classes == Any or issubclass(self.config_tables[stop].type, classes):\nstop += 1\nelse:\nbreak\nif stop &gt; start:\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(name)):\nself._write_tables(self.config_tables[start:stop], model_ids, datasets)\nreturn stop\ndef _write_tables(self,\ntables: List[FeSummaryTable],\nmodel_ids: Set[FEID],\ndatasets: Dict[FEID, Tuple[Set[str], Any]]) -&gt; None:\n\"\"\"Insert a LaTeX representation of a list of tables into the current doc.\n        Args:\n            tables: The tables to write into the doc.\n            model_ids: The ids of any known models.\n            datasets: A mapping like {ID: ({modes}, dataset)}. Useful for augmenting the displayed information.\n        \"\"\"\nfor tbl in tables:\nname_override = None\ntoc_ref = None\nextra_rows = None\nif tbl.fe_id in model_ids:\n# Link to a later detailed model description\nname_override = Hyperref(Marker(name=str(tbl.name), prefix=\"subsec\"),\ntext=NoEscape(r'\\textcolor{blue}{') + bold(tbl.name) + NoEscape('}'))\nif tbl.fe_id in datasets:\nmodes, dataset = datasets[tbl.fe_id]\ntitle = \", \".join([s.capitalize() for s in modes])\nname_override = bold(f'{tbl.name} ({title})')\n# Enhance the dataset summary\nif isinstance(dataset, FEDataset):\nextra_rows = list(dataset.summary().__getstate__().items())\nfor idx, (key, val) in enumerate(extra_rows):\nkey = f\"{prettify_metric_name(key)}:\"\nif isinstance(val, dict) and val:\nif isinstance(list(val.values())[0], (int, float, str, bool, type(None))):\nval = jsonpickle.dumps(val, unpicklable=False)\nelse:\nsubtable = Tabularx('l|X', width_argument=NoEscape(r'\\linewidth'))\nfor k, v in val.items():\nif hasattr(v, '__getstate__'):\nv = jsonpickle.dumps(v, unpicklable=False)\nsubtable.add_row((k, v))\n# To nest TabularX, have to wrap it in brackets\nsubtable = ContainerList(data=[NoEscape(\"{\"), subtable, NoEscape(\"}\")])\nval = subtable\nextra_rows[idx] = (key, val)\ntbl.render_table(self.doc, name_override=name_override, toc_ref=toc_ref, extra_rows=extra_rows)\ndef _document_models(self) -&gt; None:\n\"\"\"Add model summaries to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"Models\")):\nfor model in humansorted(self.system.network.models, key=lambda m: m.model_name):\nif not isinstance(model, (tf.keras.Model, torch.nn.Module)):\ncontinue\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(f\"{model.model_name.capitalize()}\")):\nif isinstance(model, tf.keras.Model):\n# Text Summary\nsummary = []\nmodel.summary(line_length=92, print_fn=lambda x: summary.append(x))\nsummary = \"\\n\".join(summary)\nself.doc.append(Verbatim(summary))\nwith self.doc.create(Center()):\nself.doc.append(HrefFEID(FEID(id(model)), model.model_name))\n# Visual Summary\n# noinspection PyBroadException\ntry:\nfile_path = os.path.join(self.resource_dir,\n\"{}_{}.pdf\".format(self.report_name, model.model_name))\ndot = tf.keras.utils.model_to_dot(model, show_shapes=True, expand_nested=True)\n# LaTeX \\maxdim is around 575cm (226 inches), so the image must have max dimension less than\n# 226 inches. However, the 'size' parameter doesn't account for the whole node height, so\n# set the limit lower (100 inches) to leave some wiggle room.\ndot.set('size', '100')\ndot.write(file_path, format='pdf')\nexcept Exception:\nfile_path = None\nprint(\nf\"FastEstimator-Warn: Model {model.model_name} could not be visualized by Traceability\")\nelif isinstance(model, torch.nn.Module):\nif hasattr(model, 'fe_input_spec'):\n# Text Summary\n# noinspection PyUnresolvedReferences\ninputs = model.fe_input_spec.get_dummy_input()\nself.doc.append(\nVerbatim(\npms.summary(model.module if self.system.num_devices &gt; 1 else model,\ninputs,\nprint_summary=False)))\nwith self.doc.create(Center()):\nself.doc.append(HrefFEID(FEID(id(model)), model.model_name))\n# Visual Summary\n# Import has to be done while matplotlib is using the Agg backend\nold_backend = matplotlib.get_backend() or 'Agg'\nmatplotlib.use('Agg')\n# noinspection PyBroadException\ntry:\n# Fake the IPython import when user isn't running from Jupyter\nsys.modules.setdefault('IPython', MagicMock())\nsys.modules.setdefault('IPython.display', MagicMock())\nimport hiddenlayer as hl\nwith Suppressor():\ngraph = hl.build_graph(model.module if self.system.num_devices &gt; 1 else model,\ninputs)\ngraph = graph.build_dot()\ngraph.attr(rankdir='TB')  # Switch it to Top-to-Bottom instead of Left-to-Right\n# LaTeX \\maxdim is around 575cm (226 inches), so the image must have max dimension less\n# than 226 inches. However, the 'size' parameter doesn't account for the whole node\n# height, so set the limit lower (100 inches) to leave some wiggle room.\ngraph.attr(size=\"100,100\")\ngraph.attr(margin='0')\nfile_path = graph.render(filename=\"{}_{}\".format(self.report_name, model.model_name),\ndirectory=self.resource_dir,\nformat='pdf',\ncleanup=True)\nexcept Exception:\nfile_path = None\nprint(\"FastEstimator-Warn: Model {} could not be visualized by Traceability\".format(\nmodel.model_name))\nfinally:\nmatplotlib.use(old_backend)\nelse:\nfile_path = None\nself.doc.append(\"This model was not used by the Network during training.\")\nif file_path:\nwith self.doc.create(Figure(position='ht!')) as fig:\nfig.append(Label(Marker(name=str(FEID(id(model))), prefix=\"model\")))\nfig.add_image(os.path.relpath(file_path, start=self.save_dir),\nwidth=NoEscape(r'1.0\\textwidth,height=0.95\\textheight,keepaspectratio'))\nfig.add_caption(NoEscape(HrefFEID(FEID(id(model)), model.model_name).dumps()))\ndef _document_sys_config(self) -&gt; None:\n\"\"\"Add a system config summary to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"System Configuration\")):\nwith self.doc.create(Itemize()) as itemize:\nitemize.add_item(escape_latex(f\"FastEstimator {fe.__version__}\"))\nitemize.add_item(escape_latex(f\"Python {platform.python_version()}\"))\nitemize.add_item(escape_latex(f\"OS: {sys.platform}\"))\nitemize.add_item(f\"Number of GPUs: {torch.cuda.device_count()}\")\nif fe.fe_deterministic_seed is not None:\nitemize.add_item(escape_latex(f\"Deterministic Seed: {fe.fe_deterministic_seed}\"))\nwith self.doc.create(LongTable('|lr|', pos=['h!'], booktabs=True)) as tabular:\ntabular.add_row((bold(\"Module\"), bold(\"Version\")))\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(2, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\ncolor = True\nfor name, module in humansorted(sys.modules.items(), key=lambda x: x[0]):\nif \".\" in name:\ncontinue  # Skip sub-packages\nif name.startswith(\"_\"):\ncontinue  # Skip private packages\nif isinstance(module, Base):\ncontinue  # Skip fake packages we mocked\nif hasattr(module, '__version__'):\ntabular.add_row((escape_latex(name), escape_latex(str(module.__version__))),\ncolor='black!5' if color else 'white')\ncolor = not color\nelif hasattr(module, 'VERSION'):\ntabular.add_row((escape_latex(name), escape_latex(str(module.VERSION))),\ncolor='black!5' if color else 'white')\ncolor = not color\ndef _draw_diagram(self, mode: str, epoch: int) -&gt; pydot.Dot:\n\"\"\"Draw a summary diagram of the FastEstimator Ops / Traces.\n        Args:\n            mode: The execution mode to summarize ('train', 'eval', 'test', or 'infer').\n            epoch: The epoch to summarize.\n        Returns:\n            A pydot digraph representing the execution flow.\n        \"\"\"\nds = self.system.pipeline.data[mode]\nif isinstance(ds, Scheduler):\nds = ds.get_current_value(epoch)\npipe_ops = get_current_items(self.system.pipeline.ops, run_modes=mode, epoch=epoch) if isinstance(\nds, Dataset) else []\nnet_ops = get_current_items(self.system.network.ops, run_modes=mode, epoch=epoch)\ntraces = sort_traces(get_current_items(self.system.traces, run_modes=mode, epoch=epoch))\ndiagram = pydot.Dot(compound='true')  # Compound lets you draw edges which terminate at sub-graphs\ndiagram.set('rankdir', 'TB')\ndiagram.set('dpi', 300)\ndiagram.set_node_defaults(shape='box')\n# Make the dataset the first of the pipeline ops\npipe_ops.insert(0, ds)\nlabel_last_seen = defaultdict(lambda: str(id(ds)))  # Where was this key last generated\nbatch_size = \"\"\nif isinstance(ds, Dataset) and not isinstance(ds, BatchDataset):\nbatch_size = self.system.pipeline.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\nif batch_size is not None:\nbatch_size = f\" (Batch Size: {batch_size})\"\nself._draw_subgraph(diagram, diagram, label_last_seen, f'Pipeline{batch_size}', pipe_ops)\nself._draw_subgraph(diagram, diagram, label_last_seen, 'Network', net_ops)\nself._draw_subgraph(diagram, diagram, label_last_seen, 'Traces', traces)\nreturn diagram\n@staticmethod\ndef _draw_subgraph(progenitor: pydot.Dot,\ndiagram: Union[pydot.Dot, pydot.Cluster],\nlabel_last_seen: DefaultDict[str, str],\nsubgraph_name: str,\nsubgraph_ops: List[Union[Op, Trace, Any]]) -&gt; None:\n\"\"\"Draw a subgraph of ops into an existing `diagram`.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            diagram: The diagram into which to add new Nodes.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n            subgraph_name: The name to be associated with this subgraph.\n            subgraph_ops: The ops to be wrapped in this subgraph.\n        \"\"\"\nsubgraph = pydot.Cluster(style='dashed', graph_name=subgraph_name, color='black')\nsubgraph.set('label', subgraph_name)\nsubgraph.set('labeljust', 'l')\nfor idx, op in enumerate(subgraph_ops):\nnode_id = str(id(op))\nTraceability._add_node(progenitor, subgraph, op, label_last_seen)\nif isinstance(op, Trace) and idx &gt; 0:\n# Invisibly connect traces in order so that they aren't all just squashed horizontally into the image\nprogenitor.add_edge(pydot.Edge(src=str(id(subgraph_ops[idx - 1])), dst=node_id, style='invis'))\ndiagram.add_subgraph(subgraph)\n@staticmethod\ndef _add_node(progenitor: pydot.Dot,\ndiagram: Union[pydot.Dot, pydot.Cluster],\nop: Union[Op, Trace, Any],\nlabel_last_seen: DefaultDict[str, str],\nedges: bool = True) -&gt; None:\n\"\"\"Draw a node onto a diagram based on a given op.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            diagram: The diagram to be appended to.\n            op: The op (or trace) to be visualized.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n            edges: Whether to write Edges to/from this Node.\n        \"\"\"\nnode_id = str(id(op))\nif isinstance(op, (Sometimes, SometimesT)) and op.op:\nwrapper = pydot.Cluster(style='dotted', color='red', graph_name=str(id(op)))\nwrapper.set('label', f'Sometimes ({op.prob}):')\nwrapper.set('labeljust', 'l')\nedge_srcs = defaultdict(lambda: [])\nif op.extra_inputs:\nfor inp in op.extra_inputs:\nif inp == '*':\ncontinue\nedge_srcs[label_last_seen[inp]].append(inp)\nTraceability._add_node(progenitor, wrapper, op.op, label_last_seen)\ndiagram.add_subgraph(wrapper)\ndst_id = Traceability._get_all_nodes(wrapper)[0].get_name()\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(\npydot.Edge(src=src, dst=dst_id, lhead=wrapper.get_name(), label=f\" {', '.join(labels)} \"))\nelif isinstance(op, (OneOf, OneOfT)) and op.ops:\nwrapper = pydot.Cluster(style='dotted', color='darkorchid4', graph_name=str(id(op)))\nwrapper.set('label', 'One Of:')\nwrapper.set('labeljust', 'l')\nTraceability._add_node(progenitor, wrapper, op.ops[0], label_last_seen, edges=True)\nfor sub_op in op.ops[1:]:\nTraceability._add_node(progenitor, wrapper, sub_op, label_last_seen, edges=False)\ndiagram.add_subgraph(wrapper)\nelif isinstance(op, (Fuse, FuseT)) and op.ops:\nTraceability._draw_subgraph(progenitor, diagram, label_last_seen, f'Fuse:', op.ops)\nelif isinstance(op, (Repeat, RepeatT)) and op.op:\nwrapper = pydot.Cluster(style='dotted', color='darkgreen', graph_name=str(id(op)))\nwrapper.set('label', f'Repeat:')\nwrapper.set('labeljust', 'l')\nwrapper.add_node(\npydot.Node(node_id,\nlabel=f'{op.repeat if isinstance(op.repeat, int) else \"?\"}',\nshape='doublecircle',\nwidth=0.1))\n# dot2tex doesn't seem to handle edge color conversion correctly, so have to set hex color\nprogenitor.add_edge(pydot.Edge(src=node_id + \":ne\", dst=node_id + \":w\", color='#006300'))\nTraceability._add_node(progenitor, wrapper, op.op, label_last_seen)\n# Add repeat edges\nedge_srcs = defaultdict(lambda: [])\nfor out in op.outputs:\nif out in op.inputs and out not in op.repeat_inputs:\nedge_srcs[label_last_seen[out]].append(out)\nfor inp in op.repeat_inputs:\nedge_srcs[label_last_seen[inp]].append(inp)\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(pydot.Edge(src=src, dst=node_id, constraint=False, label=f\" {', '.join(labels)} \"))\ndiagram.add_subgraph(wrapper)\nelse:\nif isinstance(op, ModelOp):\nlabel = f\"{op.__class__.__name__} ({FEID(id(op))}): {op.model.model_name}\"\nmodel_ref = Hyperref(Marker(name=str(op.model.model_name), prefix='subsec'),\ntext=NoEscape(r'\\textcolor{blue}{') + bold(op.model.model_name) +\nNoEscape('}')).dumps()\ntexlbl = f\"{HrefFEID(FEID(id(op)), name=op.__class__.__name__).dumps()}: {model_ref}\"\nelse:\nlabel = f\"{op.__class__.__name__} ({FEID(id(op))})\"\ntexlbl = HrefFEID(FEID(id(op)), name=op.__class__.__name__).dumps()\ndiagram.add_node(pydot.Node(node_id, label=label, texlbl=texlbl))\nif isinstance(op, (Op, Trace)) and edges:\n# Need the instance check since subgraph_ops might contain a tf dataset or torch dataloader\nTraceability._add_edge(progenitor, op, label_last_seen)\n@staticmethod\ndef _add_edge(progenitor: pydot.Dot, op: Union[Trace, Op], label_last_seen: Dict[str, str]):\n\"\"\"Draw edges into a given Node.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            op: The op (or trace) to be visualized.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n        \"\"\"\nnode_id = str(id(op))\nedge_srcs = defaultdict(lambda: [])\nfor inp in op.inputs:\nif inp == '*':\ncontinue\nedge_srcs[label_last_seen[inp]].append(inp)\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(pydot.Edge(src=src, dst=node_id, label=f\" {', '.join(labels)} \"))\nfor out in op.outputs:\nlabel_last_seen[out] = node_id\n@staticmethod\ndef _get_all_nodes(diagram: Union[pydot.Dot, pydot.Cluster]) -&gt; List[pydot.Node]:\n\"\"\"Recursively search through a `diagram` looking for Nodes.\n        Args:\n            diagram: The diagram to be inspected.\n        Returns:\n            All of the Nodes available within this diagram and its child diagrams.\n        \"\"\"\nnodes = diagram.get_nodes()\nfor subgraph in diagram.get_subgraphs():\nnodes.extend(Traceability._get_all_nodes(subgraph))\nreturn nodes\n@staticmethod\ndef _init_document_geometry() -&gt; Document:\n\"\"\"Init geometry setting of the document.\n        Return:\n            Initialized Document object.\n        \"\"\"\nreturn Document(geometry_options=['lmargin=2cm', 'rmargin=2cm', 'bmargin=2cm'])\n</code></pre>"}, {"location": "fastestimator/trace/metric/accuracy.html", "title": "accuracy", "text": ""}, {"location": "fastestimator/trace/metric/accuracy.html#fastestimator.fastestimator.trace.metric.accuracy.Accuracy", "title": "<code>Accuracy</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the accuracy for a given set of predictions.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'accuracy'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\accuracy.py</code> <pre><code>@traceable()\nclass Accuracy(Trace):\n\"\"\"A trace which computes the accuracy for a given set of predictions.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"accuracy\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.total = 0\nself.correct = 0\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.total = 0\nself.correct = 0\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.correct += np.sum(y_pred.ravel() == y_true.ravel())\nself.total += len(y_pred.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.correct / self.total)\n</code></pre>"}, {"location": "fastestimator/trace/metric/confusion_matrix.html", "title": "confusion_matrix", "text": ""}, {"location": "fastestimator/trace/metric/confusion_matrix.html#fastestimator.fastestimator.trace.metric.confusion_matrix.ConfusionMatrix", "title": "<code>ConfusionMatrix</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes the confusion matrix between y_true (rows) and y_predicted (columns).</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes of the confusion matrix.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'confusion_matrix'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\confusion_matrix.py</code> <pre><code>@traceable()\nclass ConfusionMatrix(Trace):\n\"\"\"Computes the confusion matrix between y_true (rows) and y_predicted (columns).\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        num_classes: Total number of classes of the confusion matrix.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nnum_classes: int,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"confusion_matrix\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.num_classes = num_classes\nself.matrix = None\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.matrix = None\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nbatch_confusion = confusion_matrix(y_true, y_pred, labels=list(range(0, self.num_classes)))\nif self.matrix is None:\nself.matrix = batch_confusion\nelse:\nself.matrix += batch_confusion\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.matrix)\n</code></pre>"}, {"location": "fastestimator/trace/metric/dice.html", "title": "dice", "text": ""}, {"location": "fastestimator/trace/metric/dice.html#fastestimator.fastestimator.trace.metric.dice.Dice", "title": "<code>Dice</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Dice score for binary classification between y_true and y_predicted.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>The key of the ground truth mask.</p> required <code>pred_key</code> <code>str</code> <p>The key of the prediction values.</p> required <code>threshold</code> <code>float</code> <p>The threshold for binarizing the prediction.</p> <code>0.5</code> <code>mode</code> <code>Union[None, str, List[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'Dice'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\dice.py</code> <pre><code>@traceable()\nclass Dice(Trace):\n\"\"\"Dice score for binary classification between y_true and y_predicted.\n    Args:\n        true_key: The key of the ground truth mask.\n        pred_key: The key of the prediction values.\n        threshold: The threshold for binarizing the prediction.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nthreshold: float = 0.5,\nmode: Union[None, str, List[str]] = (\"eval\", \"test\"),\noutput_name: str = \"Dice\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.threshold = threshold\nself.smooth = 1e-8\nself.dice = np.array([])\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.dice = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nbatch_size = y_true.shape[0]\ny_true, y_pred = y_true.reshape((batch_size, -1)), y_pred.reshape((batch_size, -1))\nprediction_label = (y_pred &gt;= self.threshold).astype(np.int32)\nintersection = np.sum(y_true * prediction_label, axis=-1)\narea_sum = np.sum(y_true, axis=-1) + np.sum(prediction_label, axis=-1)\ndice = (2. * intersection + self.smooth) / (area_sum + self.smooth)\nself.dice.extend(list(dice))\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], np.mean(self.dice))\n</code></pre>"}, {"location": "fastestimator/trace/metric/f1_score.html", "title": "f1_score", "text": ""}, {"location": "fastestimator/trace/metric/f1_score.html#fastestimator.fastestimator.trace.metric.f1_score.F1Score", "title": "<code>F1Score</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate the F1 score for a classification task and report it back to the logger.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store back to the state.</p> <code>'f1_score'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\f1_score.py</code> <pre><code>@traceable()\nclass F1Score(Trace):\n\"\"\"Calculate the F1 score for a classification task and report it back to the logger.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store back to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"f1_score\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = f1_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = f1_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/metric/mcc.html", "title": "mcc", "text": ""}, {"location": "fastestimator/trace/metric/mcc.html#fastestimator.fastestimator.trace.metric.mcc.MCC", "title": "<code>MCC</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the Matthews Correlation Coefficient for a given set of predictions.</p> <p>This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,  a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies anti-correlation (you should invert your classifier predictions in order to do better).</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'mcc'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mcc.py</code> <pre><code>@traceable()\nclass MCC(Trace):\n\"\"\"A trace which computes the Matthews Correlation Coefficient for a given set of predictions.\n    This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does\n    not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,\n     a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies\n    anti-correlation (you should invert your classifier predictions in order to do better).\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"mcc\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_true.extend(y_true)\nself.y_pred.extend(y_pred)\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], matthews_corrcoef(y_true=self.y_true, y_pred=self.y_pred))\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html", "title": "mean_average_precision", "text": "<p>COCO Mean average precisin (mAP) implementation.</p>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision", "title": "<code>MeanAveragePrecision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate COCO mean average precision.</p> <p>The value of 'y_pred' has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select is either 0 or 1. The value of 'bbox' has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label].</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Maximum <code>int</code> value for your class label. In COCO dataset we only used 80 classes, but the maxium value of the class label is <code>90</code>. In this case <code>num_classes</code> should be <code>90</code>.</p> required <p>Returns:</p> Type Description <p>Mean Average Precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>@traceable()\nclass MeanAveragePrecision(Trace):\n\"\"\"Calculate COCO mean average precision.\n    The value of 'y_pred' has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select\n    is either 0 or 1.\n    The value of 'bbox' has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label].\n    Args:\n        num_classes: Maximum `int` value for your class label. In COCO dataset we only used 80 classes, but the maxium\n            value of the class label is `90`. In this case `num_classes` should be `90`.\n    Returns:\n        Mean Average Precision.\n    \"\"\"\ndef __init__(self,\nnum_classes: int,\ntrue_key='bbox',\npred_key: str = 'pred',\nmode: str = \"eval\",\noutput_name=(\"mAP\", \"AP50\", \"AP75\")) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nassert len(self.outputs) == 3, 'MeanAvgPrecision trace adds 3 fields mAP AP50 AP75 to state dict'\nself.iou_thres = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05).astype(np.int) + 1, endpoint=True)\nself.recall_thres = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01).astype(np.int) + 1, endpoint=True)\nself.categories = range(num_classes)\nself.max_detection = 100\nself.image_ids = []\n# eval\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0  # reset per epoch\n# reset per batch\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\nself.counter = 0  # REMOVE\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef _get_id_in_epoch(self, idx_in_batch: int) -&gt; int:\n\"\"\"Get unique image id in epoch.\n        Id starts from 1.\n        Args:\n            idx_in_batch: Image id within a batch.\n        Returns:\n            Global unique id within one epoch.\n        \"\"\"\n# for this batch\nnum_unique_id_previous = len(np.unique(self.ids_unique))\nself.ids_unique.append(idx_in_batch)\nnum_unique_id = len(np.unique(self.ids_unique))\nif num_unique_id &gt; num_unique_id_previous:\n# for epoch\nself.ids_in_epoch += 1\nself.ids_batch_to_epoch[idx_in_batch] = self.ids_in_epoch\nreturn self.ids_in_epoch\ndef on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\ndef on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n@staticmethod\ndef _reshape_gt(gt_array: np.ndarray) -&gt; np.ndarray:\n\"\"\"Reshape ground truth and add local image id within batch.\n        The input ground truth array has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label] for each\n        bounding box.\n        For output we drop all padded bounding boxes (all zeros), and flatten the batch dimension. The output shape is\n        (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n        Args:\n            gt_array: Ground truth with shape (batch_size, num_bbox, 5).\n        Returns:\n            Ground truth with shape (batch_size * num_bbox, 6).\n        \"\"\"\nlocal_ids = np.repeat(range(gt_array.shape[0]), gt_array.shape[1], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\ngt_with_id = np.concatenate([local_ids, gt_array.reshape(-1, 5)], axis=1)\nkeep = ~np.all(gt_with_id[:, 1:] == 0, axis=1)  # remove rows of all 0 bounding boxes\nreturn gt_with_id[keep]\n@staticmethod\ndef _reshape_pred(pred: List[np.ndarray]) -&gt; np.ndarray:\n\"\"\"Reshape predicted bounding boxes and add local image id within batch.\n        The input pred array has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select\n        is either 0 or 1.\n        For output we flatten the batch dimension. The output shape is (total_num_bbox_in_batch, 7). The 7 is\n        [id_in_batch, x1, y1, w, h, label, score].\n        Args:\n            pred: List of predected bounding boxes for each image. Each element in the list has shape (num_bbox, 6).\n        Returns:\n            Predected bounding boxes with shape (total_num_bbox_in_batch, 7).\n        \"\"\"\npred_with_id = []\nfor id_batch in range(pred.shape[0]):\npred_single = pred[id_batch]\nlocal_ids = np.repeat([id_batch], pred_single.shape[0], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\npred_single = np.concatenate([local_ids, pred_single], axis=1)\npred_with_id.append(pred_single[pred_single[:, -1] &gt; 0, :-1])\npred_with_id = np.concatenate(pred_with_id, axis=0)\nreturn pred_with_id\ndef on_batch_end(self, data: Data):\n# begin of reading det and gt\npred = to_number(data[self.pred_key])  # pred is [batch, nms_max_outputs, 7]\npred = self._reshape_pred(pred)\ngt = to_number(data[self.true_key])  # gt is np.array (batch, box, 5), box dimension is padded\ngt = self._reshape_gt(gt)\nground_truth_bb = []\nfor gt_item in gt:\nidx_in_batch, x1, y1, w, h, label = gt_item\nlabel = int(label)\nid_epoch = self._get_id_in_epoch(idx_in_batch)\nself.batch_image_ids.append(id_epoch)\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label}\nground_truth_bb.append(tmp_dict)\npredicted_bb = []\nfor pred_item in pred:\nidx_in_batch, x1, y1, w, h, label, score = pred_item\nlabel = int(label)\nid_epoch = self.ids_batch_to_epoch[idx_in_batch]\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label, 'score': score}\npredicted_bb.append(tmp_dict)\nfor dict_elem in ground_truth_bb:\nself.gt[dict_elem['idx'], dict_elem['label']].append(dict_elem)\nfor dict_elem in predicted_bb:\nself.det[dict_elem['idx'], dict_elem['label']].append(dict_elem)\n# end of reading det and gt\n# compute iou matrix, matrix index is (img_id, cat_id), each element in matrix has shape (num_det, num_gt)\nself.ious = {(img_id, cat_id): self.compute_iou(self.det[img_id, cat_id], self.gt[img_id, cat_id])\nfor img_id in self.batch_image_ids for cat_id in self.categories}\nfor cat_id in self.categories:\nfor img_id in self.batch_image_ids:\nself.evalimgs[(cat_id, img_id)] = self.evaluate_img(cat_id, img_id)\ndef on_epoch_end(self, data: Data):\nself.accumulate()\nmean_ap = self.summarize()\nap50 = self.summarize(iou=0.5)\nap75 = self.summarize(iou=0.75)\ndata[self.outputs[0]] = mean_ap\ndata[self.outputs[1]] = ap50\ndata[self.outputs[2]] = ap75\ndef evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n        Args:\n            cat_id:\n            img_id:\n        Returns:\n        \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\ndef accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\ndef summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n        Args:\n            iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n                is the mean average precision.\n        Returns:\n            Average precision.\n        \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\ndef compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n        We leverage `maskUtils.iou`.\n        Args:\n            det: Detection array.\n            gt: Ground truth array.\n        Returns:\n            Intersection of union array.\n        \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.accumulate", "title": "<code>accumulate</code>", "text": "<p>Generate precision-recall curve.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.compute_iou", "title": "<code>compute_iou</code>", "text": "<p>Compute intersection over union.</p> <p>We leverage <code>maskUtils.iou</code>.</p> <p>Parameters:</p> Name Type Description Default <code>det</code> <code>np.ndarray</code> <p>Detection array.</p> required <code>gt</code> <code>np.ndarray</code> <p>Ground truth array.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Intersection of union array.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n    We leverage `maskUtils.iou`.\n    Args:\n        det: Detection array.\n        gt: Ground truth array.\n    Returns:\n        Intersection of union array.\n    \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.evaluate_img", "title": "<code>evaluate_img</code>", "text": "<p>Find gt matches for det given one image and one category.</p> <p>Parameters:</p> Name Type Description Default <code>cat_id</code> <code>int</code> required <code>img_id</code> <code>int</code> required Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n    Args:\n        cat_id:\n        img_id:\n    Returns:\n    \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.summarize", "title": "<code>summarize</code>", "text": "<p>Compute average precision given one intersection union threshold.</p> <p>Parameters:</p> Name Type Description Default <code>iou</code> <code>float</code> <p>Intersection over union threshold. If this value is <code>None</code>, then average all iou thresholds. The result is the mean average precision.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Average precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n    Args:\n        iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n            is the mean average precision.\n    Returns:\n        Average precision.\n    \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\n</code></pre>"}, {"location": "fastestimator/trace/metric/precision.html", "title": "precision", "text": ""}, {"location": "fastestimator/trace/metric/precision.html#fastestimator.fastestimator.trace.metric.precision.Precision", "title": "<code>Precision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes precision for a classification task and reports it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'precision'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\precision.py</code> <pre><code>@traceable()\nclass Precision(Trace):\n\"\"\"Computes precision for a classification task and reports it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"precision\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = precision_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = precision_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/metric/recall.html", "title": "recall", "text": ""}, {"location": "fastestimator/trace/metric/recall.html#fastestimator.fastestimator.trace.metric.recall.Recall", "title": "<code>Recall</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Compute recall for a classification task and report it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'recall'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\recall.py</code> <pre><code>@traceable()\nclass Recall(Trace):\n\"\"\"Compute recall for a classification task and report it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"recall\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = recall_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = recall_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/trace/xai/saliency.html#fastestimator.fastestimator.trace.xai.saliency.Saliency", "title": "<code>Saliency</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace which computes saliency maps for a given model throughout training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>A model compiled with fe.build to be analyzed.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the input values for the model.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the output values from a model.</p> required <code>class_key</code> <code>Optional[str]</code> <p>The key of the true labels corresponding to the model inputs (not required).</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>{class_string: model_output_value}.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The name of the output which will be generated by this trace.</p> <code>'saliency'</code> <code>samples</code> <code>Union[None, int, Dict[str, Any]]</code> <p>How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.</p> <code>None</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>smoothing</code> <code>int</code> <p>How many rounds of smoothing should be applied to the saliency mask (0 to disable).</p> <code>25</code> <code>integrating</code> <code>Union[int, Tuple[int, int]]</code> <p>How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was provided by the smoothing variable (useful if you want to compare techniques / save on computation time)/</p> <code>(25, 7)</code> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\saliency.py</code> <pre><code>@traceable()\nclass Saliency(Trace):\n\"\"\"A Trace which computes saliency maps for a given model throughout training.\n    Args:\n        model: A model compiled with fe.build to be analyzed.\n        model_inputs: Keys for the input values for the model.\n        model_outputs: Keys for the output values from a model.\n        class_key: The key of the true labels corresponding to the model inputs (not required).\n        label_mapping: {class_string: model_output_value}.\n        outputs: The name of the output which will be generated by this trace.\n        samples: How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        smoothing: How many rounds of smoothing should be applied to the saliency mask (0 to disable).\n        integrating: How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may\n            be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was\n            provided by the smoothing variable (useful if you want to compare techniques / save on computation time)/\n    \"\"\"\nsamples: Dict[str, Union[None, int, Dict[str, Any]]]  # {mode: val}\nn_found: Dict[str, int]  # {mode: val}\nn_required: Dict[str, int]  # {mode: val}\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\nclass_key: Optional[str] = None,\nlabel_mapping: Optional[Dict[str, Any]] = None,\noutputs: Union[str, List[str]] = \"saliency\",\nsamples: Union[None, int, Dict[str, Any]] = None,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\nsmoothing: int = 25,\nintegrating: Union[int, Tuple[int, int]] = (25, 7)) -&gt; None:\n# Model outputs are required due to inability to statically determine the number of outputs from a pytorch model\nself.class_key = class_key\nself.model_outputs = to_list(model_outputs)\nsuper().__init__(inputs=to_list(self.class_key) + to_list(model_inputs), outputs=outputs, mode=mode)\nself.smoothing = smoothing\nself.integrating = integrating\nself.samples = {}\nself.n_found = {}\nself.n_required = {}\n# TODO - handle non-hashable labels\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nfor mode in mode or (\"train\", \"eval\", \"test\"):\nself.samples[mode] = samples\nif isinstance(samples, int):\nself.samples[mode] = None\nself.n_found[mode] = 0\nself.n_required[mode] = samples\nelse:\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nif self.samples[mode] is None:\nself.samples[mode] = defaultdict(list)\nself.salnet = SaliencyNet(model=model, model_inputs=model_inputs, model_outputs=model_outputs, outputs=outputs)\ndef on_batch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif not self.samples[mode] or self.n_found[mode] &lt; self.n_required[mode]:\nn_samples = 0\nfor key in self.inputs:\nself.samples[mode][key].append(data[key])\nn_samples = len(data[key])\nself.n_found[mode] += n_samples\ndef on_epoch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif self.n_found[mode] &gt; 0:\nif self.n_required[mode] &gt; 0:\n# We are keeping a user-specified number of samples\nself.samples[mode] = {\nkey: concat(val)[:self.n_required[mode]]\nfor key, val in self.samples[mode].items()\n}\nelse:\n# We are keeping one batch of data\nself.samples[mode] = {key: val[0] for key, val in self.samples[mode].items()}\n# even if you haven't found n_required samples, you're at end of epoch so no point trying to collect more\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nmasks = self.salnet.get_masks(self.samples[mode])\nsmoothed, integrated, smint = {}, {}, {}\nif self.smoothing:\nsmoothed = self.salnet.get_smoothed_masks(self.samples[mode], nsamples=self.smoothing)\nif self.integrating:\nif isinstance(self.integrating, Tuple):\nn_integration, n_smoothing = self.integrating\nelse:\nn_integration = self.integrating\nn_smoothing = self.smoothing\nintegrated = self.salnet.get_integrated_masks(self.samples[mode], nsamples=n_integration)\nif n_smoothing:\nsmint = self.salnet.get_smoothed_masks(self.samples[mode],\nnsamples=n_smoothing,\nnintegration=n_integration)\n# Arrange the outputs\nargs = {}\nif self.class_key:\nclasses = self.samples[mode][self.class_key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\nargs[self.class_key] = classes\nfor key in self.model_outputs:\nclasses = masks[key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\nargs[key] = classes\nsal = smint or integrated or smoothed or masks\nfor key, val in self.samples[mode].items():\nif key is not self.class_key:\nargs[key] = val\n# Create a linear combination of the original image, the saliency mask, and the product of the two in\n# order to highlight regions of importance\nmin_val = reduce_min(val)\ndiff = reduce_max(val) - min_val\nfor outkey in self.outputs:\nargs[\"{} {}\".format(key, outkey)] = (0.3 * (sal[outkey] * (val - min_val) + min_val) + 0.3 * val +\n0.4 * sal[outkey] * diff + min_val)\nfor key in self.outputs:\nargs[key] = masks[key]\nif smoothed:\nargs[\"Smoothed {}\".format(key)] = smoothed[key]\nif integrated:\nargs[\"Integrated {}\".format(key)] = integrated[key]\nif smint:\nargs[\"SmInt {}\".format(key)] = smint[key]\nresult = ImgData(colormap=\"inferno\", **args)\ndata.write_without_log(self.outputs[0], result)\n</code></pre>"}, {"location": "fastestimator/util/cli_util.html", "title": "cli_util", "text": ""}, {"location": "fastestimator/util/cli_util.html#fastestimator.fastestimator.util.cli_util.SaveAction", "title": "<code>SaveAction</code>", "text": "<p>         Bases: <code>argparse.Action</code></p> <p>A customized save action for use with argparse.</p> <p>A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file is invoked directly during argument parsing.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>option_strings</code> <code>Sequence[str]</code> <p>A list of command-line option strings which should be associated with this action.</p> required <code>dest</code> <code>str</code> <p>The name of the attribute to hold the created object(s).</p> required <code>nargs</code> <code>Union[int, str, None]</code> <p>The number of command line arguments to be consumed.</p> <code>'?'</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Pass-through keyword arguments.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\cli_util.py</code> <pre><code>class SaveAction(argparse.Action):\n\"\"\"A customized save action for use with argparse.\n    A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file\n    is invoked directly during argument parsing.\n    This class is intentionally not @traceable.\n    Args:\n        option_strings: A list of command-line option strings which should be associated with this action.\n        dest: The name of the attribute to hold the created object(s).\n        nargs: The number of command line arguments to be consumed.\n        **kwargs: Pass-through keyword arguments.\n    \"\"\"\ndef __init__(self,\noption_strings: Sequence[str],\ndest: str,\nnargs: Union[int, str, None] = '?',\n**kwargs: Dict[str, Any]) -&gt; None:\nif '?' != nargs:\nraise ValueError(\"nargs must be \\'?\\'\")\nsuper().__init__(option_strings, dest, nargs, **kwargs)\ndef __call__(self,\nparser: argparse.ArgumentParser,\nnamespace: argparse.Namespace,\nvalues: Optional[str],\noption_string: Optional[str] = None) -&gt; None:\n\"\"\"Invokes the save action, writing two values into the namespace.\n        Args:\n            parser: The active argument parser (ignored by this implementation).\n            namespace: The current namespace to be written to.\n            values: The value to write into the namespace.\n            option_string: An option_string (ignored by this implementation).\n        \"\"\"\nsetattr(namespace, self.dest, True)\nsetattr(namespace, self.dest + '_dir', values if values is None else os.path.join(values, ''))\n</code></pre>"}, {"location": "fastestimator/util/cli_util.html#fastestimator.fastestimator.util.cli_util.parse_cli_to_dictionary", "title": "<code>parse_cli_to_dictionary</code>", "text": "<p>Convert a list of strings into a dictionary with python objects as values.</p> <pre><code>a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"])\n# {'epochs': 5, 'test': 'this', 'lr': 0.74}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[str]</code> <p>A list of input strings from the cli.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary constructed from the <code>input_list</code>, with values converted to python objects where applicable.</p> Source code in <code>fastestimator\\fastestimator\\util\\cli_util.py</code> <pre><code>def parse_cli_to_dictionary(input_list: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Convert a list of strings into a dictionary with python objects as values.\n    ```python\n    a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"])\n    # {'epochs': 5, 'test': 'this', 'lr': 0.74}\n    ```\n    Args:\n        input_list: A list of input strings from the cli.\n    Returns:\n        A dictionary constructed from the `input_list`, with values converted to python objects where applicable.\n    \"\"\"\nresult = {}\nif input_list is None:\nreturn result\nkey = \"\"\nval = \"\"\nidx = 0\nwhile idx &lt; len(input_list):\nif input_list[idx].startswith(\"--\"):\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nval = \"\"\nkey = input_list[idx].strip('--')\nelse:\nval += input_list[idx]\nidx += 1\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/data.html", "title": "data", "text": ""}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data", "title": "<code>Data</code>", "text": "<p>         Bases: <code>ChainMap[str, Any]</code></p> <p>A class which contains prediction and batch data.</p> <p>This class is intentionally not @traceable.</p> <p>Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not. We therefore provide helper methods to write entries into <code>Data</code> which are intended or not intended for logging.</p> <pre><code>d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\na = d[\"a\"]  # 0\nd.write_with_log(\"d\", 3)\nd.write_without_log(\"e\", 5)\nd.write_with_log(\"a\", 4)\na = d[\"a\"]  # 4\nr = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Optional[MutableMapping[str, Any]]</code> <p>The batch data dictionary. In practice this is itself often a ChainMap containing separate prediction and batch dictionaries.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>class Data(ChainMap[str, Any]):\n\"\"\"A class which contains prediction and batch data.\n    This class is intentionally not @traceable.\n    Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of\n    two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data\n    written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not.\n    We therefore provide helper methods to write entries into `Data` which are intended or not intended for logging.\n    ```python\n    d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\n    a = d[\"a\"]  # 0\n    d.write_with_log(\"d\", 3)\n    d.write_without_log(\"e\", 5)\n    d.write_with_log(\"a\", 4)\n    a = d[\"a\"]  # 4\n    r = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n    ```\n    Args:\n        batch_data: The batch data dictionary. In practice this is itself often a ChainMap containing separate\n            prediction and batch dictionaries.\n    \"\"\"\nmaps: List[MutableMapping[str, Any]]\ndef __init__(self, batch_data: Optional[MutableMapping[str, Any]] = None) -&gt; None:\nsuper().__init__({}, batch_data or {})\ndef write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n        Args:\n            key: The key to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.__setitem__(key, value)\ndef write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n        Args:\n            key: The ey to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.maps[1][key] = value\ndef read_logs(self) -&gt; Dict[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n        Returns:\n            A dictionary of all of the keys and values to be logged.\n        \"\"\"\nreturn self.maps[0]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.read_logs", "title": "<code>read_logs</code>", "text": "<p>Read all values from the <code>Data</code> dictionary which were intended to be logged.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of all of the keys and values to be logged.</p> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def read_logs(self) -&gt; Dict[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n    Returns:\n        A dictionary of all of the keys and values to be logged.\n    \"\"\"\nreturn self.maps[0]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_with_log", "title": "<code>write_with_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n    Args:\n        key: The key to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.__setitem__(key, value)\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_without_log", "title": "<code>write_without_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it not be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The ey to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n    Args:\n        key: The ey to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.maps[1][key] = value\n</code></pre>"}, {"location": "fastestimator/util/img_data.html", "title": "img_data", "text": ""}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData", "title": "<code>ImgData</code>", "text": "<p>         Bases: <code>OrderedDict</code></p> <p>A container for image related data.</p> <p>This class is intentionally not @traceable.</p> <p>This class is useful for automatically laying out collections of images for comparison and visualization.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nfig = d.paint_figure()\nplt.show()\nimg = 0.5*np.ones((4, 32, 32, 3))\nmask = np.zeros_like(img)\nmask[0, 10:20, 10:30, :] = [1, 0, 0]\nmask[1, 5:15, 5:20, :] = [0, 1, 0]\nbbox = np.array([[[3,7,10,6,'box1'], [20,20,8,8,'box2']]]*4)\nd = fe.util.ImgData(y=tf.ones((4,)), x=[img, mask, bbox])\nfig = d.paint_figure()\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>colormap</code> <code>str</code> <p>What colormap to use when rendering greyscale images. A good colorization option is 'inferno'.</p> <code>'Greys'</code> <code>**kwargs</code> <code>Union[Tensor, List[Tensor]]</code> <p>image_title / image pairs for visualization. Images with the same batch dimensions will be laid out side-by-side, with earlier kwargs entries displayed further to the left. The value part of the key/value pair can be a list of tensors, in which case the elements of the list are overlaid. This can be useful for displaying masks and bounding boxes on top of images. In such cases, the largest image should be put as the first entry in the list. Bounding boxes should be shaped like (batch, n_boxes, box), where each box is formatted like (x0, y0, width, height[, label]).</p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a list of Tensors is provided as an input, but that list has an inconsistent batch dimension.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>class ImgData(OrderedDict):\n\"\"\"A container for image related data.\n    This class is intentionally not @traceable.\n    This class is useful for automatically laying out collections of images for comparison and visualization.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    fig = d.paint_figure()\n    plt.show()\n    img = 0.5*np.ones((4, 32, 32, 3))\n    mask = np.zeros_like(img)\n    mask[0, 10:20, 10:30, :] = [1, 0, 0]\n    mask[1, 5:15, 5:20, :] = [0, 1, 0]\n    bbox = np.array([[[3,7,10,6,'box1'], [20,20,8,8,'box2']]]*4)\n    d = fe.util.ImgData(y=tf.ones((4,)), x=[img, mask, bbox])\n    fig = d.paint_figure()\n    plt.show()\n    ```\n    Args:\n        colormap: What colormap to use when rendering greyscale images. A good colorization option is 'inferno'.\n        **kwargs: image_title / image pairs for visualization. Images with the same batch dimensions will be laid out\n            side-by-side, with earlier kwargs entries displayed further to the left. The value part of the key/value\n            pair can be a list of tensors, in which case the elements of the list are overlaid. This can be useful for\n            displaying masks and bounding boxes on top of images. In such cases, the largest image should be put as the\n            first entry in the list. Bounding boxes should be shaped like (batch, n_boxes, box), where each box is\n            formatted like (x0, y0, width, height[, label]).\n    Raises:\n        AssertionError: If a list of Tensors is provided as an input, but that list has an inconsistent batch dimension.\n    \"\"\"\nn_elements: Dict[int, List[str]]\ndef __init__(self, colormap: str = \"Greys\", **kwargs: Union[Tensor, List[Tensor]]) -&gt; None:\nself.n_elements = {}  # Not a default dict b/c that complicates the computations later\nself.colormap = colormap\nsuper().__init__(**kwargs)\ndef __setitem__(self, key: str, value: Union[Tensor, List[Tensor]]):\n# Convert all values into a list for consistency\nvalue = to_list(value)\nbatch_size = value[0].shape[0]\nfor elem in value[1:]:\nassert elem.shape[0] == batch_size, \"Provided item has an inconsistent batch size\"\nsuper().__setitem__(key, value)\nself.n_elements.setdefault(batch_size, []).append(key)\ndef __delitem__(self, key: str):\nsuper().__delitem__(key)\nfor k, lst in self.n_elements.items():\nlst.remove(key)\nif len(lst) == 0:\ndel self.n_elements[k]\ndef _to_grid(self) -&gt; List[List[Tuple[str, np.ndarray]]]:\n\"\"\"Convert the elements of ImgData into a grid view.\n        One row in the grid is generated for each unique batch dimension present within the ImgData. Each column in the\n        grid is a tensor with batch dimension matching the current row. Columns are given in the order they were input\n        into the ImgData constructor.\n        Returns:\n            The ImgData arranged as a grid, with entries in the grid as (key, value) pairs.\n        \"\"\"\nsorted_sections = sorted(self.n_elements.keys())\nreturn [[(key, self[key]) for key in self.n_elements[n_rows]] for n_rows in sorted_sections]\ndef _n_rows(self) -&gt; int:\n\"\"\"Computes how many rows are present in the ImgData grid.\n        Returns:\n            The number of rows in the ImgData grid.\n        \"\"\"\nreturn len(self.n_elements)\ndef _n_cols(self) -&gt; int:\n\"\"\"Computes how many columns are present in the ImgData grid.\n        Returns:\n            The number of columns in the ImgData grid.\n        \"\"\"\nreturn max((len(elem) for elem in self.n_elements.values()))\n@staticmethod\ndef _shape_to_width(shape: Tuple[int], min_width=200) -&gt; int:\n\"\"\"Decide the width of an image for visualization.\n        Args:\n            shape: The shape of the image.\n            min_width: The minimum desired width for visualization.\n        Returns:\n            The maximum between the width specified by `shape` and the given `min_width` value.\n        \"\"\"\nif len(shape) &lt; 2:\n# text field, use default width\npass\nelif len(shape) == 2:\n# image field: width x height\nmin_width = max(shape[0], min_width)\nelse:\n# image field: batch x width x height\nmin_width = max(shape[1], min_width)\nreturn min_width\n@staticmethod\ndef _shape_to_height(shape: Tuple[int], min_height=200) -&gt; int:\n\"\"\"Decide the height of an image for visualization.\n        Args:\n            shape: The shape of the image.\n            min_height: The minimum desired width for visualization.\n        Returns:\n            The maximum between the height specified by `shape` and the given `min_height` value.\n        \"\"\"\nif len(shape) &lt; 2:\n# text field, use default width\npass\nelif len(shape) == 2:\n# image field: width x height\nmin_height = max(shape[0], min_height)\nelse:\n# image field: batch x width x height\nmin_height = max(shape[1], min_height) * shape[0]\nreturn min_height\ndef _widths(self, row: int, gap: int = 50, min_width: int = 200) -&gt; List[Tuple[int, int]]:\n\"\"\"Get the display widths of a particular row.\n        Args:\n            row: The row to measure.\n            gap: How much space to allow between each column.\n            min_width: The minimum width for a column.\n        Returns:\n            A list of (x1, x2) coordinates marking the beginning and end coordinates of each column in the `row`.\n        \"\"\"\nkeys = list(sorted(self.n_elements.keys()))\n# For overlay values consider the zeroth element for the shape\nrow = [self[key][0] for key in self.n_elements[keys[row]]]\nwidths = [(0, ImgData._shape_to_width(row[0].shape, min_width=min_width))]\nfor img in row[1:]:\nwidths.append(\n(widths[-1][1] + gap, widths[-1][1] + gap + ImgData._shape_to_width(img.shape, min_width=min_width)))\nreturn widths\ndef _total_width(self, gap: int = 50, min_width: int = 200) -&gt; int:\n\"\"\"Get the total width necessary for the image by considering the widths of each row.\n        Args:\n            gap: The horizontal space between each column in the grid.\n            min_width: The minimum width of a column.\n        Returns:\n            The total width of the image.\n        \"\"\"\nreturn max(\n(self._widths(row, gap=gap, min_width=min_width)[-1][-1] for row in range(len(self.n_elements.keys()))))\ndef _heights(self, gap: int = 100, min_height: int = 200) -&gt; List[Tuple[int, int]]:\n\"\"\"Get the display heights of each row.\n        Args:\n            gap: How much space to allow between each row.\n            min_height: The minimum height for a row.\n        Returns:\n            A list of (y1, y2) coordinates marking the top and bottom coordinates of each row in the grid.\n        \"\"\"\nkeys = list(sorted(self.n_elements.keys()))\n# For overlay values consider the zeroth element for the shape\nrows = [[self[key][0] for key in self.n_elements[keys[row]]] for row in range(self._n_rows())]\nheights = [\nmax((ImgData._shape_to_height(elem.shape, min_height=min_height) for elem in rows[i]))\nfor i in range(self._n_rows())\n]\noffset = 10\nresult = [(offset, heights[0] + offset)]\nfor height in heights[1:]:\nresult.append((result[-1][1] + gap, result[-1][1] + gap + height))\nreturn result\ndef _total_height(self, gap: int = 100, min_height: int = 200) -&gt; int:\n\"\"\"Get the total height necessary for the image by considering the heights of each row.\n        Args:\n            gap: The vertical space between each row in the grid.\n            min_height: The minimum height of a row.\n        Returns:\n            The total height of the image.\n        \"\"\"\nheights = self._heights(gap=gap, min_height=min_height)\n# Add some space at the top for the labels\nreturn heights[-1][1] + 30\ndef _batch_size(self, row: int) -&gt; int:\n\"\"\"Get the batch size associated with the given `row`.\n        Args:\n            row: The row for which to report the batch size.\n        Returns:\n            The batch size of all of the entries in the row.\n        \"\"\"\nreturn sorted(self.n_elements.keys())[row]\ndef paint_figure(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96,\nsave_path: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"Visualize the current ImgData entries in a matplotlib figure.\n        ```python\n        d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n        fig = d.paint_figure()\n        plt.show()\n        ```\n        Args:\n            height_gap: How much space to put between each row.\n            min_height: The minimum height of a row.\n            width_gap: How much space to put between each column.\n            min_width: The minimum width of a column.\n            dpi: The resolution of the image to display.\n            save_path: If provided, the figure will be saved to the given path.\n        Returns:\n            The handle to the generated matplotlib figure.\n        \"\"\"\ntotal_width = self._total_width(gap=width_gap, min_width=min_width)\ntotal_height = self._total_height(gap=height_gap, min_height=min_height)\nfig = plt.figure(figsize=(total_width / dpi, total_height / dpi), dpi=dpi)\ngrid = self._to_grid()\n# TODO - elements with batch size = 1 should be laid out in a grid like for plotting\nfor row_idx, (start_height, end_height) in enumerate(self._heights(gap=height_gap, min_height=min_height)):\nrow = grid[row_idx]\nbatch_size = self._batch_size(row_idx)\ngs = GridSpec(nrows=batch_size,\nncols=total_width,\nfigure=fig,\nleft=0.0,\nright=1.0,\nbottom=start_height / total_height,\ntop=end_height / total_height,\nhspace=0.05,\nwspace=0.0)\nfor batch_idx in range(batch_size):\nfor col_idx, width in enumerate(self._widths(row=row_idx, gap=width_gap, min_width=min_width)):\nax = fig.add_subplot(gs[batch_idx, width[0]:width[1]])\nimg_stack = [elem[batch_idx] for elem in row[col_idx][1]]\nfor idx, img in enumerate(img_stack):\nshow_image(img,\naxis=ax,\nfig=fig,\ntitle=row[col_idx][0] if (batch_idx == 0 and idx == 0) else None,\nstack_depth=idx,\ncolor_map=self.colormap)\nif save_path:\nplt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\nreturn fig\ndef paint_numpy(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96) -&gt; np.ndarray:\n\"\"\"Visualize the current ImgData entries into an image stored in a numpy array.\n        ```python\n        d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n        img = d.paint_numpy()\n        plt.imshow(img[0])\n        plt.show()\n        ```\n        Args:\n            height_gap: How much space to put between each row.\n            min_height: The minimum height of a row.\n            width_gap: How much space to put between each column.\n            min_width: The minimum width of a column.\n            dpi: The resolution of the image to display.\n        Returns:\n            A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.\n        \"\"\"\nfig = self.paint_figure(height_gap=height_gap,\nmin_height=min_height,\nwidth_gap=width_gap,\nmin_width=min_width,\ndpi=dpi)\ncanvas = plt_backend_agg.FigureCanvasAgg(fig)\ncanvas.draw()\ndata = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\nw, h = fig.canvas.get_width_height()\ndata = data.reshape([h, w, 4])[:, :, 0:3]\nplt.close(fig)\nreturn np.stack([data])  # Add a batch dimension\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData.paint_figure", "title": "<code>paint_figure</code>", "text": "<p>Visualize the current ImgData entries in a matplotlib figure.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nfig = d.paint_figure()\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>height_gap</code> <code>int</code> <p>How much space to put between each row.</p> <code>100</code> <code>min_height</code> <code>int</code> <p>The minimum height of a row.</p> <code>200</code> <code>width_gap</code> <code>int</code> <p>How much space to put between each column.</p> <code>50</code> <code>min_width</code> <code>int</code> <p>The minimum width of a column.</p> <code>200</code> <code>dpi</code> <code>int</code> <p>The resolution of the image to display.</p> <code>96</code> <code>save_path</code> <code>Optional[str]</code> <p>If provided, the figure will be saved to the given path.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>The handle to the generated matplotlib figure.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>def paint_figure(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96,\nsave_path: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"Visualize the current ImgData entries in a matplotlib figure.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    fig = d.paint_figure()\n    plt.show()\n    ```\n    Args:\n        height_gap: How much space to put between each row.\n        min_height: The minimum height of a row.\n        width_gap: How much space to put between each column.\n        min_width: The minimum width of a column.\n        dpi: The resolution of the image to display.\n        save_path: If provided, the figure will be saved to the given path.\n    Returns:\n        The handle to the generated matplotlib figure.\n    \"\"\"\ntotal_width = self._total_width(gap=width_gap, min_width=min_width)\ntotal_height = self._total_height(gap=height_gap, min_height=min_height)\nfig = plt.figure(figsize=(total_width / dpi, total_height / dpi), dpi=dpi)\ngrid = self._to_grid()\n# TODO - elements with batch size = 1 should be laid out in a grid like for plotting\nfor row_idx, (start_height, end_height) in enumerate(self._heights(gap=height_gap, min_height=min_height)):\nrow = grid[row_idx]\nbatch_size = self._batch_size(row_idx)\ngs = GridSpec(nrows=batch_size,\nncols=total_width,\nfigure=fig,\nleft=0.0,\nright=1.0,\nbottom=start_height / total_height,\ntop=end_height / total_height,\nhspace=0.05,\nwspace=0.0)\nfor batch_idx in range(batch_size):\nfor col_idx, width in enumerate(self._widths(row=row_idx, gap=width_gap, min_width=min_width)):\nax = fig.add_subplot(gs[batch_idx, width[0]:width[1]])\nimg_stack = [elem[batch_idx] for elem in row[col_idx][1]]\nfor idx, img in enumerate(img_stack):\nshow_image(img,\naxis=ax,\nfig=fig,\ntitle=row[col_idx][0] if (batch_idx == 0 and idx == 0) else None,\nstack_depth=idx,\ncolor_map=self.colormap)\nif save_path:\nplt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData.paint_numpy", "title": "<code>paint_numpy</code>", "text": "<p>Visualize the current ImgData entries into an image stored in a numpy array.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nimg = d.paint_numpy()\nplt.imshow(img[0])\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>height_gap</code> <code>int</code> <p>How much space to put between each row.</p> <code>100</code> <code>min_height</code> <code>int</code> <p>The minimum height of a row.</p> <code>200</code> <code>width_gap</code> <code>int</code> <p>How much space to put between each column.</p> <code>50</code> <code>min_width</code> <code>int</code> <p>The minimum width of a column.</p> <code>200</code> <code>dpi</code> <code>int</code> <p>The resolution of the image to display.</p> <code>96</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>def paint_numpy(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96) -&gt; np.ndarray:\n\"\"\"Visualize the current ImgData entries into an image stored in a numpy array.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    img = d.paint_numpy()\n    plt.imshow(img[0])\n    plt.show()\n    ```\n    Args:\n        height_gap: How much space to put between each row.\n        min_height: The minimum height of a row.\n        width_gap: How much space to put between each column.\n        min_width: The minimum width of a column.\n        dpi: The resolution of the image to display.\n    Returns:\n        A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.\n    \"\"\"\nfig = self.paint_figure(height_gap=height_gap,\nmin_height=min_height,\nwidth_gap=width_gap,\nmin_width=min_width,\ndpi=dpi)\ncanvas = plt_backend_agg.FigureCanvasAgg(fig)\ncanvas.draw()\ndata = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\nw, h = fig.canvas.get_width_height()\ndata = data.reshape([h, w, 4])[:, :, 0:3]\nplt.close(fig)\nreturn np.stack([data])  # Add a batch dimension\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html", "title": "latex_util", "text": ""}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.AdjustBox", "title": "<code>AdjustBox</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to adjust the size of boxes.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class AdjustBox(Environment):\n\"\"\"A class to adjust the size of boxes.\n    This class is intentionally not @traceable.\n    \"\"\"\npackages = [Package('adjustbox')]\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Center", "title": "<code>Center</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to center content in a page.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Center(Environment):\n\"\"\"A class to center content in a page.\n    This class is intentionally not @traceable.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.ContainerList", "title": "<code>ContainerList</code>", "text": "<p>         Bases: <code>Container</code></p> <p>A class to expedite combining pieces of latex together.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class ContainerList(Container):\n\"\"\"A class to expedite combining pieces of latex together.\n    This class is intentionally not @traceable.\n    \"\"\"\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this container.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nreturn self.dumps_content()\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.ContainerList.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this container.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this container.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nreturn self.dumps_content()\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Form", "title": "<code>Form</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to allow Form elements.</p> <p>This class is intentionally not @traceable. Only one Form is allowed per document.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Form(Environment):\n\"\"\"A class to allow Form elements.\n    This class is intentionally not @traceable. Only one Form is allowed per document.\n    \"\"\"\n_latex_name = 'Form'\npackages = [Package('hyperref', options='hidelinks')]\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.HrefFEID", "title": "<code>HrefFEID</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to represent a colored and underlined hyperref based on a given fe_id.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>fe_id</code> <code>FEID</code> <p>The id used to link this hyperref.</p> required <code>name</code> <code>str</code> <p>A string suffix to be printed as part of the link text.</p> required <code>link_prefix</code> <code>str</code> <p>The prefix for the hyperlink.</p> <code>'tbl'</code> <code>id_in_name</code> <code>bool</code> <p>Whether to include the id in front of the name text.</p> <code>True</code> <code>bold_name</code> <code>bool</code> <p>Whether to bold the name.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class HrefFEID(ContainerList):\n\"\"\"A class to represent a colored and underlined hyperref based on a given fe_id.\n    This class is intentionally not @traceable.\n    Args:\n        fe_id: The id used to link this hyperref.\n        name: A string suffix to be printed as part of the link text.\n        link_prefix: The prefix for the hyperlink.\n        id_in_name: Whether to include the id in front of the name text.\n        bold_name: Whether to bold the name.\n    \"\"\"\ndef __init__(self,\nfe_id: FEID,\nname: str,\nlink_prefix: str = 'tbl',\nid_in_name: bool = True,\nbold_name: bool = False):\nself.content_separator = ''\nself.packages.add(Package('hyperref', options='hidelinks'))\nself.packages.add(Package('ulem'))\nself.packages.add(Package('xcolor', options='table'))\nself.fe_id = fe_id\nself.name = name\ndata = [\nNoEscape(r'\\hyperref['), escape_latex(f\"{link_prefix}:\"), fe_id, NoEscape(r']{\\textcolor{blue}{\\uline{')\n]\nif id_in_name:\ndata.append(fe_id)\nif name:\ndata.append(\": \")\nif name:\ndata.append(bold(escape_latex(name)) if bold_name else escape_latex(name))\ndata.append(NoEscape(\"}}}\"))\nsuper().__init__(data=data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.IterJoin", "title": "<code>IterJoin</code>", "text": "<p>         Bases: <code>Container</code></p> <p>A class to convert an iterable to a latex representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable</code> <p>Data of the cell.</p> required <code>token</code> <code>str</code> <p>String to serve as separator among items of <code>data</code>.</p> required Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class IterJoin(Container):\n\"\"\"A class to convert an iterable to a latex representation.\n    Args:\n        data: Data of the cell.\n        token: String to serve as separator among items of `data`.\n    \"\"\"\ndef __init__(self, data: Iterable, token: str):\nsuper().__init__(data=data)\nself.token = token\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nreturn dumps_list(self, token=self.token)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.IterJoin.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this cell.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nreturn dumps_list(self, token=self.token)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.PyContainer", "title": "<code>PyContainer</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to convert python containers to a LaTeX representation.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, tuple, set, dict]</code> <p>The python object to be converted to LaTeX.</p> required <code>truncate</code> <code>Optional[int]</code> <p>How many values to display before truncating with an ellipsis. This should be a positive integer or None to disable truncation.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class PyContainer(ContainerList):\n\"\"\"A class to convert python containers to a LaTeX representation.\n    This class is intentionally not @traceable.\n    Args:\n        data: The python object to be converted to LaTeX.\n        truncate: How many values to display before truncating with an ellipsis. This should be a positive integer or\n            None to disable truncation.\n    \"\"\"\ndef __init__(self, data: Union[list, tuple, set, dict], truncate: Optional[int] = None):\nself.packages.add(Package('enumitem', options='inline'))\nassert isinstance(data, (list, tuple, set, dict)), f\"Unacceptable data type for PyContainer: {type(data)}\"\nopen_char = '[' if isinstance(data, list) else '(' if isinstance(data, tuple) else r'\\{'\nclose_char = ']' if isinstance(data, list) else ')' if isinstance(data, tuple) else r'\\}'\nltx = Enumerate(options=Options(NoEscape('label={}'), NoEscape('itemjoin={,}')))\nltx._star_latex_name = True  # Converts this to an inline list\nself.raw_input = data\nif isinstance(data, dict):\nfor key, val in list(data.items())[:truncate]:\nltx.add_item(ContainerList(data=[key, \": \", val]))\nelse:\nfor val in list(data)[:truncate]:\nltx.add_item(val)\nif truncate and len(data) &gt; truncate:\nltx.add_item(NoEscape(r'\\ldots'))\nsuper().__init__(data=[NoEscape(open_char), ltx, NoEscape(close_char)])\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.TextField", "title": "<code>TextField</code>", "text": "<p>         Bases: <code>ContainerCommand</code></p> <p>A class to create editable text fields.</p> <p>This class is intentionally not @traceable. It can only be used inside of a Form.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class TextField(ContainerCommand):\n\"\"\"A class to create editable text fields.\n    This class is intentionally not @traceable. It can only be used inside of a Form.\n    \"\"\"\n_latex_name = \"TextField\"\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.TextFieldBox", "title": "<code>TextFieldBox</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to wrap TextFields into padded boxes for use in nesting within tables.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to assign to this TextField. It should be unique within the document since changes to one box will impact all boxes with the same name.</p> required <code>height</code> <code>str</code> <p>How tall should the TextField box be? Note that it will be wrapped by 10pt space on the top and bottom.</p> <code>'2.5cm'</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class TextFieldBox(ContainerList):\n\"\"\"A class to wrap TextFields into padded boxes for use in nesting within tables.\n    Args:\n        name: The name to assign to this TextField. It should be unique within the document since changes to one box\n            will impact all boxes with the same name.\n        height: How tall should the TextField box be? Note that it will be wrapped by 10pt space on the top and bottom.\n    \"\"\"\npackages = [Package('xcolor', options='table')]\ndef __init__(self, name: str, height: str = '2.5cm'):\ndata = [\nNoEscape(r\"\\begin{minipage}{\\linewidth}\"),\nNoEscape(r\"\\vspace{3pt}\"),\nTextField(options=[\nNoEscape(r'width=\\linewidth'),\nNoEscape(f'height={height}'),\nNoEscape('backgroundcolor={0.97 0.97 0.97}'),\n'bordercolor=white',\n'multiline=true',\nf'name={name}'\n]),\nNoEscape(r\"\\vspace{3pt}\"),\nNoEscape(r\"\\end{minipage}\")\n]\nsuper().__init__(data=data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Verbatim", "title": "<code>Verbatim</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to put a string inside the latex verbatim environment.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The string to be wrapped.</p> required Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Verbatim(Environment):\n\"\"\"A class to put a string inside the latex verbatim environment.\n    This class is intentionally not @traceable.\n    Args:\n        data: The string to be wrapped.\n    \"\"\"\ndef __init__(self, data: str):\nsuper().__init__(options=None, arguments=None, start_arguments=None, data=NoEscape(data))\nself.content_separator = '\\n'\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.WrapText", "title": "<code>WrapText</code>", "text": "<p>         Bases: <code>LatexObject</code></p> <p>A class to convert strings or numbers to wrappable latex representation.</p> <p>This class will first convert the data to string, and then to a wrappable latex representation if its length is too long. This fixes an issue which prevents the first element placed into a latex X column from wrapping correctly.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, int, float]</code> <p>Input data to be converted.</p> required <code>threshold</code> <code>int</code> <p>When the length of <code>data</code> is greater than <code>threshold</code>, the resulting string will be made wrappable.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>data</code> is not a string, int, or float.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class WrapText(LatexObject):\n\"\"\"A class to convert strings or numbers to wrappable latex representation.\n    This class will first convert the data to string, and then to a wrappable latex representation if its length is too\n    long. This fixes an issue which prevents the first element placed into a latex X column from wrapping correctly.\n    Args:\n        data: Input data to be converted.\n        threshold: When the length of `data` is greater than `threshold`, the resulting string will be made wrappable.\n    Raises:\n        AssertionError: If `data` is not a string, int, or float.\n    \"\"\"\ndef __init__(self, data: Union[str, int, float], threshold: int):\nassert isinstance(data, (str, int, float)), \"the self.data type needs to be str, int, float\"\nself.threshold = threshold\nself.data = str(data)\nsuper().__init__()\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nif len(self.data) &gt; self.threshold:\nreturn NoEscape(r'\\seqsplit{' + escape_latex(self.data) + '}')\nelse:\nreturn escape_latex(self.data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.WrapText.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this cell.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nif len(self.data) &gt; self.threshold:\nreturn NoEscape(r'\\seqsplit{' + escape_latex(self.data) + '}')\nelse:\nreturn escape_latex(self.data)\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html", "title": "traceability_util", "text": ""}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeInputSpec", "title": "<code>FeInputSpec</code>", "text": "<p>A class to keep track of a model's input so that fake inputs can be generated.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input to the model.</p> required <code>model</code> <code>Model</code> <p>The model which corresponds to the given <code>model_input</code>.</p> required Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeInputSpec:\n\"\"\"A class to keep track of a model's input so that fake inputs can be generated.\n    This class is intentionally not @traceable.\n    Args:\n        model_input: The input to the model.\n        model: The model which corresponds to the given `model_input`.\n    \"\"\"\ndef __init__(self, model_input: Any, model: Model):\nself.shape = to_shape(model_input)\nself.dtype = to_type(model_input)\nself.tensor_func = tf.ones if isinstance(model, tf.keras.Model) else torch.ones\ndef get_dummy_input(self) -&gt; Any:\n\"\"\"Get fake input for the model.\n        Returns:\n            Input of the correct shape and dtype for the model.\n        \"\"\"\nreturn self._from_shape_and_type(self.shape, self.dtype)\ndef _from_shape_and_type(self, shape: Any, dtype: Any) -&gt; Any:\n\"\"\"Constructs tensor(s) with the specified shape and dtype.\n        It is assumed that the `shape` and `dtype` arguments have the same container structure. That is to say, if\n        `shape` is a list of 5 elements, it is required that `dtype` also be a list of 5 elements.\n        Args:\n            shape: A shape or (possibly nested) container of shapes.\n            dtype: A dtype or (possibly nested) container of dtypes.\n        Returns:\n            A tensor or collection of tensors corresponding to the shape and dtype arguments.\n        \"\"\"\nif isinstance(dtype, dict):\nreturn {key: self._from_shape_and_type(value, dtype[key]) for key, value in shape.items()}\nelif isinstance(dtype, list):\nreturn [self._from_shape_and_type(shape[i], dtype[i]) for i in range(len(shape))]\nelif isinstance(dtype, tuple):\nreturn tuple([self._from_shape_and_type(shape[i], dtype[i]) for i in range(len(shape))])\nelif isinstance(dtype, set):\nreturn set([self._from_shape_and_type(s, t) for s, t in zip(shape, dtype)])\nelse:\nreturn self.tensor_func(shape, dtype=dtype)\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeInputSpec.get_dummy_input", "title": "<code>get_dummy_input</code>", "text": "<p>Get fake input for the model.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Input of the correct shape and dtype for the model.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def get_dummy_input(self) -&gt; Any:\n\"\"\"Get fake input for the model.\n    Returns:\n        Input of the correct shape and dtype for the model.\n    \"\"\"\nreturn self._from_shape_and_type(self.shape, self.dtype)\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary", "title": "<code>FeSplitSummary</code>", "text": "<p>         Bases: <code>LatexObject</code></p> <p>A class to summarize splits performed on an FE Dataset.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeSplitSummary(LatexObject):\n\"\"\"A class to summarize splits performed on an FE Dataset.\n    This class is intentionally not @traceable.\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.data = []\ndef add_split(self, parent: Union[FEID, str], fraction: str) -&gt; None:\n\"\"\"Record another split on this dataset.\n        Args:\n            parent: The id of the parent involved in the split (or 'self' if you are the parent).\n            fraction: The string representation of the split fraction that was used.\n        \"\"\"\nself.data.append((parent, fraction))\ndef dumps(self) -&gt; str:\n\"\"\"Generate a LaTeX formatted representation of this object.\n        Returns:\n            A LaTeX string representation of this object.\n        \"\"\"\nreturn \" $\\\\rightarrow$ \".join([\nf\"{HrefFEID(parent, name='').dumps() if isinstance(parent, FEID) else parent}({escape_latex(fraction)})\"\nfor parent,\nfraction in self.data\n])\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary.add_split", "title": "<code>add_split</code>", "text": "<p>Record another split on this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Union[FEID, str]</code> <p>The id of the parent involved in the split (or 'self' if you are the parent).</p> required <code>fraction</code> <code>str</code> <p>The string representation of the split fraction that was used.</p> required Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def add_split(self, parent: Union[FEID, str], fraction: str) -&gt; None:\n\"\"\"Record another split on this dataset.\n    Args:\n        parent: The id of the parent involved in the split (or 'self' if you are the parent).\n        fraction: The string representation of the split fraction that was used.\n    \"\"\"\nself.data.append((parent, fraction))\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary.dumps", "title": "<code>dumps</code>", "text": "<p>Generate a LaTeX formatted representation of this object.</p> <p>Returns:</p> Type Description <code>str</code> <p>A LaTeX string representation of this object.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Generate a LaTeX formatted representation of this object.\n    Returns:\n        A LaTeX string representation of this object.\n    \"\"\"\nreturn \" $\\\\rightarrow$ \".join([\nf\"{HrefFEID(parent, name='').dumps() if isinstance(parent, FEID) else parent}({escape_latex(fraction)})\"\nfor parent,\nfraction in self.data\n])\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSummaryTable", "title": "<code>FeSummaryTable</code>", "text": "<p>A class containing summaries of traceability information.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The string to be used as the title line in the summary table.</p> required <code>fe_id</code> <code>FEID</code> <p>The id of this table, used for cross-referencing from other tables.</p> required <code>target_type</code> <code>Type</code> <p>The type of the object being summarized.</p> required <code>path</code> <code>Union[None, str, LatexObject]</code> <p>The import path of the object in question. Might be more complicated when methods/functions are involved.</p> <code>None</code> <code>kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>The keyword arguments used to instantiate the object being summarized.</p> <code>None</code> <code>**fields</code> <code>Any</code> <p>Any other information about the summarized object / function.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeSummaryTable:\n\"\"\"A class containing summaries of traceability information.\n    This class is intentionally not @traceable.\n    Args:\n        name: The string to be used as the title line in the summary table.\n        fe_id: The id of this table, used for cross-referencing from other tables.\n        target_type: The type of the object being summarized.\n        path: The import path of the object in question. Might be more complicated when methods/functions are involved.\n        kwargs: The keyword arguments used to instantiate the object being summarized.\n        **fields: Any other information about the summarized object / function.\n    \"\"\"\nname: Union[str, LatexObject]\nfe_id: FEID\nfields: Dict[str, Any]\ndef __init__(self,\nname: str,\nfe_id: FEID,\ntarget_type: Type,\npath: Union[None, str, LatexObject] = None,\nkwargs: Optional[Dict[str, Any]] = None,\n**fields: Any):\nself.name = name\nself.fe_id = fe_id\nself.type = target_type\nself.path = path\nself.args = fields.pop(\"args\", None)\nself.kwargs = kwargs or {}\nself.fields = fields\ndef render_table(self,\ndoc: Document,\nname_override: Optional[LatexObject] = None,\ntoc_ref: Optional[str] = None,\nextra_rows: Optional[List[Tuple[str, Any]]] = None) -&gt; None:\n\"\"\"Write this table into a LaTeX document.\n        Args:\n            doc: The LaTeX document to be appended to.\n            name_override: An optional replacement for this table's name field.\n            toc_ref: A reference to be added to the table of contents.\n            extra_rows: Any extra rows to be added to the table before the kwargs.\n        \"\"\"\nwith doc.create(Table(position='htp!')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\ntable.append(Label(Marker(name=str(self.fe_id), prefix=\"tbl\")))\nif toc_ref:\ntable.append(NoEscape(r'\\addcontentsline{toc}{subsection}{' + escape_latex(toc_ref) + '}'))\nwith doc.create(Tabularx('|lX|', booktabs=True)) as tabular:\npackage = Package('xcolor', options='table')\nif package not in tabular.packages:\n# Need to invoke a table color before invoking TextColor (bug?)\ntabular.packages.append(package)\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\ntabular.add_row((name_override if name_override else bold(self.name),\nMultiColumn(size=1, align='r|', data=TextColor('blue', self.fe_id))))\ntabular.add_hline()\ntype_str = f\"{self.type}\"\nmatch = re.fullmatch(r'^&lt;.* \\'(?P&lt;typ&gt;.*)\\'&gt;$', type_str)\ntype_str = match.group(\"typ\") if match else type_str\ntabular.add_row((\"Type: \", escape_latex(type_str)))\nif self.path:\nif isinstance(self.path, LatexObject):\ntabular.add_row((\"\", self.path))\nelse:\ntabular.add_row((\"\", escape_latex(self.path)))\nfor k, v in self.fields.items():\ntabular.add_hline()\ntabular.add_row((f\"{k.capitalize()}: \", v))\nif self.args:\ntabular.add_hline()\ntabular.add_row((\"Args: \", self.args))\nif extra_rows:\nfor (key, val) in extra_rows:\ntabular.add_hline()\ntabular.add_row(key, val)\nif self.kwargs:\ntabular.add_hline()\nfor idx, (kwarg, val) in enumerate(self.kwargs.items()):\ntabular.add_row((italic(kwarg), val), color='white' if idx % 2 else 'black!5')\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSummaryTable.render_table", "title": "<code>render_table</code>", "text": "<p>Write this table into a LaTeX document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>The LaTeX document to be appended to.</p> required <code>name_override</code> <code>Optional[LatexObject]</code> <p>An optional replacement for this table's name field.</p> <code>None</code> <code>toc_ref</code> <code>Optional[str]</code> <p>A reference to be added to the table of contents.</p> <code>None</code> <code>extra_rows</code> <code>Optional[List[Tuple[str, Any]]]</code> <p>Any extra rows to be added to the table before the kwargs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def render_table(self,\ndoc: Document,\nname_override: Optional[LatexObject] = None,\ntoc_ref: Optional[str] = None,\nextra_rows: Optional[List[Tuple[str, Any]]] = None) -&gt; None:\n\"\"\"Write this table into a LaTeX document.\n    Args:\n        doc: The LaTeX document to be appended to.\n        name_override: An optional replacement for this table's name field.\n        toc_ref: A reference to be added to the table of contents.\n        extra_rows: Any extra rows to be added to the table before the kwargs.\n    \"\"\"\nwith doc.create(Table(position='htp!')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\ntable.append(Label(Marker(name=str(self.fe_id), prefix=\"tbl\")))\nif toc_ref:\ntable.append(NoEscape(r'\\addcontentsline{toc}{subsection}{' + escape_latex(toc_ref) + '}'))\nwith doc.create(Tabularx('|lX|', booktabs=True)) as tabular:\npackage = Package('xcolor', options='table')\nif package not in tabular.packages:\n# Need to invoke a table color before invoking TextColor (bug?)\ntabular.packages.append(package)\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\ntabular.add_row((name_override if name_override else bold(self.name),\nMultiColumn(size=1, align='r|', data=TextColor('blue', self.fe_id))))\ntabular.add_hline()\ntype_str = f\"{self.type}\"\nmatch = re.fullmatch(r'^&lt;.* \\'(?P&lt;typ&gt;.*)\\'&gt;$', type_str)\ntype_str = match.group(\"typ\") if match else type_str\ntabular.add_row((\"Type: \", escape_latex(type_str)))\nif self.path:\nif isinstance(self.path, LatexObject):\ntabular.add_row((\"\", self.path))\nelse:\ntabular.add_row((\"\", escape_latex(self.path)))\nfor k, v in self.fields.items():\ntabular.add_hline()\ntabular.add_row((f\"{k.capitalize()}: \", v))\nif self.args:\ntabular.add_hline()\ntabular.add_row((\"Args: \", self.args))\nif extra_rows:\nfor (key, val) in extra_rows:\ntabular.add_hline()\ntabular.add_row(key, val)\nif self.kwargs:\ntabular.add_hline()\nfor idx, (kwarg, val) in enumerate(self.kwargs.items()):\ntabular.add_row((italic(kwarg), val), color='white' if idx % 2 else 'black!5')\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.fe_summary", "title": "<code>fe_summary</code>", "text": "<p>Return a summary of how this class was instantiated (for traceability).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The bound class instance.</p> required <p>Returns:</p> Type Description <code>List[FeSummaryTable]</code> <p>A summary of the instance.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def fe_summary(self) -&gt; List[FeSummaryTable]:\n\"\"\"Return a summary of how this class was instantiated (for traceability).\n    Args:\n        self: The bound class instance.\n    Returns:\n        A summary of the instance.\n    \"\"\"\n# Delayed imports to avoid circular dependency\nfrom fastestimator.estimator import Estimator\nfrom fastestimator.network import TFNetwork, TorchNetwork\nfrom fastestimator.pipeline import Pipeline\nfrom fastestimator.op.op import Op\nfrom fastestimator.trace.trace import Trace\nfrom fastestimator.schedule.schedule import Scheduler\nfrom torch.utils.data import Dataset\n# re-number the references for nicer viewing\nordered_items = sorted(\nself._fe_traceability_summary.items(),\nkey=lambda x: 0 if issubclass(x[1].type, Estimator) else 1\nif issubclass(x[1].type, (TFNetwork, TorchNetwork)) else 2 if issubclass(x[1].type, Pipeline) else 3\nif issubclass(x[1].type, Scheduler) else 4 if issubclass(x[1].type, Trace) else 5\nif issubclass(x[1].type, Op) else 6 if issubclass(x[1].type, (Dataset, tf.data.Dataset)) else 7\nif issubclass(x[1].type, (tf.keras.Model, torch.nn.Module)) else 8\nif issubclass(x[1].type, types.FunctionType) else 9\nif issubclass(x[1].type, (np.ndarray, tf.Tensor, tf.Variable, torch.Tensor)) else 10)\nkey_mapping = {fe_id: f\"@FE{idx}\" for idx, (fe_id, val) in enumerate(ordered_items)}\nFEID.set_translation_dict(key_mapping)\nreturn [item[1] for item in ordered_items]\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.is_restorable", "title": "<code>is_restorable</code>", "text": "<p>Determine whether a given object can be restored easily via Pickle.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The object in question.</p> required <code>memory_limit</code> <code>int</code> <p>The maximum memory size (in bytes) to allow for an object (or 0 for no limit).</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>(result, memory size) where result is True iff <code>data</code> is only comprised of 'simple' objects and does not exceed</p> <code>int</code> <p>the <code>memory_limit</code>. If the result is False, then memory size will be &lt;= the true memory size of the <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def is_restorable(data: Any, memory_limit: int = 0) -&gt; Tuple[bool, int]:\n\"\"\"Determine whether a given object can be restored easily via Pickle.\n    Args:\n        data: The object in question.\n        memory_limit: The maximum memory size (in bytes) to allow for an object (or 0 for no limit).\n    Returns:\n        (result, memory size) where result is True iff `data` is only comprised of 'simple' objects and does not exceed\n        the `memory_limit`. If the result is False, then memory size will be &lt;= the true memory size of the `data`.\n    \"\"\"\nif isinstance(data, _RestorableClasses):\nsize = sys.getsizeof(data)\nif isinstance(data, tf.Tensor):\nsize = sys.getsizeof(data.numpy())\nelif isinstance(data, torch.Tensor):\nsize = data.element_size() * data.nelement()\nreturn True, size\nelif isinstance(data, dict):\nsize = 0\nfor key, value in data.items():\nkey_stat = is_restorable(key, memory_limit)\nif key_stat[0] is False:\nreturn False, size\nsize += key_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nval_stat = is_restorable(value, memory_limit)\nif val_stat[0] is False:\nreturn False, size\nsize += val_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nreturn True, size\nelif isinstance(data, (list, tuple, set)):\nsize = 0\nfor elem in data:\nelem_stat = is_restorable(elem, memory_limit)\nif elem_stat[0] is False:\nreturn False, size\nsize += elem_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nreturn True, size\nelse:\nreturn False, 0\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.trace_model", "title": "<code>trace_model</code>", "text": "<p>A function to add traceability information to an FE-compiled model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to be made traceable.</p> required <code>model_idx</code> <code>int</code> <p>Which of the return values from the <code>model_fn</code> is this model (or -1 if only a single return value).</p> required <code>model_fn</code> <code>Any</code> <p>The function used to generate this model.</p> required <code>optimizer_fn</code> <code>Any</code> <p>The thing used to define this model's optimizer.</p> required <code>weights_path</code> <code>Any</code> <p>The path to the weights for this model.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The <code>model</code>, but now with an fe_summary() method.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def trace_model(model: Model, model_idx: int, model_fn: Any, optimizer_fn: Any, weights_path: Any) -&gt; Model:\n\"\"\"A function to add traceability information to an FE-compiled model.\n    Args:\n        model: The model to be made traceable.\n        model_idx: Which of the return values from the `model_fn` is this model (or -1 if only a single return value).\n        model_fn: The function used to generate this model.\n        optimizer_fn: The thing used to define this model's optimizer.\n        weights_path: The path to the weights for this model.\n    Returns:\n        The `model`, but now with an fe_summary() method.\n    \"\"\"\ntables = {}\ndescription = {'definition': _trace_value(model_fn, tables, ret_ref=Flag())}\nif model_idx != -1:\ndescription['index'] = model_idx\nif optimizer_fn or isinstance(optimizer_fn, list) and optimizer_fn[0] is not None:\ndescription['optimizer'] = _trace_value(\noptimizer_fn[model_idx] if isinstance(optimizer_fn, list) else optimizer_fn, tables, ret_ref=Flag())\nif weights_path:\ndescription['weights'] = _trace_value(weights_path, tables, ret_ref=Flag())\nfe_id = FEID(id(model))\ntbl = FeSummaryTable(name=model.model_name, fe_id=fe_id, target_type=type(model), **description)\ntables[fe_id] = tbl\n# Have to put this in a ChainMap b/c dict gets put into model._layers automatically somehow\nmodel._fe_traceability_summary = ChainMap(tables)\n# Use MethodType to bind the method to the class instance\nsetattr(model, 'fe_summary', types.MethodType(fe_summary, model))\nreturn model\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.traceable", "title": "<code>traceable</code>", "text": "<p>A decorator to be placed on classes in order to make them traceable and to enable a deep restore.</p> <p>Decorated classes will gain the .fe_summary() and .fe_state() methods.</p> <p>Parameters:</p> Name Type Description Default <code>whitelist</code> <code>Union[str, Tuple[str, ...]]</code> <p>Arguments which should be included in a deep restore of the decorated class.</p> <code>()</code> <code>blacklist</code> <code>Union[str, Tuple[str, ...]]</code> <p>Arguments which should be excluded from a deep restore of the decorated class.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated class.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def traceable(whitelist: Union[str, Tuple[str, ...]] = (), blacklist: Union[str, Tuple[str, ...]] = ()) -&gt; Callable:\n\"\"\"A decorator to be placed on classes in order to make them traceable and to enable a deep restore.\n    Decorated classes will gain the .fe_summary() and .fe_state() methods.\n    Args:\n        whitelist: Arguments which should be included in a deep restore of the decorated class.\n        blacklist: Arguments which should be excluded from a deep restore of the decorated class.\n    Returns:\n        The decorated class.\n    \"\"\"\nif isinstance(whitelist, str):\nwhitelist = (whitelist, )\nif isinstance(blacklist, str):\nblacklist = (blacklist, )\nif whitelist and blacklist:\nraise ValueError(\"Traceable objects may specify a whitelist or a blacklist, but not both\")\ndef make_traceable(cls):\nbase_init = getattr(cls, '__init__')\nif hasattr(base_init, '__module__') and base_init.__module__ != 'fastestimator.util.traceability_util':\n# We haven't already overridden this class' init method\n@functools.wraps(base_init)  # to preserve the original class signature\ndef init(self, *args, **kwargs):\nif not hasattr(self, '_fe_state_whitelist'):\nself._fe_state_whitelist = whitelist\nelse:\nself._fe_state_whitelist = tuple(set(self._fe_state_whitelist).union(set(whitelist)))\nif not hasattr(self, '_fe_state_blacklist'):\nself._fe_state_blacklist = blacklist + (\n'_fe_state_whitelist', '_fe_state_blacklist', '_fe_traceability_summary')\nelse:\nself._fe_state_blacklist = tuple(set(self._fe_state_blacklist).union(set(blacklist)))\nif not hasattr(self, '_fe_traceability_summary'):\nbound_args = inspect.signature(base_init).bind(self, *args, **kwargs)\nbound_args.apply_defaults()\ntables = {}\n_trace_value(_BoundFn(self, bound_args), tables, ret_ref=Flag())\nself._fe_traceability_summary = tables\nbase_init(self, *args, **kwargs)\nsetattr(cls, '__init__', init)\nbase_func = getattr(cls, 'fe_summary', None)\nif base_func is None:\nsetattr(cls, 'fe_summary', fe_summary)\nbase_func = getattr(cls, '__getstate__', None)\nif base_func is None:\nsetattr(cls, '__getstate__', __getstate__)\nbase_func = getattr(cls, '__setstate__', None)\nif base_func is None:\nsetattr(cls, '__setstate__', __setstate__)\nreturn cls\nreturn make_traceable\n</code></pre>"}, {"location": "fastestimator/util/util.html", "title": "util", "text": "<p>Utilities for FastEstimator.</p>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.DefaultKeyDict", "title": "<code>DefaultKeyDict</code>", "text": "<p>         Bases: <code>dict</code></p> <p>Like collections.defaultdict but it passes the key argument to the default function.</p> <p>This class is intentionally not @traceable.</p> <pre><code>d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\nprint(d[\"a\"])  # 4\nprint(d[\"c\"])  # \"cc\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Callable[[Any], Any]</code> <p>A function which takes a key and returns a default value based on the key.</p> required <code>**kwargs</code> <p>Initial key/value pairs for the dictionary.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class DefaultKeyDict(dict):\n\"\"\"Like collections.defaultdict but it passes the key argument to the default function.\n    This class is intentionally not @traceable.\n    ```python\n    d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\n    print(d[\"a\"])  # 4\n    print(d[\"c\"])  # \"cc\"\n    ```\n    Args:\n        default: A function which takes a key and returns a default value based on the key.\n        **kwargs: Initial key/value pairs for the dictionary.\n    \"\"\"\ndef __init__(self, default: Callable[[Any], Any], **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.factory = default\ndef __missing__(self, key: Any) -&gt; Any:\nres = self[key] = self.factory(key)\nreturn res\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.FEID", "title": "<code>FEID</code>", "text": "<p>An int wrapper class that can change how it's values are printed.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>int</code> <p>An integer id to be wrapped.</p> required Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class FEID:\n\"\"\"An int wrapper class that can change how it's values are printed.\n    This class is intentionally not @traceable.\n    Args:\n        val: An integer id to be wrapped.\n    \"\"\"\n__slots__ = ['_val']\n_translation_dict = {}\ndef __init__(self, val: int):\nself._val = val\ndef __hash__(self) -&gt; int:\nreturn hash(self._val)\ndef __eq__(self, other: Any) -&gt; bool:\nif isinstance(other, FEID):\nreturn self._val == other._val\nelse:\nreturn int.__eq__(self._val, other)\ndef __lt__(self, other: Any) -&gt; bool:\nif isinstance(other, FEID):\nother = other._val\nreturn int.__lt__(self._val, other)\ndef __str__(self) -&gt; str:\nreturn f\"{self._translation_dict.get(self._val, self._val)}\"\ndef __repr__(self) -&gt; str:\nreturn f\"{self._translation_dict.get(self._val, self._val)}\"\n@classmethod\ndef set_translation_dict(cls, mapping: Dict[int, Any]) -&gt; None:\n\"\"\"Provide a lookup table to be invoked during value printing.\n        Args:\n            mapping: A mapping of id: printable id.\n        \"\"\"\ncls._translation_dict.clear()\ncls._translation_dict.update(mapping)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.FEID.set_translation_dict", "title": "<code>set_translation_dict</code>  <code>classmethod</code>", "text": "<p>Provide a lookup table to be invoked during value printing.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[int, Any]</code> <p>A mapping of id: printable id.</p> required Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>@classmethod\ndef set_translation_dict(cls, mapping: Dict[int, Any]) -&gt; None:\n\"\"\"Provide a lookup table to be invoked during value printing.\n    Args:\n        mapping: A mapping of id: printable id.\n    \"\"\"\ncls._translation_dict.clear()\ncls._translation_dict.update(mapping)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Flag", "title": "<code>Flag</code>", "text": "<p>A mutable wrapper around a boolean.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>bool</code> <p>The initial value for the Flag.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Flag:\n\"\"\"A mutable wrapper around a boolean.\n    This class is intentionally not @traceable.\n    Args:\n        val: The initial value for the Flag.\n    \"\"\"\n__slots__ = ['_val']\ndef __init__(self, val: bool = False):\nself._val = val\ndef set_true(self):\nself._val = True\ndef set_false(self):\nself._val = False\ndef __bool__(self):\nreturn self._val\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.LogSplicer", "title": "<code>LogSplicer</code>", "text": "<p>A class to send stdout information into a file before passing it along to the normal stdout.</p> <p>Parameters:</p> Name Type Description Default <code>log_path</code> <code>str</code> <p>The path/filename into which to append the current stdout.</p> required Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class LogSplicer:\n\"\"\"A class to send stdout information into a file before passing it along to the normal stdout.\n    Args:\n        log_path: The path/filename into which to append the current stdout.\n    \"\"\"\ndef __init__(self, log_path: str):\nself.log_path = log_path\nself.stdout = None\nself.log_file = None\ndef __enter__(self) -&gt; None:\nself.log_file = open(self.log_path, 'a')\nself.stdout = sys.stdout\nsys.stdout = self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nsys.stdout = self.stdout\nself.log_file.close()\ndef write(self, output: str) -&gt; None:\nself.log_file.write(output)\nself.stdout.write(output)\ndef flush(self) -&gt; None:\nself.stdout.flush()\nself.log_file.flush()\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.NonContext", "title": "<code>NonContext</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which is used to make nothing unusual happen.</p> <p>This class is intentionally not @traceable.</p> <pre><code>a = 5\nwith fe.util.NonContext():\na = a + 37\nprint(a)  # 42\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class NonContext(object):\n\"\"\"A class which is used to make nothing unusual happen.\n    This class is intentionally not @traceable.\n    ```python\n    a = 5\n    with fe.util.NonContext():\n        a = a + 37\n    print(a)  # 42\n    ```\n    \"\"\"\ndef __enter__(self) -&gt; None:\npass\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Suppressor", "title": "<code>Suppressor</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which can be used to silence output of function calls.</p> <p>This class is intentionally not @traceable.</p> <pre><code>x = lambda: print(\"hello\")\nx()  # \"hello\"\nwith fe.util.Suppressor():\nx()  #\nx()  # \"hello\"\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Suppressor(object):\n\"\"\"A class which can be used to silence output of function calls.\n    This class is intentionally not @traceable.\n    ```python\n    x = lambda: print(\"hello\")\n    x()  # \"hello\"\n    with fe.util.Suppressor():\n        x()  #\n    x()  # \"hello\"\n    ```\n    \"\"\"\ndef __enter__(self) -&gt; None:\n# pylint: disable=attribute-defined-outside-init\nself.stdout = sys.stdout\nself.stderr = sys.stderr\n# pylint: enable=attribute-defined-outside-init\nsys.stdout = self\nsys.stderr = self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nsys.stdout = self.stdout\nsys.stderr = self.stderr\ndef write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n        Args:\n            dummy: The string which wanted to be printed.\n        \"\"\"\npass\ndef flush(self) -&gt; None:\n\"\"\"A function to empty the current print buffer. No-op in this case.\n        \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Suppressor.flush", "title": "<code>flush</code>", "text": "<p>A function to empty the current print buffer. No-op in this case.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def flush(self) -&gt; None:\n\"\"\"A function to empty the current print buffer. No-op in this case.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Suppressor.write", "title": "<code>write</code>", "text": "<p>A function which is invoked during print calls.</p> <p>Parameters:</p> Name Type Description Default <code>dummy</code> <code>str</code> <p>The string which wanted to be printed.</p> required Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n    Args:\n        dummy: The string which wanted to be printed.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Timer", "title": "<code>Timer</code>", "text": "<p>         Bases: <code>ContextDecorator</code></p> <p>A class that can be used to time things.</p> <p>This class is intentionally not @traceable.</p> <pre><code>x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\nwith fe.util.Timer():\nx()  # Task took 0.1639 seconds\n@fe.util.Timer(\"T2\")\ndef func():\nreturn x()\nfunc()  # T2 took 0.14819 seconds\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Timer(ContextDecorator):\n\"\"\"A class that can be used to time things.\n    This class is intentionally not @traceable.\n    ```python\n    x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\n    with fe.util.Timer():\n        x()  # Task took 0.1639 seconds\n    @fe.util.Timer(\"T2\")\n    def func():\n        return x()\n    func()  # T2 took 0.14819 seconds\n    ```\n    \"\"\"\ndef __init__(self, name=\"Task\") -&gt; None:\nself.name = name\nself.start = None\nself.end = None\nself.interval = None\ndef __enter__(self) -&gt; 'Timer':\nself.start = time.perf_counter()\nreturn self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nself.end = time.perf_counter()\nself.interval = self.end - self.start\ntf.print(\"{} took {} seconds\".format(self.name, self.interval))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.draw", "title": "<code>draw</code>", "text": "<p>Print our name.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def draw() -&gt; None:\n\"\"\"Print our name.\n    \"\"\"\nprint(Figlet(font=\"slant\").renderText(\"FastEstimator\"))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_batch_size", "title": "<code>get_batch_size</code>", "text": "<p>Infer batch size from a batch dictionary. It will ignore all dictionary value with data type that doesn't have \"shape\" attribute.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The batch dictionary.</p> required <p>Returns:</p> Type Description <code>int</code> <p>batch size.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_batch_size(data: Dict[str, Any]) -&gt; int:\n\"\"\"Infer batch size from a batch dictionary. It will ignore all dictionary value with data type that\n    doesn't have \"shape\" attribute.\n    Args:\n        data: The batch dictionary.\n    Returns:\n        batch size.\n    \"\"\"\nassert isinstance(data, dict), \"data input must be a dictionary\"\nbatch_size = set(data[key].shape[0] for key in data if hasattr(data[key], \"shape\") and list(data[key].shape))\nassert len(batch_size) == 1, \"invalid batch size: {}\".format(batch_size)\nreturn batch_size.pop()\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_num_devices", "title": "<code>get_num_devices</code>", "text": "<p>Determine the number of available GPUs.</p> <p>Returns:</p> Type Description <p>The number of available GPUs, or 1 if none are found.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_num_devices():\n\"\"\"Determine the number of available GPUs.\n    Returns:\n        The number of available GPUs, or 1 if none are found.\n    \"\"\"\nreturn max(torch.cuda.device_count(), 1)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_shape", "title": "<code>get_shape</code>", "text": "<p>A function to find the shapes of an object or sequence of objects.</p> <p>Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an attempt will be made to determine which of the interior dimensions are ragged.</p> <pre><code>x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\nx = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data to infer the shape of.</p> required <p>Returns:</p> Type Description <code>List[Optional[int]]</code> <p>A list representing the shape of the data.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_shape(obj: Any) -&gt; List[Optional[int]]:\n\"\"\"A function to find the shapes of an object or sequence of objects.\n    Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have\n    mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an\n    attempt will be made to determine which of the interior dimensions are ragged.\n    ```python\n    x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\n    x = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n    ```\n    Args:\n        obj: Data to infer the shape of.\n    Returns:\n        A list representing the shape of the data.\n    \"\"\"\nif hasattr(obj, \"shape\"):\nresult = list(obj.shape)\nelif isinstance(obj, (List, Tuple)):\nshapes = [get_shape(ob) for ob in obj]\nresult = [None]\nif shapes:\nrank = len(shapes[0])\nif any((len(shape) != rank for shape in shapes)):\nreturn result\nresult.extend(shapes[0])\nfor shape in shapes[1:]:\nfor idx, dim in enumerate(shape):\nif result[idx + 1] != dim:\nresult[idx + 1] = None\nelse:\nresult = []\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_type", "title": "<code>get_type</code>", "text": "<p>A function to try and infer the types of data within containers.</p> <pre><code>x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\nx = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\nx = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\nx = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\nx = fe.util.get_type(27)  # \"int\"\n</code></pre> <p>For container to look into its element's type, its type needs to be either list or tuple, and the return string will be List[...]. All container elements need to have the same data type becuase it will only check its first element.</p> <pre><code>x = fe.util.get_type({\"a\":1, \"b\":2})  # \"dict\"\nx = fe.util.get_type([1, \"a\"]) # \"List[int]\"\nx = fe.util.get_type([[[1]]]) # \"List[List[List[int]]]\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data which may be wrapped in some kind of container.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the data type of the <code>obj</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_type(obj: Any) -&gt; str:\n\"\"\"A function to try and infer the types of data within containers.\n    ```python\n    x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\n    x = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\n    x = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\n    x = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\n    x = fe.util.get_type(27)  # \"int\"\n    ```\n    For container to look into its element's type, its type needs to be either list or tuple, and the return string will\n    be List[...]. All container elements need to have the same data type becuase it will only check its first element.\n    ```python\n    x = fe.util.get_type({\"a\":1, \"b\":2})  # \"dict\"\n    x = fe.util.get_type([1, \"a\"]) # \"List[int]\"\n    x = fe.util.get_type([[[1]]]) # \"List[List[List[int]]]\"\n    ```\n    Args:\n        obj: Data which may be wrapped in some kind of container.\n    Returns:\n        A string representation of the data type of the `obj`.\n    \"\"\"\nif hasattr(obj, \"dtype\"):\nresult = str(obj.dtype)\nelif isinstance(obj, (List, Tuple)):\nif len(obj) &gt; 0:\nresult = \"List[{}]\".format(get_type(obj[0]))\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.is_number", "title": "<code>is_number</code>", "text": "<p>Check if a given string can be converted into a number.</p> <pre><code>x = fe.util.is_number(\"13.7\")  # True\nx = fe.util.is_number(\"ae13.7\")  # False\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>str</code> <p>A potentially numeric input string.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True iff <code>arg</code> represents a number.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def is_number(arg: str) -&gt; bool:\n\"\"\"Check if a given string can be converted into a number.\n    ```python\n    x = fe.util.is_number(\"13.7\")  # True\n    x = fe.util.is_number(\"ae13.7\")  # False\n    ```\n    Args:\n        arg: A potentially numeric input string.\n    Returns:\n        True iff `arg` represents a number.\n    \"\"\"\ntry:\nfloat(arg)\nreturn True\nexcept (ValueError, TypeError):\nreturn False\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_batch", "title": "<code>pad_batch</code>", "text": "<p>A function to pad a batch of data in-place by appending to the ends of the tensors. Tensor type needs to be numpy array otherwise would get ignored. (tf.Tensor and torch.Tensor will cause error)</p> <pre><code>data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\nfe.util.pad_batch(data, pad_value=0)\nprint(data)  # [{'x': [[1., 1.], [1., 1.], [0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[MutableMapping[str, np.ndarray]]</code> <p>A list of data to be padded.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to pad with.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the data within the batch do not have matching rank, or have different keys</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_batch(batch: List[MutableMapping[str, np.ndarray]], pad_value: Union[float, int]) -&gt; None:\n\"\"\"A function to pad a batch of data in-place by appending to the ends of the tensors. Tensor type needs to be\n    numpy array otherwise would get ignored. (tf.Tensor and torch.Tensor will cause error)\n    ```python\n    data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\n    fe.util.pad_batch(data, pad_value=0)\n    print(data)  # [{'x': [[1., 1.], [1., 1.], [0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n    ```\n    Args:\n        batch: A list of data to be padded.\n        pad_value: The value to pad with.\n    Raises:\n        AssertionError: If the data within the batch do not have matching rank, or have different keys\n    \"\"\"\nkeys = batch[0].keys()\nfor one_batch in batch:\nassert one_batch.keys() == keys, \"data within batch must have same keys\"\nfor key in keys:\nshapes = [data[key].shape for data in batch if hasattr(data[key], \"shape\")]\nif len(set(shapes)) &gt; 1:\nassert len(set(len(shape) for shape in shapes)) == 1, \"data within batch must have same rank\"\nmax_shapes = tuple(np.max(np.array(shapes), axis=0))\nfor data in batch:\ndata[key] = pad_data(data[key], max_shapes, pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_data", "title": "<code>pad_data</code>", "text": "<p>Pad <code>data</code> by appending <code>pad_value</code>s along it's dimensions until the <code>target_shape</code> is reached. All entris of target_shape should be larger than the data.shape, and have the same rank.</p> <pre><code>x = np.ones((1,2))\nx = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\nx = fe.util.pad_data(x, target_shape=(3, 3, 3), pad_value = -2) # error\nx = fe.util.pad_data(x, target_shape=(4, 1), pad_value = -2) # error\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The data to be padded.</p> required <code>target_shape</code> <code>Tuple[int, ...]</code> <p>The desired shape for <code>data</code>. Should have the same rank as <code>data</code>, with each dimension being &gt;= the size of the <code>data</code> dimension.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to insert into <code>data</code> if padding is required to achieve the <code>target_shape</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The <code>data</code>, padded to the <code>target_shape</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_data(data: np.ndarray, target_shape: Tuple[int, ...], pad_value: Union[float, int]) -&gt; np.ndarray:\n\"\"\"Pad `data` by appending `pad_value`s along it's dimensions until the `target_shape` is reached. All entris of\n    target_shape should be larger than the data.shape, and have the same rank.\n    ```python\n    x = np.ones((1,2))\n    x = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\n    x = fe.util.pad_data(x, target_shape=(3, 3, 3), pad_value = -2) # error\n    x = fe.util.pad_data(x, target_shape=(4, 1), pad_value = -2) # error\n    ```\n    Args:\n        data: The data to be padded.\n        target_shape: The desired shape for `data`. Should have the same rank as `data`, with each dimension being &gt;=\n            the size of the `data` dimension.\n        pad_value: The value to insert into `data` if padding is required to achieve the `target_shape`.\n    Returns:\n        The `data`, padded to the `target_shape`.\n    \"\"\"\nshape_difference = np.array(target_shape) - np.array(data.shape)\npadded_shape = np.array([np.zeros_like(shape_difference), shape_difference]).T\nreturn np.pad(data, padded_shape, 'constant', constant_values=pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.parse_modes", "title": "<code>parse_modes</code>", "text": "<p>A function to determine which modes to run on based on a set of modes potentially containing blacklist values.</p> <pre><code>m = fe.util.parse_modes({\"train\"})  # {\"train\"}\nm = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\nm = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\nm = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modes</code> <code>Set[str]</code> <p>The desired modes to run on (possibly containing blacklisted modes).</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes to run on (converted to a whitelist).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def parse_modes(modes: Set[str]) -&gt; Set[str]:\n\"\"\"A function to determine which modes to run on based on a set of modes potentially containing blacklist values.\n    ```python\n    m = fe.util.parse_modes({\"train\"})  # {\"train\"}\n    m = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\n    m = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\n    m = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n    ```\n    Args:\n        modes: The desired modes to run on (possibly containing blacklisted modes).\n    Returns:\n        The modes to run on (converted to a whitelist).\n    Raises:\n        AssertionError: If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.\n    \"\"\"\nvalid_fields = {\"train\", \"eval\", \"test\", \"infer\", \"!train\", \"!eval\", \"!test\", \"!infer\"}\nassert modes.issubset(valid_fields), \"Invalid modes argument {}\".format(modes - valid_fields)\nnegation = set([mode.startswith(\"!\") for mode in modes])\nassert len(negation) &lt; 2, \"cannot mix !mode with mode, found {}\".format(modes)\nif True in negation:\nnew_modes = {\"train\", \"eval\", \"test\", \"infer\"}\nfor mode in modes:\nnew_modes.discard(mode.strip(\"!\"))\nmodes = new_modes\nreturn modes\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.parse_string_to_python", "title": "<code>parse_string_to_python</code>", "text": "<p>Convert a string into a python object.</p> <pre><code>x = fe.util.parse_string_to_python(\"5\")  # 5\nx = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\nx = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>str</code> <p>An input string.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A python object version of the input string.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def parse_string_to_python(val: str) -&gt; Any:\n\"\"\"Convert a string into a python object.\n    ```python\n    x = fe.util.parse_string_to_python(\"5\")  # 5\n    x = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\n    x = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n    ```\n    Args:\n        val: An input string.\n    Returns:\n        A python object version of the input string.\n    \"\"\"\nif val is None or not val:\nreturn \"\"\ntry:\nreturn literal_eval(val)\nexcept (ValueError, SyntaxError):\ntry:\nreturn json.loads(val)\nexcept json.JSONDecodeError:\nreturn val\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.prettify_metric_name", "title": "<code>prettify_metric_name</code>", "text": "<p>Add spaces to camel case words, then swap _ for space, and capitalize each word.</p> <pre><code>x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>A string to be formatted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted version of 'metric'.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def prettify_metric_name(metric: str) -&gt; str:\n\"\"\"Add spaces to camel case words, then swap _ for space, and capitalize each word.\n    ```python\n    x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n    ```\n    Args:\n        metric: A string to be formatted.\n    Returns:\n        The formatted version of 'metric'.\n    \"\"\"\nreturn string.capwords(re.sub(\"([a-z])([A-Z])\", r\"\\g&lt;1&gt; \\g&lt;2&gt;\", metric).replace(\"_\", \" \"))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.show_image", "title": "<code>show_image</code>", "text": "<p>Plots a given image onto an axis. The repeated invocation of this function will cause figure plot overlap.</p> <p>If <code>im</code> is 2D and the length of second dimension are 4 or 5, it will be viewed as bounding box data (x0, y0, w, h, ). <pre><code>boxes = np.array([[0, 0, 10, 20, \"apple\"],\n[10, 20, 30, 50, \"dog\"],\n[40, 70, 200, 200, \"cat\"],\n[0, 0, 0, 0, \"not_shown\"],\n[0, 0, -10, -20, \"not_shown2\"]])\nimg = np.zeros((150, 150))\nfig, axis = plt.subplots(1, 1)\nfe.util.show_image(img, fig=fig, axis=axis) # need to plot image first\nfe.util.show_image(boxes, fig=fig, axis=axis)\n</code></pre> <p>Users can also directly plot text</p> <pre><code>fig, axis = plt.subplots(1, 1)\nfe.util.show_image(\"apple\", fig=fig, axis=axis)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>plt.Axes</code> <p>The matplotlib axis to plot on, or None for a new plot.</p> <code>None</code> <code>fig</code> <code>plt.Figure</code> <p>A reference to the figure to plot on, or None if new plot.</p> <code>None</code> <code>im</code> <code>Union[np.ndarray, Tensor]</code> <p>The image (width X height) / bounding box / text to display.</p> required <code>title</code> <code>Optional[str]</code> <p>A title for the image.</p> <code>None</code> <code>color_map</code> <code>str</code> <p>Which colormap to use for greyscale images.</p> <code>'inferno'</code> <code>stack_depth</code> <code>int</code> <p>Multiple images can be drawn onto the same axis. When stack depth is greater than zero, the <code>im</code> will be alpha blended on top of a given axis.</p> <code>0</code> <p>Returns:</p> Type Description <code>Optional[plt.Figure]</code> <p>plotted figure. It will be the same object as user have provided in the argument.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def show_image(im: Union[np.ndarray, Tensor],\naxis: plt.Axes = None,\nfig: plt.Figure = None,\ntitle: Optional[str] = None,\ncolor_map: str = \"inferno\",\nstack_depth: int = 0) -&gt; Optional[plt.Figure]:\n\"\"\"Plots a given image onto an axis. The repeated invocation of this function will cause figure plot overlap.\n    If `im` is 2D and the length of second dimension are 4 or 5, it will be viewed as bounding box data (x0, y0, w, h,\n    &lt;label&gt;).\n    ```python\n    boxes = np.array([[0, 0, 10, 20, \"apple\"],\n                      [10, 20, 30, 50, \"dog\"],\n                      [40, 70, 200, 200, \"cat\"],\n                      [0, 0, 0, 0, \"not_shown\"],\n                      [0, 0, -10, -20, \"not_shown2\"]])\n    img = np.zeros((150, 150))\n    fig, axis = plt.subplots(1, 1)\n    fe.util.show_image(img, fig=fig, axis=axis) # need to plot image first\n    fe.util.show_image(boxes, fig=fig, axis=axis)\n    ```\n    Users can also directly plot text\n    ```python\n    fig, axis = plt.subplots(1, 1)\n    fe.util.show_image(\"apple\", fig=fig, axis=axis)\n    ```\n    Args:\n        axis: The matplotlib axis to plot on, or None for a new plot.\n        fig: A reference to the figure to plot on, or None if new plot.\n        im: The image (width X height) / bounding box / text to display.\n        title: A title for the image.\n        color_map: Which colormap to use for greyscale images.\n        stack_depth: Multiple images can be drawn onto the same axis. When stack depth is greater than zero, the `im`\n            will be alpha blended on top of a given axis.\n    Returns:\n        plotted figure. It will be the same object as user have provided in the argument.\n    \"\"\"\nif axis is None:\nfig, axis = plt.subplots(1, 1)\naxis.axis('off')\n# Compute width of axis for text font size\nbbox = axis.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\nwidth, height = bbox.width * fig.dpi, bbox.height * fig.dpi\nspace = min(width, height)\nif not hasattr(im, 'shape') or len(im.shape) &lt; 2:\n# text data\nim = to_number(im)\nif hasattr(im, 'shape') and len(im.shape) == 1:\nim = im[0]\nim = im.item()\nif isinstance(im, bytes):\nim = im.decode('utf8')\ntext = \"{}\".format(im)\naxis.text(0.5,\n0.5,\nim,\nha='center',\ntransform=axis.transAxes,\nva='center',\nwrap=False,\nfamily='monospace',\nfontsize=min(45, space // len(text)))\nelif len(im.shape) == 2 and (im.shape[1] == 4 or im.shape[1] == 5):\n# Bounding Box Data. Should be (x0, y0, w, h, &lt;label&gt;)\nboxes = []\nim = to_number(im)\ncolor = [\"m\", \"r\", \"c\", \"g\", \"y\", \"b\"][stack_depth % 6]\nfor box in im:\n# Unpack the box, which may or may not have a label\nx0 = int(box[0])\ny0 = int(box[1])\nwidth = int(box[2])\nheight = int(box[3])\nlabel = None if len(box) &lt; 5 else str(box[4])\n# Don't draw empty boxes, or invalid box\nif width &lt;= 0 or height &lt;= 0:\ncontinue\nr = Rectangle((x0, y0), width=width, height=height, fill=False, edgecolor=color, linewidth=3)\nboxes.append(r)\nif label:\naxis.text(r.get_x() + 3,\nr.get_y() + 3,\nlabel,\nha='left',\nva='top',\ncolor=color,\nfontsize=max(8, min(14, width // len(label))),\nfontweight='bold',\nfamily='monospace')\npc = PatchCollection(boxes, match_original=True)\naxis.add_collection(pc)\nelse:\nif isinstance(im, torch.Tensor) and len(im.shape) &gt; 2:\n# Move channel first to channel last\nchannels = list(range(len(im.shape)))\nchannels.append(channels.pop(0))\nim = im.permute(*channels)\n# image data\nim = to_number(im)\nim_max = np.max(im)\nim_min = np.min(im)\nif np.issubdtype(im.dtype, np.integer):\n# im is already in int format\nim = im.astype(np.uint8)\nelif 0 &lt;= im_min &lt;= im_max &lt;= 1:  # im is [0,1]\nim = (im * 255).astype(np.uint8)\nelif -0.5 &lt;= im_min &lt; 0 &lt; im_max &lt;= 0.5:  # im is [-0.5, 0.5]\nim = ((im + 0.5) * 255).astype(np.uint8)\nelif -1 &lt;= im_min &lt; 0 &lt; im_max &lt;= 1:  # im is [-1, 1]\nim = ((im + 1) * 127.5).astype(np.uint8)\nelse:  # im is in some arbitrary range, probably due to the Normalize Op\nma = abs(np.max(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nmi = abs(np.min(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nim = (((im + mi) / (ma + mi)) * 255).astype(np.uint8)\n# matplotlib doesn't support (x,y,1) images, so convert them to (x,y)\nif len(im.shape) == 3 and im.shape[2] == 1:\nim = np.reshape(im, (im.shape[0], im.shape[1]))\nalpha = 1 if stack_depth == 0 else 0.3\nif len(im.shape) == 2:\naxis.imshow(im, cmap=plt.get_cmap(name=color_map), alpha=alpha)\nelse:\naxis.imshow(im, alpha=alpha)\nif title is not None:\naxis.set_title(title, fontsize=min(20, 1 + width // len(title)), family='monospace')\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.strip_prefix", "title": "<code>strip_prefix</code>", "text": "<p>Remove the given <code>prefix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\nx = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>prefix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def strip_prefix(target: Optional[str], prefix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `prefix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\n    x = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        prefix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif prefix is None or target is None:\nreturn target\ns_len = len(prefix)\nif target[:s_len] == prefix:\nreturn target[s_len:]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.strip_suffix", "title": "<code>strip_suffix</code>", "text": "<p>Remove the given <code>suffix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\nx = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def strip_suffix(target: Optional[str], suffix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `suffix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\n    x = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        suffix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif suffix is None or target is None:\nreturn target\ns_len = len(suffix)\nif target[-s_len:] == suffix:\nreturn target[:-s_len]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_list", "title": "<code>to_list</code>", "text": "<p>Convert data to a list. A single None value will be converted to the empty list.</p> <pre><code>x = fe.util.to_list(None)  # []\nx = fe.util.to_list([None])  # [None]\nx = fe.util.to_list(7)  # [7]\nx = fe.util.to_list([7, 8])  # [7,8]\nx = fe.util.to_list({7})  # [7]\nx = fe.util.to_list((7))  # [7]\nx = fe.util.to_list({'a': 7})  # [{'a': 7}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>The input <code>data</code> but inside a list instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_list(data: Any) -&gt; List[Any]:\n\"\"\"Convert data to a list. A single None value will be converted to the empty list.\n    ```python\n    x = fe.util.to_list(None)  # []\n    x = fe.util.to_list([None])  # [None]\n    x = fe.util.to_list(7)  # [7]\n    x = fe.util.to_list([7, 8])  # [7,8]\n    x = fe.util.to_list({7})  # [7]\n    x = fe.util.to_list((7))  # [7]\n    x = fe.util.to_list({'a': 7})  # [{'a': 7}]\n    ```\n    Args:\n        data: Input data, within or without a python container.\n    Returns:\n        The input `data` but inside a list instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn []\nif not isinstance(data, list):\nif isinstance(data, (tuple, set)):\ndata = list(data)\nelse:\ndata = [data]\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_number", "title": "<code>to_number</code>", "text": "<p>Convert an input value into a Numpy ndarray.</p> <p>This method can be used with Python and Numpy data: <pre><code>b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\nb = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\nn = np.array([1, 2, 3])\nb = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([1, 2, 3])\nb = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([1, 2, 3])\nb = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[tf.Tensor, torch.Tensor, np.ndarray, int, float]</code> <p>The value to be converted into a np.ndarray.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An ndarray corresponding to the given <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_number(data: Union[tf.Tensor, torch.Tensor, np.ndarray, int, float]) -&gt; np.ndarray:\n\"\"\"Convert an input value into a Numpy ndarray.\n    This method can be used with Python and Numpy data:\n    ```python\n    b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\n    b = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\n    n = np.array([1, 2, 3])\n    b = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([1, 2, 3])\n    b = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([1, 2, 3])\n    b = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    Args:\n        data: The value to be converted into a np.ndarray.\n    Returns:\n        An ndarray corresponding to the given `data`.\n    \"\"\"\nif tf.is_tensor(data):\ndata = data.numpy()\nelif isinstance(data, torch.Tensor):\nif data.requires_grad:\ndata = data.detach().numpy()\nelse:\ndata = data.numpy()\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_set", "title": "<code>to_set</code>", "text": "<p>Convert data to a set. A single None value will be converted to the empty set.</p> <pre><code>x = fe.util.to_set(None)  # set()\nx = fe.util.to_set([None])  # {None}\nx = fe.util.to_set(7)  # {7}\nx = fe.util.to_set([7, 8])  # {7,8}\nx = fe.util.to_set({7})  # {7}\nx = fe.util.to_set((7))  # {7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container. The <code>data</code> must be hashable.</p> required <p>Returns:</p> Type Description <code>Set[Any]</code> <p>The input <code>data</code> but inside a set instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_set(data: Any) -&gt; Set[Any]:\n\"\"\"Convert data to a set. A single None value will be converted to the empty set.\n    ```python\n    x = fe.util.to_set(None)  # set()\n    x = fe.util.to_set([None])  # {None}\n    x = fe.util.to_set(7)  # {7}\n    x = fe.util.to_set([7, 8])  # {7,8}\n    x = fe.util.to_set({7})  # {7}\n    x = fe.util.to_set((7))  # {7}\n    ```\n    Args:\n        data: Input data, within or without a python container. The `data` must be hashable.\n    Returns:\n        The input `data` but inside a set instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn set()\nif not isinstance(data, set):\nif isinstance(data, (tuple, list, KeysView)):\ndata = set(data)\nelse:\ndata = {data}\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html", "title": "wget_util", "text": ""}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.bar_custom", "title": "<code>bar_custom</code>", "text": "<p>Return progress bar string for given values in one of three styles depending on available width.</p> <p>This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.</p> The bar will be one of the following formats depending on available width <p>[..  ] downloaded / total downloaded / total [.. ]</p> <p>If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:     %s / unknown     %s</p> <p>If there is not enough space on the screen, do not display anything. The returned string doesn't include control characters like   used to place cursor at the beginning of the line to erase previous content.</p> <p>This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.</p> <pre><code>wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>float</code> <p>The current amount of progress.</p> required <code>total</code> <code>float</code> <p>The total amount of progress required by the task.</p> required <code>width</code> <code>int</code> <p>The available width.</p> <code>80</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string to display the current progress.</p> Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def bar_custom(current: float, total: float, width: int = 80) -&gt; str:\n\"\"\"Return progress bar string for given values in one of three styles depending on available width.\n    This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.\n    The bar will be one of the following formats depending on available width:\n        [..  ] downloaded / total\n        downloaded / total\n        [.. ]\n    If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:\n        %s / unknown\n        %s\n    If there is not enough space on the screen, do not display anything. The returned string doesn't include control\n    characters like \\r used to place cursor at the beginning of the line to erase previous content.\n    This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.\n    ```python\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        current: The current amount of progress.\n        total: The total amount of progress required by the task.\n        width: The available width.\n    Returns:\n        A formatted string to display the current progress.\n    \"\"\"\n# process special case when total size is unknown and return immediately\nif not total or total &lt; 0:\nmsg = \"{} / unknown\".format(current)\nif len(msg) &lt; width:  # leaves one character to avoid linefeed\nreturn msg\nif len(\"{}\".format(current)) &lt; width:\nreturn \"{}\".format(current)\n# --- adaptive layout algorithm ---\n#\n# [x] describe the format of the progress bar\n# [x] describe min width for each data field\n# [x] set priorities for each element\n# [x] select elements to be shown\n#   [x] choose top priority element min_width &lt; avail_width\n#   [x] lessen avail_width by value if min_width\n#   [x] exclude element from priority list and repeat\n#  10% [.. ]  10/100\n# pppp bbbbb sssssss\nmin_width = {\n'percent': 4,  # 100%\n'bar': 3,  # [.]\n'size': len(\"{}\".format(total)) * 2 + 3,  # 'xxxx / yyyy'\n}\npriority = ['percent', 'bar', 'size']\n# select elements to show\nselected = []\navail = width\nfor field in priority:\nif min_width[field] &lt; avail:\nselected.append(field)\navail -= min_width[field] + 1  # +1 is for separator or for reserved space at\n# the end of line to avoid linefeed on Windows\n# render\noutput = ''\nfor field in selected:\nif field == 'percent':\n# fixed size width for percentage\noutput += \"{}%\".format(100 * current // total).rjust(min_width['percent'])\nelif field == 'bar':  # [. ]\n# bar takes its min width + all available space\noutput += wget.bar_thermometer(current, total, min_width['bar'] + avail)\nelif field == 'size':\n# size field has a constant width (min == max)\noutput += \"{:.2f} / {:.2f} MB\".format(current / 1e6, total / 1e6).rjust(min_width['size'])\nselected = selected[1:]\nif selected:\noutput += ' '  # add field separator\nreturn output\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.callback_progress", "title": "<code>callback_progress</code>", "text": "<p>Callback function for urlretrieve that is called when a connection is created and then once for each block.</p> <p>Draws adaptive progress bar in terminal/console.</p> <p>Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a linefeed on Windows.</p> <pre><code>import wget\nwget.callback_progress = fe.util.callback_progress\nwget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocks</code> <code>int</code> <p>number of blocks transferred so far.</p> required <code>block_size</code> <code>int</code> <p>in bytes.</p> required <code>total_size</code> <code>int</code> <p>in bytes, can be -1 if server doesn't return it.</p> required <code>bar_function</code> <code>Callable[[int, int, int], str]</code> <p>another callback function to visualize progress.</p> required Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def callback_progress(blocks: int, block_size: int, total_size: int, bar_function: Callable[[int, int, int],\nstr]) -&gt; None:\n\"\"\"Callback function for urlretrieve that is called when a connection is created and then once for each block.\n    Draws adaptive progress bar in terminal/console.\n    Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a\n    linefeed on Windows.\n    ```python\n    import wget\n    wget.callback_progress = fe.util.callback_progress\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        blocks: number of blocks transferred so far.\n        block_size: in bytes.\n        total_size: in bytes, can be -1 if server doesn't return it.\n        bar_function: another callback function to visualize progress.\n    \"\"\"\nwidth = min(100, wget.get_console_width())\nif width == 0:  # sys.stdout.fileno() in get_console_width() is not supported in jupyter notebook\nwidth = 80\ncurrent_size = min(blocks * block_size, total_size)\nprogress = bar_function(current_size, total_size, width)\nif progress:\nsys.stdout.write(\"\\r{}\".format(progress))\nif current_size &gt;= total_size:\nsys.stdout.write(\"\\n\")\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet", "title": "<code>SaliencyNet</code>", "text": "<p>A class to generate saliency masks from a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model, compiled with fe.build, which is to be inspected.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model inputs within the data dictionary.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model outputs which are written into the data dictionary.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The keys(s) under which to write the generated saliency images.</p> <code>'saliency'</code> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>@traceable()\nclass SaliencyNet:\n\"\"\"A class to generate saliency masks from a given model.\n    Args:\n        model: The model, compiled with fe.build, which is to be inspected.\n        model_inputs: The key(s) corresponding to the model inputs within the data dictionary.\n        model_outputs: The key(s) corresponding to the model outputs which are written into the data dictionary.\n        outputs: The keys(s) under which to write the generated saliency images.\n    \"\"\"\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\noutputs: Union[str, List[str]] = \"saliency\"):\nmode = \"test\"\nself.model_op = ModelOp(model=model, mode=mode, inputs=model_inputs, outputs=model_outputs, trainable=False)\nself.outputs = to_list(outputs)\nself.mode = mode\nself.gather_keys = [\"SaliencyNet_Target_Index_{}\".format(key) for key in self.model_outputs]\nself.network = Network(ops=[\nWatch(inputs=self.model_inputs, mode=mode),\nself.model_op,\nGather(inputs=self.model_outputs,\nindices=self.gather_keys,\noutputs=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\nmode=mode),\nGradientOp(inputs=self.model_inputs,\nfinals=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\noutputs=deepcopy(self.outputs),\nmode=mode),\n])\n@property\ndef model_inputs(self):\nreturn deepcopy(self.model_op.inputs)\n@property\ndef model_outputs(self):\nreturn deepcopy(self.model_op.outputs)\n@staticmethod\ndef _convert_for_visualization(tensor: Tensor, tile: int = 99) -&gt; np.ndarray:\n\"\"\"Modify the range of data in a given input `tensor` to be appropriate for visualization.\n        Args:\n            tensor: Input masks, whose channel values are to be reduced by absolute value summation.\n            tile: The percentile [0-100] used to set the max value of the image.\n        Returns:\n            A (batch X width X height) image after visualization clipping is applied.\n        \"\"\"\nif isinstance(tensor, torch.Tensor):\nchannel_axis = 1\nelse:\nchannel_axis = -1\nflattened_mask = reduce_sum(abs(tensor), axis=channel_axis, keepdims=True)\nnon_batch_axes = list(range(len(flattened_mask.shape)))[1:]\nvmax = percentile(flattened_mask, tile, axis=non_batch_axes, keepdims=True)\nvmin = reduce_min(flattened_mask, axis=non_batch_axes, keepdims=True)\nreturn clip_by_value((flattened_mask - vmin) / (vmax - vmin), 0, 1)\ndef get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\ndef _get_mask(self, batch: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model outputs and the raw saliency mask(s) for the given `batch` of data. Model predictions are reduced\n            via argmax.\n        \"\"\"\nfor key in self.gather_keys:\n# If there's no target key, use an empty array which will cause the max-likelihood class to be selected\nbatch.setdefault(key, [])\nprediction = self.network.transform(data=batch, mode=self.mode)\nfor key in self.model_outputs:\nprediction[key] = argmax(prediction[key], axis=1)\nreturn prediction\ndef _get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw integrated saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n            nsamples: How many samples to consider during integration.\n        Returns:\n            The raw integrated saliency mask(s) for the given `batch` of data.\n        \"\"\"\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\ninput_baselines = [zeros_like(ins) + (reduce_max(ins) + reduce_min(ins)) / 2 for ins in model_inputs]\ninput_diffs = [\nmodel_input - input_baseline for model_input, input_baseline in zip(model_inputs, input_baselines)\n]\nresponse = {}\nfor alpha in np.linspace(0.0, 1.0, nsamples):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nx_step = input_baselines[idx] + alpha * input_diffs[idx]\nnoisy_batch[input_name] = x_step\ngrads_and_preds = self._get_mask(noisy_batch)\nfor key in self.outputs:\nif key in response:\nresponse[key] += grads_and_preds[key]\nelse:\nresponse[key] = grads_and_preds[key]\nfor key in self.outputs:\ngrad = response[key]\nfor diff in input_diffs:\ngrad = grad * diff\nresponse[key] = grad\nreturn response\ndef get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: An input batch of data.\n            stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n            nsamples: Number of samples to average across to get the smooth gradient.\n            nintegration: Number of samples to compute when integrating (None to disable).\n            magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n        Returns:\n            Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nnoisy_batch, nsamples=nintegration)\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\ndef get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n        See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n        Args:\n            batch: An input batch of data.\n            nsamples: Number of samples to average across to get the integrated gradient.\n        Returns:\n            Greyscale saliency masks smoothed via the IntegratedGradient method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_integrated_masks", "title": "<code>get_integrated_masks</code>", "text": "<p>Generates integrated greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the integrated gradient.</p> <code>25</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency masks smoothed via the IntegratedGradient method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n    See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n    Args:\n        batch: An input batch of data.\n        nsamples: Number of samples to average across to get the integrated gradient.\n    Returns:\n        Greyscale saliency masks smoothed via the IntegratedGradient method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_masks", "title": "<code>get_masks</code>", "text": "<p>Generates greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>A batch of input data to be fed to the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>The model's classification decisions and greyscale saliency mask(s) for the given <code>batch</code> of data.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: A batch of input data to be fed to the model.\n    Returns:\n        The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_smoothed_masks", "title": "<code>get_smoothed_masks</code>", "text": "<p>Generates smoothed greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>stdev_spread</code> <code>float</code> <p>Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).</p> <code>0.15</code> <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the smooth gradient.</p> <code>25</code> <code>nintegration</code> <code>Optional[int]</code> <p>Number of samples to compute when integrating (None to disable).</p> <code>None</code> <code>magnitude</code> <code>bool</code> <p>If true, computes the sum of squares of gradients instead of just the sum.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency mask(s) smoothed via the SmoothGrad method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: An input batch of data.\n        stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n        nsamples: Number of samples to average across to get the smooth gradient.\n        nintegration: Number of samples to compute when integrating (None to disable).\n        magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n    Returns:\n        Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nnoisy_batch, nsamples=nintegration)\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\n</code></pre>"}, {"location": "tutorial/advanced/t01_dataset.html", "title": "Advanced Tutorial 1: Dataset", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\ntrain_data, eval_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data train_data, eval_data = load_data() In\u00a0[2]: Copied! <pre>train_data.summary()\n</pre> train_data.summary() Out[2]: <pre>&lt;DatasetSummary {'num_instances': 60000, 'keys': {'x': &lt;KeySummary {'shape': [28, 28], 'dtype': 'uint8'}&gt;, 'y': &lt;KeySummary {'num_unique_values': 10, 'shape': [], 'dtype': 'uint8'}&gt;}}&gt;</pre> <p>Or even more simply, by invoking the print function:</p> In\u00a0[3]: Copied! <pre>print(train_data)\n</pre> print(train_data) <pre>{\"num_instances\": 60000, \"keys\": {\"x\": {\"shape\": [28, 28], \"dtype\": \"uint8\"}, \"y\": {\"num_unique_values\": 10, \"shape\": [], \"dtype\": \"uint8\"}}}\n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>test_data = eval_data.split(0.5)\n</pre> test_data = eval_data.split(0.5) <p>Or if I want to split evaluation data into two test datasets with 20% of the evaluation data each:</p> In\u00a0[5]: Copied! <pre>test_data1, test_data2 = eval_data.split(0.2, 0.2)\n</pre> test_data1, test_data2 = eval_data.split(0.2, 0.2) <p></p> In\u00a0[6]: Copied! <pre>test_data3 = eval_data.split(100)\n</pre> test_data3 = eval_data.split(100) <p>And of course, we can generate multiple datasets by providing multiple inputs:</p> In\u00a0[7]: Copied! <pre>test_data4, test_data5 = eval_data.split(100, 100)\n</pre> test_data4, test_data5 = eval_data.split(100, 100) <p></p> In\u00a0[8]: Copied! <pre>test_data6 = eval_data.split([0,1,100])\n</pre> test_data6 = eval_data.split([0,1,100]) <p>If you just want continuous index, here's an easy way to provide index:</p> In\u00a0[9]: Copied! <pre>test_data7 = eval_data.split(range(100))\n</pre> test_data7 = eval_data.split(range(100)) <p>Needless to say, you can provide multiple inputs too:</p> In\u00a0[10]: Copied! <pre>test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5])\n</pre> test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5]) <p></p> In\u00a0[11]: Copied! <pre>from fastestimator.dataset.data.breast_cancer import load_data\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_data, eval_data = load_data()\nscaler = StandardScaler()\n\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\n</pre> from fastestimator.dataset.data.breast_cancer import load_data from sklearn.preprocessing import StandardScaler  train_data, eval_data = load_data() scaler = StandardScaler()  train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) <p></p> <p></p> In\u00a0[12]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_deterministic = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[4,4])\n# ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")  dataset_deterministic = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[4,4]) # ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[13]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_distribution = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=8, probability=[0.5, 0.5])\n# ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")  dataset_distribution = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=8, probability=[0.5, 0.5]) # ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[14]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\")\ncifar_data, _ = cifar10.load_data(image_key=\"x_cifar\", label_key=\"y_cifar\")\n\ndataset_unpaired = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[1,1])\n# ready to use dataset_unpaired in Pipeline\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\") cifar_data, _ = cifar10.load_data(image_key=\"x_cifar\", label_key=\"y_cifar\")  dataset_unpaired = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[1,1]) # ready to use dataset_unpaired in Pipeline <p></p>"}, {"location": "tutorial/advanced/t01_dataset.html#advanced-tutorial-1-dataset", "title": "Advanced Tutorial 1: Dataset\u00b6", "text": ""}, {"location": "tutorial/advanced/t01_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following topics:</p> <ul> <li>Dataset Summary</li> <li>Dataset Splitting<ul> <li>Random Fraction Split</li> <li>Random Count Split</li> <li>Index Split</li> </ul> </li> <li>Global Dataset Editing</li> <li>BatchDataset<ul> <li>Deterministic Batching</li> <li>Distribution Batching</li> <li>Unpaired Dataset</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>Before going through the tutorial, it is recommended to check Beginner Tutorial 2 for basic understanding of <code>dataset</code> from PyTorch and FastEstimator. We will talk about more details about <code>fe.dataset</code> API in this tutorial.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-summary", "title": "Dataset summary\u00b6", "text": "<p>As we have mentioned in previous tutorial, users can import our inherited dataset class for easy use in <code>Pipeline</code>. But how do we know what keys are available in the dataset?   Well, obviously one easy way is just call <code>dataset[0]</code> and check the keys. However, there's a more elegant way to check information of dataset: <code>dataset.summary()</code>.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-splitting", "title": "Dataset Splitting\u00b6", "text": "<p>Dataset splitting is nothing new in machine learning. In FastEstimator, users can easily split their data in different ways.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-fraction-split", "title": "Random Fraction Split\u00b6", "text": "<p>Let's say we want to randomly split 50% of the evaluation data into test data. This is easily accomplished:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-count-split", "title": "Random Count Split\u00b6", "text": "<p>Sometimes instead of fractions, we want an actual number of examples to split; for example, randomly splitting 100 samples from the evaluation dataset:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#index-split", "title": "Index Split\u00b6", "text": "<p>There are times when we need to split the dataset in a specific way. For that, you can provide a list of indexes. For example, if we want to split the 0th, 1st and 100th element of evaluation dataset into new test set:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#global-dataset-editing", "title": "Global Dataset Editing\u00b6", "text": "<p>In deep learning, we usually process the dataset batch by batch. However, when we are handling tabular data, we might need to apply some transformation globally before the training. For example, we may want to standardize the tabular data using <code>sklearn</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#batchdataset", "title": "BatchDataset\u00b6", "text": "<p>There might be scenarios where we need to combine multiple datasets together into one dataset in a specific way. Let's consider three such use-cases now:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#deterministic-batching", "title": "Deterministic Batching\u00b6", "text": "<p>Let's say we have <code>mnist</code> and <code>cifar</code> datasets, and want to combine them with a total batch size of 8. If we always want 4 examples from <code>mnist</code> and the rest from <code>cifar</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#distribution-batching", "title": "Distribution Batching\u00b6", "text": "<p>Some people prefer randomness in a batch. For example, given total batch size of 8, let's say we want 0.5 probability of <code>mnist</code> and the other 0.5 from <code>cifar</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#unpaired-dataset", "title": "Unpaired Dataset\u00b6", "text": "<p>Some deep learning tasks require random unpaired datasets. For example, in image-to-image translation (like Cycle-GAN), the system needs to randomly sample one horse image and one zebra image for every batch. In FastEstimator, <code>BatchDataset</code> can also handle unpaired datasets. The only restriction is that: keys from two different datasets must be unique for unpaired datasets.</p> <p>For example, let's sample one image from <code>mnist</code> and one image from <code>cifar</code> for every batch:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>DNN</li> </ul>"}, {"location": "tutorial/advanced/t02_pipeline.html", "title": "Advanced Tutorial 2: Pipeline", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Iterating Through Pipeline<ul> <li>Basic Concept</li> <li>Example</li> </ul> </li> <li>Dropping Last Batch</li> <li>Padding Batch Data</li> <li>Benchmark Pipeline Speed</li> </ul> <p>In the Beginner Tutorial 4, we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the <code>Pipeline</code>, we will demonstrate some advanced concepts and how to leverage them to create efficient <code>Pipelines</code> in this tutorial.</p> <p></p> <p></p> <p>In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom fastestimator.dataset.data import cifar10\n    \n# sample numpy array to later create datasets from them\nx_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np from fastestimator.dataset.data import cifar10      # sample numpy array to later create datasets from them x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1))) train_data = {\"x\": x_train, \"y\": y_train} In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# create NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)\n</pre> import fastestimator as fe from fastestimator.dataset.numpy_dataset import NumpyDataset  # create NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3) <p>Let's get the loader object for the <code>Pipeline</code>, then iterate through the loader with a for loop:</p> In\u00a0[3]: Copied! <pre>loader_fe = pipeline_fe.get_loader(mode=\"train\")\n\nfor batch in loader_fe:\n    print(batch)\n</pre> loader_fe = pipeline_fe.get_loader(mode=\"train\")  for batch in loader_fe:     print(batch) <pre>{'x': tensor([[0.5085, 0.0519],\n        [0.8167, 0.1808],\n        [0.9175, 0.5209]], dtype=torch.float64), 'y': tensor([[0.5407],\n        [0.7994],\n        [0.4728]], dtype=torch.float64)}\n{'x': tensor([[0.9302, 0.6404],\n        [0.1795, 0.1212],\n        [0.8716, 0.9381]], dtype=torch.float64), 'y': tensor([[0.4747],\n        [0.4103],\n        [0.3916]], dtype=torch.float64)}\n{'x': tensor([[0.9929, 0.9415],\n        [0.6404, 0.8039],\n        [0.1624, 0.9285]], dtype=torch.float64), 'y': tensor([[0.1118],\n        [0.8162],\n        [0.7057]], dtype=torch.float64)}\n{'x': tensor([[0.0748, 0.8554]], dtype=torch.float64), 'y': tensor([[0.2276]], dtype=torch.float64)}\n</pre> <p></p> <p>Let's say we have CIFAR-10 dataset and we want to find global average pixel value over three channels:</p> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ncifar_train, _ = cifar10.load_data()\n</pre> from fastestimator.dataset.data import cifar10  cifar_train, _ = cifar10.load_data() <p>We will take the <code>batch_size</code> 64 and load the data into <code>Pipeline</code></p> In\u00a0[5]: Copied! <pre>pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64)\n</pre> pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64) <p>Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset.</p> In\u00a0[6]: Copied! <pre>loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False)\nmean_arr = np.zeros((3))\nfor i, batch in enumerate(loader_fe):\n    mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\nmean_arr = mean_arr / (i+1)\n</pre> loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False) mean_arr = np.zeros((3)) for i, batch in enumerate(loader_fe):     mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2)) mean_arr = mean_arr / (i+1) In\u00a0[7]: Copied! <pre>print(\"Mean pixel value over the channels are: \", mean_arr)\n</pre> print(\"Mean pixel value over the channels are: \", mean_arr) <pre>Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n</pre> <p></p> <p>If the total number of dataset elements is not divisible by the <code>batch_size</code>, by default, the last batch will have less data than other batches.  To drop the last batch we can set <code>drop_last</code> to <code>True</code>. Therefore, if the last batch is incomplete it will be dropped.</p> In\u00a0[8]: Copied! <pre>pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True)\n</pre> pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True) <p></p> <p>There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch.</p> <p>To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre># define numpy arrays with different shapes\nelem1 = np.array([4, 5])\nelem2 = np.array([1, 2, 6])\nelem3 = np.array([3])\n\n# create train dataset\nx_train = np.array([elem1, elem2, elem3])\ntrain_data = {\"x\": x_train}\ndataset_fe = NumpyDataset(train_data)\n</pre> # define numpy arrays with different shapes elem1 = np.array([4, 5]) elem2 = np.array([1, 2, 6]) elem3 = np.array([3])  # create train dataset x_train = np.array([elem1, elem2, elem3]) train_data = {\"x\": x_train} dataset_fe = NumpyDataset(train_data) <p>We will set any <code>pad_value</code> that we want to append at the end of the tensor data. <code>pad_value</code> can be either <code>int</code> or <code>float</code>:</p> In\u00a0[10]: Copied! <pre>pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0)\n</pre> pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0) <p>Now let's print the batch data after padding:</p> In\u00a0[11]: Copied! <pre>for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):\n    print(elem)\n</pre> for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):     print(elem) <pre>{'x': tensor([[4, 5, 0],\n        [1, 2, 6],\n        [3, 0, 0]])}\n</pre> <p></p> <p>It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a <code>Pipeline</code> in order to help diagnose any potential problems. The way to benchmark <code>Pipeline</code> speed in FastEstimator is very simple: call <code>Pipeline.benchmark</code>.</p> <p>For illustration, we will create a <code>Pipeline</code> for the CIFAR-10 dataset with list of Numpy operators that expand dimensions, apply <code>Minmax</code> and finally <code>Rotate</code> the input images:</p> In\u00a0[12]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\nfrom fastestimator.op.numpyop.multivariate import Rotate\n\npipeline = fe.Pipeline(train_data=cifar_train,\n                       ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),\n                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),\n                            ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],\n                       batch_size=64)\n</pre> from fastestimator.op.numpyop.univariate import Minmax, ExpandDims from fastestimator.op.numpyop.multivariate import Rotate  pipeline = fe.Pipeline(train_data=cifar_train,                        ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),                             Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),                             ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],                        batch_size=64) <p>Let's benchmark the pre-processing speed for this pipeline in training mode:</p> In\u00a0[13]: Copied! <pre>pipeline.benchmark(mode=\"train\")\n</pre> pipeline.benchmark(mode=\"train\") <pre>FastEstimator: Step: 100, Epoch: 1, Steps/sec: 355.0314672561461\nFastEstimator: Step: 200, Epoch: 1, Steps/sec: 688.959209809261\nFastEstimator: Step: 300, Epoch: 1, Steps/sec: 646.3625572114369\nFastEstimator: Step: 400, Epoch: 1, Steps/sec: 709.4726870671318\nFastEstimator: Step: 500, Epoch: 1, Steps/sec: 654.4829388343634\nFastEstimator: Step: 600, Epoch: 1, Steps/sec: 716.1617101086067\nFastEstimator: Step: 700, Epoch: 1, Steps/sec: 635.2802801079024\n\nBreakdown of time taken by Pipeline Operations (train epoch 1)\nOp         : Inputs : Outputs :  Time\n--------------------------------------\nMinmax     : x      : x_out   : 39.42%\nRotate     : x_out  : x_out   : 49.45%\nExpandDims : x_out  : x_out   : 11.13%\n</pre>"}, {"location": "tutorial/advanced/t02_pipeline.html#advanced-tutorial-2-pipeline", "title": "Advanced Tutorial 2: Pipeline\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#overview", "title": "Overview\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#iterating-through-pipeline", "title": "Iterating Through Pipeline\u00b6", "text": "<p>In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the <code>ImageNet</code> dataset, people usually use a precomputed global pixel average for each channel to normalize the images.</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#basic-concept", "title": "Basic Concept\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#example", "title": "Example\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#dropping-last-batch", "title": "Dropping Last Batch\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#padding-batch-data", "title": "Padding Batch Data\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#benchmark-pipeline-speed", "title": "Benchmark Pipeline Speed\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html", "title": "Advanced Tutorial 3: Operator", "text": "<p>Here's one simple example of an operator:</p> In\u00a0[1]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddOne(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        x, y = data\n        x = x + 1\n        y = y + 1\n        return x, y\n    \nAddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\"))\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddOne(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         x, y = data         x = x + 1         y = y + 1         return x, y      AddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\")) <p>An <code>Op</code> interacts with the required portion of this data using the keys specified through the <code>inputs</code> key, processes the data through the <code>forward</code> function and writes the values returned from the <code>forward</code> function to this data dictionary using the <code>outputs</code> key. The processes are illustrated in the diagram below:</p> <p></p> <p></p> <p></p> <p></p> <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import OneOf, Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip\nfrom fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose\n\ntrain_data, eval_data = cifar10.load_data() \n\npipeline1 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])\n\npipeline2 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45), \n                               Delete(keys=\"x_mid\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import OneOf, Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip from fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose  train_data, eval_data = cifar10.load_data()   pipeline1 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])  pipeline2 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45),                                 Delete(keys=\"x_mid\")]) In\u00a0[3]: Copied! <pre>data1 = pipeline1.get_results()\nprint(\"Keys in pipeline: \", data1.keys())\n\ndata2 = pipeline2.get_results()\nprint(\"Keys in pipeline with Delete Op: \", data2.keys())\n</pre> data1 = pipeline1.get_results() print(\"Keys in pipeline: \", data1.keys())  data2 = pipeline2.get_results() print(\"Keys in pipeline with Delete Op: \", data2.keys()) <pre>Keys in pipeline:  dict_keys(['x', 'y', 'x_mid'])\nKeys in pipeline with Delete Op:  dict_keys(['x', 'y'])\n</pre> <p></p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop import LambdaOp\n\npipeline3 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [LambdaOp(fn=lambda: 5, outputs=\"z\"), #create a new key\n                               LambdaOp(fn=lambda a: a*3, inputs=\"z\", outputs=\"z3\")\n                               ])\n</pre> from fastestimator.op.numpyop import LambdaOp  pipeline3 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [LambdaOp(fn=lambda: 5, outputs=\"z\"), #create a new key                                LambdaOp(fn=lambda a: a*3, inputs=\"z\", outputs=\"z3\")                                ]) In\u00a0[5]: Copied! <pre>data3 = pipeline3.get_results()\nprint(f\"z: {data3['z']}\")\nprint(f\"z3: {data3['z3']}\")\n</pre> data3 = pipeline3.get_results() print(f\"z: {data3['z']}\") print(f\"z3: {data3['z3']}\") <pre>z: tensor([5, 5, 5, 5])\nz3: tensor([15, 15, 15, 15])\n</pre> <p></p> In\u00a0[6]: Copied! <pre>from albumentations.augmentations.transforms import RandomCrop\nimport numpy as np\n\nclass Patch(NumpyOp):\n    def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):\n        super().__init__(inputs, outputs, mode)\n        self.num_patch = num_patch\n        self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)\n\n    def forward(self, data, state):\n        image, label = data\n        image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)\n        label = np.array([label for _ in range(self.num_patch)])\n        return [image, label]\n    \n    def _gen_patch(self, data):\n        data = self.crop_fn(image=data)\n        return data[\"image\"].astype(np.float32)\n</pre> from albumentations.augmentations.transforms import RandomCrop import numpy as np  class Patch(NumpyOp):     def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):         super().__init__(inputs, outputs, mode)         self.num_patch = num_patch         self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)      def forward(self, data, state):         image, label = data         image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)         label = np.array([label for _ in range(self.num_patch)])         return [image, label]          def _gen_patch(self, data):         data = self.crop_fn(image=data)         return data[\"image\"].astype(np.float32) <p>Let's create a pipeline and visualize the results.</p> In\u00a0[7]: Copied! <pre>pipeline4 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=8,\n                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"], \n                                   num_patch=4)])\n</pre> pipeline4 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=8,                         ops=[Minmax(inputs=\"x\", outputs=\"x\"),                              Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"],                                     num_patch=4)]) In\u00a0[8]: Copied! <pre>from fastestimator.util import to_number\n\ndata4 = pipeline4.get_results()\nimg = fe.util.ImgData(Input_Image=to_number(data4[\"x\"]), \n                      Patch_0=to_number(data4[\"x_out\"])[:,0,:,:,:], \n                      Patch_1=to_number(data4[\"x_out\"])[:,1,:,:,:], \n                      Patch_2=to_number(data4[\"x_out\"])[:,2,:,:,:], \n                      Patch_3=to_number(data4[\"x_out\"])[:,3,:,:,:])\nfig = img.paint_figure()\n</pre> from fastestimator.util import to_number  data4 = pipeline4.get_results() img = fe.util.ImgData(Input_Image=to_number(data4[\"x\"]),                        Patch_0=to_number(data4[\"x_out\"])[:,0,:,:,:],                        Patch_1=to_number(data4[\"x_out\"])[:,1,:,:,:],                        Patch_2=to_number(data4[\"x_out\"])[:,2,:,:,:],                        Patch_3=to_number(data4[\"x_out\"])[:,3,:,:,:]) fig = img.paint_figure() <p></p> <p></p> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import LambdaOp\nimport tensorflow as tf\n\nnetwork = fe.Network(ops=[\n    LambdaOp(inputs='x', outputs='y', fn=lambda a: a*5)\n])\n\ndata = network.transform(data={'x':tf.ones((2,2))}, mode='train')\nprint(f\"x: \\n{data['x']}\")\nprint(f\"y: \\n{data['y']}\")\n</pre> from fastestimator.op.tensorop import LambdaOp import tensorflow as tf  network = fe.Network(ops=[     LambdaOp(inputs='x', outputs='y', fn=lambda a: a*5) ])  data = network.transform(data={'x':tf.ones((2,2))}, mode='train') print(f\"x: \\n{data['x']}\") print(f\"y: \\n{data['y']}\") <pre>x: \n[[1. 1.]\n [1. 1.]]\ny: \n[[5. 5.]\n [5. 5.]]\n</pre> <p></p> In\u00a0[10]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass DimensionAdjustment(TensorOp):\n    def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.reduce_dim = reduce_dim\n        self.reshape_fn = None\n    \n    def build(self, framework):\n        if framework=='tf':\n            self.reshape_fn = lambda tensor: tf.reshape(tensor, shape=self._new_shape(tensor))\n        elif framework=='torch':\n            self.reshape_fn = lambda tensor: torch.reshape(tensor, shape=self._new_shape(tensor))\n    \n    def forward(self, data, state):\n        image, label = data\n        image_out = self.reshape_fn(image)\n        label_out = self.reshape_fn(label)\n        return [image_out, label_out]\n    \n    def _new_shape(self, data):\n        return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim]\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class DimensionAdjustment(TensorOp):     def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):         super().__init__(inputs, outputs, mode)         self.reduce_dim = reduce_dim         self.reshape_fn = None          def build(self, framework):         if framework=='tf':             self.reshape_fn = lambda tensor: tf.reshape(tensor, shape=self._new_shape(tensor))         elif framework=='torch':             self.reshape_fn = lambda tensor: torch.reshape(tensor, shape=self._new_shape(tensor))          def forward(self, data, state):         image, label = data         image_out = self.reshape_fn(image)         label_out = self.reshape_fn(label)         return [image_out, label_out]          def _new_shape(self, data):         return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim] In\u00a0[11]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\npipeline5 = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=8,\n                       ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                            Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"], \n                                  num_patch=4)])\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> from fastestimator.architecture.tensorflow import LeNet from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  pipeline5 = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=8,                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"],                                    num_patch=4)])  model = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) <p>Let's check the dimensions the of Pipeline output and DimensionAdjustment TensorOp output.</p> In\u00a0[12]: Copied! <pre>data5 = pipeline5.get_results()\nresult = network.transform(data5, mode=\"infer\")\n\nprint(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\")\nprint(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\")\n</pre> data5 = pipeline5.get_results() result = network.transform(data5, mode=\"infer\")  print(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\") print(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\") <pre>Pipeline Output, Image Shape: torch.Size([8, 4, 24, 24, 3]), Label Shape: torch.Size([8, 4, 1])\nResult Image Shape: (32, 24, 24, 3), Label Shape: (32, 1)\n</pre> <p><code>TensorOps</code> have three other methods which are much less commonly used, but may be overridden if you are working on a complex Op:</p> <ol> <li>get_fe_models(self) -&gt; Set</li> <li>get_fe_loss_keys(self) -&gt; Set</li> <li>fe_retain_graph(self, retain) -&gt; bool</li> </ol> <p>If your custom <code>TensorOp</code> contains one or more neural network models, you should override the get_fe_models() method to return all of those models. An example where this is done is in our <code>ModelOp</code>.</p> <p>If your custom <code>TensorOp</code> is being used to apply a loss value to a model, you should override the get_fe_loss_keys() method to return the string name(s) of all the keys which are being used as losses. An example where this is done is in our <code>UpdateOp</code>.</p> <p>Finally, if your custom Op computes gradients, it should override the fe_retain_graph method such that it can control whether or not your Op will keep the computation graph in memory or erase it after completing its forward pass. An example where this is done is in our <code>UpdateOp</code>.</p> <p></p>"}, {"location": "tutorial/advanced/t03_operator.html#advanced-tutorial-3-operator", "title": "Advanced Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Operator Mechanism<ul> <li>data</li> <li>state</li> </ul> </li> <li>NumpyOp<ul> <li>DeleteOp</li> <li>LambdaOp</li> <li>Customizing NumpyOps</li> </ul> </li> <li>TensorOp<ul> <li>LambdaOp</li> <li>Customizing TensorOps</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t03_operator.html#operator-mechanism", "title": "Operator Mechanism\u00b6", "text": "<p>We learned about the operator structure in Beginner Tutorial 3. Operators are used to build complex computation graphs in FastEstimator.</p> <p>In FastEstimator, all the available data is held in a data dictionary during execution. An <code>Op</code> runs when it's <code>mode</code> matches the current execution mode. For more information on mode, you can go through Beginner Tutorial 8.</p>"}, {"location": "tutorial/advanced/t03_operator.html#data", "title": "data\u00b6", "text": "<p>The data argument in the <code>forward</code> function passes the portion of data dictionary corresponding to the Operator's <code>inputs</code> into the forward function. If multiple keys are provided as <code>inputs</code>, the data will be a list of corresponding to the values of those keys.</p>"}, {"location": "tutorial/advanced/t03_operator.html#state", "title": "state\u00b6", "text": "<p>The state argument in the <code>forward</code> function stores meta information about training like the current mode, GradientTape for tensorflow, etc. It is very unlikely that you would need to interact with it.</p>"}, {"location": "tutorial/advanced/t03_operator.html#numpyop", "title": "NumpyOp\u00b6", "text": "<p>NumpyOp is used in <code>Pipeline</code> for data pre-processing and augmentation. You can go through Beginner Tutorial 4 to get an overview of NumpyOp and their usage. Here, we will talk about some advanced NumpyOps.</p>"}, {"location": "tutorial/advanced/t03_operator.html#deleteop", "title": "DeleteOp\u00b6", "text": "<p>Delete op is used to delete keys from the data dictionary which are no longer required by the user. This helps in improving processing speed as we are holding only the required data in the memory. Let's see its usage:</p>"}, {"location": "tutorial/advanced/t03_operator.html#lambdaop", "title": "LambdaOp\u00b6", "text": "<p>The <code>LambdaOp</code> is a flexible Op which allows you to execute arbitrary lambda functions. This can be especially useful for adding new keys to the data dictionary, or for performing simple computations without having to write a new NumpyOp. If your lambda function has a return value, it should be in the form of an np.ndarrary.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-numpyops", "title": "Customizing NumpyOps\u00b6", "text": "<p>We can create a custom NumpyOp which suits our needs. Below, we showcase a custom NumpyOp which creates multiple random patches (crops) of images from each image.</p>"}, {"location": "tutorial/advanced/t03_operator.html#tensorop", "title": "TensorOp\u00b6", "text": "<p><code>TensorOps</code> are used to process tensor data. They are used within a <code>Network</code> for graph-based operations. You can go through Beginner Tutorial 6 to get an overview of <code>TensorOps</code> and their usages.</p>"}, {"location": "tutorial/advanced/t03_operator.html#lambdaop", "title": "LambdaOp\u00b6", "text": "<p>Just like with NumpyOps, TensorOps have a <code>LambdaOp</code> too. The TensorOp version differs in that it should return a tf.Tensor or torch.Tensor rather than an np.ndarray.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-tensorops", "title": "Customizing TensorOps\u00b6", "text": "<p>We can create a custom <code>TensorOp</code> using TensorFlow or Pytorch library calls according to our requirements. Below, we showcase a custom <code>TensorOp</code> which combines the batch dimension and patch dimension from the output of the above <code>Pipeline</code> to make it compatible to the <code>Network</code>. <code>TensorOps</code> also have a .build() method which will be invoked before the Network runs so that you can make the op compatible with multiple different backends (though you don't have to do this if you don't care about cross-framework compatibility).</p>"}, {"location": "tutorial/advanced/t03_operator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>Convolutional Variational AutoEncoder</li> <li>Semantic Segmentation</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html", "title": "Advanced Tutorial 4: Trace", "text": "<p>Let's create a function to generate a pipeline, model and network to be used for the tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    \n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=batch_size,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)          pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=batch_size,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.util import to_number\nfrom fastestimator.trace import Trace\nfrom sklearn.metrics import fbeta_score\nimport numpy as np\n\nclass FBetaScore(Trace):\n    def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):\n        super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        self.beta = beta\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_batch_end(self, data):\n        y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\n        y_pred = np.argmax(y_pred, axis=-1)\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n        \n    def on_epoch_end(self, data):\n        score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")\n        data.write_with_log(self.outputs[0], score)\n</pre> from fastestimator.util import to_number from fastestimator.trace import Trace from sklearn.metrics import fbeta_score import numpy as np  class FBetaScore(Trace):     def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):         super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)         self.true_key = true_key         self.pred_key = pred_key         self.beta = beta         self.y_true = []         self.y_pred = []              def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []              def on_batch_end(self, data):         y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])         y_pred = np.argmax(y_pred, axis=-1)         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())              def on_epoch_end(self, data):         score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")         data.write_with_log(self.outputs[0], score) <p>Now let's calculate the f2-score using our custom <code>Trace</code>. f2-score gives more importance to recall.</p> In\u00a0[3]: Copied! <pre>pipeline, model, network = get_pipeline_model_network()\n\ntraces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)\n\nestimator.fit()\n</pre> pipeline, model, network = get_pipeline_model_network()  traces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)  estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.3083596; \nFastEstimator-Train: step: 1000; ce: 0.16284753; steps/sec: 656.26; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.55 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.035797507; f2_score: 0.9885909522565743; \nFastEstimator-Train: step: 2000; ce: 0.020546585; steps/sec: 615.78; \nFastEstimator-Train: step: 3000; ce: 0.0059753414; steps/sec: 713.25; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.69 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03689827; f2_score: 0.9877924021686296; \nFastEstimator-Train: step: 4000; ce: 0.02098944; steps/sec: 680.01; \nFastEstimator-Train: step: 5000; ce: 0.22268356; steps/sec: 741.56; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 2.65 sec; \nFastEstimator-Eval: step: 5625; epoch: 3; ce: 0.032033153; f2_score: 0.9901934586365465; \nFastEstimator-Train: step: 6000; ce: 0.0055854702; steps/sec: 677.84; \nFastEstimator-Train: step: 7000; ce: 0.0013257915; steps/sec: 679.31; \nFastEstimator-Train: step: 7500; epoch: 4; epoch_time: 2.8 sec; \nFastEstimator-Eval: step: 7500; epoch: 4; ce: 0.029642625; f2_score: 0.9913968204671144; \nFastEstimator-Finish: step: 7500; total_time: 17.99 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Let's see an example where we utilize the outputs of the <code>Precision</code> and <code>Recall</code> <code>Traces</code> to generate f1-score:</p> In\u00a0[4]: Copied! <pre>from fastestimator.trace.metric import Precision, Recall\n\nclass CustomF1Score(Trace):\n    def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):\n        super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)\n        self.precision_key = precision_key\n        self.recall_key = recall_key\n        \n    def on_epoch_end(self, data):\n        precision = data[self.precision_key]\n        recall = data[self.recall_key]\n        score = 2*(precision*recall)/(precision+recall)\n        data.write_with_log(self.outputs[0], score)\n        \n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = [\n    Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),\n    Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),\n    CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\")\n]\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000)\n</pre> from fastestimator.trace.metric import Precision, Recall  class CustomF1Score(Trace):     def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):         super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)         self.precision_key = precision_key         self.recall_key = recall_key              def on_epoch_end(self, data):         precision = data[self.precision_key]         recall = data[self.recall_key]         score = 2*(precision*recall)/(precision+recall)         data.write_with_log(self.outputs[0], score)           pipeline, model, network = get_pipeline_model_network()  traces = [     Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),     Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),     CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\") ] estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000) In\u00a0[5]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.305337; \nFastEstimator-Train: step: 1000; ce: 0.024452677; steps/sec: 734.32; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.76 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.0569705; \nprecision:\n[0.97585513,0.98211091,0.9752381 ,0.98080614,0.99562363,0.96210526,\n 1.        ,0.98137803,1.        ,0.97504798];\nrecall:\n[0.99589322,1.        ,0.99224806,0.99223301,0.98484848,0.9827957 ,\n 0.95850622,0.98137803,0.95503212,0.97692308];\nf1_score:\n[0.98577236,0.99097473,0.98366955,0.98648649,0.99020675,0.97234043,\n 0.97881356,0.98137803,0.9769989 ,0.97598463];\nFastEstimator-Train: step: 2000; ce: 0.0021102745; steps/sec: 674.01; \nFastEstimator-Train: step: 3000; ce: 0.0089770565; steps/sec: 688.42; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.8 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.034781747; \nprecision:\n[0.98780488,0.99097473,0.98843931,0.98841699,0.99349241,0.98908297,\n 0.99375   ,0.9905303 ,0.97468354,0.98449612];\nrecall:\n[0.99794661,1.        ,0.99418605,0.99417476,0.99134199,0.97419355,\n 0.98962656,0.97392924,0.98929336,0.97692308];\nf1_score:\n[0.99284985,0.99546691,0.99130435,0.99128751,0.99241603,0.9815818 ,\n 0.99168399,0.98215962,0.98193411,0.98069498];\nFastEstimator-Finish: step: 3750; total_time: 8.76 sec; LeNet_lr: 0.001; \n</pre> <p><code>Note:</code> precision, recall, and f1-score are displayed for each class</p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class MonitorPred(Trace):\n    def __init__(self, true_key, pred_key, mode=\"train\"):\n        super().__init__(inputs=(true_key, pred_key), mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        \n    def on_batch_end(self, data):\n        print(\"Global Step Index: \", self.system.global_step)\n        print(\"Batch Index: \", self.system.batch_idx)\n        print(\"Epoch: \", self.system.epoch_idx)\n        print(\"Batch data has following keys: \", list(data.keys()))\n        print(\"Batch true labels: \", data[self.true_key])\n        print(\"Batch predictictions: \", data[self.pred_key])\n\npipeline, model, network = get_pipeline_model_network(batch_size=4)\n\ntraces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, max_train_steps_per_epoch=2, log_steps=None)\n</pre> class MonitorPred(Trace):     def __init__(self, true_key, pred_key, mode=\"train\"):         super().__init__(inputs=(true_key, pred_key), mode=mode)         self.true_key = true_key         self.pred_key = pred_key              def on_batch_end(self, data):         print(\"Global Step Index: \", self.system.global_step)         print(\"Batch Index: \", self.system.batch_idx)         print(\"Epoch: \", self.system.epoch_idx)         print(\"Batch data has following keys: \", list(data.keys()))         print(\"Batch true labels: \", data[self.true_key])         print(\"Batch predictictions: \", data[self.pred_key])  pipeline, model, network = get_pipeline_model_network(batch_size=4)  traces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, max_train_steps_per_epoch=2, log_steps=None) In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nGlobal Step Index:  1\nBatch Index:  1\nEpoch:  1\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [1 5 8 5]\nBatch predictictions:  [[0.09878654 0.11280762 0.10882236 0.0953772  0.09711165 0.09277759\n  0.09783419 0.09401798 0.10111833 0.10134653]\n [0.10425894 0.11605782 0.11004242 0.09267453 0.08793817 0.09537386\n  0.10757758 0.08135056 0.09903805 0.10568804]\n [0.1016297  0.11371672 0.10940187 0.09458858 0.09116017 0.09185343\n  0.10174091 0.08704273 0.10234813 0.10651773]\n [0.10281158 0.10875763 0.10668261 0.08935054 0.09368025 0.10163527\n  0.10554942 0.08158974 0.09799404 0.11194893]]\nGlobal Step Index:  2\nBatch Index:  2\nEpoch:  1\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [9 7 0 9]\nBatch predictictions:  [[0.10153595 0.11117928 0.10700106 0.09030598 0.09056976 0.10074646\n  0.10491277 0.08370153 0.10058438 0.10946291]\n [0.09943405 0.11675353 0.10615741 0.09357058 0.09498165 0.09680846\n  0.09997059 0.08461777 0.09770196 0.11000396]\n [0.10712261 0.11406822 0.10380837 0.09336544 0.08995877 0.09921383\n  0.10175668 0.08751085 0.09903854 0.10415668]\n [0.10325367 0.10959569 0.10525871 0.08968467 0.09167413 0.10499243\n  0.10512233 0.08271552 0.09867672 0.10902614]]\nGlobal Step Index:  3\nBatch Index:  1\nEpoch:  2\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [4 9 5 0]\nBatch predictictions:  [[0.10507825 0.10794099 0.10248892 0.08767187 0.08906174 0.10877317\n  0.10675651 0.08316758 0.09733932 0.11172164]\n [0.10452065 0.10935836 0.10143676 0.08643056 0.08772491 0.11231022\n  0.10028692 0.08151487 0.09872114 0.11769552]\n [0.10281294 0.11222194 0.1011567  0.08917599 0.093499   0.10987655\n  0.10295148 0.08328241 0.09753096 0.10749206]\n [0.11502377 0.10897078 0.10094845 0.08484171 0.08951931 0.10733136\n  0.09949591 0.08294778 0.09814924 0.11277179]]\nGlobal Step Index:  4\nBatch Index:  2\nEpoch:  2\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [2 9 5 9]\nBatch predictictions:  [[0.10447924 0.11029453 0.09903328 0.08642756 0.09253392 0.11049397\n  0.10054693 0.08330047 0.09570859 0.11718156]\n [0.10390399 0.11127824 0.10138535 0.08615676 0.09266223 0.11076459\n  0.10240171 0.08131735 0.09794777 0.11218196]\n [0.10628477 0.10850214 0.09937814 0.08383881 0.0902461  0.11622549\n  0.103737   0.07806063 0.09677587 0.11695106]\n [0.10669366 0.10886899 0.09865166 0.08427355 0.0894412  0.117375\n  0.10394516 0.07848874 0.09449891 0.11776313]]\n</pre> <p>As you can see, we can visualize information like the global step, batch number, epoch, keys in the data dictionary, true labels, and predictions at batch level using our <code>Trace</code>.</p> <p></p>"}, {"location": "tutorial/advanced/t04_trace.html#advanced-tutorial-4-trace", "title": "Advanced Tutorial 4: Trace\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing Traces<ul> <li>Example</li> </ul> </li> <li>More About Traces<ul> <li>Inputs, Outputs, and Mode</li> <li>Data</li> <li>System</li> </ul> </li> <li>Trace Communication</li> <li>Other Trace Usages<ul> <li>Debugging/Monitoring</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html#customizing-traces", "title": "Customizing Traces\u00b6", "text": "<p>In Beginner Tutorial 7, we talked about the basic concept and structure of <code>Traces</code> and used a few <code>Traces</code> provided by FastEstimator. We can also customize a Trace to suit our needs. Let's look at an example of a custom trace implementation:</p>"}, {"location": "tutorial/advanced/t04_trace.html#example", "title": "Example\u00b6", "text": "<p>We can utilize traces to calculate any custom metric needed for monitoring or controlling training. Below, we implement a trace for calculating the F-beta score of our model.</p>"}, {"location": "tutorial/advanced/t04_trace.html#more-about-traces", "title": "More About Traces\u00b6", "text": "<p>As we have now seen a custom Trace implementaion, let's delve deeper into the structure of <code>Traces</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#inputs-outputs-and-mode", "title": "Inputs, Outputs, and Mode\u00b6", "text": "<p>These Trace arguments are similar to the Operator. To recap, the keys from the data dictionary which are required by the Trace can be specified using the <code>inputs</code> argument. The <code>outputs</code> argument is used to specify the keys which the Trace wants to write into the system buffer. Unlike with Ops, the Trace <code>inputs</code> and <code>outputs</code> are essentially on an honor system. FastEstimator will not check whether a Trace is really only reading values listed in its <code>inputs</code> and writing values listed in its <code>outputs</code>. If you are developing a new <code>Trace</code> and want your code to work well with the features provided by FastEstimator, it is important to use these fields correctly. The <code>mode</code> argument is used to specify the mode(s) for trace execution as with <code>Ops</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#data", "title": "Data\u00b6", "text": "<p>Through its data argument, Trace has access to the current data dictionary. You can use any keys which the Trace declared as its <code>inputs</code> to access information from the data dictionary. You can write the outputs into the <code>Data</code> dictionary with or without logging using the <code>write_with_log</code> and <code>write_without_log</code> methods respectively.</p>"}, {"location": "tutorial/advanced/t04_trace.html#system", "title": "System\u00b6", "text": "<p>Traces have access to the current <code>System</code> instance which has information about the <code>Network</code> and training process. The information contained in <code>System</code> is listed below:</p> <ul> <li>global_step</li> <li>num_devices</li> <li>log_steps</li> <li>total_epochs</li> <li>epoch_idx</li> <li>batch_idx</li> <li>stop_training</li> <li>network</li> <li>max_train_steps_per_epoch</li> <li>max_eval_steps_per_epoch</li> <li>summary</li> <li>experiment_time</li> </ul> <p>We will showcase <code>System</code> usage in the other trace usages section of this tutorial.</p>"}, {"location": "tutorial/advanced/t04_trace.html#trace-communication", "title": "Trace Communication\u00b6", "text": "<p>We can have multiple traces in a network where the output of one trace is utilized as an input for another, as depicted below:</p>"}, {"location": "tutorial/advanced/t04_trace.html#other-trace-usages", "title": "Other Trace Usages\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#debuggingmonitoring", "title": "Debugging/Monitoring\u00b6", "text": "<p>Lets implement a custom trace to monitor a model's predictions. Using this, any discrepancy from the expected behavior can be checked and the relevant corrections can be made:</p>"}, {"location": "tutorial/advanced/t04_trace.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html", "title": "Advanced Tutorial 5: Scheduler", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.schedule import EpochScheduler\nbatch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64})\n</pre> from fastestimator.schedule import EpochScheduler batch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64}) In\u00a0[2]: Copied! <pre>for epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 16\nAt epoch 2, batch size is 32\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 64\n</pre> In\u00a0[3]: Copied! <pre>from fastestimator.schedule import RepeatScheduler\nbatch_size = RepeatScheduler(repeat_list=[32, 64])\n\nfor epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> from fastestimator.schedule import RepeatScheduler batch_size = RepeatScheduler(repeat_list=[32, 64])  for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 32\nAt epoch 2, batch size is 64\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 32\n</pre> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.schedule import EpochScheduler\n\ntrain_data1, eval_data = mnist.load_data()\ntrain_data2, _ = mnist.load_data()\ntrain_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2})\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.schedule import EpochScheduler  train_data1, eval_data = mnist.load_data() train_data2, _ = mnist.load_data() train_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2}) In\u00a0[5]: Copied! <pre>batch_size = RepeatScheduler(repeat_list=[32,64])\n</pre> batch_size = RepeatScheduler(repeat_list=[32,64]) In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.numpyop.multivariate import Rotate\nimport fastestimator as fe\n\nrotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})\n\npipeline = fe.Pipeline(train_data=train_data, \n                       eval_data=eval_data,\n                       batch_size=batch_size, \n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.numpyop.multivariate import Rotate import fastestimator as fe  rotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})  pipeline = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                        batch_size=batch_size,                         ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\n\nmodel_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")\n\nmodel_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"), \n             3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}\n\nupdate_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}\n\nnetwork = fe.Network(ops=[EpochScheduler(model_map),\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n                          EpochScheduler(update_map)])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop.loss import CrossEntropy  model_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")  model_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"),               3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}  update_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}  network = fe.Network(ops=[EpochScheduler(model_map),                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),                           EpochScheduler(update_map)]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.io import ModelSaver\nimport tempfile\n\nsave_folder = tempfile.mkdtemp()\n\n#Disable model saving by setting None on 3rd epoch:\nmodelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})\n\nmodelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})\n\ntraces=[modelsaver1, modelsaver2]\n</pre> from fastestimator.trace.io import ModelSaver import tempfile  save_folder = tempfile.mkdtemp()  #Disable model saving by setting None on 3rd epoch: modelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})  modelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})  traces=[modelsaver1, modelsaver2] In\u00a0[10]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300)\nestimator.fit()\n</pre> estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300) estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 300; \nFastEstimator-Train: step: 1; ce: 0.016785031; \nFastEstimator-Train: step: 300; ce: 0.16430134; steps/sec: 697.26; \nFastEstimator-Train: step: 600; ce: 0.023913195; steps/sec: 728.45; \nFastEstimator-Train: step: 900; ce: 0.042380013; steps/sec: 732.24; \nFastEstimator-Train: step: 1200; ce: 0.0014684915; steps/sec: 723.88; \nFastEstimator-Train: step: 1500; ce: 0.020901386; steps/sec: 728.1; \nFastEstimator-Train: step: 1800; ce: 0.0114256; steps/sec: 724.26; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.66 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.054206606; \nFastEstimator-Train: step: 2100; ce: 0.023387551; steps/sec: 546.57; \nFastEstimator-Train: step: 2400; ce: 0.0030879583; steps/sec: 627.71; \nFastEstimator-Train: step: 2700; ce: 0.10354612; steps/sec: 631.19; \nFastEstimator-ModelSaver: Saved model to /tmp/tmph72ava81/m1_epoch_2.h5\nFastEstimator-Train: step: 2813; epoch: 2; epoch_time: 1.58 sec; \nFastEstimator-Eval: step: 2813; epoch: 2; ce: 0.040080495; \nFastEstimator-Train: step: 3000; ce: 0.0011174735; steps/sec: 627.96; \nFastEstimator-Train: step: 3300; ce: 0.019162945; steps/sec: 792.26; \nFastEstimator-Train: step: 3600; ce: 0.21189407; steps/sec: 796.04; \nFastEstimator-Train: step: 3900; ce: 0.0007937134; steps/sec: 811.5; \nFastEstimator-Train: step: 4200; ce: 0.002208311; steps/sec: 818.86; \nFastEstimator-Train: step: 4500; ce: 0.005765636; steps/sec: 815.32; \nFastEstimator-ModelSaver: Saved model to /tmp/tmph72ava81/m2_epoch_3.h5\nFastEstimator-Train: step: 4688; epoch: 3; epoch_time: 2.4 sec; \nFastEstimator-Eval: step: 4688; epoch: 3; ce: 0.033545353; \nFastEstimator-Finish: step: 4688; total_time: 10.79 sec; m2_lr: 0.001; m1_lr: 0.01; \n</pre>"}, {"location": "tutorial/advanced/t05_scheduler.html#advanced-tutorial-5-scheduler", "title": "Advanced Tutorial 5: Scheduler\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Scheduler<ul> <li>Concept</li> <li>EpochScheduler</li> <li>RepeatScheduler</li> </ul> </li> <li>Things You Can Schedule<ul> <li>Datasets</li> <li>Batch Size</li> <li>NumpyOps</li> <li>Optimizers</li> <li>TensorOps</li> <li>Traces</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#scheduler", "title": "Scheduler\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#concept", "title": "Concept\u00b6", "text": "<p>Deep learning training is getting more complicated every year. One major aspect of this complexity is time-dependent training. For example:</p> <ul> <li>Using different datasets for different training epochs.</li> <li>Applying different preprocessing for different epochs.</li> <li>Training different networks on different epochs.</li> <li>...</li> </ul> <p>The list goes on and on. In order to provide an easy way for users to accomplish time-dependent training, we provide the <code>Scheduler</code> class which can help you schedule any part of the training.</p> <p>Please note that the basic time unit that <code>Scheduler</code> can handle is <code>epochs</code>. If users want arbitrary scheduling cycles, the simplest way is to customize the length of one epoch in <code>Estimator</code> using max_train_steps_per_epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#epochscheduler", "title": "EpochScheduler\u00b6", "text": "<p>The most straightforward way to schedule things is through an epoch-value mapping. For example, If users want to schedule the batch size in the following way:</p> <ul> <li>epoch 1 - batchsize 16</li> <li>epoch 2 - batchsize 32</li> <li>epoch 3 - batchsize 32</li> <li>epoch 4 - batchsize 64</li> <li>epoch 5 - batchsize 64</li> </ul> <p>You can do the following:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#repeatscheduler", "title": "RepeatScheduler\u00b6", "text": "<p>If your schedule follows a repeating pattern, then you don't want to specify that for all epochs. <code>RepeatScheduler</code> is here to help you. Let's say we want the batch size on odd epochs to be 32, and on even epochs to be 64:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#things-you-can-schedule", "title": "Things You Can Schedule:\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#datasets", "title": "Datasets\u00b6", "text": "<p>Scheduling training or evaluation datasets is very common in deep learning. For example, in curriculum learning people will train on an easy dataset first and then gradually move on to harder datasets. For illustration purposes, let's use two different instances of the same MNIST dataset:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#batch-size", "title": "Batch Size\u00b6", "text": "<p>We can also schedule the batch size on different epochs, which may help resolve GPU resource constraints.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#numpyops", "title": "NumpyOps\u00b6", "text": "<p>Preprocessing operators can also be scheduled. For illustration purpose, we will apply a <code>Rotation</code> for the first two epochs and then not apply it for the third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#optimizers", "title": "Optimizers\u00b6", "text": "<p>For fast convergence, some people like to use different optimizers at different training phases. In our example, we will use <code>adam</code> for the first epoch and <code>sgd</code> for the second epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#tensorops", "title": "TensorOps\u00b6", "text": "<p>We can schedule <code>TensorOps</code> just like <code>NumpyOps</code>. Let's define another model <code>model_2</code> such that:</p> <ul> <li>epoch 1-2: train <code>model_1</code></li> <li>epoch 3: train <code>model_2</code></li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#traces", "title": "Traces\u00b6", "text": "<p><code>Traces</code> can also be scheduled. For example, we will save <code>model_1</code> at the end of second epoch and save <code>model_3</code> at the end of third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#let-the-training-begin", "title": "Let the training begin\u00b6", "text": "<p>Nothing special in here, create the estimator then start the training:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PGGAN</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html", "title": "Advanced Tutorial 6: Summary", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import TensorBoard\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import TensorBoard  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ] In\u00a0[2]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120)\nest.fit()\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 120; \nFastEstimator-Train: step: 1; ce: 2.2443972; model_lr: 0.0009999998; \nFastEstimator-Train: step: 120; ce: 0.12492387; steps/sec: 669.12; model_lr: 0.000997478; \nFastEstimator-Train: step: 240; ce: 0.048826903; steps/sec: 672.17; model_lr: 0.0009899376; \nFastEstimator-Train: step: 360; ce: 0.23642284; steps/sec: 725.35; model_lr: 0.0009774548; \nFastEstimator-Train: step: 480; ce: 0.09294783; steps/sec: 700.77; model_lr: 0.0009601558; \nFastEstimator-Train: step: 600; ce: 0.028917177; steps/sec: 699.82; model_lr: 0.0009382152; \nFastEstimator-Train: step: 720; ce: 0.20153509; steps/sec: 684.74; model_lr: 0.00091185456; \nFastEstimator-Train: step: 840; ce: 0.17263229; steps/sec: 716.73; model_lr: 0.00088134; \nFastEstimator-Train: step: 960; ce: 0.015276889; steps/sec: 704.36; model_lr: 0.00084697985; \nFastEstimator-Train: step: 1080; ce: 0.2609227; steps/sec: 717.46; model_lr: 0.0008091209; \nFastEstimator-Train: step: 1200; ce: 0.087061; steps/sec: 690.11; model_lr: 0.0007681455; \nFastEstimator-Train: step: 1320; ce: 0.0999055; steps/sec: 725.65; model_lr: 0.0007244674; \nFastEstimator-Train: step: 1440; ce: 0.0026154623; steps/sec: 682.23; model_lr: 0.00067852775; \nFastEstimator-Train: step: 1560; ce: 0.0033505233; steps/sec: 725.46; model_lr: 0.0006307903; \nFastEstimator-Train: step: 1680; ce: 0.03747531; steps/sec: 692.43; model_lr: 0.00058173726; \nFastEstimator-Train: step: 1800; ce: 0.01739407; steps/sec: 721.32; model_lr: 0.0005318639; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.21 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.038839005; accuracy: 0.988; \nFastEstimator-Finish: step: 1875; total_time: 4.5 sec; model_lr: 0.0005005; \n</pre> In\u00a0[3]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500)\nsummary = est.fit(\"experiment1\")\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500) summary = est.fit(\"experiment1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 500; \nFastEstimator-Train: step: 1; ce: 0.10593383; model_lr: 0.0009999998; \nFastEstimator-Train: step: 500; ce: 0.0012721822; steps/sec: 695.15; model_lr: 0.00095681596; \nFastEstimator-Train: step: 1000; ce: 0.005035706; steps/sec: 708.72; model_lr: 0.00083473074; \nFastEstimator-Train: step: 1500; ce: 0.0077150357; steps/sec: 711.83; model_lr: 0.000654854; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.73 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.037608337; accuracy: 0.9868; \nFastEstimator-Finish: step: 1875; total_time: 3.96 sec; model_lr: 0.0005005; \n</pre> <p>Lets take a look at what sort of information is contained within our <code>Summary</code> object:</p> In\u00a0[4]: Copied! <pre>summary.name\n</pre> summary.name Out[4]: <pre>'experiment1'</pre> In\u00a0[5]: Copied! <pre>summary.history\n</pre> summary.history Out[5]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'num_device': {0: array(1)},\n                          'logging_interval': {0: array(500)},\n                          'ce': {1: array(0.10593383, dtype=float32),\n                           500: array(0.00127218, dtype=float32),\n                           1000: array(0.00503571, dtype=float32),\n                           1500: array(0.00771504, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095682, dtype=float32),\n                           1000: array(0.00083473, dtype=float32),\n                           1500: array(0.00065485, dtype=float32),\n                           1875: array(0.0005005, dtype=float32)},\n                          'steps/sec': {500: array(695.15),\n                           1000: array(708.72),\n                           1500: array(711.83)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('2.73 sec', dtype='&lt;U8')},\n                          'total_time': {1875: array('3.96 sec', dtype='&lt;U8')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'ce': {1875: array(0.03760834, dtype=float32)},\n                          'accuracy': {1875: array(0.9868)}})})</pre> <p>The history field can appear a little daunting, but it is simply a dictionary laid out as follows: {mode: {key: {step: value}}}. Once you have invoked the .fit() method with an experiment name, subsequent calls to .test() will add their results into the same summary dictionary:</p> In\u00a0[6]: Copied! <pre>summary = est.test()\n</pre> summary = est.test() <pre>FastEstimator-Test: step: 1875; epoch: 1; accuracy: 0.99; \n</pre> In\u00a0[7]: Copied! <pre>summary.history\n</pre> summary.history Out[7]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'num_device': {0: array(1)},\n                          'logging_interval': {0: array(500)},\n                          'ce': {1: array(0.10593383, dtype=float32),\n                           500: array(0.00127218, dtype=float32),\n                           1000: array(0.00503571, dtype=float32),\n                           1500: array(0.00771504, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095682, dtype=float32),\n                           1000: array(0.00083473, dtype=float32),\n                           1500: array(0.00065485, dtype=float32),\n                           1875: array(0.0005005, dtype=float32)},\n                          'steps/sec': {500: array(695.15),\n                           1000: array(708.72),\n                           1500: array(711.83)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('2.73 sec', dtype='&lt;U8')},\n                          'total_time': {1875: array('3.96 sec', dtype='&lt;U8')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'ce': {1875: array(0.03760834, dtype=float32)},\n                          'accuracy': {1875: array(0.9868)}}),\n             'test': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'accuracy': {1875: array(0.99)}})})</pre> <p>Even if an experiment name was not provided during the .fit() call, it may be provided during the .test() call. The resulting summary object will, however, only contain information from the Test mode.</p> <p></p> In\u00a0[8]: Copied! <pre>summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\")\n</pre> summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\") In\u00a0[9]: Copied! <pre>summary.name\n</pre> summary.name Out[9]: <pre>'t06a_exp1'</pre> In\u00a0[10]: Copied! <pre>summary.history['eval']\n</pre> summary.history['eval'] Out[10]: <pre>defaultdict(dict,\n            {'epoch': {1875: 1.0, 3750: 2.0, 5625: 3.0},\n             'ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02382297},\n             'min_ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02343675},\n             'since_best': {1875: 0.0, 3750: 0.0, 5625: 1.0},\n             'accuracy': {1875: 0.9882, 3750: 0.992, 5625: 0.9922}})</pre> <p></p> In\u00a0[11]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary])\n</pre> fe.summary.logs.visualize_logs(experiments=[summary]) <p>If you are only interested in visualizing a subset of these log values, it is also possible to whitelist or blacklist values via the 'include_metrics' and 'ignore_metrics' arguments respectively:</p> In\u00a0[12]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"})\n</pre> fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"}) <p>It is also possible to compare logs from different experiments, which can be especially useful when fiddling with hyper-parameter values to determine their effects on training:</p> In\u00a0[13]: Copied! <pre>fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\")\n</pre> fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\") <p>All of the log files within a given directory can also be compared at the same time, either by using the parse_log_dir() method or via the command line as follows: fastestimator logs --extension .txt --smooth 0 ../resources</p> <p></p> In\u00a0[14]: Copied! <pre>import tempfile\nlog_dir = tempfile.mkdtemp()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),\n    TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\")\n]\nest = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000)\nest.fit()\n</pre> import tempfile log_dir = tempfile.mkdtemp()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),     TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\") ] est = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Tensorboard: writing logs to /tmp/tmppy1edyce/20200929-185115\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.3263385; model2_lr: 0.0009999998; \nFastEstimator-Train: step: 1000; ce: 0.02360101; steps/sec: 450.57; model2_lr: 0.00083473074; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 4.19 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.062874146; accuracy: 0.9802; \nFastEstimator-Train: step: 2000; ce: 0.003110849; steps/sec: 466.04; model2_lr: 0.00044828805; \nFastEstimator-Train: step: 3000; ce: 0.008105779; steps/sec: 466.15; model2_lr: 9.639601e-05; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 4.07 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.027781308; accuracy: 0.9904; \nFastEstimator-Train: step: 4000; ce: 0.01386946; steps/sec: 458.4; model2_lr: 0.0009890847; \nFastEstimator-Train: step: 5000; ce: 0.020999195; steps/sec: 470.05; model2_lr: 0.00075025; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 4.06 sec; \nFastEstimator-Eval: step: 5625; epoch: 3; ce: 0.028465897; accuracy: 0.9902; \nFastEstimator-Finish: step: 5625; total_time: 13.63 sec; model2_lr: 0.0005005; \n</pre> <p>Now let's launch TensorBoard to visualize our logs. Note that this call will prevent any subsequent Jupyter Notebook cells from running until you manually terminate it.</p> In\u00a0[15]: Copied! <pre>#!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe\n</pre> #!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe <p>The TensorBoard display should look something like this:</p> <p></p> <p></p>"}, {"location": "tutorial/advanced/t06_summary.html#advanced-tutorial-6-summary", "title": "Advanced Tutorial 6: Summary\u00b6", "text": ""}, {"location": "tutorial/advanced/t06_summary.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Experiment Logging</li> <li>Experiment Summaries</li> <li>Log Parsing</li> <li>Summary Visualization</li> <li>TensorBoard Visualization</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>We will first set up a basic MNIST example for the rest of the demonstrations:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-logging", "title": "Experiment Logging\u00b6", "text": "<p>As you may have noticed if you have used FastEstimator, log messages are printed to the screen during training. If you want to persist these log messages for later records, you can simply pipe them into a file when launching training from the command line, or else just copy and paste the messages from the console into a persistent file on the disk. FastEstimator allows logging to be controlled via arguments passed to the <code>Estimator</code> class, as described in the Beginner Tutorial 7. Let's see an example logging every 120 steps:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-summaries", "title": "Experiment Summaries\u00b6", "text": "<p>Having log messages on the screen can be handy, but what if you want to access these messages within python? Enter the <code>Summary</code> class. <code>Summary</code> objects contain information about the training over time, and will be automatically generated when the <code>Estimator</code> fit() method is invoked with an experiment name:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-parsing", "title": "Log Parsing\u00b6", "text": "<p>Suppose that you have a log file saved to disk, and you want to create an in-memory <code>Summary</code> representation of it. This can be done through FastEstimator logging utilities:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-visualization", "title": "Log Visualization\u00b6", "text": "<p>While seeing log data as numbers can be informative, visualizations of data are often more useful. FastEstimator provides several ways to visualize log data: from python using <code>Summary</code> objects or log files, as well as through the command line.</p>"}, {"location": "tutorial/advanced/t06_summary.html#tensorboard", "title": "TensorBoard\u00b6", "text": "<p>Of course, no modern AI framework would be complete without TensorBoard integration. In FastEstimator, all that is required to achieve TensorBoard integration is to add the TensorBoard <code>Trace</code> to the list of traces passed to the <code>Estimator</code>:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html", "title": "Advanced Tutorial 7: Learning Rate Scheduling", "text": "<p>Learning rate schedules can be implemented using the <code>LRScheduler</code> <code>Trace</code>. <code>LRScheduler</code> takes the model and learning schedule through the lr_fn parameter. lr_fn should be a function/lambda function with 'step' or 'epoch' as its input parameter. This determines whether the learning schedule will be applied at a step or epoch level.</p> <p>For more details on traces, you can visit Beginner Tutorial 7 and Advanced Tutorial 4.</p> <p>Let's create a function to generate the pipeline, model, and network to be used for this tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\"):\n    train_data, _ = mnist.load_data()\n\n    pipeline = fe.Pipeline(train_data=train_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\"):     train_data, _ = mnist.load_data()      pipeline = fe.Pipeline(train_data=train_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import LRScheduler\n\ndef lr_schedule(epoch):\n    lr = 0.001*(20-epoch+1)/20\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)\n\nhistory = estimator.fit(summary=\"Experiment_1\")\n</pre> from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import LRScheduler  def lr_schedule(epoch):     lr = 0.001*(20-epoch+1)/20     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)  history = estimator.fit(summary=\"Experiment_1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3089; LeNet_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.58078986; steps/sec: 694.76; LeNet_lr: 0.001; \nFastEstimator-Train: step: 200; ce: 0.13996598; steps/sec: 767.68; LeNet_lr: 0.001; \nFastEstimator-Train: step: 300; ce: 0.047897074; steps/sec: 784.24; LeNet_lr: 0.001; \nFastEstimator-Train: step: 400; ce: 0.046643212; steps/sec: 776.27; LeNet_lr: 0.001; \nFastEstimator-Train: step: 500; ce: 0.022375159; steps/sec: 815.21; LeNet_lr: 0.001; \nFastEstimator-Train: step: 600; ce: 0.07842708; steps/sec: 778.69; LeNet_lr: 0.001; \nFastEstimator-Train: step: 700; ce: 0.20251414; steps/sec: 802.99; LeNet_lr: 0.001; \nFastEstimator-Train: step: 800; ce: 0.035366945; steps/sec: 769.72; LeNet_lr: 0.001; \nFastEstimator-Train: step: 900; ce: 0.03398672; steps/sec: 810.71; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1000; ce: 0.112584725; steps/sec: 783.55; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1100; ce: 0.05205777; steps/sec: 689.27; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1200; ce: 0.0033754208; steps/sec: 743.87; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1300; ce: 0.0054937536; steps/sec: 803.25; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1400; ce: 0.0065217884; steps/sec: 783.11; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1500; ce: 0.011019227; steps/sec: 819.06; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1600; ce: 0.05610779; steps/sec: 783.92; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1700; ce: 0.10374484; steps/sec: 812.64; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1800; ce: 0.16797249; steps/sec: 777.94; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.96 sec; \nFastEstimator-Train: step: 1900; ce: 0.002968135; steps/sec: 456.57; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2000; ce: 0.004666821; steps/sec: 661.41; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2100; ce: 0.0124099245; steps/sec: 707.8; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2200; ce: 0.08333805; steps/sec: 765.97; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2300; ce: 0.04198639; steps/sec: 770.37; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2400; ce: 0.072333984; steps/sec: 788.7; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2500; ce: 0.0021644386; steps/sec: 783.02; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2600; ce: 0.117298014; steps/sec: 805.19; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2700; ce: 0.029399084; steps/sec: 787.32; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2800; ce: 0.025874225; steps/sec: 810.38; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2900; ce: 0.0076365666; steps/sec: 788.14; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3000; ce: 0.018179502; steps/sec: 793.57; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3100; ce: 0.002729386; steps/sec: 780.15; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3200; ce: 0.005655894; steps/sec: 785.53; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3300; ce: 0.0051174066; steps/sec: 772.76; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3400; ce: 0.03424426; steps/sec: 714.72; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3500; ce: 0.061904356; steps/sec: 692.78; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3600; ce: 0.01764475; steps/sec: 815.43; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3700; ce: 0.004598704; steps/sec: 796.09; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.54 sec; \nFastEstimator-Train: step: 3800; ce: 0.007359849; steps/sec: 466.42; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 3900; ce: 0.03665335; steps/sec: 798.22; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4000; ce: 0.010769706; steps/sec: 775.86; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4100; ce: 0.0013347296; steps/sec: 794.47; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4200; ce: 0.00937571; steps/sec: 776.57; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4300; ce: 0.0073838052; steps/sec: 798.44; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4400; ce: 0.0016001706; steps/sec: 755.96; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4500; ce: 0.0027758705; steps/sec: 789.77; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4600; ce: 0.14900081; steps/sec: 771.62; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4700; ce: 0.00067295914; steps/sec: 794.2; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4800; ce: 0.035189193; steps/sec: 687.94; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4900; ce: 0.013106734; steps/sec: 725.63; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5000; ce: 0.0010486699; steps/sec: 790.38; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5100; ce: 0.0015635535; steps/sec: 801.9; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5200; ce: 0.22061187; steps/sec: 772.13; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5300; ce: 0.0050542983; steps/sec: 806.05; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5400; ce: 0.0024875803; steps/sec: 764.54; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5500; ce: 0.0076733916; steps/sec: 832.12; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5600; ce: 0.005431102; steps/sec: 804.78; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 2.54 sec; \nFastEstimator-Finish: step: 5625; total_time: 9.77 sec; LeNet_lr: 0.0009; \n</pre> <p>The learning rate is available in the training log at steps specified using the log_steps parameter in the <code>Estimator</code>. By default, training is logged every 100 steps.</p> In\u00a0[3]: Copied! <pre>visualize_logs(history, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history, include_metrics=\"LeNet_lr\") <p>As you can see, the learning rate changes only after every epoch.</p> <p></p> In\u00a0[4]: Copied! <pre>def lr_schedule(step):\n    lr = 0.001*(7500-step+1)/7500\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory2 = estimator.fit(summary=\"Experiment_2\")\n</pre> def lr_schedule(step):     lr = 0.001*(7500-step+1)/7500     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history2 = estimator.fit(summary=\"Experiment_2\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3065164; LeNet_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.23714758; steps/sec: 655.65; LeNet_lr: 0.0009868; \nFastEstimator-Train: step: 200; ce: 0.6577442; steps/sec: 678.86; LeNet_lr: 0.00097346667; \nFastEstimator-Train: step: 300; ce: 0.14811869; steps/sec: 677.92; LeNet_lr: 0.00096013333; \nFastEstimator-Train: step: 400; ce: 0.11562818; steps/sec: 668.4; LeNet_lr: 0.0009468; \nFastEstimator-Train: step: 500; ce: 0.027212799; steps/sec: 633.48; LeNet_lr: 0.00093346665; \nFastEstimator-Train: step: 600; ce: 0.17180511; steps/sec: 570.52; LeNet_lr: 0.0009201333; \nFastEstimator-Train: step: 700; ce: 0.060723193; steps/sec: 695.62; LeNet_lr: 0.0009068; \nFastEstimator-Train: step: 800; ce: 0.072167784; steps/sec: 682.35; LeNet_lr: 0.00089346664; \nFastEstimator-Train: step: 900; ce: 0.037193242; steps/sec: 683.86; LeNet_lr: 0.00088013336; \nFastEstimator-Train: step: 1000; ce: 0.09921763; steps/sec: 605.18; LeNet_lr: 0.0008668; \nFastEstimator-Train: step: 1100; ce: 0.050317485; steps/sec: 603.58; LeNet_lr: 0.0008534667; \nFastEstimator-Train: step: 1200; ce: 0.033182904; steps/sec: 682.28; LeNet_lr: 0.00084013335; \nFastEstimator-Train: step: 1300; ce: 0.030531863; steps/sec: 707.21; LeNet_lr: 0.0008268; \nFastEstimator-Train: step: 1400; ce: 0.033350274; steps/sec: 683.06; LeNet_lr: 0.0008134667; \nFastEstimator-Train: step: 1500; ce: 0.20844415; steps/sec: 706.99; LeNet_lr: 0.00080013333; \nFastEstimator-Train: step: 1600; ce: 0.0029021623; steps/sec: 685.66; LeNet_lr: 0.0007868; \nFastEstimator-Train: step: 1700; ce: 0.009277768; steps/sec: 717.94; LeNet_lr: 0.00077346666; \nFastEstimator-Train: step: 1800; ce: 0.0021057532; steps/sec: 688.16; LeNet_lr: 0.0007601333; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.0 sec; \nFastEstimator-Train: step: 1900; ce: 0.03702537; steps/sec: 417.6; LeNet_lr: 0.0007468; \nFastEstimator-Train: step: 2000; ce: 0.053149987; steps/sec: 651.14; LeNet_lr: 0.00073346664; \nFastEstimator-Train: step: 2100; ce: 0.017920867; steps/sec: 670.72; LeNet_lr: 0.0007201333; \nFastEstimator-Train: step: 2200; ce: 0.08334316; steps/sec: 684.75; LeNet_lr: 0.0007068; \nFastEstimator-Train: step: 2300; ce: 0.003692674; steps/sec: 682.05; LeNet_lr: 0.0006934667; \nFastEstimator-Train: step: 2400; ce: 0.003553884; steps/sec: 690.95; LeNet_lr: 0.00068013335; \nFastEstimator-Train: step: 2500; ce: 0.013678698; steps/sec: 661.92; LeNet_lr: 0.0006668; \nFastEstimator-Train: step: 2600; ce: 0.07064867; steps/sec: 716.98; LeNet_lr: 0.0006534667; \nFastEstimator-Train: step: 2700; ce: 0.036846854; steps/sec: 686.8; LeNet_lr: 0.00064013334; \nFastEstimator-Train: step: 2800; ce: 0.004501665; steps/sec: 671.16; LeNet_lr: 0.0006268; \nFastEstimator-Train: step: 2900; ce: 0.2406652; steps/sec: 566.73; LeNet_lr: 0.00061346666; \nFastEstimator-Train: step: 3000; ce: 0.004612835; steps/sec: 703.64; LeNet_lr: 0.0006001333; \nFastEstimator-Train: step: 3100; ce: 0.04271071; steps/sec: 568.53; LeNet_lr: 0.0005868; \nFastEstimator-Train: step: 3200; ce: 0.124661796; steps/sec: 674.27; LeNet_lr: 0.00057346665; \nFastEstimator-Train: step: 3300; ce: 0.012699548; steps/sec: 592.47; LeNet_lr: 0.0005601333; \nFastEstimator-Train: step: 3400; ce: 0.0035635328; steps/sec: 636.74; LeNet_lr: 0.0005468; \nFastEstimator-Train: step: 3500; ce: 0.116995685; steps/sec: 683.88; LeNet_lr: 0.0005334667; \nFastEstimator-Train: step: 3600; ce: 0.007870817; steps/sec: 691.72; LeNet_lr: 0.00052013336; \nFastEstimator-Train: step: 3700; ce: 0.01830994; steps/sec: 676.26; LeNet_lr: 0.0005068; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.96 sec; \nFastEstimator-Finish: step: 3750; total_time: 7.12 sec; LeNet_lr: 0.0005001333; \n</pre> In\u00a0[5]: Copied! <pre>visualize_logs(history2, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history2, include_metrics=\"LeNet_lr\") <p></p> <p></p> In\u00a0[6]: Copied! <pre>from fastestimator.schedule import cosine_decay\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3))\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory3 = estimator.fit(summary=\"Experiment_3\")\n</pre> from fastestimator.schedule import cosine_decay  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3)) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history3 = estimator.fit(summary=\"Experiment_3\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.2930279; LeNet_lr: 0.0009999993; \nFastEstimator-Train: step: 100; ce: 0.26312476; steps/sec: 638.7; LeNet_lr: 0.000993005; \nFastEstimator-Train: step: 200; ce: 0.12205046; steps/sec: 667.97; LeNet_lr: 0.000972216; \nFastEstimator-Train: step: 300; ce: 0.14842911; steps/sec: 696.66; LeNet_lr: 0.0009382152; \nFastEstimator-Train: step: 400; ce: 0.27469447; steps/sec: 674.11; LeNet_lr: 0.00089195487; \nFastEstimator-Train: step: 500; ce: 0.01054606; steps/sec: 705.46; LeNet_lr: 0.00083473074; \nFastEstimator-Train: step: 600; ce: 0.18344438; steps/sec: 686.91; LeNet_lr: 0.0007681455; \nFastEstimator-Train: step: 700; ce: 0.029220346; steps/sec: 682.13; LeNet_lr: 0.000694064; \nFastEstimator-Train: step: 800; ce: 0.13878524; steps/sec: 676.54; LeNet_lr: 0.00061456126; \nFastEstimator-Train: step: 900; ce: 0.028735036; steps/sec: 698.14; LeNet_lr: 0.0005318639; \nFastEstimator-Train: step: 1000; ce: 0.055955518; steps/sec: 667.36; LeNet_lr: 0.00044828805; \nFastEstimator-Train: step: 1100; ce: 0.07805036; steps/sec: 699.72; LeNet_lr: 0.00036617456; \nFastEstimator-Train: step: 1200; ce: 0.0090379715; steps/sec: 683.74; LeNet_lr: 0.00028782323; \nFastEstimator-Train: step: 1300; ce: 0.077315584; steps/sec: 586.27; LeNet_lr: 0.00021542858; \nFastEstimator-Train: step: 1400; ce: 0.05346773; steps/sec: 624.91; LeNet_lr: 0.00015101816; \nFastEstimator-Train: step: 1500; ce: 0.08146605; steps/sec: 683.15; LeNet_lr: 9.639601e-05; \nFastEstimator-Train: step: 1600; ce: 0.0033195266; steps/sec: 587.82; LeNet_lr: 5.3091975e-05; \nFastEstimator-Train: step: 1700; ce: 0.12897912; steps/sec: 668.69; LeNet_lr: 2.231891e-05; \nFastEstimator-Train: step: 1800; ce: 0.004909375; steps/sec: 673.23; LeNet_lr: 4.9387068e-06; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.07 sec; \nFastEstimator-Train: step: 1900; ce: 0.15640017; steps/sec: 340.63; LeNet_lr: 0.0009995619; \nFastEstimator-Train: step: 2000; ce: 0.089771196; steps/sec: 674.14; LeNet_lr: 0.0009890847; \nFastEstimator-Train: step: 2100; ce: 0.013233781; steps/sec: 666.61; LeNet_lr: 0.00096492335; \nFastEstimator-Train: step: 2200; ce: 0.026806809; steps/sec: 679.56; LeNet_lr: 0.00092775445; \nFastEstimator-Train: step: 2300; ce: 0.011386171; steps/sec: 673.87; LeNet_lr: 0.00087861903; \nFastEstimator-Train: step: 2400; ce: 0.018773185; steps/sec: 696.83; LeNet_lr: 0.00081889326; \nFastEstimator-Train: step: 2500; ce: 0.03541358; steps/sec: 671.42; LeNet_lr: 0.00075025; \nFastEstimator-Train: step: 2600; ce: 0.11294758; steps/sec: 714.27; LeNet_lr: 0.0006746117; \nFastEstimator-Train: step: 2700; ce: 0.028423183; steps/sec: 697.1; LeNet_lr: 0.00059409696; \nFastEstimator-Train: step: 2800; ce: 0.16977276; steps/sec: 703.42; LeNet_lr: 0.00051096076; \nFastEstimator-Train: step: 2900; ce: 0.01733088; steps/sec: 669.76; LeNet_lr: 0.00042753152; \nFastEstimator-Train: step: 3000; ce: 0.02646891; steps/sec: 680.62; LeNet_lr: 0.000346146; \nFastEstimator-Train: step: 3100; ce: 0.002477122; steps/sec: 594.12; LeNet_lr: 0.00026908363; \nFastEstimator-Train: step: 3200; ce: 0.013257658; steps/sec: 621.69; LeNet_lr: 0.00019850275; \nFastEstimator-Train: step: 3300; ce: 0.003857479; steps/sec: 684.61; LeNet_lr: 0.00013638017; \nFastEstimator-Train: step: 3400; ce: 0.029402707; steps/sec: 692.26; LeNet_lr: 8.445584e-05; \nFastEstimator-Train: step: 3500; ce: 0.00379532; steps/sec: 671.9; LeNet_lr: 4.4184046e-05; \nFastEstimator-Train: step: 3600; ce: 0.0058736084; steps/sec: 705.67; LeNet_lr: 1.6692711e-05; \nFastEstimator-Train: step: 3700; ce: 0.031074949; steps/sec: 670.42; LeNet_lr: 2.7518167e-06; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.87 sec; \nFastEstimator-Finish: step: 3750; total_time: 7.14 sec; LeNet_lr: 1e-06; \n</pre> In\u00a0[7]: Copied! <pre>visualize_logs(history3, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history3, include_metrics=\"LeNet_lr\") <p></p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#advanced-tutorial-7-learning-rate-scheduling", "title": "Advanced Tutorial 7: Learning Rate Scheduling\u00b6", "text": ""}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing a Learning Rate Schedule Function<ul> <li>epoch-wise</li> <li>step-wise</li> </ul> </li> <li>Using a Built-In lr_schedule Function<ul> <li>cosine decay</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#customizing-a-learning-rate-schedule-function", "title": "Customizing a Learning Rate Schedule Function\u00b6", "text": "<p>We can specify a custom learning schedule by passing a custom function to the lr_fn parameter of <code>LRScheduler</code>. We can have this learning rate schedule applied at either the epoch or step level. Epoch and step both start from 1.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#epoch-wise", "title": "Epoch-wise\u00b6", "text": "<p>To apply learning rate scheduling at an epoch level, the custom function should have 'epoch' as its parameter. Let's look at the example below which demonstrates this. We will be using the summary parameter in the fit method to be able to visualize the learning rate later. You can go through Advanced Tutorial 6 for more details on accessing training history.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#step-wise", "title": "Step-wise\u00b6", "text": "<p>The custom function should have 'step' as its parameter for step-based learning rate schedules.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#using-built-in-lr_schedule-function", "title": "Using Built-In lr_schedule Function\u00b6", "text": "<p>Some learning rates schedules are widely popular in the deep learning community. We have implemented some of them in FastEstimator so that you don't need to write a custom schedule for them. We will be showcasing the <code>cosine decay</code> schedule below.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#cosine_decay", "title": "cosine_decay\u00b6", "text": "<p>We can specify the length of the decay cycle and initial learning rate using cycle_length and init_lr respectively. Similar to custom learning schedule, lr_fn should have step or epoch as a parameter. The FastEstimator cosine decay can be used as follows:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t08_xai.html", "title": "Advanced Tutorial 8: Explainable AI (XAI)", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import squeeze\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver, ImageViewer, TensorBoard\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.xai import Saliency\nfrom fastestimator.util import to_number\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n\nbatch_size=32\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\")],\n    num_process=0)\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),\n    Saliency(model=model,\n             model_inputs=\"x\",\n             class_key=\"y\",\n             model_outputs=\"y_pred\",\n             samples=5,\n             label_mapping=label_mapping),\n    ImageViewer(inputs=\"saliency\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=5,\n                         traces=traces,\n                         log_steps=1000)\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import squeeze from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop.univariate import Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver, ImageViewer, TensorBoard from fastestimator.trace.metric import Accuracy from fastestimator.trace.xai import Saliency from fastestimator.util import to_number  import matplotlib.pyplot as plt import numpy as np  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 }  batch_size=32  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\")],     num_process=0)  model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),     Saliency(model=model,              model_inputs=\"x\",              class_key=\"y\",              model_outputs=\"y_pred\",              samples=5,              label_mapping=label_mapping),     ImageViewer(inputs=\"saliency\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=5,                          traces=traces,                          log_steps=1000) <p>In this example we will be using the <code>ImageViewer</code> <code>Trace</code>, since it will allow us to visualize the outputs within this Notebook. If you wanted your images to appear in TensorBoard, simply construct a <code>TensorBoard</code> <code>Trace</code> with the \"write_images\" argument set to \"saliency\".</p> In\u00a0[2]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.4207764; model_lr: 0.0009999998; \nFastEstimator-Train: step: 1000; ce: 1.13122; steps/sec: 359.79; model_lr: 0.00083473074; \nFastEstimator-Train: step: 1563; epoch: 1; epoch_time: 4.91 sec; \n</pre> <pre>FastEstimator-Eval: step: 1563; epoch: 1; ce: 1.1183364; accuracy: 0.6022; \nFastEstimator-Train: step: 2000; ce: 1.0117686; steps/sec: 349.3; model_lr: 0.00044828805; \nFastEstimator-Train: step: 3000; ce: 0.84499186; steps/sec: 362.07; model_lr: 9.639601e-05; \nFastEstimator-Train: step: 3126; epoch: 2; epoch_time: 4.29 sec; \n</pre> <pre>FastEstimator-Eval: step: 3126; epoch: 2; ce: 0.9405009; accuracy: 0.674; \nFastEstimator-Train: step: 4000; ce: 0.9253516; steps/sec: 385.52; model_lr: 0.0009890847; \nFastEstimator-Train: step: 4689; epoch: 3; epoch_time: 4.11 sec; \n</pre> <pre>FastEstimator-Eval: step: 4689; epoch: 3; ce: 0.94179326; accuracy: 0.6724; \nFastEstimator-Train: step: 5000; ce: 0.66330093; steps/sec: 375.63; model_lr: 0.00075025; \nFastEstimator-Train: step: 6000; ce: 1.046946; steps/sec: 370.67; model_lr: 0.000346146; \nFastEstimator-Train: step: 6252; epoch: 4; epoch_time: 4.2 sec; \n</pre> <pre>FastEstimator-Eval: step: 6252; epoch: 4; ce: 0.8270567; accuracy: 0.7174; \nFastEstimator-Train: step: 7000; ce: 0.6407217; steps/sec: 357.48; model_lr: 4.4184046e-05; \nFastEstimator-Train: step: 7815; epoch: 5; epoch_time: 4.4 sec; \n</pre> <pre>FastEstimator-Eval: step: 7815; epoch: 5; ce: 0.9042755; accuracy: 0.6878; \nFastEstimator-Finish: step: 7815; total_time: 30.5 sec; model_lr: 0.0009827082; \n</pre> In\u00a0[3]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 7815; epoch: 5; accuracy: 0.69; \n</pre> <p>In the images above, the 'saliency' column corresponds to a raw saliency mask generated by back-propagating a model's output prediction onto the input image. 'Smoothed saliency' combines multiple saliency masks for each image 'x', where each mask is generated by slightly perturbing the input 'x' before running the forward and backward gradient passes. The number of samples to be combined is controlled by the \"smoothing\" argument in the <code>Saliency</code> <code>Trace</code> constructor. 'Integrated saliency' is a saliency mask generated by starting from a baseline blank image and linearly interpolating the image towards 'x' over a number of steps defined by the \"integrating\" argument in the Saliency constructor. The resulting masks are then combined together. The 'SmInt Saliency' (Smoothed-Integrated) column combines smoothing and integration together. SmInt is generally considered to give the most reliable indication of the important features in an image, but it also takes the longest to compute. It is possible to disable the more complex columns by setting the 'smoothing' and 'integrating' parameters to 0. The 'x saliency' column shows the input image overlaid with whatever saliency column is furthest to the right (SmInt, unless that has been disabled).</p> <p></p> In\u00a0[4]: Copied! <pre>import tempfile\nimport os\n\npipeline.batch_size = 6\nbatch = pipeline.get_results()\nbatch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow\n\nsaliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\")\nimages = saliency_generator.get_masks(batch=batch)\n\n# Let's convert 'y' and 'y_pred' from numeric values to strings for readability:\nval_to_label = {val: key for key, val in label_mapping.items()}\ny = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))])\ny_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])\n\n# Now simply load up an ImgData object and let it handle laying out the final result for you\nsave_dir = tempfile.mkdtemp()\nimages = fe.util.ImgData(colormap=\"inferno\", y=y, y_pred=y_pred, x=batch[\"x\"], saliency=images[\"saliency\"])\nfig = images.paint_figure(save_path=os.path.join(save_dir, \"t08a_saliency.png\")) # save_path is optional, but a useful feature to know about\nplt.show()\n</pre> import tempfile import os  pipeline.batch_size = 6 batch = pipeline.get_results() batch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow  saliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\") images = saliency_generator.get_masks(batch=batch)  # Let's convert 'y' and 'y_pred' from numeric values to strings for readability: val_to_label = {val: key for key, val in label_mapping.items()} y = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))]) y_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])  # Now simply load up an ImgData object and let it handle laying out the final result for you save_dir = tempfile.mkdtemp() images = fe.util.ImgData(colormap=\"inferno\", y=y, y_pred=y_pred, x=batch[\"x\"], saliency=images[\"saliency\"]) fig = images.paint_figure(save_path=os.path.join(save_dir, \"t08a_saliency.png\")) # save_path is optional, but a useful feature to know about plt.show() <p>The <code>SaliencyNet</code> class also provides 'get_smoothed_masks' and 'get_integrated_masks' methods for generating the more complicated saliency maps.</p>"}, {"location": "tutorial/advanced/t08_xai.html#advanced-tutorial-8-explainable-ai-xai", "title": "Advanced Tutorial 8: Explainable AI (XAI)\u00b6", "text": ""}, {"location": "tutorial/advanced/t08_xai.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Saliency Maps<ul> <li>With Traces</li> <li>Without Traces</li> </ul> </li> </ul>"}, {"location": "tutorial/advanced/t08_xai.html#saliency-maps", "title": "Saliency Maps\u00b6", "text": "<p>Suppose you have a neural network that is performing image classification. The network tells you that the image it is looking at is an airplane, but you want to know whether it is really detecting an airplane, or if it is 'cheating' by noticing the blue sky in the image background. To answer this question, all you need to do is add the <code>Saliency</code> <code>Trace</code> to your list of traces, and pass its output to one of either the <code>ImageSaver</code>, <code>ImageViewer</code>, or <code>TensorBoard</code> <code>Traces</code>.</p>"}, {"location": "tutorial/advanced/t08_xai.html#saliency-maps-without-traces", "title": "Saliency Maps without Traces\u00b6", "text": "<p>Suppose that you want to generate Saliency masks without using a <code>Trace</code>. This can be done through the fe.xai package:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html", "title": "Advanced Tutorial 9: Meta Ops", "text": "In\u00a0[1]: Copied! <pre>from fastestimator import Pipeline\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop import LambdaOp, NumpyOp\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip\nfrom fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose\nfrom fastestimator.util import to_number, ImgData\n\ntrain_data, eval_data = cifar10.load_data()\n\nclass AddOne(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        return data + 1\n</pre> from fastestimator import Pipeline from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop import LambdaOp, NumpyOp from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip from fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose from fastestimator.util import to_number, ImgData  train_data, eval_data = cifar10.load_data()  class AddOne(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         return data + 1 In\u00a0[2]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes  # Note that there is also a Sometimes in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), prob=0.5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Sometimes  # Note that there is also a Sometimes in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), prob=0.5)                         ]                    ) In\u00a0[3]: Copied! <pre>data = pipeline.get_results()\nimg = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"]))\nfig = img.paint_figure()\n</pre> data = pipeline.get_results() img = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"])) fig = img.paint_figure() In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import OneOf  # Note that there is also a OneOf in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         OneOf(Rotate(image_in=\"x\", image_out=\"x_out\", mode=\"train\", limit=45), \n                               VerticalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), \n                               Blur(inputs=\"x\", outputs=\"x_out\", mode=\"train\", blur_limit=7))\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import OneOf  # Note that there is also a OneOf in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          OneOf(Rotate(image_in=\"x\", image_out=\"x_out\", mode=\"train\", limit=45),                                 VerticalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),                                 Blur(inputs=\"x\", outputs=\"x_out\", mode=\"train\", blur_limit=7))                         ]                    ) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\nimg = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"]))\nfig = img.paint_figure()\n</pre> data = pipeline.get_results() img = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"])) fig = img.paint_figure() In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),\n                         Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),                          Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=5)                         ]                    ) In\u00a0[7]: Copied! <pre>data = pipeline.get_results()\nprint(data['z'])\n</pre> data = pipeline.get_results() print(data['z']) <pre>tensor([5, 5, 5, 5])\n</pre> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),\n                         Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=lambda z: z &lt; 6.5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),                          Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=lambda z: z &lt; 6.5)                         ]                    ) In\u00a0[9]: Copied! <pre>data = pipeline.get_results()\nprint(data['z'])\n</pre> data = pipeline.get_results() print(data['z']) <pre>tensor([7, 7, 7, 7])\n</pre> In\u00a0[10]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes, Fuse  # Note that Sometimes and Fuse are also available in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         Sometimes(\n                             Fuse([\n                                 HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),\n                                 VerticalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")]))\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Sometimes, Fuse  # Note that Sometimes and Fuse are also available in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          Sometimes(                              Fuse([                                  HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),                                  VerticalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")]))                         ]                    ) In\u00a0[11]: Copied! <pre>data = pipeline.get_results()\nimg = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"]))\nfig = img.paint_figure()\n</pre> data = pipeline.get_results() img = ImgData(x=to_number(data[\"x\"]), x_out=to_number(data[\"x_out\"])) fig = img.paint_figure()"}, {"location": "tutorial/advanced/t09_meta_ops.html#advanced-tutorial-9-meta-ops", "title": "Advanced Tutorial 9: Meta Ops\u00b6", "text": ""}, {"location": "tutorial/advanced/t09_meta_ops.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Meta Op Overview</li> <li>Sometimes</li> <li>OneOf</li> <li>Repeat<ul> <li>Static</li> <li>Dynamic</li> </ul> </li> <li>Fuse</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t09_meta_ops.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>Let's gather some datasets and get some imports out of the way</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#meta-op-overview", "title": "Meta Op Overview\u00b6", "text": "<p>We learned about the operator structure in Beginner Tutorial 3. Operators are used to build complex computation graphs in FastEstimator.</p> <p>Meta Ops are Operators which take other Operators as inputs and modify their functionality. These can allow for much more complicated computation graphs, as we will see in the following examples. They are available as both NumpyOps for use in a <code>Pipeline</code>, and as TensorOps for use in a <code>Network</code>.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#sometimes", "title": "Sometimes\u00b6", "text": "<p><code>Sometimes</code> is a meta op which applies a given Op with a specified probability, by default 50% of the time. The <code>Sometimes</code> Op cannot be used to create keys which do not already exist in the data dictionary, since then it would not be clear what should be done when the Op decides not to execute. One convenient way to create default values is to first use a <code>LambdaOp</code>, as described in Advanced Tutorial 3.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#oneof", "title": "OneOf\u00b6", "text": "<p><code>OneOf</code> takes a list of Ops for input, and randomly chooses one of them every step to be executed. The Ops to be selected between must all share the same inputs, outputs, and modes.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#repeat", "title": "Repeat\u00b6", "text": "<p><code>Repeat</code> takes an Op and runs it multiple times in a row. It can be set to repeat for a fixed (static) number of times, or to repeat until a given input function evaluates to False (dynamic). <code>Repeat</code> will always evaluate at least once. After performing each forward pass, it will check to see whether the stopping criteria have been met. If using an input function to determine the stopping criteria, any input arguments to that function will be looked up by name from the data dictionary and passed through to the function for evaluation.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#static", "title": "Static\u00b6", "text": "<p>We will start with a static example of the <code>Repeat</code> Op, which will always run 5 times:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#dynamic", "title": "Dynamic\u00b6", "text": "<p>Now lets see an example of a dynamic repeat op, which uses a lambda function to determine when it should stop. In this case, the repeat will continue so long as z is less than 6.5:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#fuse", "title": "Fuse\u00b6", "text": "<p><code>Fuse</code> takes a list of Ops and combines them together into a single Op. All of the fused Ops must have the same mode. This can be useful in conjunction with the other Meta Ops. For example, suppose you have Op A and Op B, and want to run Sometimes(A) but only want B to execute when A is chosen to run by the Sometimes. You could then run Sometimes(Fuse(A,B)). Or if you wanted to perform mini-batch training within a network, you could do something like Repeat(Fuse(Model, Loss, Update)). Let's try an example where we either leave an image alone, or perform both a horizontal and vertical flip on it:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Semantic Segmentation</li> </ul>"}, {"location": "tutorial/advanced/t10_report_generation.html", "title": "Advanced Tutorial 10: Automated Report Generation", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\nroot_output_dir = tempfile.mkdtemp()\n\ndef get_estimator(extra_traces):\n    # step 1\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(100)\n    test_data['id'] = [i for i in range(len(test_data))]  # Assign some data ids for the test report to look at\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n\n    # step 2\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    # step 3\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n        BestModelSaver(model=model, save_dir=root_output_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n        LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    ]\n    traces.extend(extra_traces)\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=2,\n                             traces=traces,\n                             max_train_steps_per_epoch=100,\n                             max_eval_steps_per_epoch=100,\n                             log_steps=10)\n    return estimator\n</pre> import tempfile import os import numpy as np import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  root_output_dir = tempfile.mkdtemp()  def get_estimator(extra_traces):     # step 1     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(100)     test_data['id'] = [i for i in range(len(test_data))]  # Assign some data ids for the test report to look at     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])      # step 2     model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     # step 3     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\"),         BestModelSaver(model=model, save_dir=root_output_dir, metric=\"accuracy\", save_best_mode=\"max\"),         LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))     ]     traces.extend(extra_traces)     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=2,                              traces=traces,                              max_train_steps_per_epoch=100,                              max_eval_steps_per_epoch=100,                              log_steps=10)     return estimator In\u00a0[2]: Copied! <pre>from fastestimator.trace.io import Traceability\n\nsave_dir = os.path.join(root_output_dir, 'report')\nest = get_estimator([Traceability(save_dir)])\n\nprint(f\"The root save directory is: {root_output_dir}\")\nprint(f\"The traceability report will be written to: {save_dir}\")\nprint(f\"Logs and images from the report will be written to: {os.path.join(save_dir, 'resources')}\")\n</pre> from fastestimator.trace.io import Traceability  save_dir = os.path.join(root_output_dir, 'report') est = get_estimator([Traceability(save_dir)])  print(f\"The root save directory is: {root_output_dir}\") print(f\"The traceability report will be written to: {save_dir}\") print(f\"Logs and images from the report will be written to: {os.path.join(save_dir, 'resources')}\") <pre>The root save directory is: /tmp/tmpwps4y0dd\nThe traceability report will be written to: /tmp/tmpwps4y0dd/report\nLogs and images from the report will be written to: /tmp/tmpwps4y0dd/report/resources\n</pre> <p>When using Traceability, you must pass a summary name to the Estimator.fit() call. This will become the name of your report.</p> In\u00a0[3]: Copied! <pre>est.fit(\"Sample MNIST Report\")\n</pre> est.fit(\"Sample MNIST Report\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 10; \nFastEstimator-Train: step: 1; ce: 2.3301315; model_lr: 0.0009999998; \nFastEstimator-Train: step: 10; ce: 2.0306354; steps/sec: 447.07; model_lr: 0.0009999825; \nFastEstimator-Train: step: 20; ce: 1.5300003; steps/sec: 528.49; model_lr: 0.0009999298; \nFastEstimator-Train: step: 30; ce: 0.8917824; steps/sec: 524.95; model_lr: 0.0009998423; \nFastEstimator-Train: step: 40; ce: 0.56758463; steps/sec: 563.14; model_lr: 0.0009997196; \nFastEstimator-Train: step: 50; ce: 0.33073947; steps/sec: 560.82; model_lr: 0.0009995619; \nFastEstimator-Train: step: 60; ce: 0.5340511; steps/sec: 553.62; model_lr: 0.0009993691; \nFastEstimator-Train: step: 70; ce: 0.4809867; steps/sec: 569.04; model_lr: 0.0009991414; \nFastEstimator-Train: step: 80; ce: 0.17024752; steps/sec: 688.01; model_lr: 0.0009988786; \nFastEstimator-Train: step: 90; ce: 0.29996952; steps/sec: 659.72; model_lr: 0.0009985808; \nFastEstimator-Train: step: 100; ce: 0.2563717; steps/sec: 671.72; model_lr: 0.0009982482; \nFastEstimator-Train: step: 100; epoch: 1; epoch_time: 0.76 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpwps4y0dd/model_best_accuracy.h5\nFastEstimator-Eval: step: 100; epoch: 1; ce: 0.3122454; accuracy: 0.9159375; since_best_accuracy: 0; max_accuracy: 0.9159375; \nFastEstimator-Train: step: 110; ce: 0.32248974; steps/sec: 71.81; model_lr: 0.0009978806; \nFastEstimator-Train: step: 120; ce: 0.13474032; steps/sec: 489.5; model_lr: 0.000997478; \nFastEstimator-Train: step: 130; ce: 0.21002704; steps/sec: 581.26; model_lr: 0.0009970407; \nFastEstimator-Train: step: 140; ce: 0.11852975; steps/sec: 564.03; model_lr: 0.0009965684; \nFastEstimator-Train: step: 150; ce: 0.36558276; steps/sec: 564.45; model_lr: 0.0009960613; \nFastEstimator-Train: step: 160; ce: 0.27304798; steps/sec: 556.14; model_lr: 0.0009955195; \nFastEstimator-Train: step: 170; ce: 0.16349566; steps/sec: 547.83; model_lr: 0.0009949428; \nFastEstimator-Train: step: 180; ce: 0.2748593; steps/sec: 605.25; model_lr: 0.0009943316; \nFastEstimator-Train: step: 190; ce: 0.33497655; steps/sec: 679.45; model_lr: 0.0009936856; \nFastEstimator-Train: step: 200; ce: 0.24099544; steps/sec: 678.38; model_lr: 0.000993005; \nFastEstimator-Train: step: 200; epoch: 2; epoch_time: 0.27 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpwps4y0dd/model_best_accuracy.h5\nFastEstimator-Eval: step: 200; epoch: 2; ce: 0.20991856; accuracy: 0.92875; since_best_accuracy: 0; max_accuracy: 0.92875; \nFastEstimator-Finish: step: 200; total_time: 3.82 sec; model_lr: 0.000993005; \nFastEstimator-Traceability: Report written to /tmp/tmpwps4y0dd/report/sample_mnist_report.pdf\n</pre> Out[3]: <pre>&lt;fastestimator.summary.summary.Summary at 0x7f1a7cb84080&gt;</pre> <p>If everything went according to plan, then inside your root save directory you should now have the following files:</p> <pre><code>/report\n    sample_mnist_report.pdf\n    sample_mnist_report.tex\n    /resources\n        sample_mnist_report_logs.png\n        sample_mnist_report_model.pdf\n        sample_mnist_report.txt\n</code></pre> <p>You could then switch up your experiment parameters and call .fit() with a new experiment name in order to write more reports into the same folder. A call to <code>fastestimator logs ./resources</code> would then allow you to easily compare these experiments, as described in Advanced Tutorial 6</p> <p>Our report should look something like this (use Chrome or Firefox to view):</p> In\u00a0[4]: Copied! <pre>from IPython.display import IFrame\nIFrame('../resources/t10a_traceability.pdf', width=600, height=800)\n</pre> from IPython.display import IFrame IFrame('../resources/t10a_traceability.pdf', width=600, height=800) Out[4]: <p></p> In\u00a0[5]: Copied! <pre>from fastestimator.trace.io.test_report import TestCase, TestReport\n\nsave_dir = os.path.join(root_output_dir, 'report2')\n\n# Note that the name of the input to the 'criteria' function must match a key in the data dictionary\nagg_test_easy = TestCase(description='Accuracy should be greater than 1%', criteria=lambda accuracy: accuracy &gt; 0.01)\nagg_test_hard = TestCase(description='Accuracy should be greater than 99%', criteria=lambda accuracy: accuracy &gt; 0.99)\n\ninst_test_hard = TestCase(description='All Data should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=0)\ninst_test_easy = TestCase(description='At least one image should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=len(est.pipeline.data['test'])-1)\n\nreport = TestReport(test_cases=[agg_test_easy, agg_test_hard, inst_test_easy, inst_test_hard], save_path=save_dir, data_id='id')\n\nest = get_estimator([report])\n\nprint(f\"The root save directory is: {root_output_dir}\")\nprint(f\"The test report will be written to: {save_dir}\")\nprint(f\"A json summary of the report will be written to: {os.path.join(save_dir, 'resources')}\")\n</pre> from fastestimator.trace.io.test_report import TestCase, TestReport  save_dir = os.path.join(root_output_dir, 'report2')  # Note that the name of the input to the 'criteria' function must match a key in the data dictionary agg_test_easy = TestCase(description='Accuracy should be greater than 1%', criteria=lambda accuracy: accuracy &gt; 0.01) agg_test_hard = TestCase(description='Accuracy should be greater than 99%', criteria=lambda accuracy: accuracy &gt; 0.99)  inst_test_hard = TestCase(description='All Data should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=0) inst_test_easy = TestCase(description='At least one image should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=len(est.pipeline.data['test'])-1)  report = TestReport(test_cases=[agg_test_easy, agg_test_hard, inst_test_easy, inst_test_hard], save_path=save_dir, data_id='id')  est = get_estimator([report])  print(f\"The root save directory is: {root_output_dir}\") print(f\"The test report will be written to: {save_dir}\") print(f\"A json summary of the report will be written to: {os.path.join(save_dir, 'resources')}\") <pre>The root save directory is: /tmp/tmpwps4y0dd\nThe test report will be written to: /tmp/tmpwps4y0dd/report2\nA json summary of the report will be written to: /tmp/tmpwps4y0dd/report2/resources\n</pre> In\u00a0[6]: Copied! <pre>est.fit(\"MNIST\")\nest.test()\n</pre> est.fit(\"MNIST\") est.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 10; \nFastEstimator-Train: step: 1; ce: 2.3229642; model1_lr: 0.0009999998; \nFastEstimator-Train: step: 10; ce: 2.0991652; steps/sec: 587.99; model1_lr: 0.0009999825; \nFastEstimator-Train: step: 20; ce: 1.6330333; steps/sec: 605.43; model1_lr: 0.0009999298; \nFastEstimator-Train: step: 30; ce: 1.0917461; steps/sec: 660.18; model1_lr: 0.0009998423; \nFastEstimator-Train: step: 40; ce: 1.0353419; steps/sec: 737.78; model1_lr: 0.0009997196; \nFastEstimator-Train: step: 50; ce: 0.5517947; steps/sec: 680.52; model1_lr: 0.0009995619; \nFastEstimator-Train: step: 60; ce: 0.34303236; steps/sec: 723.04; model1_lr: 0.0009993691; \nFastEstimator-Train: step: 70; ce: 0.4734753; steps/sec: 626.83; model1_lr: 0.0009991414; \nFastEstimator-Train: step: 80; ce: 0.35647863; steps/sec: 705.1; model1_lr: 0.0009988786; \nFastEstimator-Train: step: 90; ce: 0.57413185; steps/sec: 740.04; model1_lr: 0.0009985808; \nFastEstimator-Train: step: 100; ce: 0.33393666; steps/sec: 697.83; model1_lr: 0.0009982482; \nFastEstimator-Train: step: 100; epoch: 1; epoch_time: 0.35 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpwps4y0dd/model1_best_accuracy.h5\nFastEstimator-Eval: step: 100; epoch: 1; ce: 0.3576527; accuracy: 0.8921875; since_best_accuracy: 0; max_accuracy: 0.8921875; \nFastEstimator-Train: step: 110; ce: 0.43586737; steps/sec: 81.36; model1_lr: 0.0009978806; \nFastEstimator-Train: step: 120; ce: 0.2825139; steps/sec: 618.99; model1_lr: 0.000997478; \nFastEstimator-Train: step: 130; ce: 0.23715392; steps/sec: 652.88; model1_lr: 0.0009970407; \nFastEstimator-Train: step: 140; ce: 0.4731403; steps/sec: 701.43; model1_lr: 0.0009965684; \nFastEstimator-Train: step: 150; ce: 0.12009444; steps/sec: 662.69; model1_lr: 0.0009960613; \nFastEstimator-Train: step: 160; ce: 0.1876252; steps/sec: 661.42; model1_lr: 0.0009955195; \nFastEstimator-Train: step: 170; ce: 0.060715243; steps/sec: 679.76; model1_lr: 0.0009949428; \nFastEstimator-Train: step: 180; ce: 0.22843787; steps/sec: 690.74; model1_lr: 0.0009943316; \nFastEstimator-Train: step: 190; ce: 0.28863788; steps/sec: 724.46; model1_lr: 0.0009936856; \nFastEstimator-Train: step: 200; ce: 0.1017374; steps/sec: 695.11; model1_lr: 0.000993005; \nFastEstimator-Train: step: 200; epoch: 2; epoch_time: 0.26 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpwps4y0dd/model1_best_accuracy.h5\nFastEstimator-Eval: step: 200; epoch: 2; ce: 0.25012556; accuracy: 0.9221875; since_best_accuracy: 0; max_accuracy: 0.9221875; \nFastEstimator-Finish: step: 200; total_time: 3.57 sec; model1_lr: 0.000993005; \nFastEstimator-Test: step: 200; epoch: 2; accuracy: 0.93; \nFastEstimator-TestReport: Report written to /tmp/tmpwps4y0dd/report2/mnist_TestReport.pdf\n</pre> Out[6]: <pre>&lt;fastestimator.summary.summary.Summary at 0x7f1a7cb7f6d8&gt;</pre> <p>If everything went according to plan, then inside your root save directory you should now have the following files:</p> <pre><code>/report2\n    mnist_TestReport.pdf\n    mnist_TestReport.tex\n    /resources\n        mnist_TestReport.json\n</code></pre> <p>Our report should look something like this (use Chrome or Firefox to view):</p> In\u00a0[7]: Copied! <pre>from IPython.display import IFrame\nIFrame('../resources/t10a_test.pdf', width=600, height=800)\n</pre> from IPython.display import IFrame IFrame('../resources/t10a_test.pdf', width=600, height=800) Out[7]:"}, {"location": "tutorial/advanced/t10_report_generation.html#advanced-tutorial-10-automated-report-generation", "title": "Advanced Tutorial 10: Automated Report Generation\u00b6", "text": ""}, {"location": "tutorial/advanced/t10_report_generation.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Overview and Dependencies</li> <li>Traceability</li> <li>Test Report</li> </ul>"}, {"location": "tutorial/advanced/t10_report_generation.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>Let's get some imports and object construction out of the way:</p>"}, {"location": "tutorial/advanced/t10_report_generation.html#overview-and-dependencies", "title": "Overview and Dependencies\u00b6", "text": "<p>FastEstimator provides Traces which allow you to automatically generate traceability documents and test reports. These reports are written in the LaTeX file format, and then automatically compiled into PDF documents if you have LaTeX installed on your machine. If you don't have LaTeX installed on your training machine, you can still generate the report files and then move them to a different computer in order to compile them manually. Generating traceability documents also requires GraphViz which, unlike LaTeX, must be installed in order for training to proceed.</p> <pre><code>Installing Dependencies:\n    On Linux: \n        apt-get install -y graphviz texlive-latex-base texlive-latex-extra\n    On SageMaker:\n        unset PYTHONPATH\n        export DEBIAN_FRONTEND=noninteractive\n        apt-get install -y graphviz texlive-latex-base texlive-latex-extra\n    On Mac:\n        brew install graphviz\n        brew cask install mactex\n    On Windows:\n        winget install graphviz\n        winget install TeXLive\n</code></pre>"}, {"location": "tutorial/advanced/t10_report_generation.html#traceability", "title": "Traceability\u00b6", "text": "<p>Traceability reports are designed to capture all the information about the state of your system when an experiment was run. The report will include training graphs, operator architecture diagrams, model architecture diagrams, a summary of your system configuration, and the values of all variables used to instantiate objects during training. It will also automatically save a copy of your log output to disk, which can be especially useful for comparing different experiment configurations without worrying about forgetting what settings were used for each run. To generate this report, simply add a Traceability trace to your list of traces:</p>"}, {"location": "tutorial/advanced/t10_report_generation.html#test-report", "title": "Test Report\u00b6", "text": "<p>Test Reports can provide an automatically generated overview summary of how well your model is performing. This could be useful if, for example, you needed to submit documentation to a regulatory agency. Test Reports can also be used to highlight particular failure cases so that you can investigate problematic data points in more detail.</p> <p>The <code>TestReport</code> trace takes a list of <code>TestCase</code> objects as input. These are further subdivided into two types: aggregate and per-instance. Aggregate test cases run at the end of the test epoch and deal with aggregated information (typically metrics such as accuracy). Per-instance tests run at the end of every step during testing, and are meant to evaluate every element within a batch independently. If your data dictionary happens to contain data instance ids, you can also use these to find problematic inputs.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html", "title": "Tutorial 1: Getting Started", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\ntrain_data, eval_data = mnist.load_data()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  train_data, eval_data = mnist.load_data()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[2]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\") \n    ])\n</pre> from fastestimator.architecture.tensorflow import LeNet # from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model  from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")      ]) In\u00a0[3]: Copied! <pre>from fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nimport tempfile\n\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=traces)\n</pre> from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver import tempfile  traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),           BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=traces) In\u00a0[4]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.2944355; \nFastEstimator-Train: step: 100; ce: 0.17604804; steps/sec: 724.9; \nFastEstimator-Train: step: 200; ce: 0.6541523; steps/sec: 755.07; \nFastEstimator-Train: step: 300; ce: 0.22645846; steps/sec: 793.02; \nFastEstimator-Train: step: 400; ce: 0.1256088; steps/sec: 773.46; \nFastEstimator-Train: step: 500; ce: 0.18927144; steps/sec: 809.2; \nFastEstimator-Train: step: 600; ce: 0.07107867; steps/sec: 779.29; \nFastEstimator-Train: step: 700; ce: 0.07468874; steps/sec: 806.57; \nFastEstimator-Train: step: 800; ce: 0.23852134; steps/sec: 781.42; \nFastEstimator-Train: step: 900; ce: 0.028577618; steps/sec: 826.27; \nFastEstimator-Train: step: 1000; ce: 0.115206845; steps/sec: 776.94; \nFastEstimator-Train: step: 1100; ce: 0.07892787; steps/sec: 841.47; \nFastEstimator-Train: step: 1200; ce: 0.14857067; steps/sec: 791.73; \nFastEstimator-Train: step: 1300; ce: 0.049252644; steps/sec: 834.86; \nFastEstimator-Train: step: 1400; ce: 0.046725605; steps/sec: 799.79; \nFastEstimator-Train: step: 1500; ce: 0.06713241; steps/sec: 812.31; \nFastEstimator-Train: step: 1600; ce: 0.08489384; steps/sec: 803.99; \nFastEstimator-Train: step: 1700; ce: 0.00921803; steps/sec: 767.87; \nFastEstimator-Train: step: 1800; ce: 0.0072177458; steps/sec: 694.9; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.97 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpcxa1xloj/model_best_accuracy.h5\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.06391551; accuracy: 0.9802; since_best_accuracy: 0; max_accuracy: 0.9802; \nFastEstimator-Train: step: 1900; ce: 0.006937413; steps/sec: 419.42; \nFastEstimator-Train: step: 2000; ce: 0.10369404; steps/sec: 769.67; \nFastEstimator-Train: step: 2100; ce: 0.023126157; steps/sec: 787.83; \nFastEstimator-Train: step: 2200; ce: 0.013664322; steps/sec: 807.29; \nFastEstimator-Train: step: 2300; ce: 0.15465331; steps/sec: 782.67; \nFastEstimator-Train: step: 2400; ce: 0.0059421803; steps/sec: 783.07; \nFastEstimator-Train: step: 2500; ce: 0.03436095; steps/sec: 789.81; \nFastEstimator-Train: step: 2600; ce: 0.003341827; steps/sec: 813.02; \nFastEstimator-Train: step: 2700; ce: 0.009203151; steps/sec: 779.41; \nFastEstimator-Train: step: 2800; ce: 0.0031451974; steps/sec: 818.42; \nFastEstimator-Train: step: 2900; ce: 0.03497669; steps/sec: 789.2; \nFastEstimator-Train: step: 3000; ce: 0.0043699713; steps/sec: 816.05; \nFastEstimator-Train: step: 3100; ce: 0.14205246; steps/sec: 769.89; \nFastEstimator-Train: step: 3200; ce: 0.00966863; steps/sec: 827.11; \nFastEstimator-Train: step: 3300; ce: 0.005415355; steps/sec: 780.63; \nFastEstimator-Train: step: 3400; ce: 0.027803676; steps/sec: 812.07; \nFastEstimator-Train: step: 3500; ce: 0.3876436; steps/sec: 788.85; \nFastEstimator-Train: step: 3600; ce: 0.011643453; steps/sec: 809.37; \nFastEstimator-Train: step: 3700; ce: 0.20535453; steps/sec: 794.13; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.46 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpcxa1xloj/model_best_accuracy.h5\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03874958; accuracy: 0.9867; since_best_accuracy: 0; max_accuracy: 0.9867; \nFastEstimator-Finish: step: 3750; total_time: 8.86 sec; model_lr: 0.001; \n</pre> In\u00a0[5]: Copied! <pre>import numpy as np\n\ndata = eval_data[0]\ndata = pipeline.transform(data, mode=\"eval\")\ndata = network.transform(data, mode=\"eval\")\n\nprint(\"Ground truth class is {}\".format(data[\"y\"][0]))\nprint(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\nimg = fe.util.ImgData(x=data[\"x\"])\nfig = img.paint_figure()\n</pre> import numpy as np  data = eval_data[0] data = pipeline.transform(data, mode=\"eval\") data = network.transform(data, mode=\"eval\")  print(\"Ground truth class is {}\".format(data[\"y\"][0])) print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"]))) img = fe.util.ImgData(x=data[\"x\"]) fig = img.paint_figure() <pre>Ground truth class is 7\nPredicted class is 7\n</pre>"}, {"location": "tutorial/beginner/t01_getting_started.html#tutorial-1-getting-started", "title": "Tutorial 1: Getting Started\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#overview", "title": "Overview\u00b6", "text": "<p>Welcome to FastEstimator! In this tutorial we are going to cover:</p> <ul> <li>The three main APIs of FastEstimator: <code>Pipeline</code>, <code>Network</code>, <code>Estimator</code></li> <li>An image classification example<ul> <li>Pipeline</li> <li>Network</li> <li>Estimator</li> <li>Training</li> <li>Inferencing</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t01_getting_started.html#three-main-apis", "title": "Three main APIs\u00b6", "text": "<p>All deep learning training work\ufb02ows involve the following three essential components, each mapping to a critical API in FastEstimator.</p> <ul> <li><p>Data pipeline: extracts data from disk/RAM, performs transformations. -&gt;  <code>fe.Pipeline</code></p> </li> <li><p>Network: performs trainable and differentiable operations. -&gt;  <code>fe.Network</code></p> </li> <li><p>Training loop: combines the data pipeline and network in an iterative process. -&gt;  <code>fe.Estimator</code></p> </li> </ul>  Any deep learning task can be constructed by following the 3 main steps:"}, {"location": "tutorial/beginner/t01_getting_started.html#image-classification-example", "title": "Image Classification Example\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#step-1-pipeline", "title": "Step 1 - Pipeline\u00b6", "text": "<p>We use FastEstimator dataset API to load the MNIST dataset. Please check out Tutorial 2 for more details about the dataset API. In this case our data preprocessing involves:</p> <ol> <li>Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.</li> <li>Rescale pixel values from [0, 255] to [0, 1].</li> </ol> <p>Please check out Tutorial 3 for details about <code>Operator</code> and Tutorial 4 for <code>Pipeline</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-2-network", "title": "Step 2 - Network\u00b6", "text": "<p>The model definition can be either from <code>tf.keras.Model</code> or <code>torch.nn.Module</code>, for more info about network definitions, check out Tutorial 5. The differentiable operations during training are listed as follows:</p> <ol> <li>Feed the preprocessed images to the network and get prediction scores.</li> <li>Calculate <code>CrossEntropy</code> (loss) between prediction scores and ground truth.</li> <li>Update the model by minimizing <code>CrossEntropy</code>.</li> </ol> <p>For more info about <code>Network</code> and its operators, check out Tutorial 6.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-3-estimator", "title": "Step 3 - Estimator\u00b6", "text": "<p>We define the <code>Estimator</code> to connect the <code>Network</code> to the <code>Pipeline</code>, and compute accuracy as a validation metric. Please see Tutorial 7 for more about <code>Estimator</code> and <code>Traces</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>After training, we can do inferencing on new data with <code>Pipeline.transform</code> and <code>Netowork.transform</code>. Please checkout Tutorial 8 for more details. \\</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>DNN</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html", "title": "Tutorial 2: Creating a FastEstimator dataset", "text": "<p>A Dataset in FastEstimator is a class that wraps raw input data and makes it easier to ingest into your model(s). In this tutorial we will learn about the different ways we can create these Datasets.</p> <p>The FastEstimator Dataset class inherits from the PyTorch Dataset class which provides a clean and efficient interface to load raw data. Thus, any code that you have written for PyTorch will continue to work in FastEstimator too. For a refresher on PyTorch Datasets you can go here.</p> <p>In this tutorial we will focus on two key functionalities that we need to provide for the Dataset class. The first one is the ability to get an individual data entry from the Dataset and the second one is the ability to get the length of the Dataset. This is done as follows:</p> <ul> <li>len(dataset) should return the size (number of samples) of the dataset.</li> <li>dataset[i] should return the i-th sample in the dataset. The return value should be a dictionary with data values keyed by strings.</li> </ul> <p>Let's create a simple PyTorch Dataset which shows this functionality.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom torch.utils.data import Dataset\n\nclass mydataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data['x'].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n\na = {'x': np.random.rand(100,5), 'y': np.random.rand(100)}\nds = mydataset(a)\nprint(ds[0])\nprint(len(ds))\n</pre> import numpy as np from torch.utils.data import Dataset  class mydataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data['x'].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data}  a = {'x': np.random.rand(100,5), 'y': np.random.rand(100)} ds = mydataset(a) print(ds[0]) print(len(ds)) <pre>{'x': array([0.77730671, 0.99536305, 0.30362685, 0.82398129, 0.87116199]), 'y': 0.9211995152006527}\n100\n</pre> <p></p> <p>In this section we will showcase how a Dataset can be created using FastEstimator. This tutorial shows three ways to create Datasets. The first uses data from disk, the second uses data already in memory, and the third uses a generator to create a Dataset.</p> <p></p> <p>In this tutorial we will showcase two ways to create a Dataset from disk:</p> <p></p> <p>To showcase this we will first have to create a dummy directory structure representing the two classes. Then we create a few files in each of the directories. The following image shows the hierarchy of our temporary data directory:</p> <p></p> <p>Let's prepare the data according to the directory structure:</p> In\u00a0[2]: Copied! <pre>import os\nimport tempfile\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\na_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\nb_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n\na1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\")\na2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")\n\nb1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\")\nb2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\")\n</pre> import os import tempfile  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  a_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname) b_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)  a1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\") a2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")  b1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\") b2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\") <p>Once that is done, all you have to do is create a Dataset by passing the dummy directory to the <code>LabeledDirDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[3]: Copied! <pre>dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)  print(dataset[0]) print(len(dataset)) <pre>{'x': '/tmp/tmp4_th3s9a/tmphe1zvp3u/a2.txt', 'y': 1}\n4\n</pre> <p></p> <p>To showcase creating a Dataset based on a CSV file, we now create a dummy CSV file representing information for the two classes. First, let's create the data to be used as input as follows:</p> In\u00a0[4]: Copied! <pre>import os\nimport tempfile\nimport pandas as pd\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\ndata = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]}\ndf = pd.DataFrame(data=data)\ndf.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False)\n</pre> import os import tempfile import pandas as pd  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  data = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]} df = pd.DataFrame(data=data) df.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False) <p>Once that is done you can create a Dataset by passing the CSV to the <code>CSVDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[5]: Copied! <pre>dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))  print(dataset[0]) print(len(dataset)) <pre>{'x': 'a1.txt', 'y': 0}\n4\n</pre> <p></p> <p>It is also possible to create a Dataset from data stored in memory. This may be useful for smaller datasets.</p> <p></p> <p>If you already have data in memory in the form of a Numpy array, it is easy to convert this data into a FastEstimator Dataset. To accomplish this, simply pass your data dictionary into the <code>NumpyDataset</code> class constructor. The following code snippet demonstrates this:</p> In\u00a0[6]: Copied! <pre>import numpy as np\nimport tensorflow as tf\n\nimport fastestimator as fe\n\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n\nprint (train_data[0]['y'])\nprint (len(train_data))\n</pre> import numpy as np import tensorflow as tf  import fastestimator as fe  (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data() train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train}) eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})  print (train_data[0]['y']) print (len(train_data)) <pre>5\n60000\n</pre> <p></p> <p>It is also possible to create a Dataset using generators. As an example, we will first create a generator which will generate random input data for us.</p> In\u00a0[7]: Copied! <pre>import numpy as np\n\ndef inputs():\n    while True:\n        yield {'x': np.random.rand(4), 'y':np.random.randint(2)}\n</pre> import numpy as np  def inputs():     while True:         yield {'x': np.random.rand(4), 'y':np.random.randint(2)} <p>We then pass the generator as an argument to the <code>GeneratorDataset</code> class:</p> In\u00a0[8]: Copied! <pre>from fastestimator.dataset import GeneratorDataset\n\ndataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10)\nprint(dataset[0])\nprint(len(dataset))\n</pre> from fastestimator.dataset import GeneratorDataset  dataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10) print(dataset[0]) print(len(dataset)) <pre>{'x': array([0.15550239, 0.0600738 , 0.29110195, 0.09245787]), 'y': 1}\n10\n</pre> <p></p>"}, {"location": "tutorial/beginner/t02_dataset.html#tutorial-2-creating-a-fastestimator-dataset", "title": "Tutorial 2: Creating a FastEstimator dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover three different ways to create a Dataset using FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Torch Dataset Recap</li> <li>FastEstimator Dataset<ul> <li>Dataset from disk<ul> <li>LabeledDirDataset</li> <li>CSVDataset</li> </ul> </li> <li>Dataset from memory<ul> <li>NumpyDataset</li> </ul> </li> <li>Dataset from generator</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html#torch-dataset-recap", "title": "Torch Dataset Recap\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#fastestimator-dataset", "title": "FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#1-dataset-from-disk", "title": "1. Dataset from disk\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#11-labeleddirdataset", "title": "1.1 LabeledDirDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#12-csvdataset", "title": "1.2 CSVDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#2-dataset-from-memory", "title": "2. Dataset from memory\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#21-numpydataset", "title": "2.1 NumpyDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#3-dataset-from-generator", "title": "3. Dataset from Generator\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNET</li> <li>DCGAN</li> <li>Siamese Networks</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html", "title": "Tutorial 3: Operator", "text": "In\u00a0[1]: Copied! <pre>class Op:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n    \n    def forward(self, data, state):\n        return data\n</pre> class Op:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode          def forward(self, data, state):         return data"}, {"location": "tutorial/beginner/t03_operator.html#tutorial-3-operator", "title": "Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/beginner/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will introduce the <code>Operator</code> - a fundamental building block within FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Operator Definition</li> <li>Operator Structure</li> <li>Operator Expression</li> <li>Deep Learning Examples using Operators</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html#operator-definition", "title": "Operator Definition\u00b6", "text": "<p>From Tutorial 1, we know that the preprocessing in <code>Pipeline</code> and the training in <code>Network</code> can be divided into several sub-tasks:</p> <ul> <li>Pipeline: <code>Expand_dim</code> -&gt; <code>Minmax</code></li> <li>Network: <code>ModelOp</code> -&gt; <code>CrossEntropy</code> -&gt; <code>UpdateOp</code></li> </ul> <p>Each sub-task is a modular unit that takes inputs, performs an operation, and then produces outputs. We therefore call these sub-tasks <code>Operator</code>s, and they form the building blocks of the FastEstimator <code>Pipeline</code> and <code>Network</code> APIs.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-structure", "title": "Operator Structure\u00b6", "text": "<p>An Operator has 3 main components:</p> <ul> <li>inputs: the key(s) of input data</li> <li>outputs: the key(s) of output data</li> <li>forward function: the transformation to be applied</li> </ul> <p>The base class constructor also takes a <code>mode</code> argument, but for now we will ignore it since <code>mode</code> will be discussed extensively in Tutorial 9.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-expression", "title": "Operator Expression\u00b6", "text": "<p>In this section, we will demonstrate how different tasks can be concisely expressed in operators.</p>"}, {"location": "tutorial/beginner/t03_operator.html#single-operator", "title": "Single Operator\u00b6", "text": "<p>If the task only requires taking one feature as input and transforming it to overwrite the old feature (e.g, <code>Minmax</code>), it can be expressed as:</p> <p></p> <p>If the task involves taking multiple features and overwriting them respectively (e.g, rotation of both an image and its mask), it can be expressed as:</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#multiple-operators", "title": "Multiple Operators\u00b6", "text": "<p>If there are two <code>Operator</code>s executing in a sequential manner (e.g, <code>Minmax</code> followed by <code>Transpose</code>), it can be expressed as:</p> <p></p> <p><code>Operator</code>s can also easily handle more complicated data flows:</p> <p></p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#deep-learning-examples-using-operators", "title": "Deep Learning Examples using Operators\u00b6", "text": "<p>In this section, we will show you how deep learning tasks can be modularized into combinations of <code>Operator</code>s. Please note that the <code>Operator</code> expressions we provide in this section are essentially pseudo-code. Links to full python examples are also provided.</p>"}, {"location": "tutorial/beginner/t03_operator.html#image-classification", "title": "Image Classification:\u00b6", "text": "<p>MNIST</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#dc-gan", "title": "DC-GAN:\u00b6", "text": "<p>DC-GAN</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#adversarial-hardening", "title": "Adversarial Hardening:\u00b6", "text": "<p>FGSM</p> <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html", "title": "Tutorial 4: Pipeline", "text": "<p>In Tutorial 2 we demonstrated different ways to construct FastEstimator datasets. Here we will see how datasets can be loaded in the <code>Pipeline</code> and how various operations can then be applied to the data. <code>fe.Pipeline</code> handles three different types of datasets:</p> <ul> <li>tf.data.Dataset</li> <li>torch.data.Dataloader</li> <li>fe.dataset</li> </ul> <p>Let's create an example <code>tf.data.Dataset</code> and <code>torch.data.Dataloader</code> from numpy arrays and we will load them into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Make some random data to serve as the source for our datasets\nx_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np  # Make some random data to serve as the source for our datasets x_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1))) train_data = {\"x\": x_train, \"y\": y_train} <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nimport tensorflow as tf\n\n# Create a tf.data.Dataset from sample data\ndataset_tf = tf.data.Dataset.from_tensor_slices(train_data)\ndataset_tf = dataset_tf.batch(4)\n\n# Load data into the pipeline\npipeline_tf = fe.Pipeline(dataset_tf)\n</pre> import fastestimator as fe import tensorflow as tf  # Create a tf.data.Dataset from sample data dataset_tf = tf.data.Dataset.from_tensor_slices(train_data) dataset_tf = dataset_tf.batch(4)  # Load data into the pipeline pipeline_tf = fe.Pipeline(dataset_tf) <p></p> <p>We will create a custom dataset class to load our train data into a PyTorch DataLoader.</p> In\u00a0[3]: Copied! <pre>from torch.utils.data import Dataset\n\nclass TorchCustomDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data[\"x\"].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n</pre> from torch.utils.data import Dataset  class TorchCustomDataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data[\"x\"].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data} In\u00a0[4]: Copied! <pre>import torch\nfrom torch.utils import data\n\n# Create a torch.data.Dataloader from sample data\ndataset_torch = TorchCustomDataset(train_data)\ndataloader_torch = data.DataLoader(dataset_torch, batch_size=4)\n\n# Load data into the pipeline\npipeline_torch = fe.Pipeline(dataloader_torch)\n</pre> import torch from torch.utils import data  # Create a torch.data.Dataloader from sample data dataset_torch = TorchCustomDataset(train_data) dataloader_torch = data.DataLoader(dataset_torch, batch_size=4)  # Load data into the pipeline pipeline_torch = fe.Pipeline(dataloader_torch) <p></p> <p>Next, we will see how to use one of the Fastestimator Datasets in the <code>Pipeline</code>. We will create <code>fe.dataset.NumpyDataset</code> and load it into our pipeline. As we saw in Tutorial 2, <code>NumpyDataset</code> takes a dictionary with keys for the input data and ground truth labels.</p> In\u00a0[5]: Copied! <pre>from fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# Create a NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1)\n</pre> from fastestimator.dataset.numpy_dataset import NumpyDataset  # Create a NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1) <p></p> <p>After loading the data or performing preprocessing tasks, you might want to inspect the data in the <code>Pipeline</code> and ensure the output of the <code>Pipeline</code> is as you expected. <code>fe.Pipeline.get_results</code> provides this feature:</p> In\u00a0[6]: Copied! <pre>pipeline_tf.get_results(num_steps=1)\n</pre> pipeline_tf.get_results(num_steps=1) Out[6]: <pre>{'x': &lt;tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n array([[0.63098872, 0.02492519],\n        [0.20859418, 0.39825037],\n        [0.23939722, 0.21990976],\n        [0.70966992, 0.80645352]])&gt;,\n 'y': &lt;tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n array([[0.68909166],\n        [0.66729607],\n        [0.27104117],\n        [0.3786991 ]])&gt;}</pre> <p></p> <p>In Tutorial 3, we learned about <code>Operators</code> and their structure. They are used in FastEstimator for constructing workflow graphs. Here we will talk specifically about Numpy Operators (<code>NumpyOp</code>s) and how to use them in <code>Pipeline</code>.</p> <p><code>NumpyOp</code>s form the foundation of FastEstimator data augmentation within the <code>Pipeline</code>, and inherit from the <code>Op</code> base class. They perform preprocessing and augmentation tasks on non-Tensor data. With a list of <code>NumpyOp</code>s, even complicated preprocessing tasks can be implemented in only a few lines of code. Many of the augmentation operations in FastEstimator leverage the image augmentation library albumentations.</p> <p><code>NumpyOp</code> can be further subdivided into three main categories:</p> <ul> <li>Univariate <code>NumpyOp</code>s</li> <li>Multivariate <code>NumpyOp</code>s</li> <li>Meta <code>NumpyOp</code>s</li> </ul> <p>In addition to the pre-built offerings, we can customize the <code>NumpyOp</code> to perform our own operations on the data. By inheriting <code>fe.op.numpyop</code> we can create custom <code>NumpyOp</code>s and use them in our <code>Pipeline</code>. In this tutorial, we will learn about Univariate, Multivariate and Custom Numpy Operators. We will discuss Meta NumpyOp's an advanced tutorial.</p> <p>To demonstrate use of operators, we will first load the Fashion MNIST dataset in our Pipeline and then will define list of Numpy Operators for preprocessing data. We will then visualize the <code>Pipeline</code>s inputs and outputs.</p> In\u00a0[7]: Copied! <pre>from fastestimator.dataset.data import mnist\n\nmnist_train, mnist_eval = mnist.load_data()\n</pre> from fastestimator.dataset.data import mnist  mnist_train, mnist_eval = mnist.load_data() <p></p> <p>Univariate Numpy Operators perform the same operation for all input features. They take one or more input(s) and return an equal number of outputs, applying the same transformation to each input/output pair. For example, <code>Minmax</code> is an univariate Numpy Operator. No matter what feature it is given, it will perform:</p> <p>data = (data - min) / (max - min)</p> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax\n\nminmax_op = Minmax(inputs=\"x\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop.univariate import Minmax  minmax_op = Minmax(inputs=\"x\", outputs=\"x_out\") <p></p> <p>Multivariate Numpy Operators perform different operations based on the nature of the input features. For example, if you have an image with an associated mask as well as bounding boxes, rotating all three of these objects together requires the backend code to know which of the inputs is an image and which is a bounding box. Here we will demonstrate the <code>Rotate</code> Numpy Operator which will rotate images randomly by some angle in the range (-60, 60) degrees.</p> In\u00a0[9]: Copied! <pre>from fastestimator.op.numpyop.multivariate import Rotate\n\nrotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60)\n</pre> from fastestimator.op.numpyop.multivariate import Rotate  rotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60) <p></p> <p>Let's create custom Numpy Operator that adds random noise to the input images.</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddRandomNoise(NumpyOp):\n    def forward(self, data, state):\n        # generate noise array with 0 mean and 0.1 standard deviation\n        noise = np.random.normal(0, 0.1, data.shape)\n        data = data + noise\n        return data\n    \nrandom_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddRandomNoise(NumpyOp):     def forward(self, data, state):         # generate noise array with 0 mean and 0.1 standard deviation         noise = np.random.normal(0, 0.1, data.shape)         data = data + noise         return data      random_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\") <p></p> <p>Now, let's add our <code>NumpyOp</code>s into the <code>Pipeline</code> and visualize the results.</p> In\u00a0[11]: Copied! <pre>pipeline = fe.Pipeline(train_data=mnist_train,\n                       eval_data=mnist_eval,\n                       ops=[minmax_op, rotation_op, random_noise_op],\n                       batch_size=3)\n\ndata = pipeline.get_results()\nimg = fe.util.ImgData(original_image=data[\"x\"], pipeline_output=data[\"x_out\"])\nfig = img.paint_figure()\n</pre> pipeline = fe.Pipeline(train_data=mnist_train,                        eval_data=mnist_eval,                        ops=[minmax_op, rotation_op, random_noise_op],                        batch_size=3)  data = pipeline.get_results() img = fe.util.ImgData(original_image=data[\"x\"], pipeline_output=data[\"x_out\"]) fig = img.paint_figure() <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html#tutorial-4-pipeline", "title": "Tutorial 4: Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following:</p> <ul> <li>Loading data into a <code>Pipeline</code><ul> <li>Using tf.data.Dataset</li> <li>Using torch.Dataloader</li> <li>Using FastEstimator Datasets</li> </ul> </li> <li>Getting results from a <code>Pipeline</code></li> <li>How to use Numpy Operators in a <code>Pipeline</code><ul> <li>Univariate Numpy Operators</li> <li>Multivariate Numpy Operators</li> <li>Customized Numpy Operators</li> <li>Visualizing <code>Pipeline</code> Output</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>In deep learning, data preprocessing is a way of converting data from its raw form to a more usable or desired representation. It is one crucial step in model training as it directly impacts the ability of model to learn. In FastEstimator, the <code>Pipeline</code> API enables such preprocessing tasks in an efficient manner. The <code>Pipeline</code> manages everything from  extracting data from the disk up until it is fed into the model. <code>Pipeline</code> operations usually happen on the CPU.</p>"}, {"location": "tutorial/beginner/t04_pipeline.html#loading-data-into-a-pipeline", "title": "Loading data into a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-tfdatadataset", "title": "Using tf.data.Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-torchdatadataloader", "title": "Using torch.data.Dataloader\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-a-fastestimator-dataset", "title": "Using a FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#getting-results-from-a-pipeline", "title": "Getting results from a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-numpy-operators-in-pipeline", "title": "Using Numpy Operators in Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#univariate-numpyop", "title": "Univariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#multivariate-numpyop", "title": "Multivariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#custom-numpyop", "title": "Custom NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#visualizing-pipeline-outputs", "title": "Visualizing Pipeline Outputs\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> <li>Bert</li> <li>FGSM</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html", "title": "Tutorial 5: Model", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef my_model_tf(input_shape=(30, ), num_classes=2):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n    return model\n\nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\")\n</pre> import fastestimator as fe import tensorflow as tf from tensorflow.keras import layers  def my_model_tf(input_shape=(30, ), num_classes=2):     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))     return model  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\") In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\n\nclass my_model_torch(nn.Module):\n    def __init__(self, num_inputs=30, num_classes=2):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(num_inputs, 32), \n                                    nn.ReLU(inplace=True), \n                                    nn.Linear(32, 8), \n                                    nn.ReLU(inplace=True),\n                                    nn.Linear(8, num_classes))\n\n    def forward(self, x):\n        x = self.layers(x)\n        x_label = torch.softmax(x, dim=-1)\n        return x_label\n\n    \nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as fn  class my_model_torch(nn.Module):     def __init__(self, num_inputs=30, num_classes=2):         super().__init__()         self.layers = nn.Sequential(nn.Linear(num_inputs, 32),                                      nn.ReLU(inplace=True),                                      nn.Linear(32, 8),                                      nn.ReLU(inplace=True),                                     nn.Linear(8, num_classes))      def forward(self, x):         x = self.layers(x)         x_label = torch.softmax(x, dim=-1)         return x_label       model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\") In\u00a0[3]: Copied! <pre>from fastestimator.architecture.pytorch import LeNet\n# from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.pytorch import LeNet # from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[4]: Copied! <pre>resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\")\n</pre> resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\") In\u00a0[5]: Copied! <pre>from torchvision import models\n\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\")\n</pre> from torchvision import models  resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\") In\u00a0[6]: Copied! <pre># TensorFlow \nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n\n# PyTorch\nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4))\n</pre> # TensorFlow  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))  # PyTorch model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4)) <p>If a model function returns multiple models, a list of optimizers can be provided. See the pggan apphub for an example with multiple models and optimizers.</p> <p></p> In\u00a0[7]: Copied! <pre>import os\nimport tempfile\n\nmodel_dir = tempfile.mkdtemp()\n\n# TensorFlow\nfe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")\n\n# PyTorch\nfe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\")\n</pre> import os import tempfile  model_dir = tempfile.mkdtemp()  # TensorFlow fe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")  # PyTorch fe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\") Out[7]: <pre>'/tmp/tmp7iizhyd7/resnet50_torch.pt'</pre> In\u00a0[8]: Copied! <pre># TensorFlow\nresnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None), \n                       optimizer_fn=\"adam\", \n                       weights_path=os.path.join(model_dir, \"resnet50_tf.h5\"))\n</pre> # TensorFlow resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None),                         optimizer_fn=\"adam\",                         weights_path=os.path.join(model_dir, \"resnet50_tf.h5\")) In\u00a0[9]: Copied! <pre># PyTorch\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False), \n                          optimizer_fn=\"adam\", \n                          weights_path=os.path.join(model_dir, \"resnet50_torch.pt\"))\n</pre> # PyTorch resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False),                            optimizer_fn=\"adam\",                            weights_path=os.path.join(model_dir, \"resnet50_torch.pt\")) <p></p> In\u00a0[10]: Copied! <pre>model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\nprint(\"Model Name: \", model.model_name)\n</pre> model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\") print(\"Model Name: \", model.model_name) <pre>Model Name:  LeNet\n</pre> <p>If a model function returns multiple models, a list of model_names can be given. See the pggan apphub for an illustration with multiple models and model names.</p> <p></p>"}, {"location": "tutorial/beginner/t05_model.html#tutorial-5-model", "title": "Tutorial 5: Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will cover:</p> <ul> <li>Instantiating and Compiling a Model</li> <li>The Model Function<ul> <li>Custom Models</li> <li>FastEstimator Models</li> <li>Pre-Trained Models</li> </ul> </li> <li>The Optimizer Function</li> <li>Loading Model Weights</li> <li>Specifying a Model Name</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#instantiating-and-compiling-a-model", "title": "Instantiating and Compiling a model\u00b6", "text": "<p>We need to specify two things to instantiate and compile a model:</p> <ul> <li>model_fn</li> <li>optimizer_fn</li> </ul> <p>Model definitions can be implemented in Tensorflow or Pytorch and instantiated by calling <code>fe.build</code> which constructs a model instance and associates it with the specified optimizer.</p>"}, {"location": "tutorial/beginner/t05_model.html#model-function", "title": "Model Function\u00b6", "text": "<p><code>model_fn</code> should be a function/lambda function which returns either a <code>tf.keras.Model</code> or <code>torch.nn.Module</code>. FastEstimator provides several ways to specify the model architecture:</p> <ul> <li>Custom model architecture</li> <li>Importing a pre-built model architecture from FastEstimator</li> <li>Importing pre-trained models/architectures from PyTorch or TensorFlow</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#custom-model-architecture", "title": "Custom model architecture\u00b6", "text": "<p>Let's create a custom model in TensorFlow and PyTorch for demonstration.</p>"}, {"location": "tutorial/beginner/t05_model.html#tfkerasmodel", "title": "tf.keras.Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#torchnnmodule", "title": "torch.nn.Module\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#importing-model-architecture-from-fastestimator", "title": "Importing model architecture from FastEstimator\u00b6", "text": "<p>Below we import a PyTorch LeNet architecture from FastEstimator. See our Architectures folder for a full list of the architectures provided by FastEstimator.</p>"}, {"location": "tutorial/beginner/t05_model.html#importing-pre-trained-modelsarchitectures-from-pytorch-or-tensorflow", "title": "Importing pre-trained models/architectures from PyTorch or TensorFlow\u00b6", "text": "<p>Below we show how to define a model function using a pre-trained resnet model provided by TensorFlow and PyTorch respectively. We load the pre-trained models using a lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-tfkerasapplications", "title": "Pre-trained model from tf.keras.applications\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-torchvision", "title": "Pre-trained model from torchvision\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#optimizer-function", "title": "Optimizer function\u00b6", "text": "<p><code>optimizer_fn</code> can be a string or lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-string", "title": "Optimizer from String\u00b6", "text": "<p>Specifying a string for the <code>optimizer_fn</code> loads the optimizer with default parameters. The optimizer strings accepted by FastEstimator are as follows:</p> <ul> <li>Adadelta: 'adadelta'</li> <li>Adagrad: 'adagrad'</li> <li>Adam: 'adam'</li> <li>Adamax: 'adamax'</li> <li>RMSprop: 'rmsprop'</li> <li>SGD: 'sgd'</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-function", "title": "Optimizer from Function\u00b6", "text": "<p>To specify specific values for the optimizer learning rate or other parameters, we need to pass a lambda function to the <code>optimizer_fn</code>.</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-model-weights", "title": "Loading model weights\u00b6", "text": "<p>We often need to load the weights of a saved model. Model weights can be loaded by specifying the path of the saved weights using the <code>weights_path</code> parameter. Let's use the resnet models created earlier to showcase this.</p>"}, {"location": "tutorial/beginner/t05_model.html#saving-model-weights", "title": "Saving model weights\u00b6", "text": "<p>Here, we create a temporary directory and use FastEstimator backend to save the weights of our previously created resnet50 models:</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-weights-for-tensorflow-and-pytorch-models", "title": "Loading weights for TensorFlow and PyTorch models\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#specifying-a-model-name", "title": "Specifying a Model Name\u00b6", "text": "<p>The name of a model can be specified using the <code>model_name</code> parameter. The name of the model is helpful in distinguishing models when multiple are present.</p>"}, {"location": "tutorial/beginner/t05_model.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PG-GAN</li> <li>Uncertainty Weighted Loss</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html", "title": "Tutorial 6: Network", "text": "<p>As the figure shows, models (orange) are only piece of a <code>Network</code>. It also includes other operations such as loss computation (blue) and update rules (green) that will be used during the training process.</p> <p></p> <p>A <code>Network</code> is composed of basic units called <code>TensorOps</code>. All of the building blocks inside a <code>Network</code> should derive from the <code>TensorOp</code> base class. A <code>TensorOp</code> is a kind of <code>Op</code> and therefore follows the same rules described in Tutorial 3.</p> <p></p> <p>There are some common <code>TensorOp</code> classes we would like to specially mention because of their prevalence:</p> <p></p> <p></p> In\u00a0[1]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return tf.reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class ReduceMean(TensorOp):     def forward(self, data, state):         return tf.reduce_mean(data) <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport torch\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return torch.mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import torch  class ReduceMean(TensorOp):     def forward(self, data, state):         return torch.mean(data) <p></p> In\u00a0[3]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.backend import reduce_mean\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.backend import reduce_mean  class ReduceMean(TensorOp):     def forward(self, data, state):         return reduce_mean(data) <p></p>"}, {"location": "tutorial/beginner/t06_network.html#tutorial-6-network", "title": "Tutorial 6: Network\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li><code>Network</code> Scope</li> <li><code>TensorOp</code> and its Children</li> <li>How to Customize a <code>TensorOp</code><ul> <li>TensorFlow</li> <li>PyTorch</li> <li>fe.backend</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html#network-scope", "title": "Network Scope\u00b6", "text": "<p><code>Network</code> is one of the three main FastestEstimator APIs that defines not only a neural network model but also all of the operations to be performed on it. This can include the deep-learning model itself, loss calculations, model updating rules, and any other functionality that you wish to execute within a GPU.</p> <p>Here we show two <code>Network</code> example graphs to enhance the concept:</p>"}, {"location": "tutorial/beginner/t06_network.html#tensorop-and-its-children", "title": "TensorOp and its Children\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#modelop", "title": "ModelOp\u00b6", "text": "<p>Any model instance created from <code>fe.build</code> (see Tutorial 5) needs to be packaged as a <code>ModelOp</code> such that it can interact with other components inside the <code>Network</code> API. The orange blocks in the first figure are <code>ModelOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#updateop", "title": "UpdateOp\u00b6", "text": "<p>FastEstimator use <code>UpdateOp</code> to associate the model with its loss. Unlike other <code>Ops</code> that use <code>inputs</code> and <code>outputs</code> for expressing their connections, <code>UpdateOp</code> uses the arguments <code>loss</code>, and <code>model</code> instead. The green blocks in the first figure are <code>UpdateOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#others-loss-gradient-meta-etc", "title": "Others (loss, gradient, meta, etc.)\u00b6", "text": "<p>There are many ready-to-use <code>TensorOps</code> that users can directly import from <code>fe.op.tensorop</code>. Some examples include loss and gradient computation ops. There is also a category of <code>TensorOp</code> called <code>MetaOp</code>, which takes other Ops as input and generates more complex execution graphs (see Advanced Tutorial 9).</p> <p>For all available Ops please check out the FastEstimator API.</p>"}, {"location": "tutorial/beginner/t06_network.html#customize-a-tensorop", "title": "Customize a TensorOp\u00b6", "text": "<p>FastEstimator provides flexibility that allows users to customize their own <code>TensorOp</code>s by wrapping TensorFlow or PyTorch library calls, or by leveraging <code>fe.backend</code> API functions. Users only need to inherit the <code>TensorOp</code> class and overwrite its <code>forward</code> function.</p> <p>If you want to customize a <code>TensorOp</code> by directly leveraging API calls from TensorFlow or PyTorch, please make sure that all of the <code>TensorOp</code>s in the <code>Network</code> are backend-consistent. In other words, you cannot have <code>TensorOp</code>s built specifically for TensorFlow and PyTorch in the same <code>Network</code>. Note that the <code>ModelOp</code> backend is determined by which library the model function uses, and so must be consistent with any custom <code>TensorOp</code> that you write.</p> <p>Here we are going to demonstrate how to build a <code>TenorOp</code> that takes high dimensional inputs and returns an average scalar value. For more advanced tutorial of customizing a <code>TensorOp</code> please check out Advanced Tutorial 3.</p>"}, {"location": "tutorial/beginner/t06_network.html#example-using-tensorflow", "title": "Example Using TensorFlow\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-pytorch", "title": "Example Using PyTorch\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-febackend", "title": "Example Using <code>fe.backend</code>\u00b6", "text": "<p>You don't need to worry about backend consistency if you import a FastEstimator-provided <code>TensorOp</code>, or customize your <code>TenosorOp</code> using the <code>fe.backend</code> API. FastEstimator auto-magically handles everything for you.</p>"}, {"location": "tutorial/beginner/t06_network.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>DC-GAN</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html", "title": "Tutorial 7: Estimator", "text": "<p><code>Estimator</code> is the API that manages everything related to the training loop. It combines <code>Pipeline</code> and <code>Network</code> together and provides users with fine-grain control over the training loop. Before we demonstrate different ways to control the training loop let's define a template similar to tutorial 1, but this time we will use a PyTorch model.</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.pytorch import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nimport tempfile\n\ndef get_estimator(log_steps=100, monitor_names=None, use_trace=False, max_train_steps_per_epoch=None, epochs=2):\n    # step 1\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])\n    # step 2\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    # step 3\n    traces = None\n    if use_trace:\n        traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), \n                  BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             max_train_steps_per_epoch=max_train_steps_per_epoch,\n                             log_steps=log_steps,\n                             monitor_names=monitor_names)\n    return estimator\n</pre> import fastestimator as fe from fastestimator.architecture.pytorch import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp import tempfile  def get_estimator(log_steps=100, monitor_names=None, use_trace=False, max_train_steps_per_epoch=None, epochs=2):     # step 1     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])     # step 2     model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     # step 3     traces = None     if use_trace:         traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                    BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              max_train_steps_per_epoch=max_train_steps_per_epoch,                              log_steps=log_steps,                              monitor_names=monitor_names)     return estimator <p>Let's train our model using the default <code>Estimator</code> arguments:</p> In\u00a0[2]: Copied! <pre>est = get_estimator()\nest.fit()\n</pre> est = get_estimator() est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.295395; \nFastEstimator-Train: step: 100; ce: 0.8820845; steps/sec: 320.73; \nFastEstimator-Train: step: 200; ce: 0.37291068; steps/sec: 319.45; \nFastEstimator-Train: step: 300; ce: 0.06651708; steps/sec: 309.93; \nFastEstimator-Train: step: 400; ce: 0.21876352; steps/sec: 309.78; \nFastEstimator-Train: step: 500; ce: 0.08403016; steps/sec: 309.19; \nFastEstimator-Train: step: 600; ce: 0.35541984; steps/sec: 308.78; \nFastEstimator-Train: step: 700; ce: 0.06964149; steps/sec: 300.41; \nFastEstimator-Train: step: 800; ce: 0.13983297; steps/sec: 309.22; \nFastEstimator-Train: step: 900; ce: 0.037845124; steps/sec: 312.94; \nFastEstimator-Train: step: 1000; ce: 0.13029681; steps/sec: 316.27; \nFastEstimator-Train: step: 1100; ce: 0.022184685; steps/sec: 312.62; \nFastEstimator-Train: step: 1200; ce: 0.039918672; steps/sec: 315.24; \nFastEstimator-Train: step: 1300; ce: 0.05553157; steps/sec: 313.87; \nFastEstimator-Train: step: 1400; ce: 0.0021400168; steps/sec: 343.22; \nFastEstimator-Train: step: 1500; ce: 0.07833527; steps/sec: 336.56; \nFastEstimator-Train: step: 1600; ce: 0.09543828; steps/sec: 324.81; \nFastEstimator-Train: step: 1700; ce: 0.14825855; steps/sec: 318.52; \nFastEstimator-Train: step: 1800; ce: 0.01032154; steps/sec: 322.95; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 5.99 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.06244616; \nFastEstimator-Train: step: 1900; ce: 0.015050106; steps/sec: 263.35; \nFastEstimator-Train: step: 2000; ce: 0.003486173; steps/sec: 295.2; \nFastEstimator-Train: step: 2100; ce: 0.06401425; steps/sec: 310.64; \nFastEstimator-Train: step: 2200; ce: 0.008118075; steps/sec: 297.08; \nFastEstimator-Train: step: 2300; ce: 0.05136842; steps/sec: 289.31; \nFastEstimator-Train: step: 2400; ce: 0.10011706; steps/sec: 290.44; \nFastEstimator-Train: step: 2500; ce: 0.007041894; steps/sec: 287.94; \nFastEstimator-Train: step: 2600; ce: 0.041005336; steps/sec: 301.21; \nFastEstimator-Train: step: 2700; ce: 0.0023359149; steps/sec: 311.66; \nFastEstimator-Train: step: 2800; ce: 0.034970395; steps/sec: 278.47; \nFastEstimator-Train: step: 2900; ce: 0.024958389; steps/sec: 294.08; \nFastEstimator-Train: step: 3000; ce: 0.0038549905; steps/sec: 291.1; \nFastEstimator-Train: step: 3100; ce: 0.14712071; steps/sec: 311.67; \nFastEstimator-Train: step: 3200; ce: 0.14290668; steps/sec: 316.4; \nFastEstimator-Train: step: 3300; ce: 0.34252185; steps/sec: 304.94; \nFastEstimator-Train: step: 3400; ce: 0.0059393854; steps/sec: 297.43; \nFastEstimator-Train: step: 3500; ce: 0.2493474; steps/sec: 323.9; \nFastEstimator-Train: step: 3600; ce: 0.004362625; steps/sec: 322.78; \nFastEstimator-Train: step: 3700; ce: 0.0058870725; steps/sec: 296.6; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 6.31 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.053535815; \nFastEstimator-Finish: step: 3750; total_time: 14.81 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[3]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4)\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.311253; \nFastEstimator-Train: step: 100; ce: 0.66614795; steps/sec: 274.95; \nFastEstimator-Train: step: 200; ce: 0.46526748; steps/sec: 309.99; \nFastEstimator-Train: step: 300; ce: 0.11188476; steps/sec: 336.57; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 0.99 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.17669827; \nFastEstimator-Train: step: 400; ce: 0.2917202; steps/sec: 332.81; \nFastEstimator-Train: step: 500; ce: 0.047290877; steps/sec: 323.56; \nFastEstimator-Train: step: 600; ce: 0.053344093; steps/sec: 315.52; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 0.93 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.11926653; \nFastEstimator-Train: step: 700; ce: 0.06439964; steps/sec: 300.28; \nFastEstimator-Train: step: 800; ce: 0.026502458; steps/sec: 300.13; \nFastEstimator-Train: step: 900; ce: 0.34012184; steps/sec: 303.24; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 1.0 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.075678065; \nFastEstimator-Train: step: 1000; ce: 0.044892587; steps/sec: 285.03; \nFastEstimator-Train: step: 1100; ce: 0.037321247; steps/sec: 293.51; \nFastEstimator-Train: step: 1200; ce: 0.011022182; steps/sec: 294.91; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 1.03 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.06031439; \nFastEstimator-Finish: step: 1200; total_time: 9.29 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=0)\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=0) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 0; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.16322972; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.10085282; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.08177921; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.0629242; \nFastEstimator-Finish: step: 1200; total_time: 9.14 sec; LeNet_lr: 0.001; \n</pre> <p></p> In\u00a0[5]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\")\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\") est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 150; \nFastEstimator-Train: step: 1; ce: 2.2930875; ce1: 2.2930875; \nFastEstimator-Train: step: 150; ce: 0.29343712; ce1: 0.29343712; steps/sec: 292.71; \nFastEstimator-Train: step: 300; ce: 0.37277684; ce1: 0.37277684; steps/sec: 284.64; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 1.05 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.21327984; ce1: 0.21327984; \nFastEstimator-Train: step: 450; ce: 0.3631664; ce1: 0.3631664; steps/sec: 277.43; \nFastEstimator-Train: step: 600; ce: 0.2957161; ce1: 0.2957161; steps/sec: 304.11; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 1.03 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.10858435; ce1: 0.10858435; \nFastEstimator-Train: step: 750; ce: 0.1193773; ce1: 0.1193773; steps/sec: 301.03; \nFastEstimator-Train: step: 900; ce: 0.05718822; ce1: 0.05718822; steps/sec: 294.92; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 1.01 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.093043245; ce1: 0.093043245; \nFastEstimator-Train: step: 1050; ce: 0.102503434; ce1: 0.102503434; steps/sec: 297.27; \nFastEstimator-Train: step: 1200; ce: 0.011180073; ce1: 0.011180073; steps/sec: 296.55; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 1.01 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.082674295; ce1: 0.082674295; \nFastEstimator-Finish: step: 1200; total_time: 9.62 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see, both <code>ce</code> and <code>ce1</code> showed up in the log above. Unsurprisingly, their values are identical because because they have the same inputs and forward function.</p> <p></p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class Trace:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n\n    def on_begin(self, data):\n\"\"\"Runs once at the beginning of training\"\"\"\n\n    def on_epoch_begin(self, data):\n\"\"\"Runs at the beginning of each epoch\"\"\"\n\n    def on_batch_begin(self, data):\n\"\"\"Runs at the beginning of each batch\"\"\"\n\n    def on_batch_end(self, data):\n\"\"\"Runs at the end of each batch\"\"\"\n\n    def on_epoch_end(self, data):\n\"\"\"Runs at the end of each epoch\"\"\"\n\n    def on_end(self, data):\n\"\"\"Runs once at the end training\"\"\"\n</pre> class Trace:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode      def on_begin(self, data):         \"\"\"Runs once at the beginning of training\"\"\"      def on_epoch_begin(self, data):         \"\"\"Runs at the beginning of each epoch\"\"\"      def on_batch_begin(self, data):         \"\"\"Runs at the beginning of each batch\"\"\"      def on_batch_end(self, data):         \"\"\"Runs at the end of each batch\"\"\"      def on_epoch_end(self, data):         \"\"\"Runs at the end of each epoch\"\"\"      def on_end(self, data):         \"\"\"Runs once at the end training\"\"\" <p>Given the structure, users can customize their own functions at different stages and insert them into the training loop. We will leave the customization of <code>Traces</code> to the advanced tutorial. For now, let's use some pre-built <code>Traces</code> from FastEstimator.</p> <p>During the training loop in our earlier example, we want 2 things to happen:</p> <ol> <li>Save the model weights if the evaluation loss is the best we have seen so far</li> <li>Calculate the model accuracy during evaluation</li> </ol> <p></p> In\u00a0[7]: Copied! <pre>from fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\nest = get_estimator(use_trace=True)\nest.fit()\n</pre> from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  est = get_estimator(use_trace=True) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.303516; \nFastEstimator-Train: step: 100; ce: 1.004676; steps/sec: 286.36; \nFastEstimator-Train: step: 200; ce: 0.49630624; steps/sec: 286.83; \nFastEstimator-Train: step: 300; ce: 0.12231735; steps/sec: 291.9; \nFastEstimator-Train: step: 400; ce: 0.14592598; steps/sec: 315.7; \nFastEstimator-Train: step: 500; ce: 0.25857; steps/sec: 326.27; \nFastEstimator-Train: step: 600; ce: 0.13771628; steps/sec: 331.77; \nFastEstimator-Train: step: 700; ce: 0.38054478; steps/sec: 301.89; \nFastEstimator-Train: step: 800; ce: 0.07086247; steps/sec: 291.58; \nFastEstimator-Train: step: 900; ce: 0.16959156; steps/sec: 308.7; \nFastEstimator-Train: step: 1000; ce: 0.021332668; steps/sec: 324.17; \nFastEstimator-Train: step: 1100; ce: 0.055990797; steps/sec: 287.57; \nFastEstimator-Train: step: 1200; ce: 0.2849428; steps/sec: 292.77; \nFastEstimator-Train: step: 1300; ce: 0.20509654; steps/sec: 288.14; \nFastEstimator-Train: step: 1400; ce: 0.08241908; steps/sec: 321.32; \nFastEstimator-Train: step: 1500; ce: 0.024668839; steps/sec: 320.73; \nFastEstimator-Train: step: 1600; ce: 0.01093893; steps/sec: 325.12; \nFastEstimator-Train: step: 1700; ce: 0.012216274; steps/sec: 330.77; \nFastEstimator-Train: step: 1800; ce: 0.01524183; steps/sec: 328.2; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 6.15 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmplhuyv721/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.048887283; accuracy: 0.9814; since_best_accuracy: 0; max_accuracy: 0.9814; \nFastEstimator-Train: step: 1900; ce: 0.0056912354; steps/sec: 267.68; \nFastEstimator-Train: step: 2000; ce: 0.06863687; steps/sec: 312.62; \nFastEstimator-Train: step: 2100; ce: 0.071683794; steps/sec: 319.51; \nFastEstimator-Train: step: 2200; ce: 0.023103738; steps/sec: 313.75; \nFastEstimator-Train: step: 2300; ce: 0.011231604; steps/sec: 315.5; \nFastEstimator-Train: step: 2400; ce: 0.17630634; steps/sec: 310.87; \nFastEstimator-Train: step: 2500; ce: 0.01526911; steps/sec: 315.78; \nFastEstimator-Train: step: 2600; ce: 0.06935612; steps/sec: 310.69; \nFastEstimator-Train: step: 2700; ce: 0.14090665; steps/sec: 308.39; \nFastEstimator-Train: step: 2800; ce: 0.0023762842; steps/sec: 309.23; \nFastEstimator-Train: step: 2900; ce: 0.025511805; steps/sec: 309.84; \nFastEstimator-Train: step: 3000; ce: 0.094952986; steps/sec: 318.57; \nFastEstimator-Train: step: 3100; ce: 0.011754904; steps/sec: 299.48; \nFastEstimator-Train: step: 3200; ce: 0.033963054; steps/sec: 303.24; \nFastEstimator-Train: step: 3300; ce: 0.013373202; steps/sec: 317.35; \nFastEstimator-Train: step: 3400; ce: 0.064900294; steps/sec: 295.58; \nFastEstimator-Train: step: 3500; ce: 0.29719537; steps/sec: 307.13; \nFastEstimator-Train: step: 3600; ce: 0.185368; steps/sec: 307.28; \nFastEstimator-Train: step: 3700; ce: 0.005988597; steps/sec: 278.04; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 6.19 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmplhuyv721/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03341377; accuracy: 0.9896; since_best_accuracy: 0; max_accuracy: 0.9896; \nFastEstimator-Finish: step: 3750; total_time: 14.96 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see from the log, the model is saved in a predefined location and the accuracy is displayed during evaluation.</p> <p></p> In\u00a0[8]: Copied! <pre>est.test()\n</pre> est.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.9894; \n</pre> <p>This will feed all of your test dataset through the <code>Pipeline</code> and <code>Network</code>, and finally execute the traces (in our case, compute accuracy) just like during the training.</p> <p></p>"}, {"location": "tutorial/beginner/t07_estimator.html#tutorial-7-estimator", "title": "Tutorial 7: Estimator\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Estimator API<ul> <li>Reducing the number of training steps per epoch</li> <li>Reducing the number of evaluation steps per epoch</li> <li>Changing logging behavior</li> <li>Monitoring intermediate results during training</li> </ul> </li> <li>Trace<ul> <li>Concept</li> <li>Structure</li> <li>Usage</li> </ul> </li> <li>Model Testing</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html#estimator-api", "title": "Estimator API\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-training-steps-per-epoch", "title": "Reduce the number of training steps per epoch\u00b6", "text": "<p>In general, one epoch of training means that every element in the training dataset will be visited exactly one time. If evaluation data is available, evaluation happens after every epoch by default. Consider the following two scenarios:</p> <ul> <li>The training dataset is very large such that evaluation needs to happen multiple times during one epoch.</li> <li>Different training datasets are being used for different epochs, but the number of training steps should be consistent between each epoch.</li> </ul> <p>One easy solution to the above scenarios is to limit the number of training steps per epoch. For example, if we want to train for only 300 steps per epoch, with training lasting for 4 epochs (1200 steps total), we would do the following:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-evaluation-steps-per-epoch", "title": "Reduce the number of evaluation steps per epoch\u00b6", "text": "<p>One may need to reduce the number of evaluation steps for debugging purpose. This can be easily done by setting the <code>max_eval_steps_per_epoch</code> argument in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#change-logging-behavior", "title": "Change logging behavior\u00b6", "text": "<p>When the number of training epochs is large, the log can become verbose. You can change the logging behavior by choosing one of following options:</p> <ul> <li>set <code>log_steps</code> to <code>None</code> if you do not want to see any training logs printed.</li> <li>set <code>log_steps</code> to 0 if you only wish to see the evaluation logs.</li> <li>set <code>log_steps</code> to some integer 'x' if you want training logs to be printed every 'x' steps.</li> </ul> <p>Let's set the <code>log_steps</code> to 0:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#monitor-intermediate-results", "title": "Monitor intermediate results\u00b6", "text": "<p>You might have noticed that in our example <code>Network</code> there is an op: <code>CrossEntropy(inputs=(\"y_pred\", \"y\") outputs=\"ce1\")</code>. However, the <code>ce1</code> never shows up in the training log above. This is because FastEstimator identifies and filters out unused variables to reduce unnecessary communication between the GPU and CPU. On the contrary, <code>ce</code> shows up in the log because by default we log all loss values that are used to update models.</p> <p>But what if we want to see the value of <code>ce1</code> throughout training?</p> <p>Easy: just add <code>ce1</code> to <code>monitor_names</code> in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#trace", "title": "Trace\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#concept", "title": "Concept\u00b6", "text": "<p>Now you might be thinking: 'changing logging behavior and monitoring extra keys is cool, but where is the fine-grained access to the training loop?'</p> <p>The answer is <code>Trace</code>. <code>Trace</code> is a module that can offer you access to different training stages and allow you \"do stuff\" with them. Here are some examples of what a <code>Trace</code> can do:</p> <ul> <li>print any training data at any training step</li> <li>write results to a file during training</li> <li>change learning rate based on some loss conditions</li> <li>calculate any metrics</li> <li>order you a pizza after training ends</li> <li>...</li> </ul> <p>So what are the different training stages? They are:</p> <ul> <li>Beginning of training</li> <li>Beginning of epoch</li> <li>Beginning of batch</li> <li>End of batch</li> <li>End of epoch</li> <li>End of training</li> </ul> <p></p> <p>As we can see from the illustration above, the training process is essentially a nested combination of batch loops and epoch loops. Over the course of training, <code>Trace</code> places 6 different \"road blocks\" for you to leverage.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#structure", "title": "Structure\u00b6", "text": "<p>If you are familiar with Keras, you will notice that the structure of <code>Trace</code> is very similar to the <code>Callback</code> in keras.  Despite the structural similarity, <code>Trace</code> gives you a lot more flexibility which we will talk about in depth in advanced tutorial 4. Implementation-wise, <code>Trace</code> is a python class with the following structure:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Sometimes you have a separate testing dataset other than training and evaluation data. If you want to evalate the model metrics on test data, you can simply call:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNet</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html", "title": "Tutorial 8: Mode", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]"}, {"location": "tutorial/beginner/t08_mode.html#tutorial-8-mode", "title": "Tutorial 8: Mode\u00b6", "text": ""}, {"location": "tutorial/beginner/t08_mode.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Modes</li> <li>When Modes are Activated</li> <li>How to Set Modes</li> <li>A Code Example</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#modes", "title": "Modes\u00b6", "text": "<p>The development cycle of a deep learning application can usually be broken into 4 phases: training, evaluation, testing, and inference. FastEstimator provides 4 corresponding modes: <code>train</code>, <code>eval</code>, <code>test</code>, and <code>infer</code> that allow users to manage each phase independently. Users have the flexibility to construct the <code>Network</code> and <code>Pipeline</code> in different ways for each of those modes. Only a single mode can ever be active at a time, and for each given mode the corresponding graph topology will be computed and executed.</p>"}, {"location": "tutorial/beginner/t08_mode.html#when-modes-are-activated", "title": "When Modes are Activated\u00b6", "text": "<ul> <li>train: <code>estimator.fit()</code> being called, during training cycle</li> <li>eval: <code>estimator.fit()</code> being called, during evaluation cycle</li> <li>test: <code>estimator.test()</code> being called</li> <li>infer: <code>pipeline.transform(mode=\"infer\")</code> or <code>network.transform(mode=\"infer\")</code> being called (inference will be covered in Tutorial 9)</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#how-to-set-modes", "title": "How to Set Modes\u00b6", "text": "<p>From the previous tutorials we already know that <code>Ops</code> define the workflow of <code>Networks</code> and <code>Pipelines</code>, whereas <code>Traces</code> control the training process. All <code>Ops</code> and <code>Traces</code> can be specified to run in one or more modes. Here are all 5 ways to set the modes:</p> <ol> <li><p>Setting a single mode Specify the desired mode as string. Ex: Op(mode=\"train\")</p> </li> <li><p>Setting multiple modes Put all desired modes in a tuple or list as an argument. Ex: Trace(mode=[\"train\", \"test\"]) </p> </li> <li><p>Setting an exception mode Prefix a \"!\" on a mode, and then the object will execute during all modes that are NOT the specified one. Ex: Op(mode=\"!train\") </p> </li> <li><p>Setting all modes Set the mode argument equal to None. Ex: Trace(mode=None) </p> </li> <li><p>Using the default mode setting Don't specify anything in mode argument. Different <code>Ops</code> and <code>Traces</code> have different default mode settings. Ex: <code>UpdateOp</code> -&gt; default mode: train  Ex: <code>Accuracy</code> trace -&gt; default mode: eval, test</p> </li> </ol>"}, {"location": "tutorial/beginner/t08_mode.html#code-example", "title": "Code Example\u00b6", "text": "<p>Let's see come example code and visualize the topology of the corresponding execution graphs for each mode:</p>"}, {"location": "tutorial/beginner/t08_mode.html#train-mode", "title": "Train Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"train\" mode. It has a complete data pipeline including the <code>CoarseDropout</code> data augmentation Op. The data source of the pipeline is \"train_data\". The <code>Accuracy</code> Trace will not exist in this mode because the default mode of that trace is \"eval\" and \"test\".</p>"}, {"location": "tutorial/beginner/t08_mode.html#eval-mode", "title": "Eval Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"eval\" mode. The data augmentation block is missing and the pipeline data source is \"eval_data\". The <code>Accuracy</code> block exist in this mode because of its default trace setting.</p>"}, {"location": "tutorial/beginner/t08_mode.html#test-mode", "title": "Test Mode\u00b6", "text": "<p>Everything in the \"test\" mode is the same as the \"eval\" mode, except that the data source of pipeline has switched to \"test_data\":</p>"}, {"location": "tutorial/beginner/t08_mode.html#infer-mode", "title": "Infer Mode\u00b6", "text": "<p>\"Infer\" mode only has the minimum operations that model inference requires. The data source is not defined yet because input data will not be passed until the inference function is invoked. See Tutorial 9 for more details.</p>"}, {"location": "tutorial/beginner/t08_mode.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html", "title": "Tutorial 9: Inference", "text": "<p>Running inference means using a trained deep learning model to get a prediction from some input data. Users can use <code>pipeline.transform</code> and <code>network.transform</code> to feed the data forward and get the computed result in any operation mode. Here we are going to use an end-to-end example (the same example code from Tutorial 8) on MNIST image classification to demonstrate how to run inference.</p> <p>We first train a deep leaning model with the following code:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=1,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\nestimator.fit()\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=1,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test] estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3060365; \nFastEstimator-Train: step: 100; ce: 1.3945999; steps/sec: 607.74; \nFastEstimator-Train: step: 200; ce: 1.2612264; steps/sec: 608.3; \nFastEstimator-Train: step: 300; ce: 1.0923076; steps/sec: 637.68; \nFastEstimator-Train: step: 400; ce: 1.0432141; steps/sec: 618.6; \nFastEstimator-Train: step: 500; ce: 1.1291115; steps/sec: 632.26; \nFastEstimator-Train: step: 600; ce: 1.1621585; steps/sec: 631.08; \nFastEstimator-Train: step: 700; ce: 0.8800679; steps/sec: 635.6; \nFastEstimator-Train: step: 800; ce: 0.881644; steps/sec: 626.44; \nFastEstimator-Train: step: 900; ce: 1.0101147; steps/sec: 634.27; \nFastEstimator-Train: step: 1000; ce: 0.6896187; steps/sec: 625.37; \nFastEstimator-Train: step: 1100; ce: 0.8749423; steps/sec: 622.53; \nFastEstimator-Train: step: 1200; ce: 0.92175865; steps/sec: 638.35; \nFastEstimator-Train: step: 1300; ce: 0.863528; steps/sec: 597.19; \nFastEstimator-Train: step: 1400; ce: 0.97089654; steps/sec: 564.34; \nFastEstimator-Train: step: 1500; ce: 0.80536413; steps/sec: 657.6; \nFastEstimator-Train: step: 1600; ce: 1.2070308; steps/sec: 624.45; \nFastEstimator-Train: step: 1700; ce: 0.7665318; steps/sec: 639.24; \nFastEstimator-Train: step: 1800; ce: 0.79776144; steps/sec: 629.49; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.58 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.16841692; accuracy: 0.9488; \nFastEstimator-Finish: step: 1875; total_time: 5.2 sec; model_lr: 0.001; \n</pre> <p>Let's create a customized print function to showcase our inferencing easier:</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport tensorflow as tf\n\ndef print_dict_but_value(data):\n    for key, value in data.items():\n        if isinstance(value, np.ndarray):\n            print(\"{}: ndarray with shape {}\".format(key, value.shape))\n        \n        elif isinstance(value, tf.Tensor):\n            print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))\n        \n        else:\n            print(\"{}: {}\".format(key, value))\n</pre> import numpy as np import tensorflow as tf  def print_dict_but_value(data):     for key, value in data.items():         if isinstance(value, np.ndarray):             print(\"{}: ndarray with shape {}\".format(key, value.shape))                  elif isinstance(value, tf.Tensor):             print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))                  else:             print(\"{}: {}\".format(key, value)) <p>The following figure shows the complete execution graph (consisting <code>Pipeline</code> and <code>Network</code>) for the \"infer\" mode:</p> <p></p> <p>Our goal is to provide an input image \"x\" and get the prediction result \"y_pred\".</p> <p></p> In\u00a0[3]: Copied! <pre>import copy \n\ninfer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])}\nprint_dict_but_value(infer_data)\n</pre> import copy   infer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])} print_dict_but_value(infer_data) <pre>x: ndarray with shape (28, 28)\n</pre> In\u00a0[4]: Copied! <pre>infer_data = pipeline.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = pipeline.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: ndarray with shape (1, 28, 28, 1)\nx_out: ndarray with shape (1, 28, 28, 1)\n</pre> <p></p> In\u00a0[5]: Copied! <pre>infer_data = network.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = network.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: tf.Tensor with shape (1, 28, 28, 1)\nx_out: tf.Tensor with shape (1, 28, 28, 1)\ny_pred: tf.Tensor with shape (1, 10)\n</pre> <p>Now we can visualize the input image and compare with its prediction class.</p> In\u00a0[6]: Copied! <pre>print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"])))\nimg = fe.util.ImgData(x=infer_data[\"x\"])\nfig = img.paint_figure()\n</pre> print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"]))) img = fe.util.ImgData(x=infer_data[\"x\"]) fig = img.paint_figure() <pre>Predicted class is 2\n</pre> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#tutorial-9-inference", "title": "Tutorial 9: Inference\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Running inference with the transform method<ul> <li>Pipeline.transform</li> <li>Network.transform</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html#running-inference-with-transform-method", "title": "Running inference with transform method\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#pipelinetransform", "title": "Pipeline.transform\u00b6", "text": "<p>The <code>Pipeline</code> object has a <code>transform</code> method that runs the pipeline graph (\"x\" to \"x_out\") when inference data (a dictionary of keys and values like {\"x\":image}), is inserted. The returned output will be the dictionary of computed results after applying all <code>Pipeline</code> Ops, where the dictionary values will be Numpy arrays.</p> <p></p> <p>Here we take eval_data's first image, package it into a dictionary, and then call <code>pipeline.transform</code>:</p>"}, {"location": "tutorial/beginner/t09_inference.html#networktransform", "title": "Network.transform\u00b6", "text": "<p>We then use the network object to call the <code>transform</code> method that runs the network graph (\"x_out\" to \"y_pred\"). Much like with <code>pipeline.transform</code>, it will return it's Op outputs, though this time in the form of a dictionary of Tensors. The data type of the returned values depends on the backend of the network. It is <code>tf.Tensor</code> when using the TensorFlow backend and <code>torch.Tensor</code> with PyTorch. Please check out Tutorial 6 for more details about <code>Network</code> backends).</p> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>IMDB</li> </ul>"}, {"location": "tutorial/beginner/t10_cli.html", "title": "Tutorial 10: How to use FastEstimator Command Line Interface (CLI)", "text": ""}, {"location": "tutorial/beginner/t10_cli.html#overview", "title": "Overview", "text": "<p>FastEstimator comes with a set of CLI commands that can help users train and test their models quickly. In this tutorial, we will go through the CLI usage and the arguments these CLI commands take. This tutorial is divided into the following sections:</p> <ul> <li>How Does the CLI Work</li> <li>CLI Usage</li> <li>Sending Input Args to <code>get_estimator</code><ul> <li>Using --arg</li> <li>Using a JSON file</li> </ul> </li> </ul> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#how_does_the_cli_work", "title": "How Does the CLI Work", "text": "<p>Given a python file, the FastEstimator CLI looks for a <code>get_estimator</code> function to get the estimator definition. It then calls either the <code>fit()</code> or <code>test()</code> functions on the returned estimator instance to train or test the model.</p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#cli_usage", "title": "CLI Usage", "text": "<p>In this section we will show the actual commands that we can use to train and test our models. We will use mnist_tf.py for illustration.</p> <p>To call <code>estimator.fit()</code> and start the training on terminal:</p> <pre><code>$ fastestimator train mnist_tf.py\n</code></pre> <p>To call <code>estimator.test()</code> and start testing on terminal:</p> <pre><code>$ fastestimator test mnist_tf.py\n</code></pre> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#sending_input_args_to_get_estimator", "title": "Sending Input Args to <code>get_estimator</code>", "text": "<p>We can also pass arguments to the <code>get_estimator</code> function call from the CLI. The following code snippet shows the <code>get_estimator</code> method for our MNIST example: <pre><code>def get_estimator(epochs=2, batch_size=32, ...):\n...\n</code></pre></p> <p>Next, we try to change these arguments in two ways:</p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_--arg", "title": "Using --arg", "text": "<p>To pass the arguments directly from the CLI we can use the <code>--arg</code> format. The following shows an example of how we can set the number of epochs to 3 and batch_size to 64:</p> <pre><code>$ fastestimator train mnist_tf.py --epochs 3 --batch_size 64\n</code></pre> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_a_json_file", "title": "Using a JSON file", "text": "<p>The other way we can send arguments is by using the <code>--hyperparameters</code> argument and passing it a json file containing all the training hyperparameters like epochs, batch_size, optimizer, etc. This option is really useful when you want to repeat the training job more than once and/or the list of the hyperparameter is getting really long. The following shows an example JSON file and how it could be used for our MNIST example: <pre><code>JSON:\n{\n    \"epochs\": 1,\n    \"batch_size\": 64\n}\n</code></pre> <pre><code>$ fastestimator train mnist_tf.py --hyperparameters hp.json\n</code></pre></p>"}, {"location": "tutorial/beginner/t10_cli.html#system_argument", "title": "System argument", "text": "<p>There are some default system arguments in the CLI, here are a list of them: * <code>warmup</code>: controls whether to perform warmup checking before the actual training starts. Default is True. Users can disable warmup before training by <code>--warmup False</code>. * <code>summary</code>: this is the same argument used in <code>estimator.fit()</code> or <code>estimator.test()</code>, it allows users to specify experiment name when generating reports. For example, Users can set experiment name by <code>--summary exp_name</code>.</p>"}]}