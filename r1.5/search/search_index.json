{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "installation.html", "title": "Install", "text": ""}, {"location": "installation.html#prerequisites", "title": "Prerequisites", "text": "<ul> <li>Python 3.7 - 3.9</li> <li>Nvidia Driver &gt;= 450 (GPU only)</li> <li>CUDA &gt;= 11.0 (GPU only)</li> </ul>"}, {"location": "installation.html#1_install_dependencies", "title": "1. Install Dependencies:", "text": "<ul> <li> <p>Install TensorFlow</p> <ul> <li>Linux/MAC:     <pre><code>pip install tensorflow==2.9.1\n</code></pre></li> <li>Windows:     Please follow this installation guide.</li> </ul> </li> <li> <p>Install PyTorch</p> <ul> <li>CPU:     <pre><code>pip install torch==1.10.2+cpu torchvision==0.11.3+cpu torchaudio==0.10.2+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n</code></pre></li> <li>GPU:     <pre><code>pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n</code></pre></li> </ul> </li> <li> <p>Extra Dependencies:</p> <ul> <li> <p>Windows:</p> <ul> <li> <p>Install Build Tools for Visual Studio 2019 here.</p> </li> <li> <p>Install latest Visual C++ redistributable here and choose x86 for 32 bit OS, x64 for 64 bit OS.</p> </li> </ul> </li> <li> <p>Linux:     <pre><code>$ apt-get install libglib2.0-0 libsm6 libxrender1 libxext6\n</code></pre></p> </li> <li> <p>Mac:     <pre><code>$ echo No extra dependency needed \":)\"\n</code></pre></p> </li> </ul> </li> </ul>"}, {"location": "installation.html#2_install_fastestimator", "title": "2. Install FastEstimator:", "text": "<ul> <li> <p>Stable (Linux/Mac):     <pre><code>$ pip install fastestimator\n</code></pre></p> </li> <li> <p>Stable (Windows):</p> <p>First download zip file from available releases <pre><code>$ pip install fastestimator-x.x.x.zip\n</code></pre></p> </li> <li> <p>Nightly (Linux/Mac):     <pre><code>$ pip install fastestimator-nightly\n</code></pre></p> </li> <li> <p>Nightly (Windows):</p> <p>First download zip file here <pre><code>$ pip install fastestimator-master.zip\n</code></pre></p> </li> </ul>"}, {"location": "installation.html#docker_hub", "title": "Docker Hub", "text": "<p>Docker containers create isolated virtual environments that share resources with a host machine. Docker provides an easy way to set up a FastEstimator environment. You can simply pull our image from Docker Hub and get started: * Stable:     * GPU:         <pre><code>docker pull fastestimator/fastestimator:latest-gpu\n</code></pre>     * CPU:         <pre><code>docker pull fastestimator/fastestimator:latest-cpu\n</code></pre> * Nighly:     * GPU:         <pre><code>docker pull fastestimator/fastestimator:nightly-gpu\n</code></pre>     * CPU:         <pre><code>docker pull fastestimator/fastestimator:nightly-cpu\n</code></pre></p>"}, {"location": "apphub/index.html", "title": "FastEstimator Application Hub", "text": "<p>Welcome to the FastEstimator Application Hub! Here we showcase different end-to-end AI examples implemented in FastEstimator. We will keep implementing new AI ideas and making state-of-the-art solutions accessible to everyone.</p>"}, {"location": "apphub/index.html#purpose_of_application_hub", "title": "Purpose of Application Hub", "text": "<ul> <li>Provide a place to learn implementation details of state-of-the-art solutions</li> <li>Showcase FastEstimator functionalities in an end-to-end fashion</li> <li>Offer ready-made AI solutions for people to use in their own projects/products</li> </ul>"}, {"location": "apphub/index.html#why_not_just_learn_from_official_implementations", "title": "Why not just learn from official implementations?", "text": "<p>If you have ever spent time reading AI research papers, you will often find yourself asking: did I just spent 3 hours reading a paper where the underlying idea can be expressed in 3 minutes?</p> <p>Similarly, people may use 5000 lines of code to implement an idea which could have been expressed in 500 lines using a different AI framework. In FastEstimator, we strive to make things simpler and more intuitive while preserving flexibility. As a result, many state-of-the-art AI implementations can be simplified greatly such that the code directly reflects the key ideas. As an example, the official implementation of PGGAN includes 5000+ lines of code whereas our implementation requires less than 500.</p> <p>To summarize, we spent time learning from the official implementation, so you can save time by learning from us!</p>"}, {"location": "apphub/index.html#whats_included_in_each_example", "title": "What's included in each example?", "text": "<p>Each example contains three files:</p> <ol> <li>A TensorFlow python file (.py): The FastEstimator source code needed to run the example with TensorFlow.</li> <li>A PyTorch python file (.py): The FastEstimator source code needed to run the example with PyTorch.</li> <li>A jupyter notebook (.ipynb): A notebook that provides step-by-step instructions and explanations about the implementation.</li> </ol>"}, {"location": "apphub/index.html#how_do_i_run_each_example", "title": "How do I run each example", "text": "<p>One can simply execute the python file of any example: <pre><code>$ python mnist_tf.py\n</code></pre></p> <p>Or use our Command-Line Interface (CLI):</p> <pre><code>$ fastestimator train mnist_torch.py\n</code></pre> <p>One benefit of the CLI is that it allows users to configure the input args of <code>get_estimator</code>:</p> <pre><code>$ fastestimator train lenet_mnist.py --batch_size 64 --epochs 4\n</code></pre>"}, {"location": "apphub/NLP/imdb/imdb.html", "title": "Sentiment Prediction in IMDB Reviews using an LSTM", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport fastestimator as fe\nfrom fastestimator.dataset.data import imdb_review\nfrom fastestimator.op.numpyop.univariate.reshape import Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.backend import load_model\n</pre> import tempfile import os import numpy as np import torch import torch.nn as nn import fastestimator as fe from fastestimator.dataset.data import imdb_review from fastestimator.op.numpyop.univariate.reshape import Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.backend import load_model In\u00a0[2]: parameters Copied! <pre>MAX_WORDS = 10000\nMAX_LEN = 500\nbatch_size = 64\nepochs = 10\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\n</pre> MAX_WORDS = 10000 MAX_LEN = 500 batch_size = 64 epochs = 10 train_steps_per_epoch = None eval_steps_per_epoch = None Building components <p>We are loading the dataset from tf.keras.datasets.imdb which contains movie reviews and sentiment scores. All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the <code>Pipeline</code>.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=batch_size,\n                       ops=Reshape(1, inputs=\"y\", outputs=\"y\"))\n</pre> train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=batch_size,                        ops=Reshape(1, inputs=\"y\", outputs=\"y\")) <p>First, we have to define the neural network architecture, and then pass the definition, associated model name, and optimizer into fe.build:</p> In\u00a0[4]: Copied! <pre>class ReviewSentiment(nn.Module):\n    def __init__(self, embedding_size=64, hidden_units=64):\n        super().__init__()\n        self.embedding = nn.Embedding(MAX_WORDS, embedding_size)\n        self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n        self.maxpool1d = nn.MaxPool1d(kernel_size=4)\n        self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)\n        self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)\n        self.fc2 = nn.Linear(in_features=250, out_features=1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.permute((0, 2, 1))\n        x = self.conv1d(x)\n        x = torch.relu(x)\n        x = self.maxpool1d(x)\n        output, _ = self.lstm(x)\n        x = output[:, -1]  # sequence output of only last timestamp\n        x = torch.tanh(x)\n        x = self.fc1(x)\n        x = torch.relu(x)\n        x = self.fc2(x)\n        x = torch.sigmoid(x)\n        return x\n</pre> class ReviewSentiment(nn.Module):     def __init__(self, embedding_size=64, hidden_units=64):         super().__init__()         self.embedding = nn.Embedding(MAX_WORDS, embedding_size)         self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)         self.maxpool1d = nn.MaxPool1d(kernel_size=4)         self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)         self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)         self.fc2 = nn.Linear(in_features=250, out_features=1)      def forward(self, x):         x = self.embedding(x)         x = x.permute((0, 2, 1))         x = self.conv1d(x)         x = torch.relu(x)         x = self.maxpool1d(x)         output, _ = self.lstm(x)         x = output[:, -1]  # sequence output of only last timestamp         x = torch.tanh(x)         x = self.fc1(x)         x = torch.relu(x)         x = self.fc2(x)         x = torch.sigmoid(x)         return x <p><code>Network</code> is the object that defines the whole training graph, including models, loss functions, optimizers etc. A <code>Network</code> can have several different models and loss functions (ex. GANs). <code>fe.Network</code> takes a series of operators, in this case just the basic <code>ModelOp</code>, loss op, and <code>UpdateOp</code> will suffice. It should be noted that \"y_pred\" is the key in the data dictionary which will store the predictions.</p> In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p><code>Estimator</code> is the API that wraps the <code>Pipeline</code>, <code>Network</code> and other training metadata together. <code>Estimator</code> also contains <code>Traces</code>, which are similar to the callbacks of Keras.</p> <p>In the training loop, we want to measure the validation loss and save the model that has the minimum loss. <code>BestModelSaver</code> is a convenient <code>Trace</code> to achieve this. Let's also measure accuracy over time using another <code>Trace</code>:</p> In\u00a0[6]: Copied! <pre>model_dir = tempfile.mkdtemp()\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> model_dir = tempfile.mkdtemp() traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) Training In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; loss: 0.6982045;\nFastEstimator-Train: step: 100; loss: 0.69076145; steps/sec: 4.55;\nFastEstimator-Train: step: 200; loss: 0.6970146; steps/sec: 5.49;\nFastEstimator-Train: step: 300; loss: 0.67406845; steps/sec: 5.6;\nFastEstimator-Train: step: 358; epoch: 1; epoch_time: 69.22 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\nFastEstimator-Eval: step: 358; epoch: 1; accuracy: 0.6826793843485801; loss: 0.59441286; min_loss: 0.59441286; since_best_loss: 0;\nFastEstimator-Train: step: 400; loss: 0.579373; steps/sec: 5.39;\nFastEstimator-Train: step: 500; loss: 0.5601772; steps/sec: 4.79;\nFastEstimator-Train: step: 600; loss: 0.3669433; steps/sec: 5.2;\nFastEstimator-Train: step: 700; loss: 0.5050458; steps/sec: 4.86;\nFastEstimator-Train: step: 716; epoch: 2; epoch_time: 71.36 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\nFastEstimator-Eval: step: 716; epoch: 2; accuracy: 0.7672230652503793; loss: 0.48858097; min_loss: 0.48858097; since_best_loss: 0;\nFastEstimator-Train: step: 800; loss: 0.43962425; steps/sec: 5.57;\nFastEstimator-Train: step: 900; loss: 0.33729357; steps/sec: 5.71;\nFastEstimator-Train: step: 1000; loss: 0.31596264; steps/sec: 5.23;\nFastEstimator-Train: step: 1074; epoch: 3; epoch_time: 77.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\nFastEstimator-Eval: step: 1074; epoch: 3; accuracy: 0.8103186646433991; loss: 0.4192897; min_loss: 0.4192897; since_best_loss: 0;\nFastEstimator-Train: step: 1100; loss: 0.33041656; steps/sec: 3.22;\nFastEstimator-Train: step: 1200; loss: 0.41677344; steps/sec: 5.75;\nFastEstimator-Train: step: 1300; loss: 0.43493804; steps/sec: 5.68;\nFastEstimator-Train: step: 1400; loss: 0.26938343; steps/sec: 5.34;\nFastEstimator-Train: step: 1432; epoch: 4; epoch_time: 64.02 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpds6dz9wa/model_best_loss.pt\nFastEstimator-Eval: step: 1432; epoch: 4; accuracy: 0.823845653587687; loss: 0.3995199; min_loss: 0.3995199; since_best_loss: 0;\nFastEstimator-Train: step: 1500; loss: 0.323763; steps/sec: 5.76;\nFastEstimator-Train: step: 1600; loss: 0.21561582; steps/sec: 5.84;\nFastEstimator-Train: step: 1700; loss: 0.20746922; steps/sec: 5.59;\nFastEstimator-Train: step: 1790; epoch: 5; epoch_time: 63.49 sec;\nFastEstimator-Eval: step: 1790; epoch: 5; accuracy: 0.8291784088445697; loss: 0.4008124; min_loss: 0.3995199; since_best_loss: 1;\nFastEstimator-Train: step: 1800; loss: 0.2219275; steps/sec: 5.12;\nFastEstimator-Train: step: 1900; loss: 0.2188505; steps/sec: 5.11;\nFastEstimator-Train: step: 2000; loss: 0.14373234; steps/sec: 5.53;\nFastEstimator-Train: step: 2100; loss: 0.20883155; steps/sec: 1.96;\nFastEstimator-Train: step: 2148; epoch: 6; epoch_time: 100.15 sec;\nFastEstimator-Eval: step: 2148; epoch: 6; accuracy: 0.8313461955343594; loss: 0.41437832; min_loss: 0.3995199; since_best_loss: 2;\nFastEstimator-Train: step: 2200; loss: 0.20082837; steps/sec: 5.64;\nFastEstimator-Train: step: 2300; loss: 0.22870378; steps/sec: 5.65;\nFastEstimator-Train: step: 2400; loss: 0.28569937; steps/sec: 5.7;\nFastEstimator-Train: step: 2500; loss: 0.16878708; steps/sec: 5.69;\nFastEstimator-Train: step: 2506; epoch: 7; epoch_time: 63.07 sec;\nFastEstimator-Eval: step: 2506; epoch: 7; accuracy: 0.8314762627357468; loss: 0.42922923; min_loss: 0.3995199; since_best_loss: 3;\nFastEstimator-Train: step: 2600; loss: 0.20338291; steps/sec: 5.77;\nFastEstimator-Train: step: 2700; loss: 0.17639604; steps/sec: 5.68;\nFastEstimator-Train: step: 2800; loss: 0.12155069; steps/sec: 5.7;\nFastEstimator-Train: step: 2864; epoch: 8; epoch_time: 62.75 sec;\nFastEstimator-Eval: step: 2864; epoch: 8; accuracy: 0.8294818989811402; loss: 0.46396694; min_loss: 0.3995199; since_best_loss: 4;\nFastEstimator-Train: step: 2900; loss: 0.20103803; steps/sec: 5.34;\nFastEstimator-Train: step: 3000; loss: 0.10518805; steps/sec: 5.71;\nFastEstimator-Train: step: 3100; loss: 0.10425654; steps/sec: 5.64;\nFastEstimator-Train: step: 3200; loss: 0.13740686; steps/sec: 5.5;\nFastEstimator-Train: step: 3222; epoch: 9; epoch_time: 64.67 sec;\nFastEstimator-Eval: step: 3222; epoch: 9; accuracy: 0.8254498157381314; loss: 0.5149529; min_loss: 0.3995199; since_best_loss: 5;\nFastEstimator-Train: step: 3300; loss: 0.080922514; steps/sec: 5.77;\nFastEstimator-Train: step: 3400; loss: 0.088989146; steps/sec: 5.41;\nFastEstimator-Train: step: 3500; loss: 0.1620798; steps/sec: 5.3;\nFastEstimator-Train: step: 3580; epoch: 10; epoch_time: 64.87 sec;\nFastEstimator-Eval: step: 3580; epoch: 10; accuracy: 0.8214177324951225; loss: 0.5555562; min_loss: 0.3995199; since_best_loss: 6;\nFastEstimator-Finish: step: 3580; model_lr: 0.001; total_time: 1124.45 sec;\n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We previously saved model weights corresponding to our minimum loss, and now we will load the weights using <code>load_model()</code>:</p> In\u00a0[8]: Copied! <pre>model_name = 'model_best_loss.pt'\nmodel_path = os.path.join(model_dir, model_name)\nload_model(model, model_path)\n</pre> model_name = 'model_best_loss.pt' model_path = os.path.join(model_dir, model_name) load_model(model, model_path) <p>Let's get some random sequence and compare the prediction with the ground truth:</p> In\u00a0[9]: Copied! <pre>selected_idx = np.random.randint(10000)\nprint(\"Ground truth is: \",eval_data[selected_idx]['y'])\n</pre> selected_idx = np.random.randint(10000) print(\"Ground truth is: \",eval_data[selected_idx]['y']) <pre>Ground truth is:  1\n</pre> <p>Create data dictionary for the inference. The <code>Transform()</code> function in Pipeline and Network applies all the operations on the given data:</p> In\u00a0[10]: Copied! <pre>infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Finally, print the inferencing results.</p> In\u00a0[11]: Copied! <pre>print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0])\n</pre> print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0]) <pre>Prediction for the input sequence:  0.91389465\n</pre>"}, {"location": "apphub/NLP/imdb/imdb.html#sentiment-prediction-in-imdb-reviews-using-an-lstm", "title": "Sentiment Prediction in IMDB Reviews using an LSTM\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html", "title": "Languge Modeling using LSTM on Penn Treebank", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nimport numpy as np\nimport tensorflow as tf\nfrom fastestimator.op.numpyop import Batch, NumpyOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.adapt import EarlyStopping, LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\n</pre> import tempfile  import fastestimator as fe import numpy as np import tensorflow as tf from fastestimator.op.numpyop import Batch, NumpyOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace import Trace from fastestimator.trace.adapt import EarlyStopping, LRScheduler from fastestimator.trace.io import BestModelSaver In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs=30\nbatch_size=128\nseq_length=20\nvocab_size=10000\ndata_dir=None\ntrain_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # Parameters epochs=30 batch_size=128 seq_length=20 vocab_size=10000 data_dir=None train_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.penn_treebank import load_data\ntrain_data, eval_data, _, vocab = load_data(root_dir=data_dir, seq_length=seq_length + 1)\n</pre> from fastestimator.dataset.data.penn_treebank import load_data train_data, eval_data, _, vocab = load_data(root_dir=data_dir, seq_length=seq_length + 1) In\u00a0[4]: Copied! <pre>class CreateInputAndTarget(NumpyOp):\n    def forward(self, data, state):\n        return data[:-1], data[1:]\n</pre> class CreateInputAndTarget(NumpyOp):     def forward(self, data, state):         return data[:-1], data[1:] In\u00a0[5]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       ops=[CreateInputAndTarget(inputs=\"x\", outputs=(\"x\", \"y\")),\n                            Batch(batch_size=batch_size, drop_last=True)])\n</pre> pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        ops=[CreateInputAndTarget(inputs=\"x\", outputs=(\"x\", \"y\")),                             Batch(batch_size=batch_size, drop_last=True)]) In\u00a0[6]: Copied! <pre>def build_model(vocab_size, embedding_dim, rnn_units, seq_length):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[None, seq_length]),\n        tf.keras.layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model\n</pre> def build_model(vocab_size, embedding_dim, rnn_units, seq_length):     model = tf.keras.Sequential([         tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[None, seq_length]),         tf.keras.layers.LSTM(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform'),         tf.keras.layers.Dropout(0.5),         tf.keras.layers.Dense(vocab_size)     ])     return model In\u00a0[7]: Copied! <pre>model = fe.build(model_fn=lambda: build_model(vocab_size, embedding_dim=300, rnn_units=600, seq_length=seq_length),\n                     optimizer_fn=lambda: tf.optimizers.SGD(1.0, momentum=0.9))\n</pre> model = fe.build(model_fn=lambda: build_model(vocab_size, embedding_dim=300, rnn_units=600, seq_length=seq_length),                      optimizer_fn=lambda: tf.optimizers.SGD(1.0, momentum=0.9)) <p>We now define the <code>Network</code> object:</p> In\u00a0[8]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(\n        inputs=(\"y_pred\", \"y\"), outputs=\"ce\", form=\"sparse\", from_logits=True),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(         inputs=(\"y_pred\", \"y\"), outputs=\"ce\", form=\"sparse\", from_logits=True),     UpdateOp(model=model, loss_name=\"ce\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>A custom trace to calculate Perplexity.</li> <li>LRScheduler to apply custom learning rate schedule.</li> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>EarlyStopping Trace for stopping early.</li> </ol> In\u00a0[9]: Copied! <pre>def lr_schedule(step, init_lr):\n    if step &lt;= 1725:\n        lr = init_lr + init_lr * (step - 1) / 1725\n    else:\n        lr = max(2 * init_lr * ((6900 - step + 1725) / 6900), 1.0)\n    return lr\n\n\nclass Perplexity(Trace):\n    def on_epoch_end(self, data):\n        ce = data[\"ce\"]\n        data.write_with_log(self.outputs[0], np.exp(ce))\n\n\ntraces = [\n    Perplexity(inputs=\"ce\", outputs=\"perplexity\", mode=\"eval\"),\n    LRScheduler(model=model, lr_fn=lambda step: lr_schedule(step, init_lr=1.0)),\n    BestModelSaver(model=model, save_dir=save_dir, metric='perplexity', save_best_mode='min', load_best_final=True),\n    EarlyStopping(monitor=\"perplexity\", patience=5)\n]\n</pre> def lr_schedule(step, init_lr):     if step &lt;= 1725:         lr = init_lr + init_lr * (step - 1) / 1725     else:         lr = max(2 * init_lr * ((6900 - step + 1725) / 6900), 1.0)     return lr   class Perplexity(Trace):     def on_epoch_end(self, data):         ce = data[\"ce\"]         data.write_with_log(self.outputs[0], np.exp(ce))   traces = [     Perplexity(inputs=\"ce\", outputs=\"perplexity\", mode=\"eval\"),     LRScheduler(model=model, lr_fn=lambda step: lr_schedule(step, init_lr=1.0)),     BestModelSaver(model=model, save_dir=save_dir, metric='perplexity', save_best_mode='min', load_best_final=True),     EarlyStopping(monitor=\"perplexity\", patience=5) ] In\u00a0[10]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch, \n                         log_steps=300)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                           log_steps=300) In\u00a0[11]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 300; \nFastEstimator-Train: step: 1; ce: 9.210202; model_lr: 1.0; \nFastEstimator-Train: step: 300; ce: 6.110634; steps/sec: 8.33; model_lr: 1.1733333; \nFastEstimator-Train: step: 345; epoch: 1; epoch_time: 43.55 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 345; epoch: 1; ce: 5.8996396; perplexity: 364.90594; since_best_perplexity: 0; min_perplexity: 364.90594; \nFastEstimator-Train: step: 600; ce: 5.7039967; steps/sec: 8.27; model_lr: 1.3472464; \nFastEstimator-Train: step: 690; epoch: 2; epoch_time: 41.65 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 690; epoch: 2; ce: 5.5457325; perplexity: 256.14215; since_best_perplexity: 0; min_perplexity: 256.14215; \nFastEstimator-Train: step: 900; ce: 5.636613; steps/sec: 8.27; model_lr: 1.5211594; \nFastEstimator-Train: step: 1035; epoch: 3; epoch_time: 41.89 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1035; epoch: 3; ce: 5.3436885; perplexity: 209.28323; since_best_perplexity: 0; min_perplexity: 209.28323; \nFastEstimator-Train: step: 1200; ce: 5.3110213; steps/sec: 8.23; model_lr: 1.6950724; \nFastEstimator-Train: step: 1380; epoch: 4; epoch_time: 41.82 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1380; epoch: 4; ce: 5.209713; perplexity: 183.04152; since_best_perplexity: 0; min_perplexity: 183.04152; \nFastEstimator-Train: step: 1500; ce: 5.1398573; steps/sec: 8.25; model_lr: 1.8689855; \nFastEstimator-Train: step: 1725; epoch: 5; epoch_time: 41.79 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 1725; epoch: 5; ce: 5.107191; perplexity: 165.20566; since_best_perplexity: 0; min_perplexity: 165.20566; \nFastEstimator-Train: step: 1800; ce: 5.003286; steps/sec: 8.25; model_lr: 1.9782609; \nFastEstimator-Train: step: 2070; epoch: 6; epoch_time: 41.9 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2070; epoch: 6; ce: 5.018823; perplexity: 151.23322; since_best_perplexity: 0; min_perplexity: 151.23322; \nFastEstimator-Train: step: 2100; ce: 4.9688864; steps/sec: 8.23; model_lr: 1.8913044; \nFastEstimator-Train: step: 2400; ce: 4.8305387; steps/sec: 8.23; model_lr: 1.8043479; \nFastEstimator-Train: step: 2415; epoch: 7; epoch_time: 41.97 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2415; epoch: 7; ce: 4.9463377; perplexity: 140.65887; since_best_perplexity: 0; min_perplexity: 140.65887; \nFastEstimator-Train: step: 2700; ce: 4.5900016; steps/sec: 8.23; model_lr: 1.7173913; \nFastEstimator-Train: step: 2760; epoch: 8; epoch_time: 41.98 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 2760; epoch: 8; ce: 4.900966; perplexity: 134.41959; since_best_perplexity: 0; min_perplexity: 134.41959; \nFastEstimator-Train: step: 3000; ce: 4.6566253; steps/sec: 8.21; model_lr: 1.6304348; \nFastEstimator-Train: step: 3105; epoch: 9; epoch_time: 41.64 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3105; epoch: 9; ce: 4.8612027; perplexity: 129.17947; since_best_perplexity: 0; min_perplexity: 129.17947; \nFastEstimator-Train: step: 3300; ce: 4.6201677; steps/sec: 8.34; model_lr: 1.5434783; \nFastEstimator-Train: step: 3450; epoch: 10; epoch_time: 41.66 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3450; epoch: 10; ce: 4.833195; perplexity: 125.61168; since_best_perplexity: 0; min_perplexity: 125.61168; \nFastEstimator-Train: step: 3600; ce: 4.672325; steps/sec: 8.27; model_lr: 1.4565217; \nFastEstimator-Train: step: 3795; epoch: 11; epoch_time: 41.92 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 3795; epoch: 11; ce: 4.8110547; perplexity: 122.86113; since_best_perplexity: 0; min_perplexity: 122.86113; \nFastEstimator-Train: step: 3900; ce: 4.5373406; steps/sec: 8.21; model_lr: 1.3695652; \nFastEstimator-Train: step: 4140; epoch: 12; epoch_time: 41.83 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4140; epoch: 12; ce: 4.802991; perplexity: 121.87438; since_best_perplexity: 0; min_perplexity: 121.87438; \nFastEstimator-Train: step: 4200; ce: 4.412928; steps/sec: 8.26; model_lr: 1.2826087; \nFastEstimator-Train: step: 4485; epoch: 13; epoch_time: 41.72 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4485; epoch: 13; ce: 4.7911124; perplexity: 120.43527; since_best_perplexity: 0; min_perplexity: 120.43527; \nFastEstimator-Train: step: 4500; ce: 4.402304; steps/sec: 8.26; model_lr: 1.1956521; \nFastEstimator-Train: step: 4800; ce: 4.4627676; steps/sec: 8.31; model_lr: 1.1086956; \nFastEstimator-Train: step: 4830; epoch: 14; epoch_time: 41.58 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 4830; epoch: 14; ce: 4.78295; perplexity: 119.45622; since_best_perplexity: 0; min_perplexity: 119.45622; \nFastEstimator-Train: step: 5100; ce: 4.2155848; steps/sec: 8.24; model_lr: 1.0217391; \nFastEstimator-Train: step: 5175; epoch: 15; epoch_time: 41.92 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Eval: step: 5175; epoch: 15; ce: 4.777499; perplexity: 118.80688; since_best_perplexity: 0; min_perplexity: 118.80688; \nFastEstimator-Train: step: 5400; ce: 4.2096825; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 5520; epoch: 16; epoch_time: 41.88 sec; \nFastEstimator-Eval: step: 5520; epoch: 16; ce: 4.786487; perplexity: 119.87951; since_best_perplexity: 1; min_perplexity: 118.80688; \nFastEstimator-Train: step: 5700; ce: 4.19374; steps/sec: 8.22; model_lr: 1.0; \nFastEstimator-Train: step: 5865; epoch: 17; epoch_time: 41.86 sec; \nFastEstimator-Eval: step: 5865; epoch: 17; ce: 4.791095; perplexity: 120.43314; since_best_perplexity: 2; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6000; ce: 4.040009; steps/sec: 8.27; model_lr: 1.0; \nFastEstimator-Train: step: 6210; epoch: 18; epoch_time: 41.74 sec; \nFastEstimator-Eval: step: 6210; epoch: 18; ce: 4.7963467; perplexity: 121.067314; since_best_perplexity: 3; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6300; ce: 4.0719166; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 6555; epoch: 19; epoch_time: 41.94 sec; \nFastEstimator-Eval: step: 6555; epoch: 19; ce: 4.799467; perplexity: 121.44568; since_best_perplexity: 4; min_perplexity: 118.80688; \nFastEstimator-Train: step: 6600; ce: 4.110601; steps/sec: 8.24; model_lr: 1.0; \nFastEstimator-Train: step: 6900; ce: 3.9709384; steps/sec: 8.3; model_lr: 1.0; \nFastEstimator-Train: step: 6900; epoch: 20; epoch_time: 41.54 sec; \nFastEstimator-EarlyStopping: 'perplexity' triggered an early stop. Its best value was 118.80687713623047 at epoch 15\nFastEstimator-Eval: step: 6900; epoch: 20; ce: 4.8052564; perplexity: 122.150795; since_best_perplexity: 5; min_perplexity: 118.80688; \nFastEstimator-BestModelSaver: Restoring model from /tmp/tmptl5d8hgb/model_best_perplexity.h5\nFastEstimator-Finish: step: 6900; total_time: 864.19 sec; model_lr: 1.0; \n</pre> In\u00a0[12]: Copied! <pre>def get_next_word(data, vocab):\n    output = network.transform(data, mode=\"infer\") \n    index = output[\"y_pred\"].numpy().squeeze()[-1].argmax()\n    if index == 44:    # Removing unkwown predicition\n        index = output[\"y_pred\"].numpy().squeeze()[-1].argsort()[-2]\n    return index\n\ndef generate_sequence(inp_seq, vocab, min_paragraph_len=50):\n    data = pipeline.transform({\"x\": inp_seq}, mode=\"infer\")\n    generated_seq = data[\"x\"]\n    counter=0\n    next_entry=0\n    # Stopping at &lt;eos&gt; tag or after min_paragraph_len+30 words\n    while (counter&lt;min_paragraph_len or next_entry != 43) and counter&lt;min_paragraph_len+30:  \n        next_entry = get_next_word(data, vocab)\n        generated_seq = np.concatenate([generated_seq.squeeze(), [next_entry]])\n        data = {\"x\": generated_seq[-20:].reshape((1, 20))}\n        counter+=1\n\n    return \" \".join([vocab[i] for i in generated_seq])\n</pre> def get_next_word(data, vocab):     output = network.transform(data, mode=\"infer\")      index = output[\"y_pred\"].numpy().squeeze()[-1].argmax()     if index == 44:    # Removing unkwown predicition         index = output[\"y_pred\"].numpy().squeeze()[-1].argsort()[-2]     return index  def generate_sequence(inp_seq, vocab, min_paragraph_len=50):     data = pipeline.transform({\"x\": inp_seq}, mode=\"infer\")     generated_seq = data[\"x\"]     counter=0     next_entry=0     # Stopping at  tag or after min_paragraph_len+30 words     while (counter <p>We will provide a text sequence from the validation dataset to the model and generate a paragraph with the input text sequence.</p> In\u00a0[13]: Copied! <pre>for _ in range(2):\n    idx = np.random.choice(len(eval_data))\n    inp_seq = eval_data[\"x\"][idx]\n    print(\"Input Sequence:\", \" \".join([vocab[i] for i in inp_seq[:20]]))\n    gen_seq = generate_sequence(inp_seq, vocab, 50)\n    print(\"\\nGenerated Sequence:\", gen_seq)\n    print(\"\\n\")\n</pre> for _ in range(2):     idx = np.random.choice(len(eval_data))     inp_seq = eval_data[\"x\"][idx]     print(\"Input Sequence:\", \" \".join([vocab[i] for i in inp_seq[:20]]))     gen_seq = generate_sequence(inp_seq, vocab, 50)     print(\"\\nGenerated Sequence:\", gen_seq)     print(\"\\n\") <pre>Input Sequence: the pictures &lt;eos&gt; the state &lt;unk&gt; noted that &lt;unk&gt; banking practices are grounds for removing an officer or director and\n\nGenerated Sequence: the pictures &lt;eos&gt; the state &lt;unk&gt; noted that &lt;unk&gt; banking practices are grounds for removing an officer or director and chief executive officer &lt;eos&gt; mr. guber and mr. peters have been working on the board &lt;eos&gt; the company said the company will be able to pay for the $ N million of the company 's common shares outstanding &lt;eos&gt; the company said the company 's net income rose N N to $ N million from $ N million &lt;eos&gt;\n\n\nInput Sequence: the russians in iran the russians seem to have lost interest in the whole subject &lt;eos&gt; meanwhile congress is cutting\n\nGenerated Sequence: the russians in iran the russians seem to have lost interest in the whole subject &lt;eos&gt; meanwhile congress is cutting the capital-gains tax cut to the u.s. and the u.s. trade deficit &lt;eos&gt; the u.s. trade deficit has been the highest since august N &lt;eos&gt; the dollar was mixed &lt;eos&gt; the dollar was mixed &lt;eos&gt; the nasdaq composite index rose N to N &lt;eos&gt; the index gained N to N &lt;eos&gt;\n\n\n</pre> <p>As you can see, the network is able to generate meaningful sentences.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#languge-modeling-using-lstm-on-penn-treebank", "title": "Languge Modeling using LSTM on Penn Treebank\u00b6", "text": "<p>Language Modeling is the development of models to predict the next word of the sequence given the words that precede it. In this notebook we will demonstrate how to predict next word of a sequence using an LSTM. We will be using Penn Treebank dataset which contains 888K words for training, 70K for validation, and 79K for testing, with a vocabulary size of 10K.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download the Penn Treebank dataset via our dataset API.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We will create a custom NumpyOp to generate input and target sequences.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model is a LSTM.</p>"}, {"location": "apphub/NLP/language_modeling/ptb.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#training-and-testing", "title": "Training and Testing\u00b6", "text": ""}, {"location": "apphub/NLP/language_modeling/ptb.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will use the model to generate some sequences of text.</p>"}, {"location": "apphub/NLP/named_entity_recognition/bert.html", "title": "Named Entity Recognition using BERT Fine-Tuning", "text": "<p>For downstream NLP tasks such as question answering, named entity recognition, and language inference, models built on pre-trained word representations tend to perform better. BERT, which fine tunes a deep bi-directional representation on a series of tasks, achieves state-of-the-art results. Unlike traditional transformers, BERT is trained on \"masked language modeling,\" which means that it is allowed to see the whole sentence and does not limit the context it can take into account.</p> <p>For this example, we are leveraging the transformers library to load a BERT model, along with some config files:</p> In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom transformers import BertTokenizer, TFBertModel\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import mitmovie_ner\nfrom fastestimator.op.numpyop.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId\nfrom fastestimator.op.tensorop import Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.backend import feed_forward\n</pre> import tempfile import os import numpy as np  import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.models import Model from transformers import BertTokenizer, TFBertModel  import fastestimator as fe from fastestimator.dataset.data import mitmovie_ner from fastestimator.op.numpyop.numpyop import NumpyOp from fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId from fastestimator.op.tensorop import Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver from fastestimator.backend import feed_forward In\u00a0[2]: parameters Copied! <pre>max_len = 20\nbatch_size = 64\nepochs = 10\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> max_len = 20 batch_size = 64 epochs = 10 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We will need a custom <code>NumpyOp</code> that constructs attention masks for input sequences:</p> In\u00a0[3]: Copied! <pre>class AttentionMask(NumpyOp):\n    def forward(self, data, state):\n        masks = [float(i &gt; 0) for i in data]\n        return np.array(masks)\n</pre> class AttentionMask(NumpyOp):     def forward(self, data, state):         masks = [float(i &gt; 0) for i in data]         return np.array(masks) <p>Our <code>char2idx</code> function creates a look-up table to match ids and labels:</p> In\u00a0[4]: Copied! <pre>def char2idx(data):\n    tag2idx = {t: i for i, t in enumerate(data)}\n    return tag2idx\n</pre> def char2idx(data):     tag2idx = {t: i for i, t in enumerate(data)}     return tag2idx Building components <p>We are loading train and eval sequences from the MIT Movie datasets that is semantically tagged along with data and label vocabulary. For this example other nouns are omitted for the simplicity.</p> In\u00a0[5]: Copied! <pre>train_data, eval_data, data_vocab, label_vocab = mitmovie_ner.load_data(root_dir=data_dir)\n</pre> train_data, eval_data, data_vocab, label_vocab = mitmovie_ner.load_data(root_dir=data_dir) <pre>/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  y_train = np.array(y_train)\n/home/ubuntu/fe/fastestimator/fastestimator/dataset/data/mitmovie_ner.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  y_eval = np.array(y_eval)\n</pre> <p>Define a pipeline to tokenize and pad the input sequences and construct attention masks. Attention masks are used to avoid performing attention operations on padded tokens. We are using the BERT tokenizer for input sequence tokenization, and limiting our sequences to a max length of 50 for this example.</p> In\u00a0[6]: Copied! <pre>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntag2idx = char2idx(label_vocab)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),\n        WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),\n        WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),\n        PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),\n        PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),\n        AttentionMask(inputs=\"x\", outputs=\"x_masks\")\n    ])\n</pre> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) tag2idx = char2idx(label_vocab) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size,     ops=[         Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),         WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),         WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),         PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),         PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),         AttentionMask(inputs=\"x\", outputs=\"x_masks\")     ]) <p>Our neural network architecture leverages pre-trained weights as initialization for downstream tasks. The whole network is then trained during the fine-tuning.</p> In\u00a0[7]: Copied! <pre>def ner_model():\n    token_inputs = Input((max_len), dtype=tf.int32, name='input_words')\n    mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')\n    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    bert_output = bert_model(token_inputs, attention_mask=mask_inputs) # use the last hidden state\n    output = Dense(len(label_vocab) + 1, activation='softmax')(bert_output[0])\n    model = Model([token_inputs, mask_inputs], output)\n    return model\n</pre> def ner_model():     token_inputs = Input((max_len), dtype=tf.int32, name='input_words')     mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')     bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")     bert_output = bert_model(token_inputs, attention_mask=mask_inputs) # use the last hidden state     output = Dense(len(label_vocab) + 1, activation='softmax')(bert_output[0])     model = Model([token_inputs, mask_inputs], output)     return model <p>After defining the model, it is then instantiated by calling fe.build which also associates the model with a specific optimizer:</p> In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <p><code>fe.Network</code> takes a series of operators. In this case we use a <code>ModelOp</code> to run forward passes through the neural network. The <code>ReshapeOp</code> is then used to transform the prediction and ground truth to a two dimensional vector or scalar respectively before feeding them to the loss calculation.</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),\n        Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),\n        Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, len(label_vocab) + 1)),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n        UpdateOp(model=model, loss_name=\"loss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),         Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),         Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, len(label_vocab) + 1)),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),         UpdateOp(model=model, loss_name=\"loss\")     ]) <p>The <code>Estimator</code> takes four important arguments: network, pipeline, epochs, and traces. During the training, we want to compute accuracy as well as to save the model with the minimum loss. This can be done using <code>Traces</code>.</p> In\u00a0[10]: Copied! <pre>traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)]\n</pre> traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)] In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) Training In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nFastEstimator-Train: step: 1; loss: 3.534539; \nFastEstimator-Train: step: 100; loss: 0.7910271; steps/sec: 2.04; \nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nFastEstimator-Train: step: 153; epoch: 1; epoch_time: 93.27 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 153; epoch: 1; loss: 0.4999621; accuracy: 0.8571633237822349; since_best_loss: 0; min_loss: 0.4999621; \nFastEstimator-Train: step: 200; loss: 0.4584582; steps/sec: 1.71; \nFastEstimator-Train: step: 300; loss: 0.34510213; steps/sec: 1.97; \nFastEstimator-Train: step: 306; epoch: 2; epoch_time: 77.24 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 306; epoch: 2; loss: 0.33649036; accuracy: 0.9036635284486287; since_best_loss: 0; min_loss: 0.33649036; \nFastEstimator-Train: step: 400; loss: 0.28147238; steps/sec: 1.89; \nFastEstimator-Train: step: 459; epoch: 3; epoch_time: 81.27 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 459; epoch: 3; loss: 0.27175826; accuracy: 0.920384772820303; since_best_loss: 0; min_loss: 0.27175826; \nFastEstimator-Train: step: 500; loss: 0.2756353; steps/sec: 1.92; \nFastEstimator-Train: step: 600; loss: 0.22491762; steps/sec: 2.0; \nFastEstimator-Train: step: 612; epoch: 4; epoch_time: 76.74 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 612; epoch: 4; loss: 0.23077382; accuracy: 0.9328694228407696; since_best_loss: 0; min_loss: 0.23077382; \nFastEstimator-Train: step: 700; loss: 0.21992946; steps/sec: 2.0; \nFastEstimator-Train: step: 765; epoch: 5; epoch_time: 76.65 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 765; epoch: 5; loss: 0.20521004; accuracy: 0.9419566107245191; since_best_loss: 0; min_loss: 0.20521004; \nFastEstimator-Train: step: 800; loss: 0.22478268; steps/sec: 2.0; \nFastEstimator-Train: step: 900; loss: 0.18137912; steps/sec: 1.99; \nFastEstimator-Train: step: 918; epoch: 6; epoch_time: 76.67 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 918; epoch: 6; loss: 0.19312857; accuracy: 0.947359803520262; since_best_loss: 0; min_loss: 0.19312857; \nFastEstimator-Train: step: 1000; loss: 0.16234711; steps/sec: 1.99; \nFastEstimator-Train: step: 1071; epoch: 7; epoch_time: 76.88 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1071; epoch: 7; loss: 0.17946187; accuracy: 0.951105198526402; since_best_loss: 0; min_loss: 0.17946187; \nFastEstimator-Train: step: 1100; loss: 0.1411412; steps/sec: 1.99; \nFastEstimator-Train: step: 1200; loss: 0.21398099; steps/sec: 2.0; \nFastEstimator-Train: step: 1224; epoch: 8; epoch_time: 76.62 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1224; epoch: 8; loss: 0.17292295; accuracy: 0.9533974621367172; since_best_loss: 0; min_loss: 0.17292295; \nFastEstimator-Train: step: 1300; loss: 0.104645446; steps/sec: 1.98; \nFastEstimator-Train: step: 1377; epoch: 9; epoch_time: 77.22 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1377; epoch: 9; loss: 0.17160487; accuracy: 0.95440032746623; since_best_loss: 0; min_loss: 0.17160487; \nFastEstimator-Train: step: 1400; loss: 0.114030465; steps/sec: 1.99; \nFastEstimator-Train: step: 1500; loss: 0.118343726; steps/sec: 2.0; \nFastEstimator-Train: step: 1530; epoch: 10; epoch_time: 76.59 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmpjyf_2m8t/model_best_loss.h5\nFastEstimator-Eval: step: 1530; epoch: 10; loss: 0.16745299; accuracy: 0.9565083913221449; since_best_loss: 0; min_loss: 0.16745299; \nFastEstimator-Finish: step: 1530; total_time: 897.96 sec; model_lr: 1e-05; \n</pre> Inferencing <p>Load model weights using fe.build</p> In\u00a0[13]: Copied! <pre>model_name = 'model_best_loss.h5'\nmodel_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model_name = 'model_best_loss.h5' model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <p>Let's take random phrase about a movie and predict it's named entities in BIO format.</p> In\u00a0[14]: Copied! <pre>test_input = 'have you seen The dark night trilogy'\ntest_ground_truth = ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE']\n</pre> test_input = 'have you seen The dark night trilogy' test_ground_truth = ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE'] <p>Create a data dictionary for the inference. The <code>transform()</code> function in <code>Pipeline</code> and <code>Network</code> applies all their operations on the given data:</p> In\u00a0[15]: Copied! <pre>infer_data = {\"x\":test_input, \"y\":test_ground_truth}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":test_input, \"y\":test_ground_truth} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Get the predictions using feed_forward</p> In\u00a0[16]: Copied! <pre>predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False)\npredictions = np.array(predictions).reshape(20, len(label_vocab) + 1)\npredictions = np.argmax(predictions, axis=-1)\n</pre> predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False) predictions = np.array(predictions).reshape(20, len(label_vocab) + 1) predictions = np.argmax(predictions, axis=-1) In\u00a0[17]: Copied! <pre>def get_key(val): \n    for key, value in tag2idx.items(): \n         if val == value: \n            return key\n</pre> def get_key(val):      for key, value in tag2idx.items():           if val == value:              return key  In\u00a0[18]: Copied! <pre>print(\"Predictions: \", [get_key(pred) for pred in predictions])\n</pre> print(\"Predictions: \", [get_key(pred) for pred in predictions]) <pre>Predictions:  ['O', 'O', 'O', 'O', 'B-TITLE', 'I-TITLE', 'I-TITLE', None, None, None, None, None, None, None, None, None, None, None, None, None]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "apphub/NLP/named_entity_recognition/bert.html#named-entity-recognition-using-bert-fine-tuning", "title": "Named Entity Recognition using BERT Fine-Tuning\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-2-create-model-and-fastestimator-network", "title": "Step 2: Create <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/NLP/neural_machine_translation/transformer.html", "title": "Neural Machine Translation Using Transformer", "text": "In\u00a0[1]: parameters Copied! <pre>data_dir = None\nepochs=20\nem_dim=128\nbatch_size=64\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\n</pre> data_dir = None epochs=20 em_dim=128 batch_size=64 train_steps_per_epoch=None eval_steps_per_epoch=None In\u00a0[2]: Copied! <pre>from fastestimator.dataset.data import tednmt\n\ntrain_ds, eval_ds, test_ds = tednmt.load_data(data_dir, translate_option=\"pt_to_en\")\n</pre> from fastestimator.dataset.data import tednmt  train_ds, eval_ds, test_ds = tednmt.load_data(data_dir, translate_option=\"pt_to_en\") <p>Now that the dataset is downloaded, let's check what the dataset looks like:</p> In\u00a0[3]: Copied! <pre>print(\"example source language:\")\nprint(train_ds[0][\"source\"])\nprint(\"\")\nprint(\"example target language:\")\nprint(train_ds[0][\"target\"])\n</pre> print(\"example source language:\") print(train_ds[0][\"source\"]) print(\"\") print(\"example target language:\") print(train_ds[0][\"target\"]) <pre>example source language:\nentre todas as grandes priva\u00e7\u00f5es com que nos debatemos hoje \u2014 pensamos em financeiras e econ\u00f3micas primeiro \u2014 aquela que mais me preocupa \u00e9 a falta de di\u00e1logo pol\u00edtico \u2014 a nossa capacidade de abordar conflitos modernos como eles s\u00e3o , de ir \u00e0 raiz do que eles s\u00e3o e perceber os agentes-chave e lidar com eles .\n\nexample target language:\namongst all the troubling deficits we struggle with today \u2014 we think of financial and economic primarily \u2014 the ones that concern me most is the deficit of political dialogue \u2014 our ability to address modern conflicts as they are , to go to the source of what they 're all about and to understand the key players and to deal with them .\n</pre> In\u00a0[\u00a0]: Copied! <pre>import fastestimator as fe\nfrom transformers import BertTokenizer\nfrom fastestimator.op.numpyop import Batch, NumpyOp\nimport numpy as np\n\n\nclass Encode(NumpyOp):\n    def __init__(self, tokenizer, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.tokenizer = tokenizer\n\n    def forward(self, data, state):\n        return np.array(self.tokenizer.encode(data))\n\n\npt_tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\nen_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=eval_ds,\n    test_data=test_ds,\n    ops=[\n        Encode(inputs=\"source\", outputs=\"source\", tokenizer=pt_tokenizer),\n        Encode(inputs=\"target\", outputs=\"target\", tokenizer=en_tokenizer),\n        Batch(batch_size=batch_size, pad_value=0)\n    ])\n</pre> import fastestimator as fe from transformers import BertTokenizer from fastestimator.op.numpyop import Batch, NumpyOp import numpy as np   class Encode(NumpyOp):     def __init__(self, tokenizer, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.tokenizer = tokenizer      def forward(self, data, state):         return np.array(self.tokenizer.encode(data))   pt_tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\") en_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=eval_ds,     test_data=test_ds,     ops=[         Encode(inputs=\"source\", outputs=\"source\", tokenizer=pt_tokenizer),         Encode(inputs=\"target\", outputs=\"target\", tokenizer=en_tokenizer),         Batch(batch_size=batch_size, pad_value=0)     ]) <p>In the above code, <code>tokenizer.encode</code> will take the sentence and execute the step 1 - 3. The padding step is done by providing <code>pad_value=0</code> in the <code>Batch</code> Op.</p> In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\nprint(\"source after processing:\")\nprint(data[\"source\"])\nprint(\"source batch shape:\")\nprint(data[\"source\"].shape)\nprint(\"---------------------------------------------------\")\nprint(\"target after processing:\")\nprint(data[\"target\"])\nprint(\"target batch shape:\")\nprint(data[\"target\"].shape)\n</pre> data = pipeline.get_results() print(\"source after processing:\") print(data[\"source\"]) print(\"source batch shape:\") print(data[\"source\"].shape) print(\"---------------------------------------------------\") print(\"target after processing:\") print(data[\"target\"]) print(\"target batch shape:\") print(data[\"target\"].shape) <pre>source after processing:\ntensor([[  101,   420,  1485,  ...,  1061,   119,   102],\n        [  101,   538,   179,  ...,     0,     0,     0],\n        [  101,   122, 21174,  ...,     0,     0,     0],\n        ...,\n        [  101,   607,   230,  ...,     0,     0,     0],\n        [  101,   123, 10186,  ...,     0,     0,     0],\n        [  101, 11865,  3072,  ...,     0,     0,     0]])\nsource batch shape:\ntorch.Size([64, 72])\n---------------------------------------------------\ntarget after processing:\ntensor([[ 101, 5921, 2035,  ..., 2068, 1012,  102],\n        [ 101, 2057, 2040,  ...,    0,    0,    0],\n        [ 101, 1998, 1045,  ...,    0,    0,    0],\n        ...,\n        [ 101, 2045, 1005,  ...,    0,    0,    0],\n        [ 101, 1996, 5424,  ...,    0,    0,    0],\n        [ 101, 2009, 2097,  ...,    0,    0,    0]])\ntarget batch shape:\ntorch.Size([64, 70])\n</pre> In\u00a0[6]: Copied! <pre>import tensorflow as tf\n\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    num_heads, inp_length = tf.shape(scaled_attention_logits)[1], tf.shape(scaled_attention_logits)[2]\n    num_heads_mask, inp_length_mask = tf.shape(mask)[1], tf.shape(mask)[2]\n    # This manual tiling is to fix a auto-broadcasting issue with tensorflow\n    scaled_attention_logits += tf.tile(mask * -1e9, [1, num_heads // num_heads_mask, inp_length // inp_length_mask, 1])\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    return output\n\n\ndef point_wise_feed_forward_network(em_dim, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(em_dim)  # (batch_size, seq_len, em_dim)\n    ])\n</pre> import tensorflow as tf   def scaled_dot_product_attention(q, k, v, mask):     matmul_qk = tf.matmul(q, k, transpose_b=True)     dk = tf.cast(tf.shape(k)[-1], tf.float32)     scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)     num_heads, inp_length = tf.shape(scaled_attention_logits)[1], tf.shape(scaled_attention_logits)[2]     num_heads_mask, inp_length_mask = tf.shape(mask)[1], tf.shape(mask)[2]     # This manual tiling is to fix a auto-broadcasting issue with tensorflow     scaled_attention_logits += tf.tile(mask * -1e9, [1, num_heads // num_heads_mask, inp_length // inp_length_mask, 1])     attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)     output = tf.matmul(attention_weights, v)     return output   def point_wise_feed_forward_network(em_dim, dff):     return tf.keras.Sequential([         tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)         tf.keras.layers.Dense(em_dim)  # (batch_size, seq_len, em_dim)     ])  In\u00a0[7]: Copied! <pre>from tensorflow.keras import layers\n\nclass MultiHeadAttention(layers.Layer):\n    def __init__(self, em_dim, num_heads):\n        super().__init__()\n        assert em_dim % num_heads == 0, \"model dimension must be multiply of number of heads\"\n        self.num_heads = num_heads\n        self.em_dim = em_dim\n        self.depth = em_dim // self.num_heads\n        self.wq = layers.Dense(em_dim)\n        self.wk = layers.Dense(em_dim)\n        self.wv = layers.Dense(em_dim)\n        self.dense = layers.Dense(em_dim)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])  # B, num_heads, seq_len, depth\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # B, seq_len, em_dim\n        k = self.wk(k)  # B, seq_len, em_dim\n        v = self.wv(v)  # B, seq_len, em_dim\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        scaled_attention = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  #B, seq_len, num_heads, depth\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.em_dim))  # B, seq_len, em_dim\n        output = self.dense(concat_attention)\n        return output\n</pre> from tensorflow.keras import layers  class MultiHeadAttention(layers.Layer):     def __init__(self, em_dim, num_heads):         super().__init__()         assert em_dim % num_heads == 0, \"model dimension must be multiply of number of heads\"         self.num_heads = num_heads         self.em_dim = em_dim         self.depth = em_dim // self.num_heads         self.wq = layers.Dense(em_dim)         self.wk = layers.Dense(em_dim)         self.wv = layers.Dense(em_dim)         self.dense = layers.Dense(em_dim)      def split_heads(self, x, batch_size):         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))         return tf.transpose(x, perm=[0, 2, 1, 3])  # B, num_heads, seq_len, depth      def call(self, v, k, q, mask):         batch_size = tf.shape(q)[0]         q = self.wq(q)  # B, seq_len, em_dim         k = self.wk(k)  # B, seq_len, em_dim         v = self.wv(v)  # B, seq_len, em_dim         q = self.split_heads(q, batch_size)         k = self.split_heads(k, batch_size)         v = self.split_heads(v, batch_size)         scaled_attention = scaled_dot_product_attention(q, k, v, mask)         scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  #B, seq_len, num_heads, depth         concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.em_dim))  # B, seq_len, em_dim         output = self.dense(concat_attention)         return output In\u00a0[8]: Copied! <pre>class EncoderLayer(layers.Layer):\n    def __init__(self, em_dim, num_heads, dff, rate=0.1):\n        super().__init__()\n        self.mha = MultiHeadAttention(em_dim, num_heads)\n        self.ffn = point_wise_feed_forward_network(em_dim, dff)\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        return out2\n\n\nclass DecoderLayer(layers.Layer):\n    def __init__(self, em_dim, num_heads, diff, rate=0.1):\n        super().__init__()\n        self.mha1 = MultiHeadAttention(em_dim, num_heads)\n        self.mha2 = MultiHeadAttention(em_dim, num_heads)\n        self.ffn = point_wise_feed_forward_network(em_dim, diff)\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n        self.dropout3 = layers.Dropout(rate)\n\n    def call(self, x, enc_out, training, decode_mask, padding_mask):\n        attn1 = self.mha1(x, x, x, decode_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n        attn2 = self.mha2(enc_out, enc_out, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n        return out3\n</pre> class EncoderLayer(layers.Layer):     def __init__(self, em_dim, num_heads, dff, rate=0.1):         super().__init__()         self.mha = MultiHeadAttention(em_dim, num_heads)         self.ffn = point_wise_feed_forward_network(em_dim, dff)         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)         self.dropout1 = layers.Dropout(rate)         self.dropout2 = layers.Dropout(rate)      def call(self, x, training, mask):         attn_output = self.mha(x, x, x, mask)         attn_output = self.dropout1(attn_output, training=training)         out1 = self.layernorm1(x + attn_output)         ffn_output = self.ffn(out1)         ffn_output = self.dropout2(ffn_output, training=training)         out2 = self.layernorm2(out1 + ffn_output)         return out2   class DecoderLayer(layers.Layer):     def __init__(self, em_dim, num_heads, diff, rate=0.1):         super().__init__()         self.mha1 = MultiHeadAttention(em_dim, num_heads)         self.mha2 = MultiHeadAttention(em_dim, num_heads)         self.ffn = point_wise_feed_forward_network(em_dim, diff)         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)         self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)         self.dropout1 = layers.Dropout(rate)         self.dropout2 = layers.Dropout(rate)         self.dropout3 = layers.Dropout(rate)      def call(self, x, enc_out, training, decode_mask, padding_mask):         attn1 = self.mha1(x, x, x, decode_mask)         attn1 = self.dropout1(attn1, training=training)         out1 = self.layernorm1(attn1 + x)         attn2 = self.mha2(enc_out, enc_out, out1, padding_mask)         attn2 = self.dropout2(attn2, training=training)         out2 = self.layernorm2(attn2 + out1)         ffn_output = self.ffn(out2)         ffn_output = self.dropout3(ffn_output, training=training)         out3 = self.layernorm3(ffn_output + out2)         return out3 In\u00a0[9]: Copied! <pre>def get_angles(pos, i, em_dim):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(em_dim))\n    return pos * angle_rates\n\n\ndef positional_encoding(position, em_dim):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(em_dim)[np.newaxis, :], em_dim)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n\nclass Encoder(layers.Layer):\n    def __init__(self, num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=0.1):\n        super().__init__()\n        self.em_dim = em_dim\n        self.num_layers = num_layers\n        self.embedding = layers.Embedding(input_vocab, em_dim)\n        self.pos_encoding = positional_encoding(max_pos_enc, self.em_dim)\n        self.enc_layers = [EncoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, x, mask, training=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n        return x\n\n\nclass Decoder(layers.Layer):\n    def __init__(self, num_layers, em_dim, num_heads, dff, target_vocab, max_pos_enc, rate=0.1):\n        super().__init__()\n        self.em_dim = em_dim\n        self.num_layers = num_layers\n        self.embedding = layers.Embedding(target_vocab, em_dim)\n        self.pos_encoding = positional_encoding(max_pos_enc, em_dim)\n        self.dec_layers = [DecoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, x, enc_output, decode_mask, padding_mask, training=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x = self.dec_layers[i](x, enc_output, training, decode_mask, padding_mask)\n        return x\n\n\ndef transformer(num_layers, em_dim, num_heads, dff, input_vocab, target_vocab, max_pos_enc, max_pos_dec, rate=0.1):\n    inputs = layers.Input(shape=(None, ))\n    targets = layers.Input(shape=(None, ))\n    encode_mask = layers.Input(shape=(None, None, None))\n    decode_mask = layers.Input(shape=(None, None, None))\n    x = Encoder(num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=rate)(inputs, encode_mask)\n    x = Decoder(num_layers, em_dim, num_heads, dff, target_vocab, max_pos_dec, rate=rate)(targets,\n                                                                                          x,\n                                                                                          decode_mask,\n                                                                                          encode_mask)\n    x = layers.Dense(target_vocab)(x)\n    model = tf.keras.Model(inputs=[inputs, targets, encode_mask, decode_mask], outputs=x)\n    return model\n</pre> def get_angles(pos, i, em_dim):     angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(em_dim))     return pos * angle_rates   def positional_encoding(position, em_dim):     angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(em_dim)[np.newaxis, :], em_dim)     # apply sin to even indices in the array; 2i     angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])     # apply cos to odd indices in the array; 2i+1     angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])     pos_encoding = angle_rads[np.newaxis, ...]     return tf.cast(pos_encoding, dtype=tf.float32)   class Encoder(layers.Layer):     def __init__(self, num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=0.1):         super().__init__()         self.em_dim = em_dim         self.num_layers = num_layers         self.embedding = layers.Embedding(input_vocab, em_dim)         self.pos_encoding = positional_encoding(max_pos_enc, self.em_dim)         self.enc_layers = [EncoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]         self.dropout = layers.Dropout(rate)      def call(self, x, mask, training=None):         seq_len = tf.shape(x)[1]         x = self.embedding(x)         x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))         x += self.pos_encoding[:, :seq_len, :]         x = self.dropout(x, training=training)         for i in range(self.num_layers):             x = self.enc_layers[i](x, training, mask)         return x   class Decoder(layers.Layer):     def __init__(self, num_layers, em_dim, num_heads, dff, target_vocab, max_pos_enc, rate=0.1):         super().__init__()         self.em_dim = em_dim         self.num_layers = num_layers         self.embedding = layers.Embedding(target_vocab, em_dim)         self.pos_encoding = positional_encoding(max_pos_enc, em_dim)         self.dec_layers = [DecoderLayer(em_dim, num_heads, dff, rate) for _ in range(num_layers)]         self.dropout = layers.Dropout(rate)      def call(self, x, enc_output, decode_mask, padding_mask, training=None):         seq_len = tf.shape(x)[1]         x = self.embedding(x)         x *= tf.math.sqrt(tf.cast(self.em_dim, tf.float32))         x += self.pos_encoding[:, :seq_len, :]         x = self.dropout(x, training=training)         for i in range(self.num_layers):             x = self.dec_layers[i](x, enc_output, training, decode_mask, padding_mask)         return x   def transformer(num_layers, em_dim, num_heads, dff, input_vocab, target_vocab, max_pos_enc, max_pos_dec, rate=0.1):     inputs = layers.Input(shape=(None, ))     targets = layers.Input(shape=(None, ))     encode_mask = layers.Input(shape=(None, None, None))     decode_mask = layers.Input(shape=(None, None, None))     x = Encoder(num_layers, em_dim, num_heads, dff, input_vocab, max_pos_enc, rate=rate)(inputs, encode_mask)     x = Decoder(num_layers, em_dim, num_heads, dff, target_vocab, max_pos_dec, rate=rate)(targets,                                                                                           x,                                                                                           decode_mask,                                                                                           encode_mask)     x = layers.Dense(target_vocab)(x)     model = tf.keras.Model(inputs=[inputs, targets, encode_mask, decode_mask], outputs=x)     return model In\u00a0[10]: Copied! <pre>model = fe.build(\n    model_fn=lambda: transformer(num_layers=4,\n                                 em_dim=em_dim,\n                                 num_heads=8,\n                                 dff=512,\n                                 input_vocab=pt_tokenizer.vocab_size,\n                                 target_vocab=en_tokenizer.vocab_size,\n                                 max_pos_enc=1000,\n                                 max_pos_dec=1000),\n    optimizer_fn=\"adam\")\n</pre> model = fe.build(     model_fn=lambda: transformer(num_layers=4,                                  em_dim=em_dim,                                  num_heads=8,                                  dff=512,                                  input_vocab=pt_tokenizer.vocab_size,                                  target_vocab=en_tokenizer.vocab_size,                                  max_pos_enc=1000,                                  max_pos_dec=1000),     optimizer_fn=\"adam\") In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.loss import LossOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nclass CreateMasks(TensorOp):\n    def forward(self, data, state):\n        inp, tar = data\n        encode_mask = self.create_padding_mask(inp)\n        dec_look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])\n        dec_target_padding_mask = self.create_padding_mask(tar)\n        decode_mask = tf.maximum(dec_target_padding_mask, dec_look_ahead_mask)\n        return encode_mask, decode_mask\n\n    @staticmethod\n    def create_padding_mask(seq):\n        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\n    @staticmethod\n    def create_look_ahead_mask(size):\n        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n        return mask  # (seq_len, seq_len)\n\n\nclass ShiftData(TensorOp):\n    def forward(self, data, state):\n        target = data\n        return target[:, :-1], target[:, 1:]\n\n\nclass MaskedCrossEntropy(LossOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\n    def forward(self, data, state):\n        y_pred, y_true = data\n        mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.float32)\n        loss = self.loss_fn(y_true, y_pred) * mask\n        loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n        return loss\n\nnetwork = fe.Network(ops=[\n    ShiftData(inputs=\"target\", outputs=(\"target_inp\", \"target_real\")),\n    CreateMasks(inputs=(\"source\", \"target_inp\"), outputs=(\"encode_mask\", \"decode_mask\")),\n    ModelOp(model=model, inputs=(\"source\", \"target_inp\", \"encode_mask\", \"decode_mask\"), outputs=\"pred\"),\n    MaskedCrossEntropy(inputs=(\"pred\", \"target_real\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.loss import LossOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp  class CreateMasks(TensorOp):     def forward(self, data, state):         inp, tar = data         encode_mask = self.create_padding_mask(inp)         dec_look_ahead_mask = self.create_look_ahead_mask(tf.shape(tar)[1])         dec_target_padding_mask = self.create_padding_mask(tar)         decode_mask = tf.maximum(dec_target_padding_mask, dec_look_ahead_mask)         return encode_mask, decode_mask      @staticmethod     def create_padding_mask(seq):         seq = tf.cast(tf.math.equal(seq, 0), tf.float32)         return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)      @staticmethod     def create_look_ahead_mask(size):         mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)         return mask  # (seq_len, seq_len)   class ShiftData(TensorOp):     def forward(self, data, state):         target = data         return target[:, :-1], target[:, 1:]   class MaskedCrossEntropy(LossOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')      def forward(self, data, state):         y_pred, y_true = data         mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), tf.float32)         loss = self.loss_fn(y_true, y_pred) * mask         loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)         return loss  network = fe.Network(ops=[     ShiftData(inputs=\"target\", outputs=(\"target_inp\", \"target_real\")),     CreateMasks(inputs=(\"source\", \"target_inp\"), outputs=(\"encode_mask\", \"decode_mask\")),     ModelOp(model=model, inputs=(\"source\", \"target_inp\", \"encode_mask\", \"decode_mask\"), outputs=\"pred\"),     MaskedCrossEntropy(inputs=(\"pred\", \"target_real\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) In\u00a0[12]: Copied! <pre>import tempfile\n\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric.bleu_score import BleuScore\nfrom fastestimator.trace.trace import Trace\n\nmodel_dir=tempfile.mkdtemp()\n\ndef lr_fn(step, em_dim, warmupstep=4000):\n    lr = em_dim**-0.5 * min(step**-0.5, step * warmupstep**-1.5)\n    return lr\n\nclass MaskedAccuracy(Trace):\n    def on_epoch_begin(self, data):\n        self.correct = 0\n        self.total = 0\n\n    def on_batch_end(self, data):\n        y_pred, y_true = data[\"pred\"].numpy(), data[\"target_real\"].numpy()\n        mask = np.logical_not(y_true == 0)\n        matches = np.logical_and(y_true == np.argmax(y_pred, axis=2), mask)\n        self.correct += np.sum(matches)\n        self.total += np.sum(mask)\n\n    def on_epoch_end(self, data):\n        data.write_with_log(self.outputs[0], self.correct / self.total)\n\n\ntraces = [\n    MaskedAccuracy(inputs=(\"pred\", \"target_real\"), outputs=\"masked_acc\", mode=\"!train\"),\n    BleuScore(true_key=\"target_real\", pred_key =\"pred\", output_name=\"bleu_score\", n_gram=2, mode=\"!train\"),\n    BestModelSaver(model=model, save_dir=model_dir, metric=\"masked_acc\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lambda step: lr_fn(step, em_dim))]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         traces=traces,\n                         epochs=epochs,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> import tempfile  from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric.bleu_score import BleuScore from fastestimator.trace.trace import Trace  model_dir=tempfile.mkdtemp()  def lr_fn(step, em_dim, warmupstep=4000):     lr = em_dim**-0.5 * min(step**-0.5, step * warmupstep**-1.5)     return lr  class MaskedAccuracy(Trace):     def on_epoch_begin(self, data):         self.correct = 0         self.total = 0      def on_batch_end(self, data):         y_pred, y_true = data[\"pred\"].numpy(), data[\"target_real\"].numpy()         mask = np.logical_not(y_true == 0)         matches = np.logical_and(y_true == np.argmax(y_pred, axis=2), mask)         self.correct += np.sum(matches)         self.total += np.sum(mask)      def on_epoch_end(self, data):         data.write_with_log(self.outputs[0], self.correct / self.total)   traces = [     MaskedAccuracy(inputs=(\"pred\", \"target_real\"), outputs=\"masked_acc\", mode=\"!train\"),     BleuScore(true_key=\"target_real\", pred_key =\"pred\", output_name=\"bleu_score\", n_gram=2, mode=\"!train\"),     BestModelSaver(model=model, save_dir=model_dir, metric=\"masked_acc\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lambda step: lr_fn(step, em_dim))]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          traces=traces,                          epochs=epochs,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() In\u00a0[14]: Copied! <pre>def token_to_words(sample, tokenizer):\n    words = tokenizer.decode(sample)\n    if '[CLS]' in words:\n        words = words[words.index('[CLS]')+5:]\n    if '[SEP]' in words:\n        words = words[:words.index('[SEP]')]\n    return words\n\nsample_test_data = pipeline.get_results(mode=\"test\")\nsample_test_data = network.transform(data=sample_test_data, mode=\"test\")\nsource = sample_test_data[\"source\"].numpy()\npredicted = sample_test_data[\"pred\"].numpy()\npredicted = np.argmax(predicted, axis=-1)\ngrouth_truth = sample_test_data[\"target_real\"].numpy()\n</pre> def token_to_words(sample, tokenizer):     words = tokenizer.decode(sample)     if '[CLS]' in words:         words = words[words.index('[CLS]')+5:]     if '[SEP]' in words:         words = words[:words.index('[SEP]')]     return words  sample_test_data = pipeline.get_results(mode=\"test\") sample_test_data = network.transform(data=sample_test_data, mode=\"test\") source = sample_test_data[\"source\"].numpy() predicted = sample_test_data[\"pred\"].numpy() predicted = np.argmax(predicted, axis=-1) grouth_truth = sample_test_data[\"target_real\"].numpy() In\u00a0[16]: Copied! <pre>index = np.random.randint(0, source.shape[0])\nsample_source, sample_predicted, sample_groud_truth = source[index], predicted[index], grouth_truth[index]\nprint(\"Source Language: \")\nprint(token_to_words(sample_source, pt_tokenizer))\nprint(\"\")\nprint(\"Translation Ground Truth: \")\nprint(token_to_words(sample_groud_truth, en_tokenizer))\nprint(\"\")\nprint(\"Machine Translation: \")\nprint(token_to_words(sample_predicted, en_tokenizer))\n</pre> index = np.random.randint(0, source.shape[0]) sample_source, sample_predicted, sample_groud_truth = source[index], predicted[index], grouth_truth[index] print(\"Source Language: \") print(token_to_words(sample_source, pt_tokenizer)) print(\"\") print(\"Translation Ground Truth: \") print(token_to_words(sample_groud_truth, en_tokenizer)) print(\"\") print(\"Machine Translation: \") print(token_to_words(sample_predicted, en_tokenizer)) <pre>Source Language: \n muito obrigado. \n\nTranslation Ground Truth: \nthank you very much. \n\nMachine Translation: \nthank you very much. \n</pre> <p>You are welcome.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#neural-machine-translation-using-transformer", "title": "Neural Machine Translation Using Transformer\u00b6", "text": "<p>In this tutorial we will look at a sequence to sequence task: translating one language into another. The architecture used for the task is the famous <code>Transformer</code>.</p> <p>The transformer architecture was first proposed by this paper. The general idea behind the architecture is the <code>attention</code> mechanism that can perform a re-weighting of the features throughout the network. Another advantage brought by the transformer architecture is that it breaks the temporal dependency of the data, allowing more efficient parallelization of training. We will implement every detail of the transformer in this tutorial. Let's get started!</p> <p>First let's define some hyper-parameters that we will use later.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#dataset", "title": "Dataset\u00b6", "text": "<p>In this machine translation task, we will use the TED translation dataset. The dataset consists of 14 different translation tasks, such as Portuguese to English (<code>pt_to_en</code>), Russian to English (<code>ru_to_en</code>), and many others. In this tutorial, we will translate Portuguese to English. You can access this dataset through our dataset API - <code>tednmt</code>. Feel free to check the docstring of the API for other translation options.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#preprocessing-the-languages", "title": "Preprocessing the languages\u00b6", "text": "<p>Since the text by itself cannot be recognized by computers, we need to perform a series of transformations to the text. Here are the steps:</p> <ol> <li>Split the sentence into words or sub-words. For example, \"I love apple\" can be split into [\"I\", \"love\", \"apple\"]. Sometimes in order to represent more words, a word is further reduced into sub-words. For example, <code>tokenization</code> can be split into <code>token</code> and <code>_ization</code>. As a result, a word like \"civilization\" doesn't require extra space when both <code>civil</code> and <code>_ization</code> are already in the dictionary.</li> <li>Map the tokens into a discrete index according to the dictionary. In this task, we are loading a pretrained tokenizer with a built-in dictionary already.</li> <li>Add a [start] and [end] token around every index. This is mainly to help the network identify the beginning and end of each sentence.</li> <li>When creating a batch of multiple sentences, pad the shorter sentences with 0 so that each sentence in the batch has the same length.</li> </ol>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#preprocessing-results", "title": "Preprocessing Results\u00b6", "text": ""}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#transformer-architecture", "title": "Transformer Architecture\u00b6", "text": ""}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#attention-unit", "title": "Attention Unit\u00b6", "text": "<p>The basic form of the attention unit is defined in <code>scaled_dot_product_attention</code>. Given a set of queries(Q), keys(K), and values(V), it first performs the matrix multiplication of Q and K. The output of this multiplication gives the matching score between various elements of Q and K. Then all the weights are normalized across the Keys dimension. Finally, the normalized score will be multiplied by the V to get the final result.  The intuition behind the attention unit is essentially a dictionary look-up with interpolation.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#multi-head-attention", "title": "Multi-head Attention\u00b6", "text": "<p>There are two drawbacks of the attention unit above:</p> <ol> <li>The complexity of matrix multiplication is O(N^3), when batch size or embedding dimension increases, the computation will not scale well.</li> <li>A single attention head is limited in expressing local correlation between two words, because it calculates correlation by normalizing all embeddings dimensions. Sometimes this overall normalization will remove interesting local patterns.  A good analogy is to consider a single attention unit as globally averaging a signal whereas a moving average is preferred to preserve certain information.</li> </ol> <p>Multi-head attention is used to overcome the issues above. It breaks the embedding dimension into multiple heads. As a result, each head's embedding dimension is divided by the number of heads, reducing the computation complexity. Moreover, each head only takes a fraction of the embedding and can be viewed as a specialized expert for a specific context.  The final results can be combined using another dense layer.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#encoder-and-decoder-layer", "title": "Encoder and Decoder layer\u00b6", "text": "<p>Both the encoder and decoder layers will go through multi-head attention. The decoder layer will use another multi-attention module to connect the bridge between encoder outputs and targets. Specifically, in the decoders second multi-head attention module, encoded output is used as both values and keys whereas the target embedding is used as a query to \"look up\" encoder information. In the end, there is a feed-forward neural network to transform the looked-up value into something useful.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#putting-everything-together", "title": "Putting Everything Together\u00b6", "text": "<p>A transformer consists of an Encoder and Decoder, which in turn consist of multiple stacked encoder/decoder layers. One interesting property of transformers is that they do not have an intrinsic awareness of the position dimension. Therefore, a position encoding is usually done to the embedding matrix to add position context to the embedding. A nice tutorial about positional encoding can be found here.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#network-operations", "title": "Network Operations\u00b6", "text": "<p>Now that we have defined the transformer architecture, another thing that is worth mentioning is the mask. A mask is a boolean array that we created to tell the network to ignore certain words within the sentence. For example, to tell the network to ignore the words we padded, a padding mask is used. In order to not give away the answer when processing the word before it, a mask is also needed.</p> <p>The loss function of transformer is simply a masked cross entropy loss, as it will only consider predictions that are not masked.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#metrics-and-learning-rate-scheduling", "title": "Metrics and Learning Rate Scheduling\u00b6", "text": "<p>The metric used to evaluate the model is a masked accuracy, which is simply accuracy with unmasked predictions and ground truths. The learning rate scheduler uses warm-up followed by exponential decay.</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#start-the-training", "title": "Start the training\u00b6", "text": "<p>The training will take around 30 minutes on a single V100 GPU</p>"}, {"location": "apphub/NLP/neural_machine_translation/transformer.html#lets-translate-something", "title": "Let's translate something!\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html", "title": "Adversarial Robustness with Error Correcting Codes", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D\nfrom tensorflow.keras.models import Model\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.layers.tensorflow import HadamardCode\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.op.tensorop.gradient import FGSM, Watch\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import EarlyStopping\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  from tensorflow.keras import Sequential, layers from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D from tensorflow.keras.models import Model  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifair10 from fastestimator.layers.tensorflow import HadamardCode from fastestimator.op.numpyop.univariate import Normalize from fastestimator.op.tensorop.gradient import FGSM, Watch from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import EarlyStopping from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=60\nbatch_size=50\nlog_steps=500\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=60 batch_size=50 log_steps=500 train_steps_per_epoch=None eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))])\n</pre> train_data, eval_data = cifair10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))]) In\u00a0[4]: Copied! <pre>def get_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        UpdateOp(model=model, loss_name=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_ce\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch,\n                             monitor_names=[\"adv_ce\"])\n    return estimator\n</pre> def get_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         UpdateOp(model=model, loss_name=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_ce\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch,                              monitor_names=[\"adv_ce\"])     return estimator In\u00a0[5]: Copied! <pre>softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax')\n</pre> softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax') <pre>Metal device set to: Apple M1 Max\n</pre> <pre>2022-04-14 07:30:40.088441: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-04-14 07:30:40.088549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)\n</pre> In\u00a0[6]: Copied! <pre>def EccLeNet(input_shape=(32, 32, 3), classes=10):\n    model = Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(HadamardCode(classes))  # Note that this is the only difference between this model and the FE LeNet implementation\n    return model\n</pre> def EccLeNet(input_shape=(32, 32, 3), classes=10):     model = Sequential()     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.Flatten())     model.add(layers.Dense(64, activation='relu'))     model.add(HadamardCode(classes))  # Note that this is the only difference between this model and the FE LeNet implementation     return model In\u00a0[7]: Copied! <pre>ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc')\n</pre> ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc') In\u00a0[8]: Copied! <pre>def HydraEccLeNet(input_shape=(32, 32, 3), classes=10):\n    inputs = Input(input_shape)\n    conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)\n    flat = Flatten()(conv3)\n    # Create multiple heads\n    n_heads = 4\n    heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]\n    outputs = HadamardCode(classes)(heads)\n    return Model(inputs=inputs, outputs=outputs)\n</pre> def HydraEccLeNet(input_shape=(32, 32, 3), classes=10):     inputs = Input(input_shape)     conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)     pool1 = MaxPooling2D((2, 2))(conv1)     conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)     pool2 = MaxPooling2D((2, 2))(conv2)     conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)     flat = Flatten()(conv3)     # Create multiple heads     n_heads = 4     heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]     outputs = HadamardCode(classes)(heads)     return Model(inputs=inputs, outputs=outputs) In\u00a0[9]: Copied! <pre>hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc')\n</pre> hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc') In\u00a0[10]: Copied! <pre>softmax_estimator = get_estimator(softmax_model)\necc_estimator = get_estimator(ecc_model)\nhydra_estimator = get_estimator(hydra_model)\n</pre> softmax_estimator = get_estimator(softmax_model) ecc_estimator = get_estimator(ecc_model) hydra_estimator = get_estimator(hydra_model) <pre>2022-04-14 07:30:40.921443: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[11]: Copied! <pre>softmax_estimator.fit('Softmax')\nsoftmax_results = softmax_estimator.test()\n</pre> softmax_estimator.fit('Softmax') softmax_results = softmax_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_ce: 2.2992618;\nFastEstimator-Train: step: 500; base_ce: 1.512064; steps/sec: 180.41;\nFastEstimator-Train: step: 1000; base_ce: 1.3816669; steps/sec: 188.64;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 7.03 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 2.0590332; adversarial accuracy: 0.2904; base accuracy: 0.59; base_ce: 1.1634815; min_base_ce: 1.1634815; since_best_base_ce: 0;\nFastEstimator-Train: step: 1500; base_ce: 0.99346334; steps/sec: 162.71;\nFastEstimator-Train: step: 2000; base_ce: 0.8344573; steps/sec: 188.63;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 5.73 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 2.2008438; adversarial accuracy: 0.2832; base accuracy: 0.6564; base_ce: 0.98102194; min_base_ce: 0.98102194; since_best_base_ce: 0;\nFastEstimator-Train: step: 2500; base_ce: 0.6389392; steps/sec: 158.94;\nFastEstimator-Train: step: 3000; base_ce: 0.9886453; steps/sec: 175.98;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 5.99 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 2.3973176; adversarial accuracy: 0.2788; base accuracy: 0.6766; base_ce: 0.9346378; min_base_ce: 0.9346378; since_best_base_ce: 0;\nFastEstimator-Train: step: 3500; base_ce: 0.48805878; steps/sec: 162.87;\nFastEstimator-Train: step: 4000; base_ce: 0.9593257; steps/sec: 183.3;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 5.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 2.5404387; adversarial accuracy: 0.2724; base accuracy: 0.6946; base_ce: 0.90526956; min_base_ce: 0.90526956; since_best_base_ce: 0;\nFastEstimator-Train: step: 4500; base_ce: 0.65144515; steps/sec: 158.58;\nFastEstimator-Train: step: 5000; base_ce: 0.51473236; steps/sec: 179.0;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 5.95 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 2.856645; adversarial accuracy: 0.2528; base accuracy: 0.7084; base_ce: 0.87103415; min_base_ce: 0.87103415; since_best_base_ce: 0;\nFastEstimator-Train: step: 5500; base_ce: 0.31187427; steps/sec: 163.7;\nFastEstimator-Train: step: 6000; base_ce: 0.63420796; steps/sec: 184.62;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 5.76 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 2.93197; adversarial accuracy: 0.2436; base accuracy: 0.718; base_ce: 0.842367; min_base_ce: 0.842367; since_best_base_ce: 0;\nFastEstimator-Train: step: 6500; base_ce: 0.4245732; steps/sec: 164.78;\nFastEstimator-Train: step: 7000; base_ce: 0.37809974; steps/sec: 187.83;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 5.69 sec;\nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 3.2728195; adversarial accuracy: 0.233; base accuracy: 0.7182; base_ce: 0.85198694; min_base_ce: 0.842367; since_best_base_ce: 1;\nFastEstimator-Train: step: 7500; base_ce: 0.6512601; steps/sec: 166.02;\nFastEstimator-Train: step: 8000; base_ce: 0.34757873; steps/sec: 190.29;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 5.64 sec;\nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 3.677762; adversarial accuracy: 0.2322; base accuracy: 0.7178; base_ce: 0.87585616; min_base_ce: 0.842367; since_best_base_ce: 2;\nFastEstimator-Train: step: 8500; base_ce: 0.6901499; steps/sec: 164.41;\nFastEstimator-Train: step: 9000; base_ce: 0.5857948; steps/sec: 180.34;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 5.82 sec;\nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 3.9997666; adversarial accuracy: 0.2078; base accuracy: 0.7158; base_ce: 0.9179778; min_base_ce: 0.842367; since_best_base_ce: 3;\nFastEstimator-Train: step: 9500; base_ce: 0.551821; steps/sec: 164.98;\nFastEstimator-Train: step: 10000; base_ce: 0.30067065; steps/sec: 189.53;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 5.67 sec;\nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 4.3430243; adversarial accuracy: 0.1862; base accuracy: 0.724; base_ce: 0.9034187; min_base_ce: 0.842367; since_best_base_ce: 4;\nFastEstimator-Train: step: 10500; base_ce: 0.30980313; steps/sec: 166.19;\nFastEstimator-Train: step: 11000; base_ce: 0.2635348; steps/sec: 189.53;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 5.65 sec;\nFastEstimator-Eval: step: 11000; epoch: 11; adv_ce: 4.8428307; adversarial accuracy: 0.187; base accuracy: 0.73; base_ce: 0.93213964; min_base_ce: 0.842367; since_best_base_ce: 5;\nFastEstimator-Train: step: 11500; base_ce: 0.33661276; steps/sec: 168.02;\nFastEstimator-Train: step: 12000; base_ce: 0.34464568; steps/sec: 187.98;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 5.64 sec;\nFastEstimator-Eval: step: 12000; epoch: 12; adv_ce: 5.329111; adversarial accuracy: 0.1838; base accuracy: 0.7162; base_ce: 1.0212883; min_base_ce: 0.842367; since_best_base_ce: 6;\nFastEstimator-Train: step: 12500; base_ce: 0.2674638; steps/sec: 168.76;\nFastEstimator-Train: step: 13000; base_ce: 0.2755358; steps/sec: 194.63;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 5.53 sec;\nFastEstimator-Eval: step: 13000; epoch: 13; adv_ce: 5.7377477; adversarial accuracy: 0.1756; base accuracy: 0.7216; base_ce: 1.0674266; min_base_ce: 0.842367; since_best_base_ce: 7;\nFastEstimator-Train: step: 13500; base_ce: 0.43096292; steps/sec: 165.88;\nFastEstimator-Train: step: 14000; base_ce: 0.2341825; steps/sec: 192.04;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 5.63 sec;\nFastEstimator-Eval: step: 14000; epoch: 14; adv_ce: 6.306669; adversarial accuracy: 0.1518; base accuracy: 0.7022; base_ce: 1.1309965; min_base_ce: 0.842367; since_best_base_ce: 8;\nFastEstimator-Train: step: 14500; base_ce: 0.20541003; steps/sec: 166.96;\nFastEstimator-Train: step: 15000; base_ce: 0.36601982; steps/sec: 191.74;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 5.6 sec;\nFastEstimator-Eval: step: 15000; epoch: 15; adv_ce: 6.5554395; adversarial accuracy: 0.159; base accuracy: 0.7178; base_ce: 1.1321083; min_base_ce: 0.842367; since_best_base_ce: 9;\nFastEstimator-Train: step: 15500; base_ce: 0.47732756; steps/sec: 166.0;\nFastEstimator-Train: step: 16000; base_ce: 0.19374126; steps/sec: 192.74;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 5.6 sec;\nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 0.8423669934272766 at epoch 6\nFastEstimator-Eval: step: 16000; epoch: 16; adv_ce: 7.4802423; adversarial accuracy: 0.1508; base accuracy: 0.7102; base_ce: 1.2362047; min_base_ce: 0.842367; since_best_base_ce: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/softmax_best_base_ce.h5\nFastEstimator-Finish: step: 16000; softmax_lr: 0.001; total_time: 110.44 sec;\nFastEstimator-Test: step: 16000; epoch: 16; adv_ce: 2.9070234; adversarial accuracy: 0.2336; base accuracy: 0.7194; base_ce: 0.811303;\n</pre> In\u00a0[12]: Copied! <pre>ecc_estimator.fit('ECC')\necc_results = ecc_estimator.test()\n</pre> ecc_estimator.fit('ECC') ecc_results = ecc_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_ce: 2.3106103;\nFastEstimator-Train: step: 500; base_ce: 1.9567659; steps/sec: 153.58;\nFastEstimator-Train: step: 1000; base_ce: 1.5948787; steps/sec: 168.59;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 6.85 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 2.151366; adversarial accuracy: 0.3106; base accuracy: 0.4942; base_ce: 1.6855414; min_base_ce: 1.6855414; since_best_base_ce: 0;\nFastEstimator-Train: step: 1500; base_ce: 1.6709622; steps/sec: 134.1;\nFastEstimator-Train: step: 2000; base_ce: 1.6304135; steps/sec: 163.29;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 6.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 2.2654982; adversarial accuracy: 0.3354; base accuracy: 0.5846; base_ce: 1.4812744; min_base_ce: 1.4812744; since_best_base_ce: 0;\nFastEstimator-Train: step: 2500; base_ce: 1.4377127; steps/sec: 135.04;\nFastEstimator-Train: step: 3000; base_ce: 1.2552406; steps/sec: 163.37;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 6.77 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 2.3034942; adversarial accuracy: 0.342; base accuracy: 0.6134; base_ce: 1.3836565; min_base_ce: 1.3836565; since_best_base_ce: 0;\nFastEstimator-Train: step: 3500; base_ce: 1.1992656; steps/sec: 136.27;\nFastEstimator-Train: step: 4000; base_ce: 1.2122211; steps/sec: 165.07;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 6.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 2.375598; adversarial accuracy: 0.3474; base accuracy: 0.6484; base_ce: 1.299376; min_base_ce: 1.299376; since_best_base_ce: 0;\nFastEstimator-Train: step: 4500; base_ce: 0.8894017; steps/sec: 132.69;\nFastEstimator-Train: step: 5000; base_ce: 1.0109656; steps/sec: 165.32;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 6.8 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 2.466221; adversarial accuracy: 0.3408; base accuracy: 0.6542; base_ce: 1.2570887; min_base_ce: 1.2570887; since_best_base_ce: 0;\nFastEstimator-Train: step: 5500; base_ce: 1.0434448; steps/sec: 129.97;\nFastEstimator-Train: step: 6000; base_ce: 1.1524091; steps/sec: 162.84;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 6.91 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 2.5495539; adversarial accuracy: 0.3436; base accuracy: 0.6666; base_ce: 1.2395524; min_base_ce: 1.2395524; since_best_base_ce: 0;\nFastEstimator-Train: step: 6500; base_ce: 1.2838608; steps/sec: 141.08;\nFastEstimator-Train: step: 7000; base_ce: 0.98052794; steps/sec: 166.13;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 6.56 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 2.4138238; adversarial accuracy: 0.3694; base accuracy: 0.6848; base_ce: 1.1781778; min_base_ce: 1.1781778; since_best_base_ce: 0;\nFastEstimator-Train: step: 7500; base_ce: 0.78041; steps/sec: 137.73;\nFastEstimator-Train: step: 8000; base_ce: 1.0777975; steps/sec: 161.29;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 6.73 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 2.5248635; adversarial accuracy: 0.374; base accuracy: 0.6934; base_ce: 1.171383; min_base_ce: 1.171383; since_best_base_ce: 0;\nFastEstimator-Train: step: 8500; base_ce: 0.90453595; steps/sec: 137.67;\nFastEstimator-Train: step: 9000; base_ce: 0.7657153; steps/sec: 161.72;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 6.72 sec;\nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 2.5365229; adversarial accuracy: 0.3634; base accuracy: 0.6856; base_ce: 1.1813465; min_base_ce: 1.171383; since_best_base_ce: 1;\nFastEstimator-Train: step: 9500; base_ce: 1.1377211; steps/sec: 137.9;\nFastEstimator-Train: step: 10000; base_ce: 0.82338774; steps/sec: 161.54;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 6.72 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 2.5089622; adversarial accuracy: 0.3752; base accuracy: 0.7032; base_ce: 1.1360503; min_base_ce: 1.1360503; since_best_base_ce: 0;\nFastEstimator-Train: step: 10500; base_ce: 0.716062; steps/sec: 137.2;\nFastEstimator-Train: step: 11000; base_ce: 0.90397626; steps/sec: 163.97;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 6.69 sec;\nFastEstimator-Eval: step: 11000; epoch: 11; adv_ce: 2.540486; adversarial accuracy: 0.3902; base accuracy: 0.7002; base_ce: 1.1411539; min_base_ce: 1.1360503; since_best_base_ce: 1;\nFastEstimator-Train: step: 11500; base_ce: 0.7946057; steps/sec: 132.16;\nFastEstimator-Train: step: 12000; base_ce: 0.81318045; steps/sec: 159.61;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 6.92 sec;\nFastEstimator-Eval: step: 12000; epoch: 12; adv_ce: 2.6211627; adversarial accuracy: 0.3826; base accuracy: 0.704; base_ce: 1.1610231; min_base_ce: 1.1360503; since_best_base_ce: 2;\nFastEstimator-Train: step: 12500; base_ce: 0.8362641; steps/sec: 131.45;\nFastEstimator-Train: step: 13000; base_ce: 1.0650773; steps/sec: 163.39;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 6.89 sec;\nFastEstimator-Eval: step: 13000; epoch: 13; adv_ce: 2.5993066; adversarial accuracy: 0.3768; base accuracy: 0.7022; base_ce: 1.1479177; min_base_ce: 1.1360503; since_best_base_ce: 3;\nFastEstimator-Train: step: 13500; base_ce: 0.78695935; steps/sec: 130.28;\nFastEstimator-Train: step: 14000; base_ce: 0.43967634; steps/sec: 165.29;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 6.85 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Eval: step: 14000; epoch: 14; adv_ce: 2.5545912; adversarial accuracy: 0.4; base accuracy: 0.7118; base_ce: 1.1282525; min_base_ce: 1.1282525; since_best_base_ce: 0;\nFastEstimator-Train: step: 14500; base_ce: 0.7823583; steps/sec: 137.14;\nFastEstimator-Train: step: 15000; base_ce: 0.8720597; steps/sec: 165.03;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 6.68 sec;\nFastEstimator-Eval: step: 15000; epoch: 15; adv_ce: 2.605087; adversarial accuracy: 0.3916; base accuracy: 0.7014; base_ce: 1.1404293; min_base_ce: 1.1282525; since_best_base_ce: 1;\nFastEstimator-Train: step: 15500; base_ce: 0.59988195; steps/sec: 139.13;\nFastEstimator-Train: step: 16000; base_ce: 0.65357494; steps/sec: 165.72;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 6.6 sec;\nFastEstimator-Eval: step: 16000; epoch: 16; adv_ce: 2.5662081; adversarial accuracy: 0.403; base accuracy: 0.7098; base_ce: 1.1337018; min_base_ce: 1.1282525; since_best_base_ce: 2;\nFastEstimator-Train: step: 16500; base_ce: 0.5405857; steps/sec: 135.12;\nFastEstimator-Train: step: 17000; base_ce: 0.8562554; steps/sec: 157.99;\nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 6.87 sec;\nFastEstimator-Eval: step: 17000; epoch: 17; adv_ce: 2.6947477; adversarial accuracy: 0.3736; base accuracy: 0.704; base_ce: 1.144717; min_base_ce: 1.1282525; since_best_base_ce: 3;\nFastEstimator-Train: step: 17500; base_ce: 0.64489233; steps/sec: 128.86;\nFastEstimator-Train: step: 18000; base_ce: 0.6826552; steps/sec: 156.12;\nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 7.08 sec;\nFastEstimator-Eval: step: 18000; epoch: 18; adv_ce: 2.6476705; adversarial accuracy: 0.3972; base accuracy: 0.7; base_ce: 1.1826608; min_base_ce: 1.1282525; since_best_base_ce: 4;\nFastEstimator-Train: step: 18500; base_ce: 0.64138246; steps/sec: 124.32;\nFastEstimator-Train: step: 19000; base_ce: 0.793949; steps/sec: 158.96;\nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 7.17 sec;\nFastEstimator-Eval: step: 19000; epoch: 19; adv_ce: 2.7317286; adversarial accuracy: 0.3846; base accuracy: 0.7012; base_ce: 1.192893; min_base_ce: 1.1282525; since_best_base_ce: 5;\nFastEstimator-Train: step: 19500; base_ce: 0.5358307; steps/sec: 133.34;\nFastEstimator-Train: step: 20000; base_ce: 0.868741; steps/sec: 157.33;\nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 6.94 sec;\nFastEstimator-Eval: step: 20000; epoch: 20; adv_ce: 2.749324; adversarial accuracy: 0.3824; base accuracy: 0.6984; base_ce: 1.2226379; min_base_ce: 1.1282525; since_best_base_ce: 6;\nFastEstimator-Train: step: 20500; base_ce: 0.7731119; steps/sec: 136.83;\nFastEstimator-Train: step: 21000; base_ce: 0.7152124; steps/sec: 159.93;\nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 6.79 sec;\nFastEstimator-Eval: step: 21000; epoch: 21; adv_ce: 2.7222548; adversarial accuracy: 0.3898; base accuracy: 0.7008; base_ce: 1.1841533; min_base_ce: 1.1282525; since_best_base_ce: 7;\nFastEstimator-Train: step: 21500; base_ce: 0.88036644; steps/sec: 134.66;\nFastEstimator-Train: step: 22000; base_ce: 0.45785868; steps/sec: 158.36;\nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 6.86 sec;\nFastEstimator-Eval: step: 22000; epoch: 22; adv_ce: 2.764157; adversarial accuracy: 0.3804; base accuracy: 0.694; base_ce: 1.1981807; min_base_ce: 1.1282525; since_best_base_ce: 8;\nFastEstimator-Train: step: 22500; base_ce: 0.26250863; steps/sec: 132.61;\nFastEstimator-Train: step: 23000; base_ce: 0.83438385; steps/sec: 158.73;\nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 6.92 sec;\nFastEstimator-Eval: step: 23000; epoch: 23; adv_ce: 2.7135825; adversarial accuracy: 0.392; base accuracy: 0.7018; base_ce: 1.1811721; min_base_ce: 1.1282525; since_best_base_ce: 9;\nFastEstimator-Train: step: 23500; base_ce: 0.4100248; steps/sec: 135.94;\nFastEstimator-Train: step: 24000; base_ce: 0.59091467; steps/sec: 161.11;\nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 6.77 sec;\nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 1.1282525062561035 at epoch 14\nFastEstimator-Eval: step: 24000; epoch: 24; adv_ce: 2.776509; adversarial accuracy: 0.3814; base accuracy: 0.697; base_ce: 1.2234251; min_base_ce: 1.1282525; since_best_base_ce: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/ecc_best_base_ce.h5\nFastEstimator-Finish: step: 24000; ecc_lr: 0.001; total_time: 197.08 sec;\nFastEstimator-Test: step: 24000; epoch: 24; adv_ce: 2.5469933; adversarial accuracy: 0.4202; base accuracy: 0.717; base_ce: 1.1081417;\n</pre> In\u00a0[13]: Copied! <pre>hydra_estimator.fit('Hydra')\nhydra_results = hydra_estimator.test()\n</pre> hydra_estimator.fit('Hydra') hydra_results = hydra_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_ce: 2.3209429;\nFastEstimator-Train: step: 500; base_ce: 1.9059893; steps/sec: 109.68;\nFastEstimator-Train: step: 1000; base_ce: 1.599645; steps/sec: 121.69;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 9.57 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 2.1371648; adversarial accuracy: 0.314; base accuracy: 0.477; base_ce: 1.7292327; min_base_ce: 1.7292327; since_best_base_ce: 0;\nFastEstimator-Train: step: 1500; base_ce: 1.740985; steps/sec: 103.45;\nFastEstimator-Train: step: 2000; base_ce: 1.534274; steps/sec: 117.73;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 9.08 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 2.0958247; adversarial accuracy: 0.3542; base accuracy: 0.555; base_ce: 1.5324627; min_base_ce: 1.5324627; since_best_base_ce: 0;\nFastEstimator-Train: step: 2500; base_ce: 1.3189642; steps/sec: 93.57;\nFastEstimator-Train: step: 3000; base_ce: 1.1266878; steps/sec: 109.73;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 9.9 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 2.0866797; adversarial accuracy: 0.3738; base accuracy: 0.6226; base_ce: 1.3853276; min_base_ce: 1.3853276; since_best_base_ce: 0;\nFastEstimator-Train: step: 3500; base_ce: 1.1933565; steps/sec: 100.91;\nFastEstimator-Train: step: 4000; base_ce: 1.1618978; steps/sec: 122.24;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 9.05 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 2.1071148; adversarial accuracy: 0.3718; base accuracy: 0.6342; base_ce: 1.3465571; min_base_ce: 1.3465571; since_best_base_ce: 0;\nFastEstimator-Train: step: 4500; base_ce: 1.4193944; steps/sec: 98.28;\nFastEstimator-Train: step: 5000; base_ce: 0.96144146; steps/sec: 112.84;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 9.52 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 2.2091506; adversarial accuracy: 0.3728; base accuracy: 0.6512; base_ce: 1.2852292; min_base_ce: 1.2852292; since_best_base_ce: 0;\nFastEstimator-Train: step: 5500; base_ce: 1.3246837; steps/sec: 99.79;\nFastEstimator-Train: step: 6000; base_ce: 0.90941876; steps/sec: 120.71;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 9.15 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 2.2634563; adversarial accuracy: 0.365; base accuracy: 0.6618; base_ce: 1.2636027; min_base_ce: 1.2636027; since_best_base_ce: 0;\nFastEstimator-Train: step: 6500; base_ce: 0.9575111; steps/sec: 98.3;\nFastEstimator-Train: step: 7000; base_ce: 0.79568386; steps/sec: 121.21;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 9.2 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 2.225419; adversarial accuracy: 0.3908; base accuracy: 0.6736; base_ce: 1.213738; min_base_ce: 1.213738; since_best_base_ce: 0;\nFastEstimator-Train: step: 7500; base_ce: 1.2011161; steps/sec: 98.64;\nFastEstimator-Train: step: 8000; base_ce: 0.78585136; steps/sec: 122.86;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 9.14 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 2.2856061; adversarial accuracy: 0.3966; base accuracy: 0.6786; base_ce: 1.2058243; min_base_ce: 1.2058243; since_best_base_ce: 0;\nFastEstimator-Train: step: 8500; base_ce: 0.91969687; steps/sec: 97.95;\nFastEstimator-Train: step: 9000; base_ce: 1.2670665; steps/sec: 123.34;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 9.17 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 2.2384853; adversarial accuracy: 0.4022; base accuracy: 0.6932; base_ce: 1.162873; min_base_ce: 1.162873; since_best_base_ce: 0;\nFastEstimator-Train: step: 9500; base_ce: 0.7936392; steps/sec: 102.81;\nFastEstimator-Train: step: 10000; base_ce: 1.1028607; steps/sec: 124.48;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 8.87 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 2.26161; adversarial accuracy: 0.3988; base accuracy: 0.694; base_ce: 1.1394957; min_base_ce: 1.1394957; since_best_base_ce: 0;\nFastEstimator-Train: step: 10500; base_ce: 0.96117824; steps/sec: 95.28;\nFastEstimator-Train: step: 11000; base_ce: 0.8865253; steps/sec: 120.41;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 9.41 sec;\nFastEstimator-Eval: step: 11000; epoch: 11; adv_ce: 2.3774323; adversarial accuracy: 0.3826; base accuracy: 0.6836; base_ce: 1.2022808; min_base_ce: 1.1394957; since_best_base_ce: 1;\nFastEstimator-Train: step: 11500; base_ce: 0.67801213; steps/sec: 93.64;\nFastEstimator-Train: step: 12000; base_ce: 0.95619464; steps/sec: 121.61;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 9.44 sec;\nFastEstimator-Eval: step: 12000; epoch: 12; adv_ce: 2.33686; adversarial accuracy: 0.3914; base accuracy: 0.6898; base_ce: 1.1678803; min_base_ce: 1.1394957; since_best_base_ce: 2;\nFastEstimator-Train: step: 12500; base_ce: 0.80759734; steps/sec: 96.39;\nFastEstimator-Train: step: 13000; base_ce: 0.6847743; steps/sec: 124.15;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 9.21 sec;\nFastEstimator-Eval: step: 13000; epoch: 13; adv_ce: 2.338109; adversarial accuracy: 0.3966; base accuracy: 0.698; base_ce: 1.1532812; min_base_ce: 1.1394957; since_best_base_ce: 3;\nFastEstimator-Train: step: 13500; base_ce: 0.7953265; steps/sec: 89.64;\nFastEstimator-Train: step: 14000; base_ce: 1.0528977; steps/sec: 121.43;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 9.71 sec;\nFastEstimator-Eval: step: 14000; epoch: 14; adv_ce: 2.375578; adversarial accuracy: 0.3962; base accuracy: 0.7; base_ce: 1.1491884; min_base_ce: 1.1394957; since_best_base_ce: 4;\nFastEstimator-Train: step: 14500; base_ce: 0.8380149; steps/sec: 99.43;\nFastEstimator-Train: step: 15000; base_ce: 0.75329435; steps/sec: 122.08;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 9.11 sec;\nFastEstimator-Eval: step: 15000; epoch: 15; adv_ce: 2.3749874; adversarial accuracy: 0.3878; base accuracy: 0.6958; base_ce: 1.1639857; min_base_ce: 1.1394957; since_best_base_ce: 5;\nFastEstimator-Train: step: 15500; base_ce: 0.8191551; steps/sec: 98.63;\nFastEstimator-Train: step: 16000; base_ce: 0.99632496; steps/sec: 120.51;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 9.24 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 16000; epoch: 16; adv_ce: 2.3621073; adversarial accuracy: 0.4034; base accuracy: 0.7092; base_ce: 1.1288407; min_base_ce: 1.1288407; since_best_base_ce: 0;\nFastEstimator-Train: step: 16500; base_ce: 0.77608883; steps/sec: 98.23;\nFastEstimator-Train: step: 17000; base_ce: 0.6935814; steps/sec: 121.98;\nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 9.18 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 17000; epoch: 17; adv_ce: 2.3885064; adversarial accuracy: 0.4072; base accuracy: 0.7064; base_ce: 1.1193285; min_base_ce: 1.1193285; since_best_base_ce: 0;\nFastEstimator-Train: step: 17500; base_ce: 0.8365773; steps/sec: 96.56;\nFastEstimator-Train: step: 18000; base_ce: 0.938272; steps/sec: 121.61;\nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 9.29 sec;\nFastEstimator-Eval: step: 18000; epoch: 18; adv_ce: 2.3680475; adversarial accuracy: 0.4114; base accuracy: 0.712; base_ce: 1.1202908; min_base_ce: 1.1193285; since_best_base_ce: 1;\nFastEstimator-Train: step: 18500; base_ce: 0.82442915; steps/sec: 103.31;\nFastEstimator-Train: step: 19000; base_ce: 0.42202026; steps/sec: 123.8;\nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 8.87 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 19000; epoch: 19; adv_ce: 2.391935; adversarial accuracy: 0.4026; base accuracy: 0.713; base_ce: 1.1003814; min_base_ce: 1.1003814; since_best_base_ce: 0;\nFastEstimator-Train: step: 19500; base_ce: 0.93993056; steps/sec: 100.82;\nFastEstimator-Train: step: 20000; base_ce: 0.707767; steps/sec: 123.73;\nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 9.01 sec;\nFastEstimator-Eval: step: 20000; epoch: 20; adv_ce: 2.447187; adversarial accuracy: 0.3872; base accuracy: 0.7118; base_ce: 1.1085609; min_base_ce: 1.1003814; since_best_base_ce: 1;\nFastEstimator-Train: step: 20500; base_ce: 0.88275725; steps/sec: 92.15;\nFastEstimator-Train: step: 21000; base_ce: 0.83274955; steps/sec: 117.37;\nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 9.69 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Eval: step: 21000; epoch: 21; adv_ce: 2.4366765; adversarial accuracy: 0.3946; base accuracy: 0.7194; base_ce: 1.0885057; min_base_ce: 1.0885057; since_best_base_ce: 0;\nFastEstimator-Train: step: 21500; base_ce: 1.0195268; steps/sec: 94.25;\nFastEstimator-Train: step: 22000; base_ce: 0.8516718; steps/sec: 120.2;\nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 9.45 sec;\nFastEstimator-Eval: step: 22000; epoch: 22; adv_ce: 2.5237665; adversarial accuracy: 0.3792; base accuracy: 0.7104; base_ce: 1.1355748; min_base_ce: 1.0885057; since_best_base_ce: 1;\nFastEstimator-Train: step: 22500; base_ce: 0.57069576; steps/sec: 92.88;\nFastEstimator-Train: step: 23000; base_ce: 0.6884723; steps/sec: 123.25;\nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 9.44 sec;\nFastEstimator-Eval: step: 23000; epoch: 23; adv_ce: 2.5008826; adversarial accuracy: 0.3864; base accuracy: 0.7078; base_ce: 1.1259525; min_base_ce: 1.0885057; since_best_base_ce: 2;\nFastEstimator-Train: step: 23500; base_ce: 0.7877195; steps/sec: 88.34;\nFastEstimator-Train: step: 24000; base_ce: 0.625138; steps/sec: 121.91;\nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 9.77 sec;\nFastEstimator-Eval: step: 24000; epoch: 24; adv_ce: 2.491919; adversarial accuracy: 0.3924; base accuracy: 0.7072; base_ce: 1.1492797; min_base_ce: 1.0885057; since_best_base_ce: 3;\nFastEstimator-Train: step: 24500; base_ce: 1.3304155; steps/sec: 97.49;\nFastEstimator-Train: step: 25000; base_ce: 0.8153142; steps/sec: 123.25;\nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 9.18 sec;\nFastEstimator-Eval: step: 25000; epoch: 25; adv_ce: 2.4916878; adversarial accuracy: 0.389; base accuracy: 0.7124; base_ce: 1.142636; min_base_ce: 1.0885057; since_best_base_ce: 4;\nFastEstimator-Train: step: 25500; base_ce: 0.68842775; steps/sec: 96.83;\nFastEstimator-Train: step: 26000; base_ce: 0.6803902; steps/sec: 120.0;\nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 9.34 sec;\nFastEstimator-Eval: step: 26000; epoch: 26; adv_ce: 2.4946892; adversarial accuracy: 0.392; base accuracy: 0.7108; base_ce: 1.1262212; min_base_ce: 1.0885057; since_best_base_ce: 5;\nFastEstimator-Train: step: 26500; base_ce: 0.6629103; steps/sec: 100.2;\nFastEstimator-Train: step: 27000; base_ce: 0.67052734; steps/sec: 123.34;\nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 9.02 sec;\nFastEstimator-Eval: step: 27000; epoch: 27; adv_ce: 2.483842; adversarial accuracy: 0.4036; base accuracy: 0.7202; base_ce: 1.1074018; min_base_ce: 1.0885057; since_best_base_ce: 6;\nFastEstimator-Train: step: 27500; base_ce: 0.54780537; steps/sec: 98.46;\nFastEstimator-Train: step: 28000; base_ce: 0.56461316; steps/sec: 122.82;\nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 9.15 sec;\nFastEstimator-Eval: step: 28000; epoch: 28; adv_ce: 2.5989664; adversarial accuracy: 0.3676; base accuracy: 0.7062; base_ce: 1.1447918; min_base_ce: 1.0885057; since_best_base_ce: 7;\nFastEstimator-Train: step: 28500; base_ce: 0.6223859; steps/sec: 97.36;\nFastEstimator-Train: step: 29000; base_ce: 0.66837525; steps/sec: 121.58;\nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 9.25 sec;\nFastEstimator-Eval: step: 29000; epoch: 29; adv_ce: 2.603566; adversarial accuracy: 0.3688; base accuracy: 0.7108; base_ce: 1.1207267; min_base_ce: 1.0885057; since_best_base_ce: 8;\nFastEstimator-Train: step: 29500; base_ce: 0.8323528; steps/sec: 99.67;\nFastEstimator-Train: step: 30000; base_ce: 0.861635; steps/sec: 124.67;\nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 9.03 sec;\nFastEstimator-Eval: step: 30000; epoch: 30; adv_ce: 2.541872; adversarial accuracy: 0.3884; base accuracy: 0.7156; base_ce: 1.1296782; min_base_ce: 1.0885057; since_best_base_ce: 9;\nFastEstimator-Train: step: 30500; base_ce: 0.7760727; steps/sec: 93.62;\nFastEstimator-Train: step: 31000; base_ce: 0.72736603; steps/sec: 121.38;\nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 9.46 sec;\nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 1.088505744934082 at epoch 21\nFastEstimator-Eval: step: 31000; epoch: 31; adv_ce: 2.5675056; adversarial accuracy: 0.3874; base accuracy: 0.7156; base_ce: 1.1196213; min_base_ce: 1.0885057; since_best_base_ce: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpjrsdl6xj/hydra_ecc_best_base_ce.h5\nFastEstimator-Finish: step: 31000; hydra_ecc_lr: 0.001; total_time: 343.98 sec;\nFastEstimator-Test: step: 31000; epoch: 31; adv_ce: 2.4052315; adversarial accuracy: 0.4046; base accuracy: 0.721; base_ce: 1.0806081;\n</pre> In\u00a0[14]: Copied! <pre>logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'})\n</pre> logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'}) <p>As you can see, the conventional network using softmax to convert logits to class probabilities actually gets more and more vulnerable to adversarial attacks as training progresses. It also quickly overfits to the data, reaching an optimal performance around epoch 7. By simply switching the softmax layer for an error-correcting-code, the network is able to train for around 16 epochs before starting to cap out, and even then continuing to train it results in better and better adversarial performance. Creating a multi-headed ecc output layer allows still more training and higher peak performances. If you were to run the experiment out to 160 epochs you would find that the adversarial accuracy can reach between 60-70% with only a slight accuracy degradation on clean samples (performance still above 70%). This is significantly better performance than networks trained specifically to combat this attack, shown in the FGSM notebook. Note also that their is virtually no additional cost to training using ECC as opposed to softmax in terms of steps/sec. This is a big benefit over FGSM, where the training time for each step is doubled. With these benefits in mind, you may want to consider never using softmax again.</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#adversarial-robustness-with-error-correcting-codes", "title": "Adversarial Robustness with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#you-might-never-use-softmax-again", "title": "(You might never use Softmax again)\u00b6", "text": "<p>In this example we will show how using error correcting codes to convert model logits to probabilities can drastically reduce model overfitting while simultaneously increasing model robustness against adversarial attacks. In other words, why you should never use a softmax layer again. This phenomena was first publicized by the US Army in a 2019 Neurips Paper. For background on adversarial attacks, and on the attack type we will be demonstrating here, check out our FGSM apphub example. Note that in this apphub we will not be training against adversarial samples, but only performing adversarial attacks during evaluation to see how different models fair against them.</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#imports", "title": "Imports\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#getting-the-data", "title": "Getting the Data\u00b6", "text": "<p>For these experiments we will be using the ciFAIR10 Dataset (like cifar10, but with duplicate testing data replaced)</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#defining-an-estimator", "title": "Defining an Estimator\u00b6", "text": "<p>In this apphub we will be comparing three very similar models, all using the same training and evaluation routines. Hence a function to generate the estimators:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#the-models", "title": "The Models\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#1-a-lenet-model-with-softmax", "title": "1 - A LeNet model with Softmax\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#2-a-lenet-model-with-error-correcting-codes", "title": "2 - A LeNet model with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc/ecc.html#3-a-lenet-model-using-ecc-and-multiple-feature-heads", "title": "3 - A LeNet model using ECC and multiple feature heads\u00b6", "text": "<p>While it is common practice to follow the feature extraction layers of convolution networks with several fully connected layers in order to perform classification, this can lead to the final logits being interdependent which can actually reduce the robustness of the network. One way around this is to divide your classification layers into multiple smaller independent units:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#the-experiments", "title": "The Experiments\u00b6", "text": "<p>Let's get Estimators for each of these models and compare them:</p>"}, {"location": "apphub/adversarial_training/ecc/ecc.html#comparing-the-results", "title": "Comparing the Results\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html", "title": "Adversarial Robustness with Error Correcting Codes (and Hinge Loss)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras.layers import Concatenate, Conv2D, Dense, Flatten, Input, MaxPooling2D\nfrom tensorflow.keras.models import Model\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.univariate import Hadamard, Normalize\nfrom fastestimator.op.tensorop import UnHadamard\nfrom fastestimator.op.tensorop.gradient import FGSM, Watch\nfrom fastestimator.op.tensorop.loss import CrossEntropy, Hinge\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import EarlyStopping\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  from tensorflow.keras import Sequential, layers from tensorflow.keras.layers import Concatenate, Conv2D, Dense, Flatten, Input, MaxPooling2D from tensorflow.keras.models import Model  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.univariate import Hadamard, Normalize from fastestimator.op.tensorop import UnHadamard from fastestimator.op.tensorop.gradient import FGSM, Watch from fastestimator.op.tensorop.loss import CrossEntropy, Hinge from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import EarlyStopping from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=60\nbatch_size=50\nlog_steps=500\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=60 batch_size=50 log_steps=500 train_steps_per_epoch=None eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        Hadamard(inputs=\"y\", outputs=\"y_code\", n_classes=10)\n        ])\n</pre> train_data, eval_data = cifair10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         Hadamard(inputs=\"y\", outputs=\"y_code\", n_classes=10)         ]) In\u00a0[4]: Copied! <pre>def get_baseline_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        UpdateOp(model=model, loss_name=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_ce\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch,\n                             monitor_names=[\"adv_ce\"])\n    return estimator\n</pre> def get_baseline_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         UpdateOp(model=model, loss_name=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=('eval', 'test')),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\", mode=('eval', 'test'))     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_ce\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch,                              monitor_names=[\"adv_ce\"])     return estimator In\u00a0[5]: Copied! <pre>def get_hinge_estimator(model):\n    network = fe.Network(ops=[\n        Watch(inputs=\"x\", mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred_code\"),\n        Hinge(inputs=(\"y_pred_code\", \"y_code\"), outputs=\"base_hinge\"),\n        UpdateOp(model=model, loss_name=\"base_hinge\"),\n        UnHadamard(inputs=\"y_pred_code\", outputs=\"y_pred\", n_classes=10, mode=('eval', 'test')),\n        # The adversarial attack:\n        FGSM(data=\"x\", loss=\"base_hinge\", outputs=\"x_adverse_hinge\", epsilon=epsilon, mode=('eval', 'test')),\n        ModelOp(model=model, inputs=\"x_adverse_hinge\", outputs=\"y_pred_adv_hinge_code\", mode=('eval', 'test')),\n        Hinge(inputs=(\"y_pred_adv_hinge_code\", \"y_code\"), outputs=\"adv_hinge\", mode=('eval', 'test')),\n        UnHadamard(inputs=\"y_pred_adv_hinge_code\", outputs=\"y_pred_adv_hinge\", n_classes=10, mode=('eval', 'test')),\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),\n        Accuracy(true_key=\"y\", pred_key=\"y_pred_adv_hinge\", output_name=\"adversarial_accuracy\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"base_hinge\", save_best_mode=\"min\", load_best_final=True),\n        EarlyStopping(monitor=\"base_hinge\", patience=10)\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             log_steps=log_steps,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch,\n                             monitor_names=[\"adv_hinge\"])\n    return estimator\n</pre> def get_hinge_estimator(model):     network = fe.Network(ops=[         Watch(inputs=\"x\", mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred_code\"),         Hinge(inputs=(\"y_pred_code\", \"y_code\"), outputs=\"base_hinge\"),         UpdateOp(model=model, loss_name=\"base_hinge\"),         UnHadamard(inputs=\"y_pred_code\", outputs=\"y_pred\", n_classes=10, mode=('eval', 'test')),         # The adversarial attack:         FGSM(data=\"x\", loss=\"base_hinge\", outputs=\"x_adverse_hinge\", epsilon=epsilon, mode=('eval', 'test')),         ModelOp(model=model, inputs=\"x_adverse_hinge\", outputs=\"y_pred_adv_hinge_code\", mode=('eval', 'test')),         Hinge(inputs=(\"y_pred_adv_hinge_code\", \"y_code\"), outputs=\"adv_hinge\", mode=('eval', 'test')),         UnHadamard(inputs=\"y_pred_adv_hinge_code\", outputs=\"y_pred_adv_hinge\", n_classes=10, mode=('eval', 'test')),     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"base_accuracy\"),         Accuracy(true_key=\"y\", pred_key=\"y_pred_adv_hinge\", output_name=\"adversarial_accuracy\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"base_hinge\", save_best_mode=\"min\", load_best_final=True),         EarlyStopping(monitor=\"base_hinge\", patience=10)     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              log_steps=log_steps,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch,                              monitor_names=[\"adv_hinge\"])     return estimator In\u00a0[6]: Copied! <pre>softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax')\n</pre> softmax_model = fe.build(model_fn=lambda:LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name='softmax') <pre>2022-04-13 17:05:52.944653: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-04-13 17:05:52.944780: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)\n</pre> <pre>Metal device set to: Apple M1 Max\n</pre> In\u00a0[7]: Copied! <pre>def EccLeNet(input_shape=(32, 32, 3), code_length=16):\n    model = Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(code_length, activation='tanh'))  # Note that this is the only difference between this model and the FE LeNet implementation\n    return model\n</pre> def EccLeNet(input_shape=(32, 32, 3), code_length=16):     model = Sequential()     model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.MaxPooling2D((2, 2)))     model.add(layers.Conv2D(64, (3, 3), activation='relu'))     model.add(layers.Flatten())     model.add(layers.Dense(64, activation='relu'))     model.add(layers.Dense(code_length, activation='tanh'))  # Note that this is the only difference between this model and the FE LeNet implementation     return model In\u00a0[8]: Copied! <pre>ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc')\n</pre> ecc_model = fe.build(model_fn=EccLeNet, optimizer_fn=\"adam\", model_name='ecc') In\u00a0[9]: Copied! <pre>def HydraEccLeNet(input_shape=(32, 32, 3), code_length=16):\n    inputs = Input(input_shape)\n    conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)\n    flat = Flatten()(conv3)\n    # Create multiple heads\n    n_heads = 4\n    heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]\n    heads2 = [Dense(code_length // n_heads, activation='tanh')(head) for head in heads]\n    outputs = Concatenate()(heads2)\n    return Model(inputs=inputs, outputs=outputs)\n</pre> def HydraEccLeNet(input_shape=(32, 32, 3), code_length=16):     inputs = Input(input_shape)     conv1 = Conv2D(32, (3, 3), activation='relu')(inputs)     pool1 = MaxPooling2D((2, 2))(conv1)     conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)     pool2 = MaxPooling2D((2, 2))(conv2)     conv3 = Conv2D(64, (3, 3), activation='relu')(pool2)     flat = Flatten()(conv3)     # Create multiple heads     n_heads = 4     heads = [Dense(16, activation='relu')(flat) for _ in range(n_heads)]     heads2 = [Dense(code_length // n_heads, activation='tanh')(head) for head in heads]     outputs = Concatenate()(heads2)     return Model(inputs=inputs, outputs=outputs) In\u00a0[10]: Copied! <pre>hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc')\n</pre> hydra_model = fe.build(model_fn=HydraEccLeNet, optimizer_fn=\"adam\", model_name='hydra_ecc') In\u00a0[11]: Copied! <pre>softmax_estimator = get_baseline_estimator(softmax_model)\necc_estimator = get_hinge_estimator(ecc_model)\nhydra_estimator = get_hinge_estimator(hydra_model)\n</pre> softmax_estimator = get_baseline_estimator(softmax_model) ecc_estimator = get_hinge_estimator(ecc_model) hydra_estimator = get_hinge_estimator(hydra_model) <pre>2022-04-13 17:05:53.690181: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[12]: Copied! <pre>softmax_estimator.fit('Softmax')\nsoftmax_results = softmax_estimator.test()\n</pre> softmax_estimator.fit('Softmax') softmax_results = softmax_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'y_code' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_ce: 2.3505318;\nFastEstimator-Train: step: 500; base_ce: 1.2900878; steps/sec: 169.3;\nFastEstimator-Train: step: 1000; base_ce: 1.0868878; steps/sec: 171.39;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 7.54 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 1.9342302; adversarial_accuracy: 0.3144; base_accuracy: 0.5896; base_ce: 1.146428; min_base_ce: 1.146428; since_best_base_ce: 0;\nFastEstimator-Train: step: 1500; base_ce: 1.1560345; steps/sec: 164.6;\nFastEstimator-Train: step: 2000; base_ce: 1.0841315; steps/sec: 175.9;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 5.88 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 2.1664774; adversarial_accuracy: 0.301; base_accuracy: 0.65; base_ce: 1.0123901; min_base_ce: 1.0123901; since_best_base_ce: 0;\nFastEstimator-Train: step: 2500; base_ce: 0.73313004; steps/sec: 157.13;\nFastEstimator-Train: step: 3000; base_ce: 1.0141094; steps/sec: 176.9;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 6.0 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 2.3282566; adversarial_accuracy: 0.2952; base_accuracy: 0.6774; base_ce: 0.9235382; min_base_ce: 0.9235382; since_best_base_ce: 0;\nFastEstimator-Train: step: 3500; base_ce: 0.5947217; steps/sec: 163.7;\nFastEstimator-Train: step: 4000; base_ce: 1.0634463; steps/sec: 178.62;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 5.85 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 2.6093562; adversarial_accuracy: 0.2648; base_accuracy: 0.6952; base_ce: 0.9063715; min_base_ce: 0.9063715; since_best_base_ce: 0;\nFastEstimator-Train: step: 4500; base_ce: 0.81916714; steps/sec: 160.13;\nFastEstimator-Train: step: 5000; base_ce: 0.8680224; steps/sec: 178.95;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 5.92 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 2.778289; adversarial_accuracy: 0.2582; base_accuracy: 0.7124; base_ce: 0.8622628; min_base_ce: 0.8622628; since_best_base_ce: 0;\nFastEstimator-Train: step: 5500; base_ce: 1.0295881; steps/sec: 160.73;\nFastEstimator-Train: step: 6000; base_ce: 0.5846585; steps/sec: 184.45;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 5.82 sec;\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 3.056959; adversarial_accuracy: 0.235; base_accuracy: 0.7088; base_ce: 0.86487746; min_base_ce: 0.8622628; since_best_base_ce: 1;\nFastEstimator-Train: step: 6500; base_ce: 0.50238585; steps/sec: 150.58;\nFastEstimator-Train: step: 7000; base_ce: 0.54035467; steps/sec: 161.76;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 6.42 sec;\nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 3.3898497; adversarial_accuracy: 0.2372; base_accuracy: 0.7162; base_ce: 0.88421905; min_base_ce: 0.8622628; since_best_base_ce: 2;\nFastEstimator-Train: step: 7500; base_ce: 0.46544573; steps/sec: 145.52;\nFastEstimator-Train: step: 8000; base_ce: 0.71973133; steps/sec: 175.91;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 6.27 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 3.5737696; adversarial_accuracy: 0.2222; base_accuracy: 0.7214; base_ce: 0.8615385; min_base_ce: 0.8615385; since_best_base_ce: 0;\nFastEstimator-Train: step: 8500; base_ce: 0.6100302; steps/sec: 157.41;\nFastEstimator-Train: step: 9000; base_ce: 0.58645684; steps/sec: 184.26;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 5.89 sec;\nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 3.8579502; adversarial_accuracy: 0.2122; base_accuracy: 0.7154; base_ce: 0.90025634; min_base_ce: 0.8615385; since_best_base_ce: 1;\nFastEstimator-Train: step: 9500; base_ce: 0.5049728; steps/sec: 164.96;\nFastEstimator-Train: step: 10000; base_ce: 0.6656696; steps/sec: 179.83;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 5.81 sec;\nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 4.3775306; adversarial_accuracy: 0.1894; base_accuracy: 0.7056; base_ce: 0.9664465; min_base_ce: 0.8615385; since_best_base_ce: 2;\nFastEstimator-Train: step: 10500; base_ce: 0.29843685; steps/sec: 168.0;\nFastEstimator-Train: step: 11000; base_ce: 0.43359518; steps/sec: 191.24;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 5.59 sec;\nFastEstimator-Eval: step: 11000; epoch: 11; adv_ce: 5.0255003; adversarial_accuracy: 0.1828; base_accuracy: 0.707; base_ce: 1.0204912; min_base_ce: 0.8615385; since_best_base_ce: 3;\nFastEstimator-Train: step: 11500; base_ce: 0.41417968; steps/sec: 164.39;\nFastEstimator-Train: step: 12000; base_ce: 0.42944288; steps/sec: 189.79;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 5.68 sec;\nFastEstimator-Eval: step: 12000; epoch: 12; adv_ce: 5.359952; adversarial_accuracy: 0.1696; base_accuracy: 0.7042; base_ce: 1.0600693; min_base_ce: 0.8615385; since_best_base_ce: 4;\nFastEstimator-Train: step: 12500; base_ce: 0.3950038; steps/sec: 153.69;\nFastEstimator-Train: step: 13000; base_ce: 0.16283879; steps/sec: 179.13;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 6.05 sec;\nFastEstimator-Eval: step: 13000; epoch: 13; adv_ce: 5.7364006; adversarial_accuracy: 0.1794; base_accuracy: 0.7082; base_ce: 1.1232861; min_base_ce: 0.8615385; since_best_base_ce: 5;\nFastEstimator-Train: step: 13500; base_ce: 0.32468435; steps/sec: 162.28;\nFastEstimator-Train: step: 14000; base_ce: 0.19765873; steps/sec: 184.49;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 5.79 sec;\nFastEstimator-Eval: step: 14000; epoch: 14; adv_ce: 5.8575244; adversarial_accuracy: 0.1704; base_accuracy: 0.7028; base_ce: 1.1115676; min_base_ce: 0.8615385; since_best_base_ce: 6;\nFastEstimator-Train: step: 14500; base_ce: 0.26641122; steps/sec: 162.69;\nFastEstimator-Train: step: 15000; base_ce: 0.2560778; steps/sec: 178.57;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 5.87 sec;\nFastEstimator-Eval: step: 15000; epoch: 15; adv_ce: 7.1690974; adversarial_accuracy: 0.1616; base_accuracy: 0.705; base_ce: 1.2743791; min_base_ce: 0.8615385; since_best_base_ce: 7;\nFastEstimator-Train: step: 15500; base_ce: 0.28445697; steps/sec: 160.85;\nFastEstimator-Train: step: 16000; base_ce: 0.27751252; steps/sec: 179.17;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 5.91 sec;\nFastEstimator-Eval: step: 16000; epoch: 16; adv_ce: 7.266639; adversarial_accuracy: 0.1604; base_accuracy: 0.7054; base_ce: 1.2886978; min_base_ce: 0.8615385; since_best_base_ce: 8;\nFastEstimator-Train: step: 16500; base_ce: 0.2318586; steps/sec: 155.11;\nFastEstimator-Train: step: 17000; base_ce: 0.14783162; steps/sec: 184.27;\nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 5.93 sec;\nFastEstimator-Eval: step: 17000; epoch: 17; adv_ce: 8.196975; adversarial_accuracy: 0.159; base_accuracy: 0.7066; base_ce: 1.3935325; min_base_ce: 0.8615385; since_best_base_ce: 9;\nFastEstimator-Train: step: 17500; base_ce: 0.3715953; steps/sec: 166.94;\nFastEstimator-Train: step: 18000; base_ce: 0.21967937; steps/sec: 181.36;\nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 5.75 sec;\nFastEstimator-EarlyStopping: 'base_ce' triggered an early stop. Its best value was 0.8615385293960571 at epoch 8\nFastEstimator-Eval: step: 18000; epoch: 18; adv_ce: 8.571767; adversarial_accuracy: 0.1602; base_accuracy: 0.702; base_ce: 1.4734117; min_base_ce: 0.8615385; since_best_base_ce: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/softmax_best_base_ce.h5\nFastEstimator-Finish: step: 18000; softmax_lr: 0.001; total_time: 128.94 sec;\nFastEstimator-Test: step: 18000; epoch: 18; adv_ce: 3.6057885; adversarial_accuracy: 0.2186; base_accuracy: 0.7206; base_ce: 0.855904;\n</pre> In\u00a0[13]: Copied! <pre>ecc_estimator.fit('ECC')\necc_results = ecc_estimator.test()\n</pre> ecc_estimator.fit('ECC') ecc_results = ecc_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'y' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_hinge: 0.999596;\nFastEstimator-Train: step: 500; base_hinge: 0.77081025; steps/sec: 171.05;\nFastEstimator-Train: step: 1000; base_hinge: 0.62755454; steps/sec: 177.78;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 6.29 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_hinge: 0.7516223; adversarial_accuracy: 0.259; base_accuracy: 0.3914; base_hinge: 0.6413806; min_base_hinge: 0.6413806; since_best_base_hinge: 0;\nFastEstimator-Train: step: 1500; base_hinge: 0.6501051; steps/sec: 145.76;\nFastEstimator-Train: step: 2000; base_hinge: 0.47245127; steps/sec: 173.18;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 6.32 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_hinge: 0.7038234; adversarial_accuracy: 0.3176; base_accuracy: 0.5038; base_hinge: 0.5589157; min_base_hinge: 0.5589157; since_best_base_hinge: 0;\nFastEstimator-Train: step: 2500; base_hinge: 0.5615465; steps/sec: 150.65;\nFastEstimator-Train: step: 3000; base_hinge: 0.51083314; steps/sec: 174.93;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 6.17 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_hinge: 0.6786241; adversarial_accuracy: 0.3506; base_accuracy: 0.5512; base_hinge: 0.50700206; min_base_hinge: 0.50700206; since_best_base_hinge: 0;\nFastEstimator-Train: step: 3500; base_hinge: 0.49202853; steps/sec: 153.9;\nFastEstimator-Train: step: 4000; base_hinge: 0.41295394; steps/sec: 180.68;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 6.05 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_hinge: 0.68908304; adversarial_accuracy: 0.3428; base_accuracy: 0.5982; base_hinge: 0.4595022; min_base_hinge: 0.4595022; since_best_base_hinge: 0;\nFastEstimator-Train: step: 4500; base_hinge: 0.34178427; steps/sec: 137.02;\nFastEstimator-Train: step: 5000; base_hinge: 0.43433386; steps/sec: 161.73;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 6.71 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_hinge: 0.6604099; adversarial_accuracy: 0.3722; base_accuracy: 0.6326; base_hinge: 0.42740837; min_base_hinge: 0.42740837; since_best_base_hinge: 0;\nFastEstimator-Train: step: 5500; base_hinge: 0.41189644; steps/sec: 144.49;\nFastEstimator-Train: step: 6000; base_hinge: 0.4554962; steps/sec: 160.9;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 6.58 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_hinge: 0.6719003; adversarial_accuracy: 0.3536; base_accuracy: 0.6472; base_hinge: 0.40192205; min_base_hinge: 0.40192205; since_best_base_hinge: 0;\nFastEstimator-Train: step: 6500; base_hinge: 0.32702583; steps/sec: 134.67;\nFastEstimator-Train: step: 7000; base_hinge: 0.32336193; steps/sec: 176.44;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 6.53 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_hinge: 0.6758878; adversarial_accuracy: 0.351; base_accuracy: 0.656; base_hinge: 0.38305122; min_base_hinge: 0.38305122; since_best_base_hinge: 0;\nFastEstimator-Train: step: 7500; base_hinge: 0.3770857; steps/sec: 146.85;\nFastEstimator-Train: step: 8000; base_hinge: 0.36640465; steps/sec: 176.21;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 6.24 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_hinge: 0.66526324; adversarial_accuracy: 0.36; base_accuracy: 0.6564; base_hinge: 0.37443084; min_base_hinge: 0.37443084; since_best_base_hinge: 0;\nFastEstimator-Train: step: 8500; base_hinge: 0.4330097; steps/sec: 152.61;\nFastEstimator-Train: step: 9000; base_hinge: 0.28264937; steps/sec: 179.83;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 6.05 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 9000; epoch: 9; adv_hinge: 0.6605204; adversarial_accuracy: 0.3632; base_accuracy: 0.6688; base_hinge: 0.36260226; min_base_hinge: 0.36260226; since_best_base_hinge: 0;\nFastEstimator-Train: step: 9500; base_hinge: 0.25725213; steps/sec: 158.7;\nFastEstimator-Train: step: 10000; base_hinge: 0.33918726; steps/sec: 187.6;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 5.82 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_hinge: 0.6670266; adversarial_accuracy: 0.3644; base_accuracy: 0.6786; base_hinge: 0.3502937; min_base_hinge: 0.3502937; since_best_base_hinge: 0;\nFastEstimator-Train: step: 10500; base_hinge: 0.24327007; steps/sec: 156.93;\nFastEstimator-Train: step: 11000; base_hinge: 0.27944666; steps/sec: 188.6;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 5.84 sec;\nFastEstimator-Eval: step: 11000; epoch: 11; adv_hinge: 0.6592089; adversarial_accuracy: 0.3662; base_accuracy: 0.6704; base_hinge: 0.35914975; min_base_hinge: 0.3502937; since_best_base_hinge: 1;\nFastEstimator-Train: step: 11500; base_hinge: 0.32648894; steps/sec: 157.13;\nFastEstimator-Train: step: 12000; base_hinge: 0.25949845; steps/sec: 181.27;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 5.93 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 12000; epoch: 12; adv_hinge: 0.651387; adversarial_accuracy: 0.3758; base_accuracy: 0.6894; base_hinge: 0.33977306; min_base_hinge: 0.33977306; since_best_base_hinge: 0;\nFastEstimator-Train: step: 12500; base_hinge: 0.36375687; steps/sec: 149.42;\nFastEstimator-Train: step: 13000; base_hinge: 0.19662806; steps/sec: 182.06;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 6.09 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 13000; epoch: 13; adv_hinge: 0.64508665; adversarial_accuracy: 0.3786; base_accuracy: 0.7002; base_hinge: 0.3291817; min_base_hinge: 0.3291817; since_best_base_hinge: 0;\nFastEstimator-Train: step: 13500; base_hinge: 0.32477826; steps/sec: 142.68;\nFastEstimator-Train: step: 14000; base_hinge: 0.22079967; steps/sec: 169.74;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 6.47 sec;\nFastEstimator-Eval: step: 14000; epoch: 14; adv_hinge: 0.66262734; adversarial_accuracy: 0.3688; base_accuracy: 0.6934; base_hinge: 0.3377885; min_base_hinge: 0.3291817; since_best_base_hinge: 1;\nFastEstimator-Train: step: 14500; base_hinge: 0.30110896; steps/sec: 145.77;\nFastEstimator-Train: step: 15000; base_hinge: 0.19694538; steps/sec: 172.02;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 6.34 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 15000; epoch: 15; adv_hinge: 0.65070426; adversarial_accuracy: 0.3776; base_accuracy: 0.7036; base_hinge: 0.32673293; min_base_hinge: 0.32673293; since_best_base_hinge: 0;\nFastEstimator-Train: step: 15500; base_hinge: 0.29805455; steps/sec: 142.52;\nFastEstimator-Train: step: 16000; base_hinge: 0.20518483; steps/sec: 181.85;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 6.25 sec;\nFastEstimator-Eval: step: 16000; epoch: 16; adv_hinge: 0.6414124; adversarial_accuracy: 0.3878; base_accuracy: 0.7002; base_hinge: 0.33122116; min_base_hinge: 0.32673293; since_best_base_hinge: 1;\nFastEstimator-Train: step: 16500; base_hinge: 0.27236506; steps/sec: 149.75;\nFastEstimator-Train: step: 17000; base_hinge: 0.42725775; steps/sec: 182.74;\nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 6.08 sec;\nFastEstimator-Eval: step: 17000; epoch: 17; adv_hinge: 0.6625693; adversarial_accuracy: 0.3716; base_accuracy: 0.7006; base_hinge: 0.33163765; min_base_hinge: 0.32673293; since_best_base_hinge: 2;\nFastEstimator-Train: step: 17500; base_hinge: 0.2649264; steps/sec: 154.25;\nFastEstimator-Train: step: 18000; base_hinge: 0.16135535; steps/sec: 181.42;\nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 5.98 sec;\nFastEstimator-Eval: step: 18000; epoch: 18; adv_hinge: 0.6661872; adversarial_accuracy: 0.3626; base_accuracy: 0.697; base_hinge: 0.33215272; min_base_hinge: 0.32673293; since_best_base_hinge: 3;\nFastEstimator-Train: step: 18500; base_hinge: 0.22723289; steps/sec: 144.34;\nFastEstimator-Train: step: 19000; base_hinge: 0.35645804; steps/sec: 176.94;\nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 6.31 sec;\nFastEstimator-Eval: step: 19000; epoch: 19; adv_hinge: 0.6585936; adversarial_accuracy: 0.3732; base_accuracy: 0.6946; base_hinge: 0.33224463; min_base_hinge: 0.32673293; since_best_base_hinge: 4;\nFastEstimator-Train: step: 19500; base_hinge: 0.32812256; steps/sec: 151.88;\nFastEstimator-Train: step: 20000; base_hinge: 0.28209794; steps/sec: 179.77;\nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 6.05 sec;\nFastEstimator-Eval: step: 20000; epoch: 20; adv_hinge: 0.68119395; adversarial_accuracy: 0.3558; base_accuracy: 0.6958; base_hinge: 0.33329198; min_base_hinge: 0.32673293; since_best_base_hinge: 5;\nFastEstimator-Train: step: 20500; base_hinge: 0.083697535; steps/sec: 149.25;\nFastEstimator-Train: step: 21000; base_hinge: 0.32213867; steps/sec: 183.06;\nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 6.1 sec;\nFastEstimator-Eval: step: 21000; epoch: 21; adv_hinge: 0.65694094; adversarial_accuracy: 0.3712; base_accuracy: 0.6992; base_hinge: 0.3307393; min_base_hinge: 0.32673293; since_best_base_hinge: 6;\nFastEstimator-Train: step: 21500; base_hinge: 0.21509376; steps/sec: 145.06;\nFastEstimator-Train: step: 22000; base_hinge: 0.19089276; steps/sec: 176.3;\nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 6.28 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 22000; epoch: 22; adv_hinge: 0.65987563; adversarial_accuracy: 0.3686; base_accuracy: 0.7086; base_hinge: 0.3198288; min_base_hinge: 0.3198288; since_best_base_hinge: 0;\nFastEstimator-Train: step: 22500; base_hinge: 0.27360404; steps/sec: 148.15;\nFastEstimator-Train: step: 23000; base_hinge: 0.20834382; steps/sec: 181.64;\nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 6.12 sec;\nFastEstimator-Eval: step: 23000; epoch: 23; adv_hinge: 0.6662962; adversarial_accuracy: 0.3658; base_accuracy: 0.7042; base_hinge: 0.32622346; min_base_hinge: 0.3198288; since_best_base_hinge: 1;\nFastEstimator-Train: step: 23500; base_hinge: 0.2153913; steps/sec: 155.29;\nFastEstimator-Train: step: 24000; base_hinge: 0.23683456; steps/sec: 179.49;\nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 6.0 sec;\nFastEstimator-Eval: step: 24000; epoch: 24; adv_hinge: 0.6681502; adversarial_accuracy: 0.3654; base_accuracy: 0.703; base_hinge: 0.3277005; min_base_hinge: 0.3198288; since_best_base_hinge: 2;\nFastEstimator-Train: step: 24500; base_hinge: 0.17390369; steps/sec: 146.98;\nFastEstimator-Train: step: 25000; base_hinge: 0.18928257; steps/sec: 177.46;\nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 6.24 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 25000; epoch: 25; adv_hinge: 0.6698162; adversarial_accuracy: 0.3616; base_accuracy: 0.715; base_hinge: 0.31426823; min_base_hinge: 0.31426823; since_best_base_hinge: 0;\nFastEstimator-Train: step: 25500; base_hinge: 0.16106723; steps/sec: 145.95;\nFastEstimator-Train: step: 26000; base_hinge: 0.19729549; steps/sec: 165.93;\nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 6.45 sec;\nFastEstimator-Eval: step: 26000; epoch: 26; adv_hinge: 0.67746514; adversarial_accuracy: 0.3552; base_accuracy: 0.7008; base_hinge: 0.3292477; min_base_hinge: 0.31426823; since_best_base_hinge: 1;\nFastEstimator-Train: step: 26500; base_hinge: 0.19739527; steps/sec: 150.1;\nFastEstimator-Train: step: 27000; base_hinge: 0.29701933; steps/sec: 168.93;\nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 6.27 sec;\nFastEstimator-Eval: step: 27000; epoch: 27; adv_hinge: 0.68560266; adversarial_accuracy: 0.3502; base_accuracy: 0.684; base_hinge: 0.3441289; min_base_hinge: 0.31426823; since_best_base_hinge: 2;\nFastEstimator-Train: step: 27500; base_hinge: 0.2622476; steps/sec: 131.3;\nFastEstimator-Train: step: 28000; base_hinge: 0.29236382; steps/sec: 164.56;\nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 6.87 sec;\nFastEstimator-Eval: step: 28000; epoch: 28; adv_hinge: 0.67048955; adversarial_accuracy: 0.361; base_accuracy: 0.705; base_hinge: 0.32279128; min_base_hinge: 0.31426823; since_best_base_hinge: 3;\nFastEstimator-Train: step: 28500; base_hinge: 0.12868015; steps/sec: 136.05;\nFastEstimator-Train: step: 29000; base_hinge: 0.16600317; steps/sec: 162.03;\nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 6.75 sec;\nFastEstimator-Eval: step: 29000; epoch: 29; adv_hinge: 0.6665552; adversarial_accuracy: 0.3682; base_accuracy: 0.7064; base_hinge: 0.32228085; min_base_hinge: 0.31426823; since_best_base_hinge: 4;\nFastEstimator-Train: step: 29500; base_hinge: 0.2084124; steps/sec: 148.01;\nFastEstimator-Train: step: 30000; base_hinge: 0.25564948; steps/sec: 173.39;\nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 6.24 sec;\nFastEstimator-Eval: step: 30000; epoch: 30; adv_hinge: 0.6741418; adversarial_accuracy: 0.3604; base_accuracy: 0.7048; base_hinge: 0.32388806; min_base_hinge: 0.31426823; since_best_base_hinge: 5;\nFastEstimator-Train: step: 30500; base_hinge: 0.16594283; steps/sec: 138.24;\nFastEstimator-Train: step: 31000; base_hinge: 0.17639668; steps/sec: 174.51;\nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 6.5 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 31000; epoch: 31; adv_hinge: 0.67253065; adversarial_accuracy: 0.3598; base_accuracy: 0.7096; base_hinge: 0.3093056; min_base_hinge: 0.3093056; since_best_base_hinge: 0;\nFastEstimator-Train: step: 31500; base_hinge: 0.22921798; steps/sec: 147.73;\nFastEstimator-Train: step: 32000; base_hinge: 0.18706411; steps/sec: 184.5;\nFastEstimator-Train: step: 32000; epoch: 32; epoch_time: 6.09 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 32000; epoch: 32; adv_hinge: 0.6728549; adversarial_accuracy: 0.356; base_accuracy: 0.7112; base_hinge: 0.30470937; min_base_hinge: 0.30470937; since_best_base_hinge: 0;\nFastEstimator-Train: step: 32500; base_hinge: 0.2344648; steps/sec: 148.86;\nFastEstimator-Train: step: 33000; base_hinge: 0.10746834; steps/sec: 181.12;\nFastEstimator-Train: step: 33000; epoch: 33; epoch_time: 6.12 sec;\nFastEstimator-Eval: step: 33000; epoch: 33; adv_hinge: 0.6760917; adversarial_accuracy: 0.3552; base_accuracy: 0.6868; base_hinge: 0.32750443; min_base_hinge: 0.30470937; since_best_base_hinge: 1;\nFastEstimator-Train: step: 33500; base_hinge: 0.20909442; steps/sec: 146.21;\nFastEstimator-Train: step: 34000; base_hinge: 0.15598711; steps/sec: 177.38;\nFastEstimator-Train: step: 34000; epoch: 34; epoch_time: 6.23 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 34000; epoch: 34; adv_hinge: 0.67582107; adversarial_accuracy: 0.36; base_accuracy: 0.7152; base_hinge: 0.30120456; min_base_hinge: 0.30120456; since_best_base_hinge: 0;\nFastEstimator-Train: step: 34500; base_hinge: 0.102830425; steps/sec: 149.09;\nFastEstimator-Train: step: 35000; base_hinge: 0.16139644; steps/sec: 182.41;\nFastEstimator-Train: step: 35000; epoch: 35; epoch_time: 6.1 sec;\nFastEstimator-Eval: step: 35000; epoch: 35; adv_hinge: 0.67635447; adversarial_accuracy: 0.3556; base_accuracy: 0.7058; base_hinge: 0.3057574; min_base_hinge: 0.30120456; since_best_base_hinge: 1;\nFastEstimator-Train: step: 35500; base_hinge: 0.10657302; steps/sec: 145.67;\nFastEstimator-Train: step: 36000; base_hinge: 0.17006141; steps/sec: 174.59;\nFastEstimator-Train: step: 36000; epoch: 36; epoch_time: 6.3 sec;\nFastEstimator-Eval: step: 36000; epoch: 36; adv_hinge: 0.67683244; adversarial_accuracy: 0.3572; base_accuracy: 0.7102; base_hinge: 0.30349904; min_base_hinge: 0.30120456; since_best_base_hinge: 2;\nFastEstimator-Train: step: 36500; base_hinge: 0.15623659; steps/sec: 145.94;\nFastEstimator-Train: step: 37000; base_hinge: 0.23949586; steps/sec: 181.96;\nFastEstimator-Train: step: 37000; epoch: 37; epoch_time: 6.17 sec;\nFastEstimator-Eval: step: 37000; epoch: 37; adv_hinge: 0.66844594; adversarial_accuracy: 0.3638; base_accuracy: 0.6966; base_hinge: 0.31694946; min_base_hinge: 0.30120456; since_best_base_hinge: 3;\nFastEstimator-Train: step: 37500; base_hinge: 0.14576639; steps/sec: 147.11;\nFastEstimator-Train: step: 38000; base_hinge: 0.18066089; steps/sec: 183.07;\nFastEstimator-Train: step: 38000; epoch: 38; epoch_time: 6.13 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 38000; epoch: 38; adv_hinge: 0.68135715; adversarial_accuracy: 0.3566; base_accuracy: 0.7182; base_hinge: 0.29901487; min_base_hinge: 0.29901487; since_best_base_hinge: 0;\nFastEstimator-Train: step: 38500; base_hinge: 0.09862997; steps/sec: 144.95;\nFastEstimator-Train: step: 39000; base_hinge: 0.23862968; steps/sec: 179.02;\nFastEstimator-Train: step: 39000; epoch: 39; epoch_time: 6.24 sec;\nFastEstimator-Eval: step: 39000; epoch: 39; adv_hinge: 0.6850794; adversarial_accuracy: 0.3468; base_accuracy: 0.695; base_hinge: 0.318525; min_base_hinge: 0.29901487; since_best_base_hinge: 1;\nFastEstimator-Train: step: 39500; base_hinge: 0.24544053; steps/sec: 142.26;\nFastEstimator-Train: step: 40000; base_hinge: 0.09076198; steps/sec: 176.09;\nFastEstimator-Train: step: 40000; epoch: 40; epoch_time: 6.34 sec;\nFastEstimator-Eval: step: 40000; epoch: 40; adv_hinge: 0.6828284; adversarial_accuracy: 0.351; base_accuracy: 0.7042; base_hinge: 0.30795157; min_base_hinge: 0.29901487; since_best_base_hinge: 2;\nFastEstimator-Train: step: 40500; base_hinge: 0.124062; steps/sec: 143.74;\nFastEstimator-Train: step: 41000; base_hinge: 0.21180153; steps/sec: 177.15;\nFastEstimator-Train: step: 41000; epoch: 41; epoch_time: 6.31 sec;\nFastEstimator-Eval: step: 41000; epoch: 41; adv_hinge: 0.69568163; adversarial_accuracy: 0.3372; base_accuracy: 0.7032; base_hinge: 0.3095613; min_base_hinge: 0.29901487; since_best_base_hinge: 3;\nFastEstimator-Train: step: 41500; base_hinge: 0.116228275; steps/sec: 142.75;\nFastEstimator-Train: step: 42000; base_hinge: 0.20703846; steps/sec: 172.43;\nFastEstimator-Train: step: 42000; epoch: 42; epoch_time: 6.4 sec;\nFastEstimator-Eval: step: 42000; epoch: 42; adv_hinge: 0.7017528; adversarial_accuracy: 0.3346; base_accuracy: 0.7094; base_hinge: 0.3047698; min_base_hinge: 0.29901487; since_best_base_hinge: 4;\nFastEstimator-Train: step: 42500; base_hinge: 0.105107; steps/sec: 140.82;\nFastEstimator-Train: step: 43000; base_hinge: 0.18722667; steps/sec: 174.07;\nFastEstimator-Train: step: 43000; epoch: 43; epoch_time: 6.43 sec;\nFastEstimator-Eval: step: 43000; epoch: 43; adv_hinge: 0.6941656; adversarial_accuracy: 0.34; base_accuracy: 0.7082; base_hinge: 0.30632982; min_base_hinge: 0.29901487; since_best_base_hinge: 5;\nFastEstimator-Train: step: 43500; base_hinge: 0.15885285; steps/sec: 146.18;\nFastEstimator-Train: step: 44000; base_hinge: 0.06321986; steps/sec: 185.0;\nFastEstimator-Train: step: 44000; epoch: 44; epoch_time: 6.12 sec;\nFastEstimator-Eval: step: 44000; epoch: 44; adv_hinge: 0.6966642; adversarial_accuracy: 0.3352; base_accuracy: 0.711; base_hinge: 0.30576453; min_base_hinge: 0.29901487; since_best_base_hinge: 6;\nFastEstimator-Train: step: 44500; base_hinge: 0.12628566; steps/sec: 142.54;\nFastEstimator-Train: step: 45000; base_hinge: 0.199184; steps/sec: 175.02;\nFastEstimator-Train: step: 45000; epoch: 45; epoch_time: 6.37 sec;\nFastEstimator-Eval: step: 45000; epoch: 45; adv_hinge: 0.6890245; adversarial_accuracy: 0.3488; base_accuracy: 0.711; base_hinge: 0.3026847; min_base_hinge: 0.29901487; since_best_base_hinge: 7;\nFastEstimator-Train: step: 45500; base_hinge: 0.17868556; steps/sec: 140.97;\nFastEstimator-Train: step: 46000; base_hinge: 0.12751797; steps/sec: 174.32;\nFastEstimator-Train: step: 46000; epoch: 46; epoch_time: 6.42 sec;\nFastEstimator-Eval: step: 46000; epoch: 46; adv_hinge: 0.6926506; adversarial_accuracy: 0.3408; base_accuracy: 0.7074; base_hinge: 0.30677107; min_base_hinge: 0.29901487; since_best_base_hinge: 8;\nFastEstimator-Train: step: 46500; base_hinge: 0.17218891; steps/sec: 143.82;\nFastEstimator-Train: step: 47000; base_hinge: 0.11857964; steps/sec: 183.44;\nFastEstimator-Train: step: 47000; epoch: 47; epoch_time: 6.2 sec;\nFastEstimator-Eval: step: 47000; epoch: 47; adv_hinge: 0.68311316; adversarial_accuracy: 0.3482; base_accuracy: 0.71; base_hinge: 0.3053293; min_base_hinge: 0.29901487; since_best_base_hinge: 9;\nFastEstimator-Train: step: 47500; base_hinge: 0.14120819; steps/sec: 140.98;\nFastEstimator-Train: step: 48000; base_hinge: 0.12556206; steps/sec: 174.2;\nFastEstimator-Train: step: 48000; epoch: 48; epoch_time: 6.41 sec;\nFastEstimator-EarlyStopping: 'base_hinge' triggered an early stop. Its best value was 0.2990148663520813 at epoch 38\nFastEstimator-Eval: step: 48000; epoch: 48; adv_hinge: 0.69799834; adversarial_accuracy: 0.3362; base_accuracy: 0.7064; base_hinge: 0.3066258; min_base_hinge: 0.29901487; since_best_base_hinge: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/ecc_best_base_hinge.h5\nFastEstimator-Finish: step: 48000; ecc_lr: 0.001; total_time: 369.69 sec;\nFastEstimator-Test: step: 48000; epoch: 48; adv_hinge: 0.68220603; adversarial_accuracy: 0.3522; base_accuracy: 0.7166; base_hinge: 0.29723442;\n</pre> In\u00a0[14]: Copied! <pre>hydra_estimator.fit('Hydra')\nhydra_results = hydra_estimator.test()\n</pre> hydra_estimator.fit('Hydra') hydra_results = hydra_estimator.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; base_hinge: 1.005537;\nFastEstimator-Train: step: 500; base_hinge: 0.6636967; steps/sec: 120.64;\nFastEstimator-Train: step: 1000; base_hinge: 0.6184193; steps/sec: 126.98;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 8.95 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_hinge: 0.76180106; adversarial_accuracy: 0.2508; base_accuracy: 0.3742; base_hinge: 0.6694584; min_base_hinge: 0.6694584; since_best_base_hinge: 0;\nFastEstimator-Train: step: 1500; base_hinge: 0.60712904; steps/sec: 105.12;\nFastEstimator-Train: step: 2000; base_hinge: 0.586785; steps/sec: 128.26;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 8.66 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_hinge: 0.7100552; adversarial_accuracy: 0.3074; base_accuracy: 0.4702; base_hinge: 0.5881349; min_base_hinge: 0.5881349; since_best_base_hinge: 0;\nFastEstimator-Train: step: 2500; base_hinge: 0.5594202; steps/sec: 106.0;\nFastEstimator-Train: step: 3000; base_hinge: 0.51175797; steps/sec: 130.09;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 8.56 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_hinge: 0.6757895; adversarial_accuracy: 0.3456; base_accuracy: 0.5496; base_hinge: 0.5054883; min_base_hinge: 0.5054883; since_best_base_hinge: 0;\nFastEstimator-Train: step: 3500; base_hinge: 0.51829374; steps/sec: 104.11;\nFastEstimator-Train: step: 4000; base_hinge: 0.534348; steps/sec: 126.79;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 8.75 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_hinge: 0.66557425; adversarial_accuracy: 0.3584; base_accuracy: 0.5726; base_hinge: 0.47270003; min_base_hinge: 0.47270003; since_best_base_hinge: 0;\nFastEstimator-Train: step: 4500; base_hinge: 0.43916184; steps/sec: 102.37;\nFastEstimator-Train: step: 5000; base_hinge: 0.44243538; steps/sec: 124.66;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 8.9 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_hinge: 0.65442306; adversarial_accuracy: 0.3744; base_accuracy: 0.6044; base_hinge: 0.4457506; min_base_hinge: 0.4457506; since_best_base_hinge: 0;\nFastEstimator-Train: step: 5500; base_hinge: 0.40786383; steps/sec: 100.59;\nFastEstimator-Train: step: 6000; base_hinge: 0.4591022; steps/sec: 124.97;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 8.97 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_hinge: 0.6492514; adversarial_accuracy: 0.3838; base_accuracy: 0.6262; base_hinge: 0.4214871; min_base_hinge: 0.4214871; since_best_base_hinge: 0;\nFastEstimator-Train: step: 6500; base_hinge: 0.39158216; steps/sec: 96.31;\nFastEstimator-Train: step: 7000; base_hinge: 0.30446494; steps/sec: 115.93;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 9.5 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_hinge: 0.6397081; adversarial_accuracy: 0.391; base_accuracy: 0.6558; base_hinge: 0.39896628; min_base_hinge: 0.39896628; since_best_base_hinge: 0;\nFastEstimator-Train: step: 7500; base_hinge: 0.37816882; steps/sec: 100.91;\nFastEstimator-Train: step: 8000; base_hinge: 0.42791244; steps/sec: 130.84;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 8.78 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 8000; epoch: 8; adv_hinge: 0.647455; adversarial_accuracy: 0.3816; base_accuracy: 0.6634; base_hinge: 0.39305332; min_base_hinge: 0.39305332; since_best_base_hinge: 0;\nFastEstimator-Train: step: 8500; base_hinge: 0.4074018; steps/sec: 96.92;\nFastEstimator-Train: step: 9000; base_hinge: 0.36726004; steps/sec: 121.02;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 9.29 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 9000; epoch: 9; adv_hinge: 0.64966685; adversarial_accuracy: 0.3758; base_accuracy: 0.6612; base_hinge: 0.39002314; min_base_hinge: 0.39002314; since_best_base_hinge: 0;\nFastEstimator-Train: step: 9500; base_hinge: 0.38659644; steps/sec: 95.88;\nFastEstimator-Train: step: 10000; base_hinge: 0.32991368; steps/sec: 118.87;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 9.42 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_hinge: 0.6384187; adversarial_accuracy: 0.3874; base_accuracy: 0.684; base_hinge: 0.37144434; min_base_hinge: 0.37144434; since_best_base_hinge: 0;\nFastEstimator-Train: step: 10500; base_hinge: 0.31555474; steps/sec: 102.34;\nFastEstimator-Train: step: 11000; base_hinge: 0.3198191; steps/sec: 127.85;\nFastEstimator-Train: step: 11000; epoch: 11; epoch_time: 8.8 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 11000; epoch: 11; adv_hinge: 0.6411109; adversarial_accuracy: 0.3818; base_accuracy: 0.6904; base_hinge: 0.3641706; min_base_hinge: 0.3641706; since_best_base_hinge: 0;\nFastEstimator-Train: step: 11500; base_hinge: 0.30043656; steps/sec: 97.15;\nFastEstimator-Train: step: 12000; base_hinge: 0.31473994; steps/sec: 117.96;\nFastEstimator-Train: step: 12000; epoch: 12; epoch_time: 9.39 sec;\nFastEstimator-Eval: step: 12000; epoch: 12; adv_hinge: 0.6478225; adversarial_accuracy: 0.3748; base_accuracy: 0.6806; base_hinge: 0.3647029; min_base_hinge: 0.3641706; since_best_base_hinge: 1;\nFastEstimator-Train: step: 12500; base_hinge: 0.3006091; steps/sec: 98.24;\nFastEstimator-Train: step: 13000; base_hinge: 0.37336874; steps/sec: 123.79;\nFastEstimator-Train: step: 13000; epoch: 13; epoch_time: 9.14 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 13000; epoch: 13; adv_hinge: 0.64424914; adversarial_accuracy: 0.3846; base_accuracy: 0.6882; base_hinge: 0.35658637; min_base_hinge: 0.35658637; since_best_base_hinge: 0;\nFastEstimator-Train: step: 13500; base_hinge: 0.27503738; steps/sec: 104.17;\nFastEstimator-Train: step: 14000; base_hinge: 0.31450596; steps/sec: 129.9;\nFastEstimator-Train: step: 14000; epoch: 14; epoch_time: 8.65 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 14000; epoch: 14; adv_hinge: 0.6362553; adversarial_accuracy: 0.3898; base_accuracy: 0.6954; base_hinge: 0.34577057; min_base_hinge: 0.34577057; since_best_base_hinge: 0;\nFastEstimator-Train: step: 14500; base_hinge: 0.29565284; steps/sec: 100.6;\nFastEstimator-Train: step: 15000; base_hinge: 0.35400718; steps/sec: 122.85;\nFastEstimator-Train: step: 15000; epoch: 15; epoch_time: 9.04 sec;\nFastEstimator-Eval: step: 15000; epoch: 15; adv_hinge: 0.6411361; adversarial_accuracy: 0.385; base_accuracy: 0.6904; base_hinge: 0.34718743; min_base_hinge: 0.34577057; since_best_base_hinge: 1;\nFastEstimator-Train: step: 15500; base_hinge: 0.35471046; steps/sec: 100.02;\nFastEstimator-Train: step: 16000; base_hinge: 0.26543427; steps/sec: 124.82;\nFastEstimator-Train: step: 16000; epoch: 16; epoch_time: 8.98 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 16000; epoch: 16; adv_hinge: 0.6434831; adversarial_accuracy: 0.3834; base_accuracy: 0.6978; base_hinge: 0.34060454; min_base_hinge: 0.34060454; since_best_base_hinge: 0;\nFastEstimator-Train: step: 16500; base_hinge: 0.21976732; steps/sec: 100.93;\nFastEstimator-Train: step: 17000; base_hinge: 0.29889458; steps/sec: 122.76;\nFastEstimator-Train: step: 17000; epoch: 17; epoch_time: 9.05 sec;\nFastEstimator-Eval: step: 17000; epoch: 17; adv_hinge: 0.656668; adversarial_accuracy: 0.3722; base_accuracy: 0.6982; base_hinge: 0.34171763; min_base_hinge: 0.34060454; since_best_base_hinge: 1;\nFastEstimator-Train: step: 17500; base_hinge: 0.14615066; steps/sec: 101.85;\nFastEstimator-Train: step: 18000; base_hinge: 0.26993346; steps/sec: 128.53;\nFastEstimator-Train: step: 18000; epoch: 18; epoch_time: 8.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 18000; epoch: 18; adv_hinge: 0.64573526; adversarial_accuracy: 0.3792; base_accuracy: 0.7016; base_hinge: 0.33208665; min_base_hinge: 0.33208665; since_best_base_hinge: 0;\nFastEstimator-Train: step: 18500; base_hinge: 0.25417802; steps/sec: 103.1;\nFastEstimator-Train: step: 19000; base_hinge: 0.2457754; steps/sec: 128.98;\nFastEstimator-Train: step: 19000; epoch: 19; epoch_time: 8.74 sec;\nFastEstimator-Eval: step: 19000; epoch: 19; adv_hinge: 0.66556203; adversarial_accuracy: 0.3622; base_accuracy: 0.7068; base_hinge: 0.33437443; min_base_hinge: 0.33208665; since_best_base_hinge: 1;\nFastEstimator-Train: step: 19500; base_hinge: 0.14611092; steps/sec: 97.02;\nFastEstimator-Train: step: 20000; base_hinge: 0.2760552; steps/sec: 130.41;\nFastEstimator-Train: step: 20000; epoch: 20; epoch_time: 8.99 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 20000; epoch: 20; adv_hinge: 0.6541269; adversarial_accuracy: 0.3698; base_accuracy: 0.715; base_hinge: 0.32563835; min_base_hinge: 0.32563835; since_best_base_hinge: 0;\nFastEstimator-Train: step: 20500; base_hinge: 0.14353013; steps/sec: 96.85;\nFastEstimator-Train: step: 21000; base_hinge: 0.20877028; steps/sec: 120.18;\nFastEstimator-Train: step: 21000; epoch: 21; epoch_time: 9.32 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 21000; epoch: 21; adv_hinge: 0.6526895; adversarial_accuracy: 0.3674; base_accuracy: 0.7182; base_hinge: 0.31998664; min_base_hinge: 0.31998664; since_best_base_hinge: 0;\nFastEstimator-Train: step: 21500; base_hinge: 0.20144889; steps/sec: 101.66;\nFastEstimator-Train: step: 22000; base_hinge: 0.28719658; steps/sec: 128.25;\nFastEstimator-Train: step: 22000; epoch: 22; epoch_time: 8.84 sec;\nFastEstimator-Eval: step: 22000; epoch: 22; adv_hinge: 0.6620615; adversarial_accuracy: 0.3674; base_accuracy: 0.7204; base_hinge: 0.32229984; min_base_hinge: 0.31998664; since_best_base_hinge: 1;\nFastEstimator-Train: step: 22500; base_hinge: 0.16186333; steps/sec: 98.64;\nFastEstimator-Train: step: 23000; base_hinge: 0.27358246; steps/sec: 122.3;\nFastEstimator-Train: step: 23000; epoch: 23; epoch_time: 9.16 sec;\nFastEstimator-Eval: step: 23000; epoch: 23; adv_hinge: 0.6603125; adversarial_accuracy: 0.3658; base_accuracy: 0.7124; base_hinge: 0.32517755; min_base_hinge: 0.31998664; since_best_base_hinge: 2;\nFastEstimator-Train: step: 23500; base_hinge: 0.1928054; steps/sec: 93.02;\nFastEstimator-Train: step: 24000; base_hinge: 0.22112942; steps/sec: 111.7;\nFastEstimator-Train: step: 24000; epoch: 24; epoch_time: 9.83 sec;\nFastEstimator-Eval: step: 24000; epoch: 24; adv_hinge: 0.6635312; adversarial_accuracy: 0.362; base_accuracy: 0.7162; base_hinge: 0.32222956; min_base_hinge: 0.31998664; since_best_base_hinge: 3;\nFastEstimator-Train: step: 24500; base_hinge: 0.18369232; steps/sec: 93.88;\nFastEstimator-Train: step: 25000; base_hinge: 0.3199598; steps/sec: 116.33;\nFastEstimator-Train: step: 25000; epoch: 25; epoch_time: 9.62 sec;\nFastEstimator-Eval: step: 25000; epoch: 25; adv_hinge: 0.6594223; adversarial_accuracy: 0.3656; base_accuracy: 0.7094; base_hinge: 0.32736686; min_base_hinge: 0.31998664; since_best_base_hinge: 4;\nFastEstimator-Train: step: 25500; base_hinge: 0.27891782; steps/sec: 95.58;\nFastEstimator-Train: step: 26000; base_hinge: 0.21763876; steps/sec: 121.42;\nFastEstimator-Train: step: 26000; epoch: 26; epoch_time: 9.35 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 26000; epoch: 26; adv_hinge: 0.66260374; adversarial_accuracy: 0.3638; base_accuracy: 0.722; base_hinge: 0.31514966; min_base_hinge: 0.31514966; since_best_base_hinge: 0;\nFastEstimator-Train: step: 26500; base_hinge: 0.21337055; steps/sec: 97.9;\nFastEstimator-Train: step: 27000; base_hinge: 0.2819699; steps/sec: 124.37;\nFastEstimator-Train: step: 27000; epoch: 27; epoch_time: 9.12 sec;\nFastEstimator-Eval: step: 27000; epoch: 27; adv_hinge: 0.6798921; adversarial_accuracy: 0.3464; base_accuracy: 0.7214; base_hinge: 0.31680486; min_base_hinge: 0.31514966; since_best_base_hinge: 1;\nFastEstimator-Train: step: 27500; base_hinge: 0.19231571; steps/sec: 105.91;\nFastEstimator-Train: step: 28000; base_hinge: 0.28198642; steps/sec: 131.18;\nFastEstimator-Train: step: 28000; epoch: 28; epoch_time: 8.53 sec;\nFastEstimator-Eval: step: 28000; epoch: 28; adv_hinge: 0.66766024; adversarial_accuracy: 0.3614; base_accuracy: 0.7218; base_hinge: 0.31777543; min_base_hinge: 0.31514966; since_best_base_hinge: 2;\nFastEstimator-Train: step: 28500; base_hinge: 0.19805568; steps/sec: 103.93;\nFastEstimator-Train: step: 29000; base_hinge: 0.2087434; steps/sec: 127.45;\nFastEstimator-Train: step: 29000; epoch: 29; epoch_time: 8.75 sec;\nFastEstimator-Eval: step: 29000; epoch: 29; adv_hinge: 0.67013097; adversarial_accuracy: 0.359; base_accuracy: 0.7214; base_hinge: 0.31696904; min_base_hinge: 0.31514966; since_best_base_hinge: 3;\nFastEstimator-Train: step: 29500; base_hinge: 0.19910683; steps/sec: 103.06;\nFastEstimator-Train: step: 30000; base_hinge: 0.23703016; steps/sec: 128.09;\nFastEstimator-Train: step: 30000; epoch: 30; epoch_time: 8.75 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 30000; epoch: 30; adv_hinge: 0.6716123; adversarial_accuracy: 0.3554; base_accuracy: 0.722; base_hinge: 0.31445774; min_base_hinge: 0.31445774; since_best_base_hinge: 0;\nFastEstimator-Train: step: 30500; base_hinge: 0.25689024; steps/sec: 104.6;\nFastEstimator-Train: step: 31000; base_hinge: 0.16964242; steps/sec: 130.99;\nFastEstimator-Train: step: 31000; epoch: 31; epoch_time: 8.6 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 31000; epoch: 31; adv_hinge: 0.6674604; adversarial_accuracy: 0.3616; base_accuracy: 0.727; base_hinge: 0.3101713; min_base_hinge: 0.3101713; since_best_base_hinge: 0;\nFastEstimator-Train: step: 31500; base_hinge: 0.24562128; steps/sec: 103.56;\nFastEstimator-Train: step: 32000; base_hinge: 0.2353131; steps/sec: 128.88;\nFastEstimator-Train: step: 32000; epoch: 32; epoch_time: 8.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 32000; epoch: 32; adv_hinge: 0.68069166; adversarial_accuracy: 0.3494; base_accuracy: 0.7338; base_hinge: 0.3074732; min_base_hinge: 0.3074732; since_best_base_hinge: 0;\nFastEstimator-Train: step: 32500; base_hinge: 0.14915092; steps/sec: 100.17;\nFastEstimator-Train: step: 33000; base_hinge: 0.22575116; steps/sec: 123.11;\nFastEstimator-Train: step: 33000; epoch: 33; epoch_time: 9.05 sec;\nFastEstimator-Eval: step: 33000; epoch: 33; adv_hinge: 0.68312544; adversarial_accuracy: 0.3436; base_accuracy: 0.7224; base_hinge: 0.310704; min_base_hinge: 0.3074732; since_best_base_hinge: 1;\nFastEstimator-Train: step: 33500; base_hinge: 0.21910556; steps/sec: 105.79;\nFastEstimator-Train: step: 34000; base_hinge: 0.20641476; steps/sec: 131.46;\nFastEstimator-Train: step: 34000; epoch: 34; epoch_time: 8.53 sec;\nFastEstimator-Eval: step: 34000; epoch: 34; adv_hinge: 0.6892244; adversarial_accuracy: 0.342; base_accuracy: 0.7234; base_hinge: 0.31533897; min_base_hinge: 0.3074732; since_best_base_hinge: 2;\nFastEstimator-Train: step: 34500; base_hinge: 0.17468475; steps/sec: 101.75;\nFastEstimator-Train: step: 35000; base_hinge: 0.19306578; steps/sec: 126.45;\nFastEstimator-Train: step: 35000; epoch: 35; epoch_time: 8.86 sec;\nFastEstimator-Eval: step: 35000; epoch: 35; adv_hinge: 0.6868973; adversarial_accuracy: 0.3454; base_accuracy: 0.7208; base_hinge: 0.31726614; min_base_hinge: 0.3074732; since_best_base_hinge: 3;\nFastEstimator-Train: step: 35500; base_hinge: 0.20393735; steps/sec: 94.62;\nFastEstimator-Train: step: 36000; base_hinge: 0.16484459; steps/sec: 119.51;\nFastEstimator-Train: step: 36000; epoch: 36; epoch_time: 9.47 sec;\nFastEstimator-Eval: step: 36000; epoch: 36; adv_hinge: 0.6915254; adversarial_accuracy: 0.3434; base_accuracy: 0.7142; base_hinge: 0.31838906; min_base_hinge: 0.3074732; since_best_base_hinge: 4;\nFastEstimator-Train: step: 36500; base_hinge: 0.30909944; steps/sec: 97.77;\nFastEstimator-Train: step: 37000; base_hinge: 0.23421745; steps/sec: 122.0;\nFastEstimator-Train: step: 37000; epoch: 37; epoch_time: 9.22 sec;\nFastEstimator-Eval: step: 37000; epoch: 37; adv_hinge: 0.6845604; adversarial_accuracy: 0.3424; base_accuracy: 0.7224; base_hinge: 0.31112215; min_base_hinge: 0.3074732; since_best_base_hinge: 5;\nFastEstimator-Train: step: 37500; base_hinge: 0.16261695; steps/sec: 100.76;\nFastEstimator-Train: step: 38000; base_hinge: 0.19233434; steps/sec: 129.8;\nFastEstimator-Train: step: 38000; epoch: 38; epoch_time: 8.79 sec;\nFastEstimator-Eval: step: 38000; epoch: 38; adv_hinge: 0.67739683; adversarial_accuracy: 0.3542; base_accuracy: 0.7242; base_hinge: 0.31000316; min_base_hinge: 0.3074732; since_best_base_hinge: 6;\nFastEstimator-Train: step: 38500; base_hinge: 0.20121937; steps/sec: 93.57;\nFastEstimator-Train: step: 39000; base_hinge: 0.13809688; steps/sec: 115.84;\nFastEstimator-Train: step: 39000; epoch: 39; epoch_time: 9.68 sec;\nFastEstimator-Eval: step: 39000; epoch: 39; adv_hinge: 0.6804919; adversarial_accuracy: 0.3528; base_accuracy: 0.7204; base_hinge: 0.3120458; min_base_hinge: 0.3074732; since_best_base_hinge: 7;\nFastEstimator-Train: step: 39500; base_hinge: 0.18955024; steps/sec: 91.47;\nFastEstimator-Train: step: 40000; base_hinge: 0.207903; steps/sec: 110.3;\nFastEstimator-Train: step: 40000; epoch: 40; epoch_time: 10.0 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Eval: step: 40000; epoch: 40; adv_hinge: 0.6855146; adversarial_accuracy: 0.3444; base_accuracy: 0.731; base_hinge: 0.30590805; min_base_hinge: 0.30590805; since_best_base_hinge: 0;\nFastEstimator-Train: step: 40500; base_hinge: 0.083864; steps/sec: 99.87;\nFastEstimator-Train: step: 41000; base_hinge: 0.21492122; steps/sec: 129.7;\nFastEstimator-Train: step: 41000; epoch: 41; epoch_time: 8.87 sec;\nFastEstimator-Eval: step: 41000; epoch: 41; adv_hinge: 0.67458624; adversarial_accuracy: 0.3572; base_accuracy: 0.7192; base_hinge: 0.31428564; min_base_hinge: 0.30590805; since_best_base_hinge: 1;\nFastEstimator-Train: step: 41500; base_hinge: 0.11375672; steps/sec: 99.68;\nFastEstimator-Train: step: 42000; base_hinge: 0.12876363; steps/sec: 129.34;\nFastEstimator-Train: step: 42000; epoch: 42; epoch_time: 8.88 sec;\nFastEstimator-Eval: step: 42000; epoch: 42; adv_hinge: 0.6846123; adversarial_accuracy: 0.3488; base_accuracy: 0.7218; base_hinge: 0.312879; min_base_hinge: 0.30590805; since_best_base_hinge: 2;\nFastEstimator-Train: step: 42500; base_hinge: 0.23576143; steps/sec: 99.47;\nFastEstimator-Train: step: 43000; base_hinge: 0.23091209; steps/sec: 128.26;\nFastEstimator-Train: step: 43000; epoch: 43; epoch_time: 8.93 sec;\nFastEstimator-Eval: step: 43000; epoch: 43; adv_hinge: 0.6828584; adversarial_accuracy: 0.3462; base_accuracy: 0.7256; base_hinge: 0.31104147; min_base_hinge: 0.30590805; since_best_base_hinge: 3;\nFastEstimator-Train: step: 43500; base_hinge: 0.16894166; steps/sec: 99.16;\nFastEstimator-Train: step: 44000; base_hinge: 0.11686382; steps/sec: 127.34;\nFastEstimator-Train: step: 44000; epoch: 44; epoch_time: 8.97 sec;\nFastEstimator-Eval: step: 44000; epoch: 44; adv_hinge: 0.68396837; adversarial_accuracy: 0.3502; base_accuracy: 0.7228; base_hinge: 0.31255093; min_base_hinge: 0.30590805; since_best_base_hinge: 4;\nFastEstimator-Train: step: 44500; base_hinge: 0.21182784; steps/sec: 97.19;\nFastEstimator-Train: step: 45000; base_hinge: 0.20819433; steps/sec: 125.21;\nFastEstimator-Train: step: 45000; epoch: 45; epoch_time: 9.13 sec;\nFastEstimator-Eval: step: 45000; epoch: 45; adv_hinge: 0.67581666; adversarial_accuracy: 0.3584; base_accuracy: 0.7186; base_hinge: 0.3161821; min_base_hinge: 0.30590805; since_best_base_hinge: 5;\nFastEstimator-Train: step: 45500; base_hinge: 0.13874252; steps/sec: 98.17;\nFastEstimator-Train: step: 46000; base_hinge: 0.11777561; steps/sec: 126.43;\nFastEstimator-Train: step: 46000; epoch: 46; epoch_time: 9.05 sec;\nFastEstimator-Eval: step: 46000; epoch: 46; adv_hinge: 0.6749865; adversarial_accuracy: 0.3574; base_accuracy: 0.7286; base_hinge: 0.3084245; min_base_hinge: 0.30590805; since_best_base_hinge: 6;\nFastEstimator-Train: step: 46500; base_hinge: 0.12897556; steps/sec: 94.37;\nFastEstimator-Train: step: 47000; base_hinge: 0.1589519; steps/sec: 120.69;\nFastEstimator-Train: step: 47000; epoch: 47; epoch_time: 9.45 sec;\nFastEstimator-Eval: step: 47000; epoch: 47; adv_hinge: 0.6834855; adversarial_accuracy: 0.3484; base_accuracy: 0.7268; base_hinge: 0.30969155; min_base_hinge: 0.30590805; since_best_base_hinge: 7;\nFastEstimator-Train: step: 47500; base_hinge: 0.21197909; steps/sec: 99.77;\nFastEstimator-Train: step: 48000; base_hinge: 0.26024407; steps/sec: 128.18;\nFastEstimator-Train: step: 48000; epoch: 48; epoch_time: 8.91 sec;\nFastEstimator-Eval: step: 48000; epoch: 48; adv_hinge: 0.66962576; adversarial_accuracy: 0.3646; base_accuracy: 0.7244; base_hinge: 0.3099848; min_base_hinge: 0.30590805; since_best_base_hinge: 8;\nFastEstimator-Train: step: 48500; base_hinge: 0.15549628; steps/sec: 101.18;\nFastEstimator-Train: step: 49000; base_hinge: 0.17724009; steps/sec: 133.42;\nFastEstimator-Train: step: 49000; epoch: 49; epoch_time: 8.69 sec;\nFastEstimator-Eval: step: 49000; epoch: 49; adv_hinge: 0.6587408; adversarial_accuracy: 0.3744; base_accuracy: 0.7304; base_hinge: 0.30658415; min_base_hinge: 0.30590805; since_best_base_hinge: 9;\nFastEstimator-Train: step: 49500; base_hinge: 0.2888211; steps/sec: 95.17;\nFastEstimator-Train: step: 50000; base_hinge: 0.1778168; steps/sec: 122.93;\nFastEstimator-Train: step: 50000; epoch: 50; epoch_time: 9.32 sec;\nFastEstimator-EarlyStopping: 'base_hinge' triggered an early stop. Its best value was 0.30590805411338806 at epoch 40\nFastEstimator-Eval: step: 50000; epoch: 50; adv_hinge: 0.67139465; adversarial_accuracy: 0.3584; base_accuracy: 0.7264; base_hinge: 0.30914956; min_base_hinge: 0.30590805; since_best_base_hinge: 10;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp0u08o2n6/hydra_ecc_best_base_hinge.h5\nFastEstimator-Finish: step: 50000; hydra_ecc_lr: 0.001; total_time: 549.55 sec;\nFastEstimator-Test: step: 50000; epoch: 50; adv_hinge: 0.69422853; adversarial_accuracy: 0.3418; base_accuracy: 0.7168; base_hinge: 0.31933317;\n</pre> In\u00a0[15]: Copied! <pre>logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'})\n</pre> logs = visualize_logs(experiments=[softmax_results, ecc_results, hydra_results], ignore_metrics={'ecc_lr', 'hydra_ecc_lr', 'softmax_lr', 'logging_interval', 'num_device', 'epoch_time', 'min_base_ce', 'adv_ce', 'total_time'}) <p>As you can see, the conventional network using softmax to convert logits to class probabilities actually gets more and more vulnerable to adversarial attacks as training progresses. It also quickly overfits to the data, reaching an optimal performance around epoch 7. By switching the output layer of the model to generate an error correcting code and training with hinge loss, the model is able to train almost 6 times longer before reaching peak conventional accuracy. Moreover, the adversarial performance of the network continues to improve even after the main training runs out. This is significantly better performance than networks trained specifically to combat this attack, shown in the FGSM notebook. It can also be seen that there is no additional cost to training using ECC as opposed to softmax in terms of steps/sec. This is a big benefit over FGSM, where the training time for each step is doubled. With these benefits in mind, you may want to consider never using softmax again.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#adversarial-robustness-with-error-correcting-codes-and-hinge-loss", "title": "Adversarial Robustness with Error Correcting Codes (and Hinge Loss)\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#never-use-softmax-again", "title": "(Never use Softmax again)\u00b6", "text": "<p>In this example we will show how using error correcting codes as a model output can drastically reduce model overfitting while simultaneously increasing model robustness against adversarial attacks. In other words, why you should never use a softmax layer again. This is slightly more complicated than the our other ECC apphub example, but it allows for more accurate final probability estimates (the FE Hadamard network layer results in probability smoothing which prevents the network from ever being 100% confident in a class choice). The usefulness of error correcting codes was first publicized by the US Army in a 2019 Neurips Paper. For background on adversarial attacks, and on the attack type we will be demonstrating here, check out our FGSM apphub example. Note that in this apphub we will not be training against adversarial samples, but only performing adversarial attacks during evaluation to see how different models fair against them.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#imports", "title": "Imports\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#getting-the-data", "title": "Getting the Data\u00b6", "text": "<p>For these experiments we will be using the ciFAIR10 Dataset (like cifar10, but with duplicate testing data replaced)</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#defining-estimators", "title": "Defining Estimators\u00b6", "text": "<p>In this apphub we will be comparing a baseline model against two models using hinge loss to enable training with error correcting codes. The setting up the hinge loss models requires a few extra Ops along the way.</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#the-models", "title": "The Models\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#1-a-lenet-model-with-softmax", "title": "1 - A LeNet model with Softmax\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#2-a-lenet-model-with-error-correcting-codes", "title": "2 - A LeNet model with Error Correcting Codes\u00b6", "text": ""}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#3-a-lenet-model-using-ecc-and-multiple-feature-heads", "title": "3 - A LeNet model using ECC and multiple feature heads\u00b6", "text": "<p>While it is common practice to follow the feature extraction layers of convolution networks with several fully connected layers in order to perform classification, this can lead to the final logits being interdependent which can actually reduce the robustness of the network. One way around this is to divide your classification layers into multiple smaller independent units:</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#the-experiments", "title": "The Experiments\u00b6", "text": "<p>Let's get Estimators for each of these models and compare them:</p>"}, {"location": "apphub/adversarial_training/ecc_hinge/ecc_hinge.html#comparing-the-results", "title": "Comparing the Results\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\n\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import argmax\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.op.tensorop import Average\nfrom fastestimator.op.tensorop.gradient import Watch, FGSM\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import BatchDisplay, GridDisplay, to_number\n</pre> import tempfile import os  import numpy as np  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import argmax from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.univariate import Normalize from fastestimator.op.tensorop import Average from fastestimator.op.tensorop.gradient import Watch, FGSM from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import BatchDisplay, GridDisplay, to_number In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=10\nbatch_size=50\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=10 batch_size=50 train_steps_per_epoch=None eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifair10\n\ntrain_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifair10  train_data, eval_data = cifair10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        test_data=test_data,\n        batch_size=batch_size,\n        ops=[\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n        ])\n</pre> pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         test_data=test_data,         batch_size=batch_size,         ops=[             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))         ]) In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\")\n</pre> model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\") <pre>2022-05-20 22:36:45.228031: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-20 22:36:45.738337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n</pre> In\u00a0[6]: Copied! <pre>network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),\n        Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),\n        UpdateOp(model=model, loss_name=\"avg_ce\")\n    ])\n</pre> network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),         Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),         UpdateOp(model=model, loss_name=\"avg_ce\")     ]) <pre>2022-05-20 22:36:46.307613: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[7]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch,\n                         monitor_names=[\"base_ce\", \"adv_ce\"],\n                         log_steps=1000)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch,                          monitor_names=[\"base_ce\", \"adv_ce\"],                          log_steps=1000) In\u00a0[8]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\n</pre> <pre>2022-05-20 22:36:53.369613: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n2022-05-20 22:36:55.266149: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Start: step: 1; logging_interval: 1000; num_device: 1;\nFastEstimator-Train: step: 1; adv_ce: 2.5153358; avg_ce: 2.4195828; base_ce: 2.3238301;\nFastEstimator-Train: step: 1000; adv_ce: 1.783967; avg_ce: 1.5844657; base_ce: 1.3849646; steps/sec: 213.41;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 9.83 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 196.03;\nEval Progress: 66/100; steps/sec: 334.2;\nEval Progress: 100/100; steps/sec: 327.79;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adv_ce: 1.6806334; adversarial_accuracy: 0.383; avg_ce: 1.4971651; base_ce: 1.3136971; clean_accuracy: 0.5406; min_base_ce: 1.3136971; since_best_base_ce: 0;\nFastEstimator-Train: step: 2000; adv_ce: 1.2669967; avg_ce: 1.0859692; base_ce: 0.90494174; steps/sec: 116.82;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 8.58 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 104.79;\nEval Progress: 66/100; steps/sec: 334.85;\nEval Progress: 100/100; steps/sec: 349.56;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adv_ce: 1.5987177; adversarial_accuracy: 0.4104; avg_ce: 1.3795794; base_ce: 1.1604412; clean_accuracy: 0.5996; min_base_ce: 1.1604412; since_best_base_ce: 0;\nFastEstimator-Train: step: 3000; adv_ce: 1.407628; avg_ce: 1.189529; base_ce: 0.9714299; steps/sec: 116.46;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 8.58 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 243.84;\nEval Progress: 66/100; steps/sec: 325.83;\nEval Progress: 100/100; steps/sec: 350.65;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adv_ce: 1.5771703; adversarial_accuracy: 0.4138; avg_ce: 1.3338548; base_ce: 1.0905393; clean_accuracy: 0.6222; min_base_ce: 1.0905393; since_best_base_ce: 0;\nFastEstimator-Train: step: 4000; adv_ce: 1.2062066; avg_ce: 0.99354184; base_ce: 0.78087705; steps/sec: 120.81;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 8.28 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 187.77;\nEval Progress: 66/100; steps/sec: 338.71;\nEval Progress: 100/100; steps/sec: 346.98;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adv_ce: 1.5070952; adversarial_accuracy: 0.4386; avg_ce: 1.2632713; base_ce: 1.0194472; clean_accuracy: 0.65; min_base_ce: 1.0194472; since_best_base_ce: 0;\nFastEstimator-Train: step: 5000; adv_ce: 1.5848972; avg_ce: 1.3050139; base_ce: 1.0251305; steps/sec: 113.37;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 8.81 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 213.82;\nEval Progress: 66/100; steps/sec: 306.58;\nEval Progress: 100/100; steps/sec: 336.49;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adv_ce: 1.5114572; adversarial_accuracy: 0.4458; avg_ce: 1.2476054; base_ce: 0.983754; clean_accuracy: 0.6604; min_base_ce: 0.983754; since_best_base_ce: 0;\nFastEstimator-Train: step: 6000; adv_ce: 1.3588514; avg_ce: 1.1306725; base_ce: 0.9024936; steps/sec: 109.93;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 9.13 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 178.84;\nEval Progress: 66/100; steps/sec: 233.34;\nEval Progress: 100/100; steps/sec: 286.35;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; adv_ce: 1.4922972; adversarial_accuracy: 0.4526; avg_ce: 1.2247608; base_ce: 0.9572241; clean_accuracy: 0.6738; min_base_ce: 0.9572241; since_best_base_ce: 0;\nFastEstimator-Train: step: 7000; adv_ce: 1.0841745; avg_ce: 0.86581266; base_ce: 0.6474508; steps/sec: 107.96;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 9.24 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 202.41;\nEval Progress: 66/100; steps/sec: 302.44;\nEval Progress: 100/100; steps/sec: 275.01;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adv_ce: 1.4921566; adversarial_accuracy: 0.4516; avg_ce: 1.2078352; base_ce: 0.9235137; clean_accuracy: 0.6832; min_base_ce: 0.9235137; since_best_base_ce: 0;\nFastEstimator-Train: step: 8000; adv_ce: 1.4827701; avg_ce: 1.2075636; base_ce: 0.9323573; steps/sec: 101.22;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 9.88 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 146.92;\nEval Progress: 66/100; steps/sec: 287.61;\nEval Progress: 100/100; steps/sec: 351.64;\nFastEstimator-Eval: step: 8000; epoch: 8; adv_ce: 1.4929062; adversarial_accuracy: 0.4568; avg_ce: 1.2131438; base_ce: 0.9333817; clean_accuracy: 0.6758; min_base_ce: 0.9235137; since_best_base_ce: 1;\nFastEstimator-Train: step: 9000; adv_ce: 1.3180299; avg_ce: 1.0481406; base_ce: 0.77825135; steps/sec: 99.96;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 10.05 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 176.89;\nEval Progress: 66/100; steps/sec: 333.66;\nEval Progress: 100/100; steps/sec: 355.6;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 9000; epoch: 9; adv_ce: 1.4940474; adversarial_accuracy: 0.468; avg_ce: 1.2010516; base_ce: 0.90805554; clean_accuracy: 0.688; min_base_ce: 0.90805554; since_best_base_ce: 0;\nFastEstimator-Train: step: 10000; adv_ce: 1.1294048; avg_ce: 0.86557364; base_ce: 0.60174257; steps/sec: 119.36;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 8.35 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 240.79;\nEval Progress: 66/100; steps/sec: 344.99;\nEval Progress: 100/100; steps/sec: 310.51;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 10000; epoch: 10; adv_ce: 1.5040021; adversarial_accuracy: 0.4578; avg_ce: 1.1994938; base_ce: 0.8949854; clean_accuracy: 0.691; min_base_ce: 0.8949854; since_best_base_ce: 0;\nFastEstimator-Finish: step: 10000; adv_model_lr: 0.001; total_time: 137.41 sec;\n</pre> In\u00a0[9]: Copied! <pre>model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\"))\n</pre> model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\")) In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 10000; epoch: 10; adv_ce: 1.5188892; adversarial_accuracy: 0.4582; avg_ce: 1.2149158; base_ce: 0.91094244; clean_accuracy: 0.6756;\n</pre> <p>In spite of our training the network using adversarially crafted images, the adversarial attack is still effective at reducing the accuracy of the network. This does not, however, mean that the efforts were wasted.</p> In\u00a0[11]: Copied! <pre>clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\")\nclean_network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),\n        ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),\n        UpdateOp(model=clean_model, loss_name=\"base_ce\")\n    ])\nclean_traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nclean_estimator = fe.Estimator(pipeline=pipeline,\n                         network=clean_network,\n                         epochs=epochs,\n                         traces=clean_traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch,\n                         log_steps=1000)\nclean_estimator.fit()\n</pre> clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\") clean_network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),         ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),         UpdateOp(model=clean_model, loss_name=\"base_ce\")     ]) clean_traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] clean_estimator = fe.Estimator(pipeline=pipeline,                          network=clean_network,                          epochs=epochs,                          traces=clean_traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch,                          log_steps=1000) clean_estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 1000; num_device: 1;\nFastEstimator-Train: step: 1; base_ce: 2.3480155;\nFastEstimator-Train: step: 1000; base_ce: 1.0126663; steps/sec: 378.6;\nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 7.12 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 252.21;\nEval Progress: 66/100; steps/sec: 295.22;\nEval Progress: 100/100; steps/sec: 352.87;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; adversarial_accuracy: 0.2802; base_ce: 1.1429971; clean_accuracy: 0.5974; min_base_ce: 1.1429971; since_best_base_ce: 0;\nFastEstimator-Train: step: 2000; base_ce: 0.93743587; steps/sec: 146.03;\nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 6.9 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 190.84;\nEval Progress: 66/100; steps/sec: 322.43;\nEval Progress: 100/100; steps/sec: 299.47;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; adversarial_accuracy: 0.2862; base_ce: 0.9852774; clean_accuracy: 0.6566; min_base_ce: 0.9852774; since_best_base_ce: 0;\nFastEstimator-Train: step: 3000; base_ce: 0.93427765; steps/sec: 135.9;\nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 7.33 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 213.71;\nEval Progress: 66/100; steps/sec: 228.4;\nEval Progress: 100/100; steps/sec: 224.45;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; adversarial_accuracy: 0.2596; base_ce: 0.92780364; clean_accuracy: 0.6772; min_base_ce: 0.92780364; since_best_base_ce: 0;\nFastEstimator-Train: step: 4000; base_ce: 0.84205955; steps/sec: 141.3;\nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 7.08 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 110.76;\nEval Progress: 66/100; steps/sec: 293.52;\nEval Progress: 100/100; steps/sec: 346.19;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; adversarial_accuracy: 0.2622; base_ce: 0.8744281; clean_accuracy: 0.6994; min_base_ce: 0.8744281; since_best_base_ce: 0;\nFastEstimator-Train: step: 5000; base_ce: 0.8608519; steps/sec: 145.1;\nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 6.91 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 91.69;\nEval Progress: 66/100; steps/sec: 289.68;\nEval Progress: 100/100; steps/sec: 360.17;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; adversarial_accuracy: 0.232; base_ce: 0.8674622; clean_accuracy: 0.6994; min_base_ce: 0.8674622; since_best_base_ce: 0;\nFastEstimator-Train: step: 6000; base_ce: 0.5080797; steps/sec: 142.61;\nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 6.98 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 159.47;\nEval Progress: 66/100; steps/sec: 262.54;\nEval Progress: 100/100; steps/sec: 264.57;\nFastEstimator-Eval: step: 6000; epoch: 6; adversarial_accuracy: 0.2304; base_ce: 0.8677709; clean_accuracy: 0.7164; min_base_ce: 0.8674622; since_best_base_ce: 1;\nFastEstimator-Train: step: 7000; base_ce: 0.4574232; steps/sec: 129.46;\nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 7.73 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 82.33;\nEval Progress: 66/100; steps/sec: 264.86;\nEval Progress: 100/100; steps/sec: 319.15;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpr5wa3jli/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; adversarial_accuracy: 0.2068; base_ce: 0.8525951; clean_accuracy: 0.7198; min_base_ce: 0.8525951; since_best_base_ce: 0;\nFastEstimator-Train: step: 8000; base_ce: 0.4699703; steps/sec: 136.61;\nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 7.36 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 89.21;\nEval Progress: 66/100; steps/sec: 250.32;\nEval Progress: 100/100; steps/sec: 253.36;\nFastEstimator-Eval: step: 8000; epoch: 8; adversarial_accuracy: 0.232; base_ce: 0.85909796; clean_accuracy: 0.7298; min_base_ce: 0.8525951; since_best_base_ce: 1;\nFastEstimator-Train: step: 9000; base_ce: 0.3419727; steps/sec: 139.87;\nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 7.12 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 107.44;\nEval Progress: 66/100; steps/sec: 278.48;\nEval Progress: 100/100; steps/sec: 278.87;\nFastEstimator-Eval: step: 9000; epoch: 9; adversarial_accuracy: 0.2098; base_ce: 0.88124627; clean_accuracy: 0.7212; min_base_ce: 0.8525951; since_best_base_ce: 2;\nFastEstimator-Train: step: 10000; base_ce: 0.5316359; steps/sec: 144.29;\nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 6.94 sec;\nEval Progress: 1/100;\nEval Progress: 33/100; steps/sec: 100.67;\nEval Progress: 66/100; steps/sec: 302.67;\nEval Progress: 100/100; steps/sec: 341.97;\nFastEstimator-Eval: step: 10000; epoch: 10; adversarial_accuracy: 0.1938; base_ce: 0.92761403; clean_accuracy: 0.7238; min_base_ce: 0.8525951; since_best_base_ce: 3;\nFastEstimator-Finish: step: 10000; clean_model_lr: 0.001; total_time: 119.58 sec;\n</pre> <p>As before, we will reload the best weights and the test the model</p> In\u00a0[12]: Copied! <pre>clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\"))\n</pre> clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\")) In\u00a0[13]: Copied! <pre>print(\"Normal Network:\")\nnormal_results = clean_estimator.test(\"normal\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\nprint(\"Adversarially Trained Network:\")\nadversarial_results = estimator.test(\"adversarial\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\n</pre> print(\"Normal Network:\") normal_results = clean_estimator.test(\"normal\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") print(\"Adversarially Trained Network:\") adversarial_results = estimator.test(\"adversarial\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") <pre>Normal Network:\nFastEstimator-Test: step: 10000; epoch: 10; adversarial_accuracy: 0.2182; base_ce: 0.83215594; clean_accuracy: 0.7244;\nThe whitebox FGSM attack reduced accuracy by 0.51\n-----------\nAdversarially Trained Network:\nFastEstimator-Test: step: 10000; epoch: 10; adv_ce: 1.5188892; adversarial_accuracy: 0.4582; avg_ce: 1.2149158; base_ce: 0.91094244; clean_accuracy: 0.6756;\nThe whitebox FGSM attack reduced accuracy by 0.22\n-----------\n</pre> <p>As we can see, the normal network is significantly less robust against adversarial attacks than the one which was trained to resist them. The downside is that the adversarial network requires more epochs of training to converge, and the training steps take about twice as long since they require two forward pass operations. It is also interesting to note that as the regular model was training, it actually saw progressively worse adversarial accuracy. This may be an indication that the network is developing very brittle decision boundaries.</p> In\u00a0[14]: Copied! <pre>class_dictionary = {\n    0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n}\nbatch = pipeline.get_results(mode=\"test\")\n</pre> class_dictionary = {     0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\" } batch = pipeline.get_results(mode=\"test\") <p>Now let's run our sample data through the network and then visualize the results</p> In\u00a0[15]: Copied! <pre>batch = clean_network.transform(batch, mode=\"test\")\n</pre> batch = clean_network.transform(batch, mode=\"test\") In\u00a0[16]: Copied! <pre>n_samples = 10\ny = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])])\ny_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])])\ny_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])])\n\nGridDisplay([BatchDisplay(image=batch[\"x\"][0:n_samples], title=\"x\"),\n             BatchDisplay(image=batch[\"x_adverse\"][0:n_samples], title=\"x_adv\"),\n             BatchDisplay(text=y, title=\"y\"),\n             BatchDisplay(text=y_pred, title=\"y_pred\"),\n             BatchDisplay(text=y_adv, title=\"y_adv\")\n            ]).show()\n</pre> n_samples = 10 y = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])]) y_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])]) y_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])])  GridDisplay([BatchDisplay(image=batch[\"x\"][0:n_samples], title=\"x\"),              BatchDisplay(image=batch[\"x_adverse\"][0:n_samples], title=\"x_adv\"),              BatchDisplay(text=y, title=\"y\"),              BatchDisplay(text=y_pred, title=\"y_pred\"),              BatchDisplay(text=y_adv, title=\"y_adv\")             ]).show() <p>As you can see, the adversarial images appear very similar to the unmodified images, and yet they are often able to modify the class predictions of the network. Note that if a network's prediction is already wrong, the attack is unlikely to change the incorrect prediction, but rather to increase the model's confidence in its incorrect prediction.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#adversarial-training-using-the-fast-gradient-sign-method-fgsm", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)\u00b6", "text": "<p>In this example we will demonstrate how to train a model to resist adversarial attacks constructed using the Fast Gradient Sign Method. For more background on adversarial attacks, visit: https://arxiv.org/abs/1412.6572</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load ciFAIR10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the ciFAIR10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#prepare-the-pipeline", "title": "Prepare the <code>Pipeline</code>\u00b6", "text": "<p>We will use a simple pipeline that just normalizes the images</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-construction", "title": "Model Construction\u00b6", "text": "<p>Here we will leverage the LeNet implementation built in to FastEstimator</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#network-defintion", "title": "<code>Network</code> defintion\u00b6", "text": "<p>This is where the adversarial attack will be implemented. To perform an FGSM attack, we first need to monitor gradients with respect to the input image. This can be accomplished in FastEstimator using the <code>Watch</code> TensorOp. We then will run the model forward once, compute the loss, and then pass the loss value into the <code>FGSM</code> TensorOp in order to create an adversarial image. We will then run the adversarial image through the model, compute the loss again, and average the two results together in order to update the model.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to connect the <code>Network</code> with the <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>) and save the best model (<code>BestModelSaver</code>) along the way. We will compute accuracy both with respect to the clean input images ('clean accuracy') as well as with respect to the adversarial input images ('adversarial accuracy'). At the end, we use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Let's start by re-loading the weights from the best model, since the model may have overfit during training</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#comparison-vs-network-without-adversarial-training", "title": "Comparison vs Network without Adversarial Training\u00b6", "text": "<p>To see whether training using adversarial hardening was actually useful, we will compare it to a network which is trained without considering any adversarial images. The setup will be similar to before, but we will only use the adversarial images for evaluation purposes and so the second <code>CrossEntropy</code> Op as well as the <code>Average</code> Op can be omitted.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#visualizing-adversarial-samples", "title": "Visualizing Adversarial Samples\u00b6", "text": "<p>Lets visualize some images generated by these adversarial attacks to make sure that everything is working as we would expect. The first step is to get some sample data from the pipeline:</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html", "title": "Anomaly Detection with Fastestimator", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nimport numpy as np\nimport tensorflow as tf\nfrom fastestimator.backend import binary_crossentropy\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Normalize\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.util import ImageDisplay, GridDisplay, to_number\nfrom sklearn.metrics import auc, f1_score, roc_curve\nfrom tensorflow.keras import layers\n</pre> import tempfile  import fastestimator as fe import numpy as np import tensorflow as tf from fastestimator.backend import binary_crossentropy from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.univariate import ExpandDims, Normalize from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace import Trace from fastestimator.trace.io import BestModelSaver from fastestimator.util import ImageDisplay, GridDisplay, to_number from sklearn.metrics import auc, f1_score, roc_curve from tensorflow.keras import layers In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs=20\nbatch_size=128\ntrain_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # Parameters epochs=20 batch_size=128 train_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\n\n# Create Training Dataset\nx_train, y_train = x_train[np.where((y_train == 1))], np.zeros(y_train[np.where((y_train == 1))].shape)\ntrain_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\n\n# Create Validation Dataset\nx_eval0, y_eval0 = x_eval[np.where((y_eval == 1))], np.ones(y_eval[np.where((y_eval == 1))].shape)\nx_eval1, y_eval1 = x_eval[np.where((y_eval != 1))], y_eval[np.where((y_eval != 1))]\n\n# Ensuring outliers comprise 50% of the dataset\nindex = np.random.choice(x_eval1.shape[0], int(x_eval0.shape[0]), replace=False)\nx_eval1, y_eval1 = x_eval1[index], np.zeros(y_eval1[index].shape)\n\nx_eval, y_eval = np.concatenate([x_eval0, x_eval1]), np.concatenate([y_eval0, y_eval1])\neval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n</pre> (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()  # Create Training Dataset x_train, y_train = x_train[np.where((y_train == 1))], np.zeros(y_train[np.where((y_train == 1))].shape) train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})  # Create Validation Dataset x_eval0, y_eval0 = x_eval[np.where((y_eval == 1))], np.ones(y_eval[np.where((y_eval == 1))].shape) x_eval1, y_eval1 = x_eval[np.where((y_eval != 1))], y_eval[np.where((y_eval != 1))]  # Ensuring outliers comprise 50% of the dataset index = np.random.choice(x_eval1.shape[0], int(x_eval0.shape[0]), replace=False) x_eval1, y_eval1 = x_eval1[index], np.zeros(y_eval1[index].shape)  x_eval, y_eval = np.concatenate([x_eval0, x_eval1]), np.concatenate([y_eval0, y_eval1]) eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval}) In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x\"),\n        Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        LambdaOp(fn=lambda x: x + np.random.normal(loc=0.0, scale=0.155, size=(28, 28, 1)),\n                 inputs=\"x\",\n                 outputs=\"x_w_noise\",\n                 mode=\"train\")\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x\"),         Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),         LambdaOp(fn=lambda x: x + np.random.normal(loc=0.0, scale=0.155, size=(28, 28, 1)),                  inputs=\"x\",                  outputs=\"x_w_noise\",                  mode=\"train\")     ]) <p>We can visualize sample images from our <code>Pipeline</code> using the 'get_results' method.</p> In\u00a0[5]: Copied! <pre>sample_batch = pipeline.get_results()\n\nGridDisplay([ImageDisplay(image=sample_batch[\"x\"][0], color_map='greys', title=\"Image\"),\n             ImageDisplay(image=sample_batch[\"x_w_noise\"][0], color_map='greys', title=\"Noisy Image\")\n            ]).show()\n</pre> sample_batch = pipeline.get_results()  GridDisplay([ImageDisplay(image=sample_batch[\"x\"][0], color_map='greys', title=\"Image\"),              ImageDisplay(image=sample_batch[\"x_w_noise\"][0], color_map='greys', title=\"Noisy Image\")             ]).show() In\u00a0[6]: Copied! <pre>def reconstructor(input_shape=(28, 28, 1)):\n    model = tf.keras.Sequential()\n    # Encoder Block\n    model.add(\n        layers.Conv2D(32, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      input_shape=input_shape))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(64, (5, 5),\n                      strides=(2, 2),\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(128, (5, 5),\n                      strides=(2, 2),\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n\n    # Decoder Block\n    model.add(\n        layers.Conv2DTranspose(32, (5, 5),\n                               strides=(2, 2),\n                               output_padding=(0, 0),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(\n        layers.Conv2DTranspose(16, (5, 5),\n                               strides=(2, 2),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.ReLU())\n    model.add(\n        layers.Conv2DTranspose(1, (5, 5),\n                               strides=(2, 2),\n                               padding='same',\n                               kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02),\n                               activation='tanh'))\n    return model\n\n\ndef discriminator(input_shape=(28, 28, 1)):\n    model = tf.keras.Sequential()\n    model.add(\n        layers.Conv2D(16, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n                      input_shape=input_shape))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(32, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(64, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(0.2))\n    model.add(\n        layers.Conv2D(128, (5, 5),\n                      strides=(2, 2),\n                      padding='same',\n                      kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))\n    model.add(layers.LeakyReLU(0.2))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation=\"sigmoid\"))\n    return model\n</pre> def reconstructor(input_shape=(28, 28, 1)):     model = tf.keras.Sequential()     # Encoder Block     model.add(         layers.Conv2D(32, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       input_shape=input_shape))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(64, (5, 5),                       strides=(2, 2),                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       padding='same'))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(128, (5, 5),                       strides=(2, 2),                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       padding='same'))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))      # Decoder Block     model.add(         layers.Conv2DTranspose(32, (5, 5),                                strides=(2, 2),                                output_padding=(0, 0),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.ReLU())     model.add(         layers.Conv2DTranspose(16, (5, 5),                                strides=(2, 2),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.ReLU())     model.add(         layers.Conv2DTranspose(1, (5, 5),                                strides=(2, 2),                                padding='same',                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02),                                activation='tanh'))     return model   def discriminator(input_shape=(28, 28, 1)):     model = tf.keras.Sequential()     model.add(         layers.Conv2D(16, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),                       input_shape=input_shape))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(32, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(64, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU(0.2))     model.add(         layers.Conv2D(128, (5, 5),                       strides=(2, 2),                       padding='same',                       kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02)))     model.add(layers.LeakyReLU(0.2))     model.add(layers.Flatten())     model.add(layers.Dense(1, activation=\"sigmoid\"))     return model In\u00a0[7]: Copied! <pre>recon_model = fe.build(model_fn=reconstructor, optimizer_fn=lambda: tf.optimizers.RMSprop(2e-4), model_name=\"reconstructor\")\ndisc_model = fe.build(model_fn=discriminator,\n                      optimizer_fn=lambda: tf.optimizers.RMSprop(1e-4),\n                      model_name=\"discriminator\")\n</pre> recon_model = fe.build(model_fn=reconstructor, optimizer_fn=lambda: tf.optimizers.RMSprop(2e-4), model_name=\"reconstructor\") disc_model = fe.build(model_fn=discriminator,                       optimizer_fn=lambda: tf.optimizers.RMSprop(1e-4),                       model_name=\"discriminator\") <pre>2022-05-20 22:51:09.938098: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-20 22:51:10.451389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n</pre> In\u00a0[8]: Copied! <pre>class RLoss(TensorOp):\n    def __init__(self, alpha=0.2, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.alpha = alpha\n\n    def forward(self, data, state):\n        fake_score, x_fake, x = data\n        recon_loss = binary_crossentropy(y_true=x, y_pred=x_fake, from_logits=True)\n        adv_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.ones_like(fake_score), from_logits=True)\n        return adv_loss + self.alpha * recon_loss\n\n\nclass DLoss(TensorOp):\n    def forward(self, data, state):\n        true_score, fake_score = data\n        real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)\n        fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)\n        total_loss = real_loss + fake_loss\n        return total_loss\n</pre> class RLoss(TensorOp):     def __init__(self, alpha=0.2, inputs=None, outputs=None, mode=None):         super().__init__(inputs, outputs, mode)         self.alpha = alpha      def forward(self, data, state):         fake_score, x_fake, x = data         recon_loss = binary_crossentropy(y_true=x, y_pred=x_fake, from_logits=True)         adv_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.ones_like(fake_score), from_logits=True)         return adv_loss + self.alpha * recon_loss   class DLoss(TensorOp):     def forward(self, data, state):         true_score, fake_score = data         real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)         fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)         total_loss = real_loss + fake_loss         return total_loss <p>We now define the <code>Network</code> object:</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=recon_model, inputs=\"x_w_noise\", outputs=\"x_fake\", mode=\"train\"),\n    ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\", mode=\"eval\"),\n    ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),\n    ModelOp(model=disc_model, inputs=\"x\", outputs=\"true_score\"),\n    RLoss(inputs=(\"fake_score\", \"x_fake\", \"x\"), outputs=\"rloss\"),\n    UpdateOp(model=recon_model, loss_name=\"rloss\"),\n    DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),\n    UpdateOp(model=disc_model, loss_name=\"dloss\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=recon_model, inputs=\"x_w_noise\", outputs=\"x_fake\", mode=\"train\"),     ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\", mode=\"eval\"),     ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),     ModelOp(model=disc_model, inputs=\"x\", outputs=\"true_score\"),     RLoss(inputs=(\"fake_score\", \"x_fake\", \"x\"), outputs=\"rloss\"),     UpdateOp(model=recon_model, loss_name=\"rloss\"),     DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),     UpdateOp(model=disc_model, loss_name=\"dloss\") ]) <pre>2022-05-20 22:51:11.153783: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> <p>In this example we will also use the following traces:</p> <ol> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>A custom trace to calculate Area Under the Curve and F1-Score.</li> </ol> In\u00a0[10]: Copied! <pre>class F1AUCScores(Trace):\n\"\"\"Computes F1-Score and AUC Score for a classification task and reports it back to the logger.\n    \"\"\"\n    def __init__(self, true_key, pred_key, mode=(\"eval\", \"test\"), output_name=[\"auc_score\", \"f1_score\"]):\n        super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\n        self.y_true = []\n        self.y_pred = []\n\n    @property\n    def true_key(self):\n        return self.inputs[0]\n\n    @property\n    def pred_key(self):\n        return self.inputs[1]\n\n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n\n    def on_batch_end(self, data):\n        y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\n        assert y_pred.size == y_true.size\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n\n    def on_epoch_end(self, data):\n        fpr, tpr, thresholds = roc_curve(self.y_true, self.y_pred, pos_label=1)\n        roc_auc = auc(fpr, tpr)\n        eer_threshold = thresholds[np.nanargmin(np.absolute((1 - tpr - fpr)))]\n        y_pred_class = np.copy(self.y_pred)\n        y_pred_class[y_pred_class &gt;= eer_threshold] = 1\n        y_pred_class[y_pred_class &lt; eer_threshold] = 0\n        f_score = f1_score(self.y_true, y_pred_class)\n\n        data.write_with_log(self.outputs[0], roc_auc)\n        data.write_with_log(self.outputs[1], f_score)\n        \n\ntraces = [\n    F1AUCScores(true_key=\"y\", pred_key=\"fake_score\", mode=\"eval\", output_name=[\"auc_score\", \"f1_score\"]),\n    BestModelSaver(model=recon_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True),\n    BestModelSaver(model=disc_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True)\n]\n</pre> class F1AUCScores(Trace):     \"\"\"Computes F1-Score and AUC Score for a classification task and reports it back to the logger.     \"\"\"     def __init__(self, true_key, pred_key, mode=(\"eval\", \"test\"), output_name=[\"auc_score\", \"f1_score\"]):         super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)         self.y_true = []         self.y_pred = []      @property     def true_key(self):         return self.inputs[0]      @property     def pred_key(self):         return self.inputs[1]      def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []      def on_batch_end(self, data):         y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])         assert y_pred.size == y_true.size         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())      def on_epoch_end(self, data):         fpr, tpr, thresholds = roc_curve(self.y_true, self.y_pred, pos_label=1)         roc_auc = auc(fpr, tpr)         eer_threshold = thresholds[np.nanargmin(np.absolute((1 - tpr - fpr)))]         y_pred_class = np.copy(self.y_pred)         y_pred_class[y_pred_class &gt;= eer_threshold] = 1         y_pred_class[y_pred_class &lt; eer_threshold] = 0         f_score = f1_score(self.y_true, y_pred_class)          data.write_with_log(self.outputs[0], roc_auc)         data.write_with_log(self.outputs[1], f_score)           traces = [     F1AUCScores(true_key=\"y\", pred_key=\"fake_score\", mode=\"eval\", output_name=[\"auc_score\", \"f1_score\"]),     BestModelSaver(model=recon_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True),     BestModelSaver(model=disc_model, save_dir=save_dir, metric='f1_score', save_best_mode='max', load_best_final=True) ] In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\n</pre> <pre>/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning:\n\n\"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n\n2022-05-20 22:51:17.273882: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n</pre> <pre>FastEstimator-Warn: the key 'y' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\n</pre> <pre>2022-05-20 22:51:29.792069: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Train: step: 1; dloss: 1.4055667; rloss: 0.9982634;\nFastEstimator-Train: step: 53; epoch: 1; epoch_time: 8.74 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 155.95;\nEval Progress: 11/17; steps/sec: 185.13;\nEval Progress: 17/17; steps/sec: 172.52;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 53; epoch: 1; auc_score: 0.4215160395117313; dloss: 1.4071065; f1_score: 0.4431718061674009; max_f1_score: 0.4431718061674009; rloss: 1.0221571; since_best_f1_score: 0;\nFastEstimator-Train: step: 100; dloss: 0.18782794; rloss: 3.4576418; steps/sec: 12.69;\nFastEstimator-Train: step: 106; epoch: 2; epoch_time: 5.69 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 158.05;\nEval Progress: 11/17; steps/sec: 237.7;\nEval Progress: 17/17; steps/sec: 212.97;\nFastEstimator-Eval: step: 106; epoch: 2; auc_score: 0.0593130082089697; dloss: 1.7725676; f1_score: 0.12945838837516513; max_f1_score: 0.4431718061674009; rloss: 1.739687; since_best_f1_score: 1;\nFastEstimator-Train: step: 159; epoch: 3; epoch_time: 5.57 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 203.15;\nEval Progress: 11/17; steps/sec: 257.38;\nEval Progress: 17/17; steps/sec: 267.37;\nFastEstimator-Eval: step: 159; epoch: 3; auc_score: 0.3312002949795261; dloss: 3.8447435; f1_score: 0.3762114537444934; max_f1_score: 0.4431718061674009; rloss: 3.5073256; since_best_f1_score: 2;\nFastEstimator-Train: step: 200; dloss: 0.0074149473; rloss: 5.2848363; steps/sec: 8.9;\nFastEstimator-Train: step: 212; epoch: 4; epoch_time: 5.7 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 133.68;\nEval Progress: 11/17; steps/sec: 182.39;\nEval Progress: 17/17; steps/sec: 172.01;\nFastEstimator-Eval: step: 212; epoch: 4; auc_score: 0.12731859729472725; dloss: 10.529196; f1_score: 0.20000000000000004; max_f1_score: 0.4431718061674009; rloss: 6.2233853; since_best_f1_score: 3;\nFastEstimator-Train: step: 265; epoch: 5; epoch_time: 5.84 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 113.41;\nEval Progress: 11/17; steps/sec: 165.91;\nEval Progress: 17/17; steps/sec: 268.33;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 265; epoch: 5; auc_score: 0.9694777697995305; dloss: 16.567978; f1_score: 0.9053280493174812; max_f1_score: 0.9053280493174812; rloss: 9.16089; since_best_f1_score: 0;\nFastEstimator-Train: step: 300; dloss: 9.0785594e-05; rloss: 9.5743; steps/sec: 8.69;\nFastEstimator-Train: step: 318; epoch: 6; epoch_time: 5.87 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 153.15;\nEval Progress: 11/17; steps/sec: 223.88;\nEval Progress: 17/17; steps/sec: 217.8;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 318; epoch: 6; auc_score: 0.9818909740146323; dloss: 20.46954; f1_score: 0.9263992948435434; max_f1_score: 0.9263992948435434; rloss: 10.067438; since_best_f1_score: 0;\nFastEstimator-Train: step: 371; epoch: 7; epoch_time: 5.85 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 202.2;\nEval Progress: 11/17; steps/sec: 238.39;\nEval Progress: 17/17; steps/sec: 224.66;\nFastEstimator-Eval: step: 371; epoch: 7; auc_score: 0.6487768052941063; dloss: 17.003485; f1_score: 0.6144366197183099; max_f1_score: 0.9263992948435434; rloss: 13.551922; since_best_f1_score: 1;\nFastEstimator-Train: step: 400; dloss: 2.1421674e-06; rloss: 13.66077; steps/sec: 8.66;\nFastEstimator-Train: step: 424; epoch: 8; epoch_time: 5.64 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 139.75;\nEval Progress: 11/17; steps/sec: 249.63;\nEval Progress: 17/17; steps/sec: 213.55;\nFastEstimator-Eval: step: 424; epoch: 8; auc_score: 0.5244898988918861; dloss: 26.341074; f1_score: 0.5312775330396475; max_f1_score: 0.9263992948435434; rloss: 14.630283; since_best_f1_score: 2;\nFastEstimator-Train: step: 477; epoch: 9; epoch_time: 5.71 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 118.19;\nEval Progress: 11/17; steps/sec: 188.76;\nEval Progress: 17/17; steps/sec: 175.64;\nFastEstimator-Eval: step: 477; epoch: 9; auc_score: 0.8110687185856508; dloss: 21.545362; f1_score: 0.7548500881834214; max_f1_score: 0.9263992948435434; rloss: 15.638036; since_best_f1_score: 3;\nFastEstimator-Train: step: 500; dloss: 4.0664025e-05; rloss: 13.955134; steps/sec: 8.85;\nFastEstimator-Train: step: 530; epoch: 10; epoch_time: 5.67 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 169.3;\nEval Progress: 11/17; steps/sec: 207.95;\nEval Progress: 17/17; steps/sec: 202.13;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 530; epoch: 10; auc_score: 0.9777368083991539; dloss: 17.341671; f1_score: 0.9463028169014085; max_f1_score: 0.9463028169014085; rloss: 12.420723; since_best_f1_score: 0;\nFastEstimator-Train: step: 583; epoch: 11; epoch_time: 5.64 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 128.68;\nEval Progress: 11/17; steps/sec: 247.49;\nEval Progress: 17/17; steps/sec: 240.88;\nFastEstimator-Eval: step: 583; epoch: 11; auc_score: 0.9810429078771178; dloss: 12.695894; f1_score: 0.9219920669898635; max_f1_score: 0.9463028169014085; rloss: 6.138731; since_best_f1_score: 1;\nFastEstimator-Train: step: 600; dloss: 0.1742793; rloss: 2.5519087; steps/sec: 8.71;\nFastEstimator-Train: step: 636; epoch: 12; epoch_time: 5.91 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 69.28;\nEval Progress: 11/17; steps/sec: 187.24;\nEval Progress: 17/17; steps/sec: 212.75;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Eval: step: 636; epoch: 12; auc_score: 0.99651613654447; dloss: 3.250877; f1_score: 0.9788546255506608; max_f1_score: 0.9788546255506608; rloss: 0.61733353; since_best_f1_score: 0;\nFastEstimator-Train: step: 689; epoch: 13; epoch_time: 5.54 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 170.37;\nEval Progress: 11/17; steps/sec: 251.05;\nEval Progress: 17/17; steps/sec: 223.27;\nFastEstimator-Eval: step: 689; epoch: 13; auc_score: 0.9530811775893187; dloss: 3.0862265; f1_score: 0.8974020255394101; max_f1_score: 0.9788546255506608; rloss: 2.3505096; since_best_f1_score: 1;\nFastEstimator-Train: step: 700; dloss: 0.09519278; rloss: 5.07388; steps/sec: 8.95;\nFastEstimator-Train: step: 742; epoch: 14; epoch_time: 5.68 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 189.04;\nEval Progress: 11/17; steps/sec: 235.52;\nEval Progress: 17/17; steps/sec: 271.66;\nFastEstimator-Eval: step: 742; epoch: 14; auc_score: 0.9910233072638709; dloss: 2.9711509; f1_score: 0.9616571176729837; max_f1_score: 0.9788546255506608; rloss: 0.5347926; since_best_f1_score: 2;\nFastEstimator-Train: step: 795; epoch: 15; epoch_time: 5.67 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 153.04;\nEval Progress: 11/17; steps/sec: 247.51;\nEval Progress: 17/17; steps/sec: 232.68;\nFastEstimator-Eval: step: 795; epoch: 15; auc_score: 0.9850724834559181; dloss: 1.6044965; f1_score: 0.9494060712714474; max_f1_score: 0.9788546255506608; rloss: 3.59912; since_best_f1_score: 3;\nFastEstimator-Train: step: 800; dloss: 0.30007884; rloss: 2.662114; steps/sec: 8.83;\nFastEstimator-Train: step: 848; epoch: 16; epoch_time: 5.9 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 72.03;\nEval Progress: 11/17; steps/sec: 156.27;\nEval Progress: 17/17; steps/sec: 209.58;\nFastEstimator-Eval: step: 848; epoch: 16; auc_score: 0.9890345242484814; dloss: 2.4346852; f1_score: 0.9379128137384413; max_f1_score: 0.9788546255506608; rloss: 1.1848502; since_best_f1_score: 4;\nFastEstimator-Train: step: 900; dloss: 2.2080877; rloss: 0.1052717; steps/sec: 15.36;\nFastEstimator-Train: step: 901; epoch: 17; epoch_time: 5.79 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 115.86;\nEval Progress: 11/17; steps/sec: 181.74;\nEval Progress: 17/17; steps/sec: 141.38;\nFastEstimator-Eval: step: 901; epoch: 17; auc_score: 0.9592586698752159; dloss: 1.7382531; f1_score: 0.8748898678414097; max_f1_score: 0.9788546255506608; rloss: 1.7491591; since_best_f1_score: 5;\nFastEstimator-Train: step: 954; epoch: 18; epoch_time: 5.87 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 88.9;\nEval Progress: 11/17; steps/sec: 133.96;\nEval Progress: 17/17; steps/sec: 209.24;\nFastEstimator-Eval: step: 954; epoch: 18; auc_score: 0.97444080032603; dloss: 2.4483051; f1_score: 0.9131776112825033; max_f1_score: 0.9788546255506608; rloss: 2.4951015; since_best_f1_score: 6;\nFastEstimator-Train: step: 1000; dloss: 1.7254086; rloss: 0.19289978; steps/sec: 8.57;\nFastEstimator-Train: step: 1007; epoch: 19; epoch_time: 5.89 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 115.95;\nEval Progress: 11/17; steps/sec: 184.74;\nEval Progress: 17/17; steps/sec: 213.9;\nFastEstimator-Eval: step: 1007; epoch: 19; auc_score: 0.9717246599002504; dloss: 1.33784; f1_score: 0.9040492957746478; max_f1_score: 0.9788546255506608; rloss: 0.684746; since_best_f1_score: 7;\nFastEstimator-Train: step: 1060; epoch: 20; epoch_time: 5.94 sec;\nEval Progress: 1/17;\nEval Progress: 5/17; steps/sec: 141.37;\nEval Progress: 11/17; steps/sec: 212.39;\nEval Progress: 17/17; steps/sec: 228.58;\nFastEstimator-Eval: step: 1060; epoch: 20; auc_score: 0.7852941062314424; dloss: 1.8164662; f1_score: 0.7163652404058226; max_f1_score: 0.9788546255506608; rloss: 1.6960378; since_best_f1_score: 8;\nFastEstimator-BestModelSaver: Restoring model from /tmp/tmp2w1fltl6/reconstructor_best_f1_score.h5\nFastEstimator-BestModelSaver: Restoring model from /tmp/tmp2w1fltl6/discriminator_best_f1_score.h5\nFastEstimator-Finish: step: 1060; discriminator_lr: 1e-04; reconstructor_lr: 0.0002; total_time: 212.81 sec;\n</pre> In\u00a0[13]: Copied! <pre>idx0 = np.random.randint(len(x_eval0))\nidx1 = np.random.randint(len(x_eval1))\n\ndata = [{\"x\": x_eval0[idx0]}, {\"x\": x_eval1[idx1]}]\nresult = [pipeline.transform(data[i], mode=\"infer\") for i in range(len(data))]\n</pre> idx0 = np.random.randint(len(x_eval0)) idx1 = np.random.randint(len(x_eval1))  data = [{\"x\": x_eval0[idx0]}, {\"x\": x_eval1[idx1]}] result = [pipeline.transform(data[i], mode=\"infer\") for i in range(len(data))] In\u00a0[14]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\"),\n    ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\")\n])\n\noutput_imgs = [network.transform(result[i], mode=\"infer\") for i in range(len(result))]\n</pre> network = fe.Network(ops=[     ModelOp(model=recon_model, inputs=\"x\", outputs=\"x_fake\"),     ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\") ])  output_imgs = [network.transform(result[i], mode=\"infer\") for i in range(len(result))] In\u00a0[16]: Copied! <pre>base_image = output_imgs[0][\"x\"].numpy()\nanomaly_image = output_imgs[1][\"x\"].numpy()\n\nrecon_base_image = output_imgs[0][\"x_fake\"].numpy()\nrecon_anomaly_image = output_imgs[1][\"x_fake\"].numpy()\n\nGridDisplay([ImageDisplay(image=base_image[0], color_map='greys', title=\"Input Image\"),\n             ImageDisplay(image=recon_base_image[0], color_map='greys', title=\"Reconstructed Image\")\n            ]).show()\n\nGridDisplay([ImageDisplay(image=anomaly_image[0], color_map='greys', title=\"Input Image\"),\n             ImageDisplay(image=recon_anomaly_image[0], color_map='greys', title=\"Reconstructed Image\")\n            ]).show()\n</pre> base_image = output_imgs[0][\"x\"].numpy() anomaly_image = output_imgs[1][\"x\"].numpy()  recon_base_image = output_imgs[0][\"x_fake\"].numpy() recon_anomaly_image = output_imgs[1][\"x_fake\"].numpy()  GridDisplay([ImageDisplay(image=base_image[0], color_map='greys', title=\"Input Image\"),              ImageDisplay(image=recon_base_image[0], color_map='greys', title=\"Reconstructed Image\")             ]).show()  GridDisplay([ImageDisplay(image=anomaly_image[0], color_map='greys', title=\"Input Image\"),              ImageDisplay(image=recon_anomaly_image[0], color_map='greys', title=\"Reconstructed Image\")             ]).show() <p>Note that the network is trained on inliers, so it's able to properly reconstruct them but does a poor job at reconstructing the outliers, thereby making it easier for discriminator to detect the outliers.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#anomaly-detection-with-fastestimator", "title": "Anomaly Detection with Fastestimator\u00b6", "text": "<p>In this notebook we will demonstrate how to do anomaly detection using one class classifier as described in Adversarially Learned One-Class Classifier for Novelty Detection. In real world, outliers or novelty class is often absent from the training dataset. Such problems can be efficiently modeled using one class classifiers. In the algorihm demonstrated below, two networks are trained to compete with each other where one network acts as a novelty detector and other enhaces the inliers and distorts the outliers. We use images of digit \"1\" from MNIST dataset for training and images of other digits as outliers.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download training images using tensorflow API. We will use images of digit <code>1</code> for training and test images of <code>1</code> as inliers and images of other digits as outliers. Outliers comprise 50% of our validation dataset.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We will use the <code>LambdaOp</code> to add noise to the images during training.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model consists of an Autoencoder (ecoder-decoder) network and a Discriminator network. [Credit: https://arxiv.org/pdf/1802.09088.pdf]</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#defining-loss", "title": "Defining Loss\u00b6", "text": "<p>The losses of both the networks are smilar to a standarad GAN network with the exception of the autoencoder having and additional reconstruction loss term to enforce similarity between the input and the reconstructed image. We first define custom <code>TensorOp</code>s to calculate the losses of both the networks.</p>"}, {"location": "apphub/anomaly_detection/alocc/alocc.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/anomaly_detection/alocc/alocc.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will apply the model to visualize the reconstructed image of the inliers and outliers.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html", "title": "RandAug:  automated data augmentation with reduced search space", "text": "In\u00a0[1]: Copied! <pre>import random\n\nimport numpy as np\nfrom PIL import Image, ImageEnhance, ImageOps, ImageTransform\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data.cifair10 import load_data\nfrom fastestimator.op.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.meta import OneOf\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import BatchDisplay, GridDisplay\n</pre> import random  import numpy as np from PIL import Image, ImageEnhance, ImageOps, ImageTransform  import fastestimator as fe from fastestimator.dataset.data.cifair10 import load_data from fastestimator.op.numpyop import NumpyOp from fastestimator.op.numpyop.meta import OneOf from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.metric import Accuracy from fastestimator.util import BatchDisplay, GridDisplay In\u00a0[2]: Copied! <pre>class Rotate(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.degree = level * 3.0\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        degree = self.degree * random.choice([1.0, -1.0])\n        im = im.rotate(degree)\n        return np.asarray(im)\n\n\nclass Identity(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n\nclass AutoContrast(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        im = ImageOps.autocontrast(im)\n        return np.copy(np.asarray(im))\n\n\nclass Equalize(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        im = ImageOps.equalize(im)\n        return np.copy(np.asarray(im))\n\n\nclass Posterize(NumpyOp):\n    # resuce the number of bits for each channel, this may be inconsistent with original implementation\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.bits = 8 - int((level / 10) * 4)\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        im = ImageOps.posterize(im, self.bits)\n        return np.copy(np.asarray(im))\n\n\nclass Solarize(NumpyOp):\n    # this may be inconsistent with original implementation\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.threshold = 256 - int(level * 25.6)\n\n    def forward(self, data, state):\n        data = np.where(data &lt; self.threshold, data, 255 - data)\n        return data\n\n\nclass Sharpness(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.diff = 0.09 * level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        factor = 1.0 + self.diff * random.choice([1.0, -1.0])\n        im = ImageEnhance.Sharpness(im).enhance(factor)\n        return np.copy(np.asarray(im))\n\n\nclass Contrast(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.diff = 0.09 * level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        factor = 1.0 + self.diff * random.choice([1.0, -1.0])\n        im = ImageEnhance.Contrast(im).enhance(factor)\n        return np.copy(np.asarray(im))\n\n\nclass Color(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.diff = 0.09 * level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        factor = 1.0 + self.diff * random.choice([1.0, -1.0])\n        im = ImageEnhance.Color(im).enhance(factor)\n        return np.copy(np.asarray(im))\n\n\nclass Brightness(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.diff = 0.09 * level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        factor = 1.0 + self.diff * random.choice([1.0, -1.0])\n        im = ImageEnhance.Brightness(im).enhance(factor)\n        return np.copy(np.asarray(im))\n\n\nclass ShearX(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.shear_coef = level * 0.03\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        shear_coeff = self.shear_coef * random.choice([1.0, -1.0])\n        width, height = im.size\n        xshift = int(round(self.shear_coef * width))\n        new_width = width + xshift\n        im = im.transform((new_width, height),\n                          ImageTransform.AffineTransform(\n                              (1.0, shear_coeff, -xshift if shear_coeff &gt; 0 else 0.0, 0.0, 1.0, 0.0)),\n                          resample=Image.BICUBIC)\n        if self.shear_coef &gt; 0:\n            im = im.resize((width, height))\n        return np.copy(np.asarray(im))\n\n\nclass ShearY(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.shear_coef = level * 0.03\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        shear_coeff = self.shear_coef * random.choice([1.0, -1.0])\n        width, height = im.size\n        yshift = int(round(self.shear_coef * height))\n        newheight = height + yshift\n        im = im.transform((width, newheight),\n                          ImageTransform.AffineTransform(\n                              (1.0, 0.0, 0.0, shear_coeff, 1.0, -yshift if shear_coeff &gt; 0 else 0.0)),\n                          resample=Image.BICUBIC)\n        if self.shear_coef &gt; 0:\n            im = im.resize((width, height))\n        return np.copy(np.asarray(im))\n\n\nclass TranslateX(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.level = level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        width, height = im.size\n        displacement = int(self.level / 10 * width / 3 * random.choice([1.0, -1.0]))\n        im = im.transform((width, height),\n                          ImageTransform.AffineTransform((1.0, 0.0, displacement, 0.0, 1.0, 0.0)),\n                          resample=Image.BICUBIC)\n        return np.copy(np.asarray(im))\n\n\nclass TranslateY(NumpyOp):\n    def __init__(self, level, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.level = level\n\n    def forward(self, data, state):\n        im = Image.fromarray(data)\n        width, height = im.size\n        displacement = int(self.level / 10 * height / 3 * random.choice([1.0, -1.0]))\n        im = im.transform((width, height),\n                          ImageTransform.AffineTransform((1.0, 0.0, 0.0, 0.0, 1.0, displacement)),\n                          resample=Image.BICUBIC)\n        return np.copy(np.asarray(im))\n</pre> class Rotate(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.degree = level * 3.0      def forward(self, data, state):         im = Image.fromarray(data)         degree = self.degree * random.choice([1.0, -1.0])         im = im.rotate(degree)         return np.asarray(im)   class Identity(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)   class AutoContrast(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)      def forward(self, data, state):         im = Image.fromarray(data)         im = ImageOps.autocontrast(im)         return np.copy(np.asarray(im))   class Equalize(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)      def forward(self, data, state):         im = Image.fromarray(data)         im = ImageOps.equalize(im)         return np.copy(np.asarray(im))   class Posterize(NumpyOp):     # resuce the number of bits for each channel, this may be inconsistent with original implementation     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.bits = 8 - int((level / 10) * 4)      def forward(self, data, state):         im = Image.fromarray(data)         im = ImageOps.posterize(im, self.bits)         return np.copy(np.asarray(im))   class Solarize(NumpyOp):     # this may be inconsistent with original implementation     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.threshold = 256 - int(level * 25.6)      def forward(self, data, state):         data = np.where(data &lt; self.threshold, data, 255 - data)         return data   class Sharpness(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.diff = 0.09 * level      def forward(self, data, state):         im = Image.fromarray(data)         factor = 1.0 + self.diff * random.choice([1.0, -1.0])         im = ImageEnhance.Sharpness(im).enhance(factor)         return np.copy(np.asarray(im))   class Contrast(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.diff = 0.09 * level      def forward(self, data, state):         im = Image.fromarray(data)         factor = 1.0 + self.diff * random.choice([1.0, -1.0])         im = ImageEnhance.Contrast(im).enhance(factor)         return np.copy(np.asarray(im))   class Color(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.diff = 0.09 * level      def forward(self, data, state):         im = Image.fromarray(data)         factor = 1.0 + self.diff * random.choice([1.0, -1.0])         im = ImageEnhance.Color(im).enhance(factor)         return np.copy(np.asarray(im))   class Brightness(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.diff = 0.09 * level      def forward(self, data, state):         im = Image.fromarray(data)         factor = 1.0 + self.diff * random.choice([1.0, -1.0])         im = ImageEnhance.Brightness(im).enhance(factor)         return np.copy(np.asarray(im))   class ShearX(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.shear_coef = level * 0.03      def forward(self, data, state):         im = Image.fromarray(data)         shear_coeff = self.shear_coef * random.choice([1.0, -1.0])         width, height = im.size         xshift = int(round(self.shear_coef * width))         new_width = width + xshift         im = im.transform((new_width, height),                           ImageTransform.AffineTransform(                               (1.0, shear_coeff, -xshift if shear_coeff &gt; 0 else 0.0, 0.0, 1.0, 0.0)),                           resample=Image.BICUBIC)         if self.shear_coef &gt; 0:             im = im.resize((width, height))         return np.copy(np.asarray(im))   class ShearY(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.shear_coef = level * 0.03      def forward(self, data, state):         im = Image.fromarray(data)         shear_coeff = self.shear_coef * random.choice([1.0, -1.0])         width, height = im.size         yshift = int(round(self.shear_coef * height))         newheight = height + yshift         im = im.transform((width, newheight),                           ImageTransform.AffineTransform(                               (1.0, 0.0, 0.0, shear_coeff, 1.0, -yshift if shear_coeff &gt; 0 else 0.0)),                           resample=Image.BICUBIC)         if self.shear_coef &gt; 0:             im = im.resize((width, height))         return np.copy(np.asarray(im))   class TranslateX(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.level = level      def forward(self, data, state):         im = Image.fromarray(data)         width, height = im.size         displacement = int(self.level / 10 * width / 3 * random.choice([1.0, -1.0]))         im = im.transform((width, height),                           ImageTransform.AffineTransform((1.0, 0.0, displacement, 0.0, 1.0, 0.0)),                           resample=Image.BICUBIC)         return np.copy(np.asarray(im))   class TranslateY(NumpyOp):     def __init__(self, level, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.level = level      def forward(self, data, state):         im = Image.fromarray(data)         width, height = im.size         displacement = int(self.level / 10 * height / 3 * random.choice([1.0, -1.0]))         im = im.transform((width, height),                           ImageTransform.AffineTransform((1.0, 0.0, 0.0, 0.0, 1.0, displacement)),                           resample=Image.BICUBIC)         return np.copy(np.asarray(im)) In\u00a0[3]: Copied! <pre>def get_pipeline(level, num_augment, batch_size):\n    assert 0 &lt;= level &lt;= 10, \"the level should be between 0 and 10\"\n    train_data, test_data = load_data()\n    aug_ops = [\n        OneOf(\n            Rotate(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Identity(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            AutoContrast(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Equalize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Posterize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Solarize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Sharpness(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Contrast(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Color(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            Brightness(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            ShearX(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            ShearY(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            TranslateX(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n            TranslateY(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),\n        ) for _ in range(num_augment)\n    ]\n    pipeline = fe.Pipeline(\n        train_data=train_data,\n        test_data=test_data,\n        batch_size=batch_size,\n        ops=aug_ops + [\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n            ChannelTranspose(inputs=\"x\", outputs=\"x\"),\n        ])\n    return pipeline\n</pre> def get_pipeline(level, num_augment, batch_size):     assert 0 &lt;= level &lt;= 10, \"the level should be between 0 and 10\"     train_data, test_data = load_data()     aug_ops = [         OneOf(             Rotate(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Identity(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             AutoContrast(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Equalize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Posterize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Solarize(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Sharpness(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Contrast(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Color(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             Brightness(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             ShearX(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             ShearY(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             TranslateX(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),             TranslateY(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),         ) for _ in range(num_augment)     ]     pipeline = fe.Pipeline(         train_data=train_data,         test_data=test_data,         batch_size=batch_size,         ops=aug_ops + [             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),             ChannelTranspose(inputs=\"x\", outputs=\"x\"),         ])     return pipeline In\u00a0[4]: Copied! <pre>pipeline = get_pipeline(level=4, num_augment=4, batch_size=8)\ndata = pipeline.get_results()\nclass_dictionary = {\n    0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n}\ny = np.array([class_dictionary[clazz.item()] for clazz in fe.util.to_number(data[\"y\"])])\n\nfig = GridDisplay([BatchDisplay(image=data['x'], title='x'),\n                   BatchDisplay(text=y, title='label')\n                  ])\nfig.show()\n</pre> pipeline = get_pipeline(level=4, num_augment=4, batch_size=8) data = pipeline.get_results() class_dictionary = {     0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\" } y = np.array([class_dictionary[clazz.item()] for clazz in fe.util.to_number(data[\"y\"])])  fig = GridDisplay([BatchDisplay(image=data['x'], title='x'),                    BatchDisplay(text=y, title='label')                   ]) fig.show() In\u00a0[5]: Copied! <pre>from fastestimator.architecture.pytorch import ResNet9\n\ndef get_network():\n    model = fe.build(model_fn=ResNet9, optimizer_fn=\"adam\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    return network\n</pre> from fastestimator.architecture.pytorch import ResNet9  def get_network():     model = fe.build(model_fn=ResNet9, optimizer_fn=\"adam\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     return network In\u00a0[6]: Copied! <pre>def get_estimator(level,\n                  num_augment,\n                  epochs=24,\n                  batch_size=512,\n                  train_steps_per_epoch=None,\n                  eval_steps_per_epoch=None):\n    pipeline = get_pipeline(batch_size=batch_size, level=level, num_augment=num_augment)\n    network = get_network()\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch)\n    return estimator\n</pre> def get_estimator(level,                   num_augment,                   epochs=24,                   batch_size=512,                   train_steps_per_epoch=None,                   eval_steps_per_epoch=None):     pipeline = get_pipeline(batch_size=batch_size, level=level, num_augment=num_augment)     network = get_network()     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch)     return estimator In\u00a0[7]: parameters Copied! <pre>#training parameters\nepochs = 50  # 50\nbatch_size = 512\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nmax_level = 10\nmax_num_augment = 10\nfinal_step = 4900\n</pre> #training parameters epochs = 50  # 50 batch_size = 512 train_steps_per_epoch = None eval_steps_per_epoch = None max_level = 10 max_num_augment = 10 final_step = 4900 In\u00a0[8]: Copied! <pre>from fastestimator.search import GridSearch\n\ndef eval_fn(search_idx, level, n_augment):\n    est = get_estimator(level=level,\n                        num_augment=n_augment,\n                        epochs=epochs,\n                        train_steps_per_epoch=train_steps_per_epoch,\n                        eval_steps_per_epoch=eval_steps_per_epoch,\n                        batch_size=batch_size\n                       )\n    est.fit()\n    result = est.test(summary='exp')\n    return {'accuracy': list(result.history['test']['accuracy'].values())[-1]}\n\nsearch = GridSearch(eval_fn=eval_fn, \n                    params={'level': [i for i in range(1, max_level+1)],\n                            'n_augment': [i for i in range(1, max_num_augment+1)]\n                           },\n                    best_mode='max'\n                   )\n\nsearch.fit()\n</pre> from fastestimator.search import GridSearch  def eval_fn(search_idx, level, n_augment):     est = get_estimator(level=level,                         num_augment=n_augment,                         epochs=epochs,                         train_steps_per_epoch=train_steps_per_epoch,                         eval_steps_per_epoch=eval_steps_per_epoch,                         batch_size=batch_size                        )     est.fit()     result = est.test(summary='exp')     return {'accuracy': list(result.history['test']['accuracy'].values())[-1]}  search = GridSearch(eval_fn=eval_fn,                      params={'level': [i for i in range(1, max_level+1)],                             'n_augment': [i for i in range(1, max_num_augment+1)]                            },                     best_mode='max'                    )  search.fit() In\u00a0[9]: Copied! <pre>fe.search.visualize.visualize_search(search)  \n# The output saved here was computed from a short version of the search (2 epochs rather than 50). The full training will look different\n</pre> fe.search.visualize.visualize_search(search)   # The output saved here was computed from a short version of the search (2 epochs rather than 50). The full training will look different"}, {"location": "apphub/automl/rand_augment/rand_augment.html#randaug-automated-data-augmentation-with-reduced-search-space", "title": "RandAug:  automated data augmentation with reduced search space\u00b6", "text": "<p>In this application, we are demonstrating automated augmentation as proposed here.</p> <p>The core idea of the paper is that it parameterizes data augmentation into two parameters: M and N. M represents the global augmentation intensity, which controls the magnitude of each augmentation. N represents the number of augmenting transformation to be applied.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/automl/rand_augment/rand_augment.html#define-the-transformations", "title": "Define the transformations\u00b6", "text": "<p>Each transformation is randomly selected from the following 14 operations, the official source code can be found here.</p> <ul> <li>Identity</li> <li>Rotate</li> <li>Posterize</li> <li>Sharpness</li> <li>AutoContrast</li> <li>Solarize</li> <li>Contrast</li> <li>Equalize</li> <li>Color</li> <li>Brightness</li> <li>Shear-x</li> <li>Shear-y</li> <li>Translate-x</li> <li>Translate-y</li> </ul> <p>On top of that, we use argument <code>level</code> to control the intensity of each augmentation.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#create-pipeline", "title": "Create Pipeline\u00b6", "text": "<p>We will use <code>OneOf</code> Op to randomly select 1 among 14 augmentations, then apply the <code>OneOf</code> for N (num_augment) times. After that, we will use <code>Normalize</code> to scale down the pixel values and finally <code>ChannelTranspose</code> to make image data channel-first.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#visualize-preprocessing-results", "title": "Visualize Preprocessing Results\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch of pipeline output, then we use <code>fe.util.GridDisplay</code> to visualize the result:</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#network-construction", "title": "<code>Network</code> construction\u00b6", "text": "<p>In this example, we will use the same network setup as the Fast Cifar10 example.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#putting-everything-together", "title": "Putting everything together\u00b6", "text": "<p>Now we are ready to combine every pieces together by creating an estimator instance:</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#applying-grid-search-to-find-the-best-levelm-and-num_augmentn", "title": "Applying Grid Search to find the best level(M) and num_augment(N):\u00b6", "text": "<p>As suggested by the paper, N and M ranges from 1 to 10, grid search can be used effectively to find the best parameter. Running 100 experiments takes ~20 hours on single V100 GPU.</p>"}, {"location": "apphub/automl/rand_augment/rand_augment.html#visualize-the-grid-search-results", "title": "Visualize the grid search results:\u00b6", "text": ""}, {"location": "apphub/contrastive_learning/simclr/simclr.html", "title": "SimCLR on CIFAIR10 Image Classification (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import ColorJitter, GaussianBlur, ToFloat, ToGray\nfrom fastestimator.op.tensorop import LambdaOp, TensorOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver, ModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  import tensorflow as tf from tensorflow.keras import layers  import fastestimator as fe from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import ColorJitter, GaussianBlur, ToFloat, ToGray from fastestimator.op.tensorop import LambdaOp, TensorOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver, ModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs_pretrain = 50\nepochs_finetune = 10\nbatch_size = 512\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs_pretrain = 50 epochs_finetune = 10 batch_size = 512 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = cifair10.load_data()\n</pre> train_data, eval_data = cifair10.load_data() In\u00a0[4]: Copied! <pre>pipeline_pretrain = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\"),\n\n        # augmentation 1\n        RandomCrop(32, 32, image_in=\"x\", image_out=\"x_aug\"),\n        Sometimes(HorizontalFlip(image_in=\"x_aug\", image_out=\"x_aug\"), prob=0.5),\n        Sometimes(\n                ColorJitter(inputs=\"x_aug\", outputs=\"x_aug\", brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n                prob=0.8),\n        Sometimes(ToGray(inputs=\"x_aug\", outputs=\"x_aug\"), prob=0.2),\n        Sometimes(GaussianBlur(inputs=\"x_aug\", outputs=\"x_aug\", blur_limit=(3, 3), sigma_limit=(0.1, 2.0)), prob=0.5),\n        ToFloat(inputs=\"x_aug\", outputs=\"x_aug\"),\n        \n        # augmentation 2\n        RandomCrop(32, 32, image_in=\"x\", image_out=\"x_aug2\"),\n        Sometimes(HorizontalFlip(image_in=\"x_aug2\", image_out=\"x_aug2\"), prob=0.5),\n        Sometimes(\n                ColorJitter(inputs=\"x_aug2\", outputs=\"x_aug2\", brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),\n                prob=0.8),\n        Sometimes(ToGray(inputs=\"x_aug2\", outputs=\"x_aug2\"), prob=0.2),\n        Sometimes(GaussianBlur(inputs=\"x_aug2\", outputs=\"x_aug2\", blur_limit=(3, 3), sigma_limit=(0.1, 2.0)), prob=0.5),\n        ToFloat(inputs=\"x_aug2\", outputs=\"x_aug2\")\n    ])\n</pre> pipeline_pretrain = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\"),          # augmentation 1         RandomCrop(32, 32, image_in=\"x\", image_out=\"x_aug\"),         Sometimes(HorizontalFlip(image_in=\"x_aug\", image_out=\"x_aug\"), prob=0.5),         Sometimes(                 ColorJitter(inputs=\"x_aug\", outputs=\"x_aug\", brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),                 prob=0.8),         Sometimes(ToGray(inputs=\"x_aug\", outputs=\"x_aug\"), prob=0.2),         Sometimes(GaussianBlur(inputs=\"x_aug\", outputs=\"x_aug\", blur_limit=(3, 3), sigma_limit=(0.1, 2.0)), prob=0.5),         ToFloat(inputs=\"x_aug\", outputs=\"x_aug\"),                  # augmentation 2         RandomCrop(32, 32, image_in=\"x\", image_out=\"x_aug2\"),         Sometimes(HorizontalFlip(image_in=\"x_aug2\", image_out=\"x_aug2\"), prob=0.5),         Sometimes(                 ColorJitter(inputs=\"x_aug2\", outputs=\"x_aug2\", brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2),                 prob=0.8),         Sometimes(ToGray(inputs=\"x_aug2\", outputs=\"x_aug2\"), prob=0.2),         Sometimes(GaussianBlur(inputs=\"x_aug2\", outputs=\"x_aug2\", blur_limit=(3, 3), sigma_limit=(0.1, 2.0)), prob=0.5),         ToFloat(inputs=\"x_aug2\", outputs=\"x_aug2\")     ]) In\u00a0[5]: Copied! <pre>def ResNet9(input_size=(32, 32, 3), dims=128, classes=10):\n\"\"\"A small 9-layer ResNet Tensorflow model for cifar10 image classification.\n    The model architecture is from https://github.com/davidcpage/cifar10-fast\n\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n        classes: The number of outputs the model should generate.\n\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[0] or `input_size`[1] is not a multiple of 16.\n\n    Returns:\n        A TensorFlow ResNet9 model.\n    \"\"\"\n\n    # prep layers\n    inp = layers.Input(shape=input_size)\n    x = layers.Conv2D(64, 3, padding='same')(inp)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    # layer1\n    x = layers.Conv2D(128, 3, padding='same')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Add()([x, residual(x, 128)])\n    # layer2\n    x = layers.Conv2D(256, 3, padding='same')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    # layer3\n    x = layers.Conv2D(512, 3, padding='same')(x)\n    x = layers.MaxPool2D()(x)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Add()([x, residual(x, 512)])\n    # layers4\n    x = layers.GlobalMaxPool2D()(x)\n    code = layers.Flatten()(x)\n\n    p_head = layers.Dense(dims)(code)\n    model_con = tf.keras.Model(inputs=inp, outputs=p_head)\n\n    s_head = layers.Dense(classes)(code)\n    s_head = layers.Activation('softmax', dtype='float32')(s_head)\n    model_finetune = tf.keras.Model(inputs=inp, outputs=s_head)\n\n    return model_con, model_finetune\n\n\ndef residual(x, num_channel: int):\n\"\"\"A ResNet unit for ResNet9.\n\n    Args:\n        x: Input Keras tensor.\n        num_channel: The number of layer channel.\n\n    Return:\n        Output Keras tensor.\n    \"\"\"\n    x = layers.Conv2D(num_channel, 3, padding='same')(x)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    x = layers.Conv2D(num_channel, 3, padding='same')(x)\n    x = layers.BatchNormalization(momentum=0.8)(x)\n    x = layers.LeakyReLU(alpha=0.1)(x)\n    return x\n\n\nmodel_con, model_finetune = fe.build(model_fn=ResNet9, optimizer_fn=[\"adam\", \"adam\"])\n</pre> def ResNet9(input_size=(32, 32, 3), dims=128, classes=10):     \"\"\"A small 9-layer ResNet Tensorflow model for cifar10 image classification.     The model architecture is from https://github.com/davidcpage/cifar10-fast      Args:         input_size: The size of the input tensor (height, width, channels).         classes: The number of outputs the model should generate.      Raises:         ValueError: Length of `input_size` is not 3.         ValueError: `input_size`[0] or `input_size`[1] is not a multiple of 16.      Returns:         A TensorFlow ResNet9 model.     \"\"\"      # prep layers     inp = layers.Input(shape=input_size)     x = layers.Conv2D(64, 3, padding='same')(inp)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     # layer1     x = layers.Conv2D(128, 3, padding='same')(x)     x = layers.MaxPool2D()(x)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     x = layers.Add()([x, residual(x, 128)])     # layer2     x = layers.Conv2D(256, 3, padding='same')(x)     x = layers.MaxPool2D()(x)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     # layer3     x = layers.Conv2D(512, 3, padding='same')(x)     x = layers.MaxPool2D()(x)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     x = layers.Add()([x, residual(x, 512)])     # layers4     x = layers.GlobalMaxPool2D()(x)     code = layers.Flatten()(x)      p_head = layers.Dense(dims)(code)     model_con = tf.keras.Model(inputs=inp, outputs=p_head)      s_head = layers.Dense(classes)(code)     s_head = layers.Activation('softmax', dtype='float32')(s_head)     model_finetune = tf.keras.Model(inputs=inp, outputs=s_head)      return model_con, model_finetune   def residual(x, num_channel: int):     \"\"\"A ResNet unit for ResNet9.      Args:         x: Input Keras tensor.         num_channel: The number of layer channel.      Return:         Output Keras tensor.     \"\"\"     x = layers.Conv2D(num_channel, 3, padding='same')(x)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     x = layers.Conv2D(num_channel, 3, padding='same')(x)     x = layers.BatchNormalization(momentum=0.8)(x)     x = layers.LeakyReLU(alpha=0.1)(x)     return x   model_con, model_finetune = fe.build(model_fn=ResNet9, optimizer_fn=[\"adam\", \"adam\"]) In\u00a0[6]: Copied! <pre>class NTXentOp(TensorOp):\n    def __init__(self, arg1, arg2, outputs, temperature=1.0, mode=None):\n        super().__init__(inputs=(arg1, arg2), outputs=outputs, mode=mode)\n        self.temperature = temperature\n\n    def forward(self, data, state):\n        arg1, arg2 = data\n        loss = NTXent(arg1, arg2, self.temperature)\n        return loss\n\n\ndef NTXent(A, B, temperature):\n    large_number = 1e9\n    batch_size = tf.shape(A)[0]\n    A = tf.math.l2_normalize(A, -1)\n    B = tf.math.l2_normalize(B, -1)\n\n    mask = tf.one_hot(tf.range(batch_size), batch_size)\n    labels = tf.one_hot(tf.range(batch_size), 2 * batch_size)\n\n    aa = tf.matmul(A, A, transpose_b=True) / temperature\n    aa = aa - mask * large_number\n    ab = tf.matmul(A, B, transpose_b=True) / temperature\n    bb = tf.matmul(B, B, transpose_b=True) / temperature\n    bb = bb - mask * large_number\n    ba = tf.matmul(B, A, transpose_b=True) / temperature\n    loss_a = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([ab, aa], 1))\n    loss_b = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([ba, bb], 1))\n    loss = tf.reduce_mean(loss_a + loss_b)\n\n    return loss, ab, labels\n\n\nnetwork_pretrain = fe.Network(ops=[\n    LambdaOp(lambda x,y: tf.concat([x, y], axis=0), inputs=[\"x_aug\", \"x_aug2\"], outputs=\"x_com\"),\n    ModelOp(model=model_con, inputs=\"x_com\", outputs=\"y_com\"),\n    LambdaOp(lambda x: tf.split(x, 2, axis=0), inputs=\"y_com\", outputs=[\"y_pred\", \"y_pred2\"]),\n    NTXentOp(arg1=\"y_pred\", arg2=\"y_pred2\", outputs=[\"NTXent\", \"logit\", \"label\"]),\n    UpdateOp(model=model_con, loss_name=\"NTXent\")\n])\n</pre> class NTXentOp(TensorOp):     def __init__(self, arg1, arg2, outputs, temperature=1.0, mode=None):         super().__init__(inputs=(arg1, arg2), outputs=outputs, mode=mode)         self.temperature = temperature      def forward(self, data, state):         arg1, arg2 = data         loss = NTXent(arg1, arg2, self.temperature)         return loss   def NTXent(A, B, temperature):     large_number = 1e9     batch_size = tf.shape(A)[0]     A = tf.math.l2_normalize(A, -1)     B = tf.math.l2_normalize(B, -1)      mask = tf.one_hot(tf.range(batch_size), batch_size)     labels = tf.one_hot(tf.range(batch_size), 2 * batch_size)      aa = tf.matmul(A, A, transpose_b=True) / temperature     aa = aa - mask * large_number     ab = tf.matmul(A, B, transpose_b=True) / temperature     bb = tf.matmul(B, B, transpose_b=True) / temperature     bb = bb - mask * large_number     ba = tf.matmul(B, A, transpose_b=True) / temperature     loss_a = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([ab, aa], 1))     loss_b = tf.nn.softmax_cross_entropy_with_logits(labels, tf.concat([ba, bb], 1))     loss = tf.reduce_mean(loss_a + loss_b)      return loss, ab, labels   network_pretrain = fe.Network(ops=[     LambdaOp(lambda x,y: tf.concat([x, y], axis=0), inputs=[\"x_aug\", \"x_aug2\"], outputs=\"x_com\"),     ModelOp(model=model_con, inputs=\"x_com\", outputs=\"y_com\"),     LambdaOp(lambda x: tf.split(x, 2, axis=0), inputs=\"y_com\", outputs=[\"y_pred\", \"y_pred2\"]),     NTXentOp(arg1=\"y_pred\", arg2=\"y_pred2\", outputs=[\"NTXent\", \"logit\", \"label\"]),     UpdateOp(model=model_con, loss_name=\"NTXent\") ]) In\u00a0[7]: Copied! <pre>traces = [\n    Accuracy(true_key=\"label\", pred_key=\"logit\", mode=\"train\", output_name=\"contrastive_accuracy\"),\n    ModelSaver(model=model_con, save_dir=save_dir)\n]\n\nestimator_pretrain = fe.Estimator(pipeline=pipeline_pretrain,\n                                  network=network_pretrain,\n                                  epochs=epochs_pretrain,\n                                  traces=traces,\n                                  train_steps_per_epoch=train_steps_per_epoch)\nestimator_pretrain.fit()\n</pre> traces = [     Accuracy(true_key=\"label\", pred_key=\"logit\", mode=\"train\", output_name=\"contrastive_accuracy\"),     ModelSaver(model=model_con, save_dir=save_dir) ]  estimator_pretrain = fe.Estimator(pipeline=pipeline_pretrain,                                   network=network_pretrain,                                   epochs=epochs_pretrain,                                   traces=traces,                                   train_steps_per_epoch=train_steps_per_epoch) estimator_pretrain.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; NTXent: 13.829769;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_1.h5\nFastEstimator-Train: step: 98; epoch: 1; contrastive_accuracy: 0.191; epoch_time: 26.89 sec;\nFastEstimator-Train: step: 100; NTXent: 12.382189; steps/sec: 4.52;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_2.h5\nFastEstimator-Train: step: 196; epoch: 2; contrastive_accuracy: 0.5078; epoch_time: 20.97 sec;\nFastEstimator-Train: step: 200; NTXent: 12.23844; steps/sec: 4.67;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_3.h5\nFastEstimator-Train: step: 294; epoch: 3; contrastive_accuracy: 0.7107; epoch_time: 20.46 sec;\nFastEstimator-Train: step: 300; NTXent: 12.149595; steps/sec: 4.79;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_4.h5\nFastEstimator-Train: step: 392; epoch: 4; contrastive_accuracy: 0.8136; epoch_time: 21.08 sec;\nFastEstimator-Train: step: 400; NTXent: 12.122614; steps/sec: 4.64;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_5.h5\nFastEstimator-Train: step: 490; epoch: 5; contrastive_accuracy: 0.87096; epoch_time: 20.76 sec;\nFastEstimator-Train: step: 500; NTXent: 12.102264; steps/sec: 4.75;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_6.h5\nFastEstimator-Train: step: 588; epoch: 6; contrastive_accuracy: 0.91048; epoch_time: 20.02 sec;\nFastEstimator-Train: step: 600; NTXent: 12.082749; steps/sec: 4.88;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_7.h5\nFastEstimator-Train: step: 686; epoch: 7; contrastive_accuracy: 0.93192; epoch_time: 20.53 sec;\nFastEstimator-Train: step: 700; NTXent: 12.061149; steps/sec: 4.8;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_8.h5\nFastEstimator-Train: step: 784; epoch: 8; contrastive_accuracy: 0.9465; epoch_time: 19.97 sec;\nFastEstimator-Train: step: 800; NTXent: 12.045004; steps/sec: 4.86;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_9.h5\nFastEstimator-Train: step: 882; epoch: 9; contrastive_accuracy: 0.96042; epoch_time: 20.63 sec;\nFastEstimator-Train: step: 900; NTXent: 12.045875; steps/sec: 4.77;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_10.h5\nFastEstimator-Train: step: 980; epoch: 10; contrastive_accuracy: 0.96834; epoch_time: 20.79 sec;\nFastEstimator-Train: step: 1000; NTXent: 12.04475; steps/sec: 4.71;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_11.h5\nFastEstimator-Train: step: 1078; epoch: 11; contrastive_accuracy: 0.97196; epoch_time: 20.5 sec;\nFastEstimator-Train: step: 1100; NTXent: 12.030878; steps/sec: 4.78;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_12.h5\nFastEstimator-Train: step: 1176; epoch: 12; contrastive_accuracy: 0.97612; epoch_time: 20.54 sec;\nFastEstimator-Train: step: 1200; NTXent: 12.024199; steps/sec: 4.78;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_13.h5\nFastEstimator-Train: step: 1274; epoch: 13; contrastive_accuracy: 0.97904; epoch_time: 20.43 sec;\nFastEstimator-Train: step: 1300; NTXent: 12.031016; steps/sec: 4.78;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_14.h5\nFastEstimator-Train: step: 1372; epoch: 14; contrastive_accuracy: 0.97972; epoch_time: 20.6 sec;\nFastEstimator-Train: step: 1400; NTXent: 12.023199; steps/sec: 4.75;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_15.h5\nFastEstimator-Train: step: 1470; epoch: 15; contrastive_accuracy: 0.98242; epoch_time: 21.16 sec;\nFastEstimator-Train: step: 1500; NTXent: 12.017285; steps/sec: 4.63;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_16.h5\nFastEstimator-Train: step: 1568; epoch: 16; contrastive_accuracy: 0.98304; epoch_time: 20.61 sec;\nFastEstimator-Train: step: 1600; NTXent: 12.014565; steps/sec: 4.74;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_17.h5\nFastEstimator-Train: step: 1666; epoch: 17; contrastive_accuracy: 0.98406; epoch_time: 20.65 sec;\nFastEstimator-Train: step: 1700; NTXent: 12.009141; steps/sec: 4.83;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_18.h5\nFastEstimator-Train: step: 1764; epoch: 18; contrastive_accuracy: 0.9849; epoch_time: 19.89 sec;\nFastEstimator-Train: step: 1800; NTXent: 12.001139; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_19.h5\nFastEstimator-Train: step: 1862; epoch: 19; contrastive_accuracy: 0.98566; epoch_time: 19.94 sec;\nFastEstimator-Train: step: 1900; NTXent: 12.003536; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_20.h5\nFastEstimator-Train: step: 1960; epoch: 20; contrastive_accuracy: 0.98588; epoch_time: 19.95 sec;\nFastEstimator-Train: step: 2000; NTXent: 11.999655; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_21.h5\nFastEstimator-Train: step: 2058; epoch: 21; contrastive_accuracy: 0.98662; epoch_time: 19.97 sec;\nFastEstimator-Train: step: 2100; NTXent: 11.992434; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_22.h5\nFastEstimator-Train: step: 2156; epoch: 22; contrastive_accuracy: 0.98726; epoch_time: 19.96 sec;\nFastEstimator-Train: step: 2200; NTXent: 11.998148; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_23.h5\nFastEstimator-Train: step: 2254; epoch: 23; contrastive_accuracy: 0.988; epoch_time: 19.93 sec;\nFastEstimator-Train: step: 2300; NTXent: 12.004744; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_24.h5\nFastEstimator-Train: step: 2352; epoch: 24; contrastive_accuracy: 0.98886; epoch_time: 19.88 sec;\nFastEstimator-Train: step: 2400; NTXent: 11.997645; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_25.h5\nFastEstimator-Train: step: 2450; epoch: 25; contrastive_accuracy: 0.98946; epoch_time: 19.91 sec;\nFastEstimator-Train: step: 2500; NTXent: 11.9974985; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_26.h5\nFastEstimator-Train: step: 2548; epoch: 26; contrastive_accuracy: 0.98968; epoch_time: 19.96 sec;\nFastEstimator-Train: step: 2600; NTXent: 11.988766; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_27.h5\nFastEstimator-Train: step: 2646; epoch: 27; contrastive_accuracy: 0.98896; epoch_time: 19.91 sec;\nFastEstimator-Train: step: 2700; NTXent: 11.992538; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_28.h5\nFastEstimator-Train: step: 2744; epoch: 28; contrastive_accuracy: 0.98936; epoch_time: 19.95 sec;\nFastEstimator-Train: step: 2800; NTXent: 11.984715; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_29.h5\nFastEstimator-Train: step: 2842; epoch: 29; contrastive_accuracy: 0.99066; epoch_time: 19.95 sec;\nFastEstimator-Train: step: 2900; NTXent: 11.989294; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_30.h5\nFastEstimator-Train: step: 2940; epoch: 30; contrastive_accuracy: 0.99126; epoch_time: 19.89 sec;\nFastEstimator-Train: step: 3000; NTXent: 11.980862; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_31.h5\nFastEstimator-Train: step: 3038; epoch: 31; contrastive_accuracy: 0.99174; epoch_time: 19.9 sec;\nFastEstimator-Train: step: 3100; NTXent: 11.988753; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_32.h5\nFastEstimator-Train: step: 3136; epoch: 32; contrastive_accuracy: 0.99158; epoch_time: 19.94 sec;\nFastEstimator-Train: step: 3200; NTXent: 11.982931; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_33.h5\nFastEstimator-Train: step: 3234; epoch: 33; contrastive_accuracy: 0.99192; epoch_time: 19.91 sec;\nFastEstimator-Train: step: 3300; NTXent: 11.983704; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_34.h5\nFastEstimator-Train: step: 3332; epoch: 34; contrastive_accuracy: 0.99288; epoch_time: 19.94 sec;\nFastEstimator-Train: step: 3400; NTXent: 11.982264; steps/sec: 4.9;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_35.h5\nFastEstimator-Train: step: 3430; epoch: 35; contrastive_accuracy: 0.99274; epoch_time: 19.95 sec;\nFastEstimator-Train: step: 3500; NTXent: 11.976917; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_36.h5\nFastEstimator-Train: step: 3528; epoch: 36; contrastive_accuracy: 0.99184; epoch_time: 19.97 sec;\nFastEstimator-Train: step: 3600; NTXent: 11.985281; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_37.h5\nFastEstimator-Train: step: 3626; epoch: 37; contrastive_accuracy: 0.99332; epoch_time: 19.97 sec;\nFastEstimator-Train: step: 3700; NTXent: 11.973089; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_38.h5\nFastEstimator-Train: step: 3724; epoch: 38; contrastive_accuracy: 0.99292; epoch_time: 19.97 sec;\nFastEstimator-Train: step: 3800; NTXent: 11.979197; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_39.h5\nFastEstimator-Train: step: 3822; epoch: 39; contrastive_accuracy: 0.99344; epoch_time: 19.92 sec;\nFastEstimator-Train: step: 3900; NTXent: 11.972714; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_40.h5\nFastEstimator-Train: step: 3920; epoch: 40; contrastive_accuracy: 0.99342; epoch_time: 19.93 sec;\nFastEstimator-Train: step: 4000; NTXent: 11.978983; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_41.h5\nFastEstimator-Train: step: 4018; epoch: 41; contrastive_accuracy: 0.99362; epoch_time: 19.92 sec;\nFastEstimator-Train: step: 4100; NTXent: 11.970781; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_42.h5\nFastEstimator-Train: step: 4116; epoch: 42; contrastive_accuracy: 0.9934; epoch_time: 19.9 sec;\nFastEstimator-Train: step: 4200; NTXent: 11.967566; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_43.h5\nFastEstimator-Train: step: 4214; epoch: 43; contrastive_accuracy: 0.99374; epoch_time: 19.93 sec;\nFastEstimator-Train: step: 4300; NTXent: 11.967752; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_44.h5\nFastEstimator-Train: step: 4312; epoch: 44; contrastive_accuracy: 0.99356; epoch_time: 19.96 sec;\nFastEstimator-Train: step: 4400; NTXent: 11.965156; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_45.h5\nFastEstimator-Train: step: 4410; epoch: 45; contrastive_accuracy: 0.99422; epoch_time: 19.88 sec;\nFastEstimator-Train: step: 4500; NTXent: 11.964204; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_46.h5\nFastEstimator-Train: step: 4508; epoch: 46; contrastive_accuracy: 0.9936; epoch_time: 19.92 sec;\nFastEstimator-Train: step: 4600; NTXent: 11.970972; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_47.h5\nFastEstimator-Train: step: 4606; epoch: 47; contrastive_accuracy: 0.99444; epoch_time: 19.9 sec;\nFastEstimator-Train: step: 4700; NTXent: 11.972122; steps/sec: 4.92;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_48.h5\nFastEstimator-Train: step: 4704; epoch: 48; contrastive_accuracy: 0.9936; epoch_time: 19.93 sec;\nFastEstimator-Train: step: 4800; NTXent: 11.961956; steps/sec: 4.93;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_49.h5\nFastEstimator-Train: step: 4802; epoch: 49; contrastive_accuracy: 0.99414; epoch_time: 19.89 sec;\nFastEstimator-Train: step: 4900; NTXent: 11.124333; steps/sec: 4.91;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp33wb3rot/model_epoch_50.h5\nFastEstimator-Train: step: 4900; epoch: 50; contrastive_accuracy: 0.99446; epoch_time: 19.94 sec;\nFastEstimator-Finish: step: 4900; model_lr: 0.001; total_time: 1055.67 sec;\n</pre> In\u00a0[8]: Copied! <pre>split_train = train_data.split(0.1)\n\npipeline_finetune = fe.Pipeline(\n    train_data=split_train,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        ToFloat(inputs=\"x\", outputs=\"x\")\n    ])\n\nnetwork_finetune = fe.Network(ops=[\n    ModelOp(model=model_finetune, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=[\"y_pred\", \"y\"], outputs=\"ce\"),\n    UpdateOp(model=model_finetune, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model_finetune, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")\n]\n\nest_finetune = fe.Estimator(pipeline=pipeline_finetune,\n                            network=network_finetune,\n                            epochs=epochs_finetune,\n                            traces=traces,\n                            train_steps_per_epoch=train_steps_per_epoch)\nest_finetune.fit()\n</pre> split_train = train_data.split(0.1)  pipeline_finetune = fe.Pipeline(     train_data=split_train,     eval_data=eval_data,     batch_size=batch_size,     ops=[         ToFloat(inputs=\"x\", outputs=\"x\")     ])  network_finetune = fe.Network(ops=[     ModelOp(model=model_finetune, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=[\"y_pred\", \"y\"], outputs=\"ce\"),     UpdateOp(model=model_finetune, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model_finetune, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\") ]  est_finetune = fe.Estimator(pipeline=pipeline_finetune,                             network=network_finetune,                             epochs=epochs_finetune,                             traces=traces,                             train_steps_per_epoch=train_steps_per_epoch) est_finetune.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; ce: 6.394948;\nFastEstimator-Train: step: 9; epoch: 1; epoch_time: 3.6 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 9; epoch: 1; accuracy: 0.4504; ce: 1.8657482; max_accuracy: 0.4504; since_best_accuracy: 0;\nFastEstimator-Train: step: 18; epoch: 2; epoch_time: 0.78 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 18; epoch: 2; accuracy: 0.5904; ce: 1.1950318; max_accuracy: 0.5904; since_best_accuracy: 0;\nFastEstimator-Train: step: 27; epoch: 3; epoch_time: 0.78 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 27; epoch: 3; accuracy: 0.6299; ce: 1.0542316; max_accuracy: 0.6299; since_best_accuracy: 0;\nFastEstimator-Train: step: 36; epoch: 4; epoch_time: 0.8 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 36; epoch: 4; accuracy: 0.6627; ce: 0.99589044; max_accuracy: 0.6627; since_best_accuracy: 0;\nFastEstimator-Train: step: 45; epoch: 5; epoch_time: 0.79 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 45; epoch: 5; accuracy: 0.667; ce: 0.95694005; max_accuracy: 0.667; since_best_accuracy: 0;\nFastEstimator-Train: step: 54; epoch: 6; epoch_time: 0.78 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 54; epoch: 6; accuracy: 0.6795; ce: 0.9522039; max_accuracy: 0.6795; since_best_accuracy: 0;\nFastEstimator-Train: step: 63; epoch: 7; epoch_time: 0.78 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 63; epoch: 7; accuracy: 0.6907; ce: 0.9476255; max_accuracy: 0.6907; since_best_accuracy: 0;\nFastEstimator-Train: step: 72; epoch: 8; epoch_time: 0.79 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 72; epoch: 8; accuracy: 0.698; ce: 0.9569526; max_accuracy: 0.698; since_best_accuracy: 0;\nFastEstimator-Train: step: 81; epoch: 9; epoch_time: 0.81 sec;\nFastEstimator-Eval: step: 81; epoch: 9; accuracy: 0.6973; ce: 0.96826965; max_accuracy: 0.698; since_best_accuracy: 1;\nFastEstimator-Train: step: 90; epoch: 10; epoch_time: 0.82 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp33wb3rot/model1_best_accuracy.h5\nFastEstimator-Eval: step: 90; epoch: 10; accuracy: 0.7009; ce: 0.98749244; max_accuracy: 0.7009; since_best_accuracy: 0;\nFastEstimator-Finish: step: 90; model1_lr: 0.001; total_time: 27.06 sec;\n</pre>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#simclr-on-cifair10-image-classification-tensorflow-backend", "title": "SimCLR on CIFAIR10 Image Classification (Tensorflow Backend)\u00b6", "text": "<p>Labeled datasets are much more expensive than their unlabeled counterparts. It is thus often the case that only a small fraction of total available data can be labeled. Therefore, self-supervised learning algorithms, which don't require labeled data during training, have become a huge topic in ML research recently. In 2020 SimCLR was proposed and achieved 85.8% top-5 accuracy using only 1% of the available labels on the ImageNet dataset.</p> <p>The idea of SimCLR is to separate visual tasks into two parts: an encoder and a classifier. The encoder projects images to a representation space which is then used by the classifier to make decisions. The encoder doesn't need to know the image class, but it does need to project an \"image group\" (a group of images generated from the same image with data augmentation) to a cluster. By increasing the similarity of encoded images from the same image groups while reducing similarity between different groups, the encoder can be trained without explicit labels. The process of training the encoder is called \"pretraining\". Later, users can attach any classifier after the pretrained encoder and finetune the whole model for specific visual tasks. According to the paper, this can achieve good results with only a small fraction of the available data being labeled.</p> <p>In this tutorial we will demonstrate the implementation of SimCLR with the ciFAIR10 dataset. Some details of this implementation will be different from the original paper. This implementation draws upon the code provided here.</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#pre-training-pipeline", "title": "Pre-Training Pipeline\u00b6", "text": "<p>In the SimCLR paper they emphasized the importance of data augmentation steps and how these can directly impact the quality of the pretrained model. The preprocessing steps include: random cropping, random color jitter, and random Gaussian blur. An image will go through the pipeline and generate two augmented images which constitute an image group (or pair to be more specific). The batch of augmented image pairs will later be used for model pretraining.</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#model", "title": "Model\u00b6", "text": "<p>During SimCLR contrastive learning, the training can be separated into two parts: pretraining and finetuning. In the pretraining step, the encoder is attached to a series of MLPs called the \"projection head\". During finetuning, the encoder is attached to a classifier called the \"supervision head\". The paper claimed that using the projection head can help make data more clustered in the representation space.</p> <p>Although in the original paper they used a ResNet50 model architecture, we will use ResNet9 for faster convergence.</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#pre-training-network", "title": "Pre-Training Network\u00b6", "text": "<p>SimCLR uses NT-Xent (the normalized temperature-scaled cross entropy loss) to train the encoder. By reducing the loss it will increase the similarity of positive augemented pairs and decrease the similarity of negative pairs as the following GIF demonstrates. For a detailed formula, please refer to the orginal paper.  (source: https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html)</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#pre-training-estimator", "title": "Pre-Training Estimator\u00b6", "text": "<p>Next we are going to combine the pretraining pipeline and network together in the estimator class with an <code>Accuracy</code> trace to monitor the contrastive accuracy and a <code>ModelSaver</code> trace to save the pretrained model. We can then start the training.</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#finetune-the-model-on-an-image-classification-task", "title": "Finetune the model on an image classification task\u00b6", "text": "<p>Once the model is pretrained, we can finetune the model on a specific task. In this case we are going to use this pretrained model on ciFAIR10 image classification. Remember in the previous section we built both <code>model_con</code> and <code>model_finetune</code>. Because those two models share the same encoder object, by (pre)training the <code>model_con</code>, the encoder of <code>model_fintune</code> is also trained. The finetuing of the model is literally just supervised training with the pretrained encoder. In order to demonstrate the benefit of SimCLR, we are going to fine-tune the network using only 10% of the labeled training data and compare with how well a model could do trained from scratch with the same data limitation.</p>"}, {"location": "apphub/contrastive_learning/simclr/simclr.html#results", "title": "Results\u00b6", "text": "<p>We can see that SimCLR achieved 70% accuracy using only 10% of the labeled data. With the same configuration, a vanilla ResNet9 can only achieve around 57%.</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html", "title": "Curriculum Learning with SuperLoss (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import math\nimport tempfile\n\nimport numpy as np\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, MaxPooling2D\nfrom tensorflow.keras.models import Sequential\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import cifair100\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy, SuperLoss\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import MCC\nfrom fastestimator.trace.xai import LabelTracker\n</pre> import math import tempfile  import numpy as np from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Flatten, MaxPooling2D from tensorflow.keras.models import Sequential  import fastestimator as fe from fastestimator.dataset.data import cifair100 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy, SuperLoss from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import MCC from fastestimator.trace.xai import LabelTracker In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 50\nbatch_size = 128\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 50 batch_size = 128 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifair100\n\ntrain_data, eval_data = cifair100.load_data()\ntest_data = eval_data.split(0.5)\n\ndef corrupt_dataset(dataset, n_classes=100, corruption_fraction=0.4):\n    # Keep track of which samples were corrupted for visualization later\n    corrupted = [0 for _ in range(len(dataset))]\n    # Perform the actual label corruption\n    n_samples_per_class = len(dataset) // n_classes\n    n_to_corrupt_per_class = math.floor(corruption_fraction * n_samples_per_class)\n    n_corrupted = [0] * n_classes\n    i = 0\n    while any([elem &lt; n_to_corrupt_per_class for elem in n_corrupted]):\n        current_class = dataset[i]['y'].item()\n        if n_corrupted[current_class] &lt; n_to_corrupt_per_class:\n            dataset[i]['y'] = (dataset[i]['y'] + np.random.randint(1, n_classes)) % n_classes\n            n_corrupted[current_class] += 1\n            corrupted[i] = 1\n        i += 1\n    # Put the corruption labels into the dataset for visualization\n    dataset['data_labels'] = np.array(corrupted, dtype=int).reshape((len(dataset), 1))\n\ncorrupt_dataset(train_data)\n</pre> from fastestimator.dataset.data import cifair100  train_data, eval_data = cifair100.load_data() test_data = eval_data.split(0.5)  def corrupt_dataset(dataset, n_classes=100, corruption_fraction=0.4):     # Keep track of which samples were corrupted for visualization later     corrupted = [0 for _ in range(len(dataset))]     # Perform the actual label corruption     n_samples_per_class = len(dataset) // n_classes     n_to_corrupt_per_class = math.floor(corruption_fraction * n_samples_per_class)     n_corrupted = [0] * n_classes     i = 0     while any([elem &lt; n_to_corrupt_per_class for elem in n_corrupted]):         current_class = dataset[i]['y'].item()         if n_corrupted[current_class] &lt; n_to_corrupt_per_class:             dataset[i]['y'] = (dataset[i]['y'] + np.random.randint(1, n_classes)) % n_classes             n_corrupted[current_class] += 1             corrupted[i] = 1         i += 1     # Put the corruption labels into the dataset for visualization     dataset['data_labels'] = np.array(corrupted, dtype=int).reshape((len(dataset), 1))  corrupt_dataset(train_data) In\u00a0[4]: Copied! <pre>def big_lenet(classes=100, input_shape=(32, 32, 3)):\n    # Like a LeNet model, but bigger. \n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='swish', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='swish'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='swish'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='swish'))\n    model.add(BatchNormalization())\n    model.add(Dense(classes, activation='softmax'))\n    return model\n\ndef build_estimator(loss_op):\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=batch_size,\n                           ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n                                PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n                                RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n                                Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n                                CoarseDropout(inputs=\"x\", outputs=\"x\", max_holes=1, mode=\"train\"),\n                                ])\n    model = fe.build(model_fn=big_lenet, optimizer_fn='adam')\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        loss_op,  # &lt;&lt;&lt;----------------------------- This is where the secret sauce will go\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    traces = [\n        MCC(true_key=\"y\", pred_key=\"y_pred\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),\n        # We will also visualize the difference between the normal and corrupted image confidence scores. You could follow this with an\n        # ImageViewer trace, but we will get the data out of the system summary instead later for viewing.\n        LabelTracker(metric=\"confidence\", label=\"data_labels\", label_mapping={\"Normal\": 0, \"Corrupted\": 1}, mode=\"train\", outputs=\"label_confidence\"),\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch,\n                             log_steps=300)\n    return estimator\n</pre> def big_lenet(classes=100, input_shape=(32, 32, 3)):     # Like a LeNet model, but bigger.      model = Sequential()     model.add(Conv2D(32, (3, 3), activation='swish', input_shape=input_shape))     model.add(BatchNormalization())     model.add(MaxPooling2D((2, 2)))     model.add(Conv2D(64, (3, 3), activation='swish'))     model.add(BatchNormalization())     model.add(MaxPooling2D((2, 2)))     model.add(Conv2D(128, (3, 3), activation='swish'))     model.add(BatchNormalization())     model.add(MaxPooling2D((2, 2)))     model.add(Flatten())     model.add(Dense(128, activation='swish'))     model.add(BatchNormalization())     model.add(Dense(classes, activation='softmax'))     return model  def build_estimator(loss_op):     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=batch_size,                            ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),                                 PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),                                 RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),                                 Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),                                 CoarseDropout(inputs=\"x\", outputs=\"x\", max_holes=1, mode=\"train\"),                                 ])     model = fe.build(model_fn=big_lenet, optimizer_fn='adam')     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         loss_op,  # &lt;&lt;&lt;----------------------------- This is where the secret sauce will go         UpdateOp(model=model, loss_name=\"ce\")     ])     traces = [         MCC(true_key=\"y\", pred_key=\"y_pred\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),         # We will also visualize the difference between the normal and corrupted image confidence scores. You could follow this with an         # ImageViewer trace, but we will get the data out of the system summary instead later for viewing.         LabelTracker(metric=\"confidence\", label=\"data_labels\", label_mapping={\"Normal\": 0, \"Corrupted\": 1}, mode=\"train\", outputs=\"label_confidence\"),     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch,                              log_steps=300)     return estimator In\u00a0[5]: Copied! <pre>class FakeSuperLoss(SuperLoss):\n    def forward(self, data, state):\n        superloss, confidence = super().forward(data, state)\n        regularloss = fe.backend.reduce_mean(self.loss.forward(data, state))\n        return [regularloss, confidence]\n\nloss = FakeSuperLoss(CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"), output_confidence=\"confidence\")\nestimator_regular = build_estimator(loss)\nregular = estimator_regular.fit(\"RegularLoss\")\n</pre> class FakeSuperLoss(SuperLoss):     def forward(self, data, state):         superloss, confidence = super().forward(data, state)         regularloss = fe.backend.reduce_mean(self.loss.forward(data, state))         return [regularloss, confidence]  loss = FakeSuperLoss(CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"), output_confidence=\"confidence\") estimator_regular = build_estimator(loss) regular = estimator_regular.fit(\"RegularLoss\") <pre>Metal device set to: Apple M1 Max\n</pre> <pre>2022-04-14 08:04:52.500693: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-04-14 08:04:52.500861: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)\n2022-04-14 08:04:52.968498: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Train: step: 1; ce: 5.0845623;\nFastEstimator-Train: step: 300; ce: 4.4716263; steps/sec: 37.57;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 15.59 sec;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 391; epoch: 1; ce: 3.9165292; max_mcc: 0.11637703081989584; mcc: 0.11637703081989584; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 4.143276; steps/sec: 26.6;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 11.49 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; ce: 3.4811482; max_mcc: 0.19463705463257847; mcc: 0.19463705463257847; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 4.118492; steps/sec: 33.88;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 12.09 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; ce: 3.2860541; max_mcc: 0.22608677082453127; mcc: 0.22608677082453127; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 3.9007201; steps/sec: 31.42;\nFastEstimator-Train: step: 1500; ce: 4.1510983; steps/sec: 36.11;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 11.54 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; ce: 3.2074647; max_mcc: 0.24498756546447986; mcc: 0.24498756546447986; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 3.9860764; steps/sec: 33.85;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 11.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; ce: 3.0909023; max_mcc: 0.2673344783186453; mcc: 0.2673344783186453; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 3.7885048; steps/sec: 32.45;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 11.26 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 2346; epoch: 6; ce: 3.0296268; max_mcc: 0.28257658668430546; mcc: 0.28257658668430546; since_best_mcc: 0;\nFastEstimator-Train: step: 2400; ce: 3.7728481; steps/sec: 34.22;\nFastEstimator-Train: step: 2700; ce: 3.9727435; steps/sec: 37.17;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 11.3 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; ce: 2.9461927; max_mcc: 0.3041788741339668; mcc: 0.3041788741339668; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 3.6648512; steps/sec: 33.29;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 11.41 sec;\nFastEstimator-Eval: step: 3128; epoch: 8; ce: 2.9334276; max_mcc: 0.3041788741339668; mcc: 0.3011113825398861; since_best_mcc: 1;\nFastEstimator-Train: step: 3300; ce: 3.9563556; steps/sec: 32.21;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 12.17 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 3519; epoch: 9; ce: 2.9065385; max_mcc: 0.30455805637425065; mcc: 0.30455805637425065; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: 3.9232101; steps/sec: 32.61;\nFastEstimator-Train: step: 3900; ce: 3.7361774; steps/sec: 36.74;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 11.33 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 3910; epoch: 10; ce: 2.8042293; max_mcc: 0.3314968221510782; mcc: 0.3314968221510782; since_best_mcc: 0;\nFastEstimator-Train: step: 4200; ce: 3.647646; steps/sec: 32.71;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 11.76 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 4301; epoch: 11; ce: 2.7552197; max_mcc: 0.3357567841576629; mcc: 0.3357567841576629; since_best_mcc: 0;\nFastEstimator-Train: step: 4500; ce: 3.4892626; steps/sec: 32.86;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 11.36 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 4692; epoch: 12; ce: 2.736461; max_mcc: 0.3510061360991489; mcc: 0.3510061360991489; since_best_mcc: 0;\nFastEstimator-Train: step: 4800; ce: 3.8051057; steps/sec: 34.65;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 11.26 sec;\nFastEstimator-Eval: step: 5083; epoch: 13; ce: 2.75954; max_mcc: 0.3510061360991489; mcc: 0.33251882862713855; since_best_mcc: 1;\nFastEstimator-Train: step: 5100; ce: 3.652556; steps/sec: 33.73;\nFastEstimator-Train: step: 5400; ce: 3.8262262; steps/sec: 37.64;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 11.13 sec;\nFastEstimator-Eval: step: 5474; epoch: 14; ce: 2.7278647; max_mcc: 0.3510061360991489; mcc: 0.3468396191191313; since_best_mcc: 2;\nFastEstimator-Train: step: 5700; ce: 3.76147; steps/sec: 33.38;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 11.52 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 5865; epoch: 15; ce: 2.698529; max_mcc: 0.35409691305967284; mcc: 0.35409691305967284; since_best_mcc: 0;\nFastEstimator-Train: step: 6000; ce: 3.7462912; steps/sec: 32.96;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 11.82 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 6256; epoch: 16; ce: 2.6763847; max_mcc: 0.36294897145164334; mcc: 0.36294897145164334; since_best_mcc: 0;\nFastEstimator-Train: step: 6300; ce: 3.6404994; steps/sec: 32.52;\nFastEstimator-Train: step: 6600; ce: 3.8307505; steps/sec: 35.45;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 11.86 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 6647; epoch: 17; ce: 2.6446831; max_mcc: 0.37233087727784103; mcc: 0.37233087727784103; since_best_mcc: 0;\nFastEstimator-Train: step: 6900; ce: 3.7684755; steps/sec: 32.51;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 11.69 sec;\nFastEstimator-Eval: step: 7038; epoch: 18; ce: 2.6735303; max_mcc: 0.37233087727784103; mcc: 0.3589859924598546; since_best_mcc: 1;\nFastEstimator-Train: step: 7200; ce: 3.9045172; steps/sec: 33.63;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 11.27 sec;\nFastEstimator-Eval: step: 7429; epoch: 19; ce: 2.677819; max_mcc: 0.37233087727784103; mcc: 0.3722223425197881; since_best_mcc: 2;\nFastEstimator-Train: step: 7500; ce: 3.513299; steps/sec: 32.48;\nFastEstimator-Train: step: 7800; ce: 3.6698349; steps/sec: 36.47;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 11.83 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 7820; epoch: 20; ce: 2.6317315; max_mcc: 0.38029253313466144; mcc: 0.38029253313466144; since_best_mcc: 0;\nFastEstimator-Train: step: 8100; ce: 3.586587; steps/sec: 33.85;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 11.52 sec;\nFastEstimator-Eval: step: 8211; epoch: 21; ce: 2.6565547; max_mcc: 0.38029253313466144; mcc: 0.36699826561040333; since_best_mcc: 1;\nFastEstimator-Train: step: 8400; ce: 3.7047858; steps/sec: 33.03;\nFastEstimator-Train: step: 8602; epoch: 22; epoch_time: 11.46 sec;\nFastEstimator-Eval: step: 8602; epoch: 22; ce: 2.6064296; max_mcc: 0.38029253313466144; mcc: 0.3740086404189197; since_best_mcc: 2;\nFastEstimator-Train: step: 8700; ce: 3.3841472; steps/sec: 32.07;\nFastEstimator-Train: step: 8993; epoch: 23; epoch_time: 12.04 sec;\nFastEstimator-Eval: step: 8993; epoch: 23; ce: 2.6085975; max_mcc: 0.38029253313466144; mcc: 0.36779678534767773; since_best_mcc: 3;\nFastEstimator-Train: step: 9000; ce: 3.6447806; steps/sec: 32.41;\nFastEstimator-Train: step: 9300; ce: 3.3836112; steps/sec: 37.36;\nFastEstimator-Train: step: 9384; epoch: 24; epoch_time: 11.19 sec;\nFastEstimator-Eval: step: 9384; epoch: 24; ce: 2.6242566; max_mcc: 0.38029253313466144; mcc: 0.37316928622026396; since_best_mcc: 4;\nFastEstimator-Train: step: 9600; ce: 3.5648074; steps/sec: 33.78;\nFastEstimator-Train: step: 9775; epoch: 25; epoch_time: 11.38 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 9775; epoch: 25; ce: 2.5628254; max_mcc: 0.3911783657392671; mcc: 0.3911783657392671; since_best_mcc: 0;\nFastEstimator-Train: step: 9900; ce: 3.4466887; steps/sec: 34.02;\nFastEstimator-Train: step: 10166; epoch: 26; epoch_time: 11.1 sec;\nFastEstimator-Eval: step: 10166; epoch: 26; ce: 2.5749164; max_mcc: 0.3911783657392671; mcc: 0.3824618003063329; since_best_mcc: 1;\nFastEstimator-Train: step: 10200; ce: 3.2873046; steps/sec: 34.09;\nFastEstimator-Train: step: 10500; ce: 3.365408; steps/sec: 37.09;\nFastEstimator-Train: step: 10557; epoch: 27; epoch_time: 11.45 sec;\nFastEstimator-Eval: step: 10557; epoch: 27; ce: 2.579395; max_mcc: 0.3911783657392671; mcc: 0.38492637219709647; since_best_mcc: 2;\nFastEstimator-Train: step: 10800; ce: 3.4366512; steps/sec: 32.48;\nFastEstimator-Train: step: 10948; epoch: 28; epoch_time: 11.89 sec;\nFastEstimator-Eval: step: 10948; epoch: 28; ce: 2.557443; max_mcc: 0.3911783657392671; mcc: 0.38992180085047434; since_best_mcc: 3;\nFastEstimator-Train: step: 11100; ce: 3.2275214; steps/sec: 32.12;\nFastEstimator-Train: step: 11339; epoch: 29; epoch_time: 11.59 sec;\nFastEstimator-Eval: step: 11339; epoch: 29; ce: 2.555817; max_mcc: 0.3911783657392671; mcc: 0.3834724048810534; since_best_mcc: 4;\nFastEstimator-Train: step: 11400; ce: 3.8311727; steps/sec: 33.09;\nFastEstimator-Train: step: 11700; ce: 3.437436; steps/sec: 36.89;\nFastEstimator-Train: step: 11730; epoch: 30; epoch_time: 11.57 sec;\nFastEstimator-Eval: step: 11730; epoch: 30; ce: 2.6084797; max_mcc: 0.3911783657392671; mcc: 0.38230879829021064; since_best_mcc: 5;\nFastEstimator-Train: step: 12000; ce: 3.2445602; steps/sec: 32.12;\nFastEstimator-Train: step: 12121; epoch: 31; epoch_time: 11.81 sec;\nFastEstimator-Eval: step: 12121; epoch: 31; ce: 2.5444205; max_mcc: 0.3911783657392671; mcc: 0.3903882701332637; since_best_mcc: 6;\nFastEstimator-Train: step: 12300; ce: 3.177442; steps/sec: 32.86;\nFastEstimator-Train: step: 12512; epoch: 32; epoch_time: 11.59 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 12512; epoch: 32; ce: 2.555135; max_mcc: 0.39336794619867993; mcc: 0.39336794619867993; since_best_mcc: 0;\nFastEstimator-Train: step: 12600; ce: 3.6959205; steps/sec: 32.51;\nFastEstimator-Train: step: 12900; ce: 3.7039824; steps/sec: 37.48;\nFastEstimator-Train: step: 12903; epoch: 33; epoch_time: 11.6 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 12903; epoch: 33; ce: 2.5638745; max_mcc: 0.3997956449593042; mcc: 0.3997956449593042; since_best_mcc: 0;\nFastEstimator-Train: step: 13200; ce: 3.453362; steps/sec: 31.32;\nFastEstimator-Train: step: 13294; epoch: 34; epoch_time: 12.01 sec;\nFastEstimator-Eval: step: 13294; epoch: 34; ce: 2.581514; max_mcc: 0.3997956449593042; mcc: 0.3840599318109791; since_best_mcc: 1;\nFastEstimator-Train: step: 13500; ce: 3.4583755; steps/sec: 32.65;\nFastEstimator-Train: step: 13685; epoch: 35; epoch_time: 11.63 sec;\nFastEstimator-Eval: step: 13685; epoch: 35; ce: 2.5393128; max_mcc: 0.3997956449593042; mcc: 0.39358100358310194; since_best_mcc: 2;\nFastEstimator-Train: step: 13800; ce: 3.3511138; steps/sec: 32.94;\nFastEstimator-Train: step: 14076; epoch: 36; epoch_time: 11.58 sec;\nFastEstimator-Eval: step: 14076; epoch: 36; ce: 2.5451062; max_mcc: 0.3997956449593042; mcc: 0.3909940656566139; since_best_mcc: 3;\nFastEstimator-Train: step: 14100; ce: 3.7378292; steps/sec: 32.85;\nFastEstimator-Train: step: 14400; ce: 3.579475; steps/sec: 35.62;\nFastEstimator-Train: step: 14467; epoch: 37; epoch_time: 11.87 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 14467; epoch: 37; ce: 2.4956336; max_mcc: 0.4018185133565588; mcc: 0.4018185133565588; since_best_mcc: 0;\nFastEstimator-Train: step: 14700; ce: 3.309771; steps/sec: 31.69;\nFastEstimator-Train: step: 14858; epoch: 38; epoch_time: 11.94 sec;\nFastEstimator-Eval: step: 14858; epoch: 38; ce: 2.5473502; max_mcc: 0.4018185133565588; mcc: 0.39070522900122134; since_best_mcc: 1;\nFastEstimator-Train: step: 15000; ce: 3.3463337; steps/sec: 31.3;\nFastEstimator-Train: step: 15249; epoch: 39; epoch_time: 12.21 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 15249; epoch: 39; ce: 2.5087152; max_mcc: 0.40282440628039246; mcc: 0.40282440628039246; since_best_mcc: 0;\nFastEstimator-Train: step: 15300; ce: 3.443113; steps/sec: 31.58;\nFastEstimator-Train: step: 15600; ce: 3.3580484; steps/sec: 36.07;\nFastEstimator-Train: step: 15640; epoch: 40; epoch_time: 12.05 sec;\nFastEstimator-Eval: step: 15640; epoch: 40; ce: 2.5329254; max_mcc: 0.40282440628039246; mcc: 0.4009165593189717; since_best_mcc: 1;\nFastEstimator-Train: step: 15900; ce: 3.464141; steps/sec: 32.94;\nFastEstimator-Train: step: 16031; epoch: 41; epoch_time: 11.49 sec;\nFastEstimator-Eval: step: 16031; epoch: 41; ce: 2.5511973; max_mcc: 0.40282440628039246; mcc: 0.38809165892740816; since_best_mcc: 2;\nFastEstimator-Train: step: 16200; ce: 3.129107; steps/sec: 33.28;\nFastEstimator-Train: step: 16422; epoch: 42; epoch_time: 11.4 sec;\nFastEstimator-Eval: step: 16422; epoch: 42; ce: 2.5401368; max_mcc: 0.40282440628039246; mcc: 0.4002505965558874; since_best_mcc: 3;\nFastEstimator-Train: step: 16500; ce: 3.323578; steps/sec: 33.3;\nFastEstimator-Train: step: 16800; ce: 3.6085215; steps/sec: 37.19;\nFastEstimator-Train: step: 16813; epoch: 43; epoch_time: 11.56 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 16813; epoch: 43; ce: 2.5123787; max_mcc: 0.403635709044651; mcc: 0.403635709044651; since_best_mcc: 0;\nFastEstimator-Train: step: 17100; ce: 3.4821346; steps/sec: 32.94;\nFastEstimator-Train: step: 17204; epoch: 44; epoch_time: 11.51 sec;\nFastEstimator-Eval: step: 17204; epoch: 44; ce: 2.5150542; max_mcc: 0.403635709044651; mcc: 0.3967572301803061; since_best_mcc: 1;\nFastEstimator-Train: step: 17400; ce: 3.422583; steps/sec: 32.87;\nFastEstimator-Train: step: 17595; epoch: 45; epoch_time: 11.48 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Eval: step: 17595; epoch: 45; ce: 2.5106926; max_mcc: 0.4070381050333296; mcc: 0.4070381050333296; since_best_mcc: 0;\nFastEstimator-Train: step: 17700; ce: 3.7156749; steps/sec: 33.6;\nFastEstimator-Train: step: 17986; epoch: 46; epoch_time: 11.4 sec;\nFastEstimator-Eval: step: 17986; epoch: 46; ce: 2.5253594; max_mcc: 0.4070381050333296; mcc: 0.399264786449083; since_best_mcc: 1;\nFastEstimator-Train: step: 18000; ce: 3.2805777; steps/sec: 33.8;\nFastEstimator-Train: step: 18300; ce: 3.4379218; steps/sec: 37.31;\nFastEstimator-Train: step: 18377; epoch: 47; epoch_time: 11.43 sec;\nFastEstimator-Eval: step: 18377; epoch: 47; ce: 2.5333138; max_mcc: 0.4070381050333296; mcc: 0.39595400336978015; since_best_mcc: 2;\nFastEstimator-Train: step: 18600; ce: 3.5096176; steps/sec: 32.47;\nFastEstimator-Train: step: 18768; epoch: 48; epoch_time: 11.75 sec;\nFastEstimator-Eval: step: 18768; epoch: 48; ce: 2.5043597; max_mcc: 0.4070381050333296; mcc: 0.39856495181853063; since_best_mcc: 3;\nFastEstimator-Train: step: 18900; ce: 3.3612788; steps/sec: 32.64;\nFastEstimator-Train: step: 19159; epoch: 49; epoch_time: 11.54 sec;\nFastEstimator-Eval: step: 19159; epoch: 49; ce: 2.5006938; max_mcc: 0.4070381050333296; mcc: 0.4048320575822205; since_best_mcc: 4;\nFastEstimator-Train: step: 19200; ce: 3.2576284; steps/sec: 32.78;\nFastEstimator-Train: step: 19500; ce: 3.5873413; steps/sec: 37.8;\nFastEstimator-Train: step: 19550; epoch: 50; epoch_time: 11.47 sec;\nFastEstimator-Eval: step: 19550; epoch: 50; ce: 2.511425; max_mcc: 0.4070381050333296; mcc: 0.3974247087308549; since_best_mcc: 5;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model_best_mcc.h5\nFastEstimator-Finish: step: 19550; model_lr: 0.001; total_time: 624.5 sec;\n</pre> In\u00a0[6]: Copied! <pre>loss = SuperLoss(CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"), output_confidence=\"confidence\")  # The output_confidence arg is only needed if you want to visualize\nestimator_super = build_estimator(loss)\nsuperL = estimator_super.fit(\"SuperLoss\")\n</pre> loss = SuperLoss(CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"), output_confidence=\"confidence\")  # The output_confidence arg is only needed if you want to visualize estimator_super = build_estimator(loss) superL = estimator_super.fit(\"SuperLoss\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Train: step: 1; ce: -0.45840234;\nFastEstimator-Train: step: 300; ce: -0.9035268; steps/sec: 26.71;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 16.66 sec;\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 391; epoch: 1; ce: -0.8853086; max_mcc: 0.14302881374073162; mcc: 0.14302881374073162; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: -1.1198719; steps/sec: 22.74;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 16.01 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; ce: -1.2232248; max_mcc: 0.22132539422808534; mcc: 0.22132539422808534; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: -1.1911894; steps/sec: 24.11;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 15.42 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; ce: -1.1744673; max_mcc: 0.26100251264473845; mcc: 0.26100251264473845; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: -1.7832377; steps/sec: 24.8;\nFastEstimator-Train: step: 1500; ce: -2.0550075; steps/sec: 27.36;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 15.59 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; ce: -1.1474909; max_mcc: 0.27333068073148203; mcc: 0.27333068073148203; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: -1.9247782; steps/sec: 23.79;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 16.05 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; ce: -1.1647666; max_mcc: 0.3067382097563979; mcc: 0.3067382097563979; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: -1.8525904; steps/sec: 24.02;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 15.9 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 2346; epoch: 6; ce: -1.1477449; max_mcc: 0.32147236431980514; mcc: 0.32147236431980514; since_best_mcc: 0;\nFastEstimator-Train: step: 2400; ce: -1.9646461; steps/sec: 23.73;\nFastEstimator-Train: step: 2700; ce: -1.6338027; steps/sec: 27.83;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 15.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; ce: -1.1068447; max_mcc: 0.3412255408982984; mcc: 0.3412255408982984; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: -1.8994336; steps/sec: 23.62;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 16.16 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 3128; epoch: 8; ce: -1.1532923; max_mcc: 0.34505209454575014; mcc: 0.34505209454575014; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: -2.3463767; steps/sec: 23.92;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 15.81 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 3519; epoch: 9; ce: -1.072229; max_mcc: 0.35470884254952145; mcc: 0.35470884254952145; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: -1.9894075; steps/sec: 23.73;\nFastEstimator-Train: step: 3900; ce: -2.5543215; steps/sec: 26.77;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 16.08 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 3910; epoch: 10; ce: -1.1424695; max_mcc: 0.3699711914588184; mcc: 0.3699711914588184; since_best_mcc: 0;\nFastEstimator-Train: step: 4200; ce: -2.374371; steps/sec: 23.8;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 15.88 sec;\nFastEstimator-Eval: step: 4301; epoch: 11; ce: -1.1189286; max_mcc: 0.3699711914588184; mcc: 0.3585887867728886; since_best_mcc: 1;\nFastEstimator-Train: step: 4500; ce: -1.3748977; steps/sec: 24.14;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 15.62 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 4692; epoch: 12; ce: -1.1282852; max_mcc: 0.376192949299191; mcc: 0.376192949299191; since_best_mcc: 0;\nFastEstimator-Train: step: 4800; ce: -2.409959; steps/sec: 24.27;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 15.92 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 5083; epoch: 13; ce: -1.1448267; max_mcc: 0.3781592646816613; mcc: 0.3781592646816613; since_best_mcc: 0;\nFastEstimator-Train: step: 5100; ce: -2.205565; steps/sec: 23.81;\nFastEstimator-Train: step: 5400; ce: -2.0702982; steps/sec: 26.67;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 16.16 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 5474; epoch: 14; ce: -1.1674703; max_mcc: 0.39153773775904055; mcc: 0.39153773775904055; since_best_mcc: 0;\nFastEstimator-Train: step: 5700; ce: -1.7408903; steps/sec: 23.54;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 16.0 sec;\nFastEstimator-Eval: step: 5865; epoch: 15; ce: -1.0727899; max_mcc: 0.39153773775904055; mcc: 0.3706045023403391; since_best_mcc: 1;\nFastEstimator-Train: step: 6000; ce: -2.7495584; steps/sec: 23.83;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 15.86 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 6256; epoch: 16; ce: -1.1405761; max_mcc: 0.4041517220685679; mcc: 0.4041517220685679; since_best_mcc: 0;\nFastEstimator-Train: step: 6300; ce: -1.6117443; steps/sec: 24.29;\nFastEstimator-Train: step: 6600; ce: -2.2560148; steps/sec: 27.6;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 15.71 sec;\nFastEstimator-Eval: step: 6647; epoch: 17; ce: -1.1009356; max_mcc: 0.4041517220685679; mcc: 0.39731992137483313; since_best_mcc: 1;\nFastEstimator-Train: step: 6900; ce: -1.6276731; steps/sec: 21.85;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 17.07 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 7038; epoch: 18; ce: -1.1205976; max_mcc: 0.40548032886265584; mcc: 0.40548032886265584; since_best_mcc: 0;\nFastEstimator-Train: step: 7200; ce: -2.260793; steps/sec: 23.97;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 15.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 7429; epoch: 19; ce: -1.1357507; max_mcc: 0.40837250526187713; mcc: 0.40837250526187713; since_best_mcc: 0;\nFastEstimator-Train: step: 7500; ce: -2.2957263; steps/sec: 24.12;\nFastEstimator-Train: step: 7800; ce: -1.9242468; steps/sec: 26.72;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 16.15 sec;\nFastEstimator-Eval: step: 7820; epoch: 20; ce: -1.1415228; max_mcc: 0.40837250526187713; mcc: 0.4074934800213416; since_best_mcc: 1;\nFastEstimator-Train: step: 8100; ce: -2.5596945; steps/sec: 23.75;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 16.05 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 8211; epoch: 21; ce: -1.113405; max_mcc: 0.42303159513114147; mcc: 0.42303159513114147; since_best_mcc: 0;\nFastEstimator-Train: step: 8400; ce: -2.0298104; steps/sec: 23.88;\nFastEstimator-Train: step: 8602; epoch: 22; epoch_time: 15.93 sec;\nFastEstimator-Eval: step: 8602; epoch: 22; ce: -1.1111972; max_mcc: 0.42303159513114147; mcc: 0.41537172896026864; since_best_mcc: 1;\nFastEstimator-Train: step: 8700; ce: -1.9526964; steps/sec: 23.83;\nFastEstimator-Train: step: 8993; epoch: 23; epoch_time: 15.82 sec;\nFastEstimator-Eval: step: 8993; epoch: 23; ce: -1.1185064; max_mcc: 0.42303159513114147; mcc: 0.41804243248667144; since_best_mcc: 2;\nFastEstimator-Train: step: 9000; ce: -2.109986; steps/sec: 24.24;\nFastEstimator-Train: step: 9300; ce: -2.568654; steps/sec: 26.65;\nFastEstimator-Train: step: 9384; epoch: 24; epoch_time: 16.01 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 9384; epoch: 24; ce: -1.135175; max_mcc: 0.4369457266318141; mcc: 0.4369457266318141; since_best_mcc: 0;\nFastEstimator-Train: step: 9600; ce: -1.6442108; steps/sec: 24.03;\nFastEstimator-Train: step: 9775; epoch: 25; epoch_time: 15.84 sec;\nFastEstimator-Eval: step: 9775; epoch: 25; ce: -1.107126; max_mcc: 0.4369457266318141; mcc: 0.42386157171628613; since_best_mcc: 1;\nFastEstimator-Train: step: 9900; ce: -2.8970966; steps/sec: 23.59;\nFastEstimator-Train: step: 10166; epoch: 26; epoch_time: 16.39 sec;\nFastEstimator-Eval: step: 10166; epoch: 26; ce: -1.1384557; max_mcc: 0.4369457266318141; mcc: 0.4180638344673729; since_best_mcc: 2;\nFastEstimator-Train: step: 10200; ce: -2.1301792; steps/sec: 23.13;\nFastEstimator-Train: step: 10500; ce: -1.8132625; steps/sec: 27.19;\nFastEstimator-Train: step: 10557; epoch: 27; epoch_time: 15.96 sec;\nFastEstimator-Eval: step: 10557; epoch: 27; ce: -1.1131722; max_mcc: 0.4369457266318141; mcc: 0.42625128157245296; since_best_mcc: 3;\nFastEstimator-Train: step: 10800; ce: -1.998105; steps/sec: 23.81;\nFastEstimator-Train: step: 10948; epoch: 28; epoch_time: 15.93 sec;\nFastEstimator-Eval: step: 10948; epoch: 28; ce: -1.1155487; max_mcc: 0.4369457266318141; mcc: 0.42908067568626684; since_best_mcc: 4;\nFastEstimator-Train: step: 11100; ce: -2.3713639; steps/sec: 23.7;\nFastEstimator-Train: step: 11339; epoch: 29; epoch_time: 16.33 sec;\nFastEstimator-Eval: step: 11339; epoch: 29; ce: -1.1220049; max_mcc: 0.4369457266318141; mcc: 0.4260630601839669; since_best_mcc: 5;\nFastEstimator-Train: step: 11400; ce: -2.169163; steps/sec: 23.17;\nFastEstimator-Train: step: 11700; ce: -2.1599045; steps/sec: 27.62;\nFastEstimator-Train: step: 11730; epoch: 30; epoch_time: 15.79 sec;\nFastEstimator-Eval: step: 11730; epoch: 30; ce: -1.0984906; max_mcc: 0.4369457266318141; mcc: 0.4285067929184998; since_best_mcc: 6;\nFastEstimator-Train: step: 12000; ce: -1.8843775; steps/sec: 24.1;\nFastEstimator-Train: step: 12121; epoch: 31; epoch_time: 15.78 sec;\nFastEstimator-Eval: step: 12121; epoch: 31; ce: -1.1562611; max_mcc: 0.4369457266318141; mcc: 0.42826860921000354; since_best_mcc: 7;\nFastEstimator-Train: step: 12300; ce: -2.6584306; steps/sec: 24.0;\nFastEstimator-Train: step: 12512; epoch: 32; epoch_time: 15.73 sec;\nFastEstimator-Eval: step: 12512; epoch: 32; ce: -1.1435204; max_mcc: 0.4369457266318141; mcc: 0.4333150458296645; since_best_mcc: 8;\nFastEstimator-Train: step: 12600; ce: -2.306051; steps/sec: 24.09;\nFastEstimator-Train: step: 12900; ce: -1.3515851; steps/sec: 27.23;\nFastEstimator-Train: step: 12903; epoch: 33; epoch_time: 15.95 sec;\nFastEstimator-Eval: step: 12903; epoch: 33; ce: -1.1314979; max_mcc: 0.4369457266318141; mcc: 0.43645579540783064; since_best_mcc: 9;\nFastEstimator-Train: step: 13200; ce: -2.8421557; steps/sec: 23.81;\nFastEstimator-Train: step: 13294; epoch: 34; epoch_time: 15.81 sec;\nFastEstimator-Eval: step: 13294; epoch: 34; ce: -1.0980072; max_mcc: 0.4369457266318141; mcc: 0.43075809854762503; since_best_mcc: 10;\nFastEstimator-Train: step: 13500; ce: -2.8916705; steps/sec: 24.25;\nFastEstimator-Train: step: 13685; epoch: 35; epoch_time: 16.25 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 13685; epoch: 35; ce: -1.1472872; max_mcc: 0.44365012520788033; mcc: 0.44365012520788033; since_best_mcc: 0;\nFastEstimator-Train: step: 13800; ce: -2.8401277; steps/sec: 23.06;\nFastEstimator-Train: step: 14076; epoch: 36; epoch_time: 15.82 sec;\nFastEstimator-Eval: step: 14076; epoch: 36; ce: -1.1562262; max_mcc: 0.44365012520788033; mcc: 0.4313645449516555; since_best_mcc: 1;\nFastEstimator-Train: step: 14100; ce: -2.5230148; steps/sec: 24.17;\nFastEstimator-Train: step: 14400; ce: -2.8517623; steps/sec: 26.92;\nFastEstimator-Train: step: 14467; epoch: 37; epoch_time: 15.98 sec;\nFastEstimator-Eval: step: 14467; epoch: 37; ce: -1.2001235; max_mcc: 0.44365012520788033; mcc: 0.430704291123061; since_best_mcc: 2;\nFastEstimator-Train: step: 14700; ce: -2.1312857; steps/sec: 24.05;\nFastEstimator-Train: step: 14858; epoch: 38; epoch_time: 15.73 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 14858; epoch: 38; ce: -1.1431301; max_mcc: 0.4472147715296702; mcc: 0.4472147715296702; since_best_mcc: 0;\nFastEstimator-Train: step: 15000; ce: -2.7680423; steps/sec: 23.84;\nFastEstimator-Train: step: 15249; epoch: 39; epoch_time: 15.99 sec;\nFastEstimator-Eval: step: 15249; epoch: 39; ce: -1.1730366; max_mcc: 0.4472147715296702; mcc: 0.43998571300322525; since_best_mcc: 1;\nFastEstimator-Train: step: 15300; ce: -2.3842983; steps/sec: 23.82;\nFastEstimator-Train: step: 15600; ce: -2.61199; steps/sec: 27.26;\nFastEstimator-Train: step: 15640; epoch: 40; epoch_time: 16.01 sec;\nFastEstimator-Eval: step: 15640; epoch: 40; ce: -1.1539762; max_mcc: 0.4472147715296702; mcc: 0.4367261360226644; since_best_mcc: 2;\nFastEstimator-Train: step: 15900; ce: -2.4703374; steps/sec: 22.97;\nFastEstimator-Train: step: 16031; epoch: 41; epoch_time: 16.48 sec;\nFastEstimator-Eval: step: 16031; epoch: 41; ce: -1.1659281; max_mcc: 0.4472147715296702; mcc: 0.4353277252173622; since_best_mcc: 3;\nFastEstimator-Train: step: 16200; ce: -2.339718; steps/sec: 23.3;\nFastEstimator-Train: step: 16422; epoch: 42; epoch_time: 15.97 sec;\nFastEstimator-Eval: step: 16422; epoch: 42; ce: -1.1800143; max_mcc: 0.4472147715296702; mcc: 0.4397506284062948; since_best_mcc: 4;\nFastEstimator-Train: step: 16500; ce: -2.7436943; steps/sec: 23.55;\nFastEstimator-Train: step: 16800; ce: -2.3053098; steps/sec: 27.47;\nFastEstimator-Train: step: 16813; epoch: 43; epoch_time: 16.11 sec;\nFastEstimator-Eval: step: 16813; epoch: 43; ce: -1.1810557; max_mcc: 0.4472147715296702; mcc: 0.4442676528937523; since_best_mcc: 5;\nFastEstimator-Train: step: 17100; ce: -2.8315272; steps/sec: 24.14;\nFastEstimator-Train: step: 17204; epoch: 44; epoch_time: 15.73 sec;\nFastEstimator-Eval: step: 17204; epoch: 44; ce: -1.1615522; max_mcc: 0.4472147715296702; mcc: 0.43377874758841084; since_best_mcc: 6;\nFastEstimator-Train: step: 17400; ce: -2.6666136; steps/sec: 23.62;\nFastEstimator-Train: step: 17595; epoch: 45; epoch_time: 16.26 sec;\nFastEstimator-Eval: step: 17595; epoch: 45; ce: -1.1887122; max_mcc: 0.4472147715296702; mcc: 0.43760807300504523; since_best_mcc: 7;\nFastEstimator-Train: step: 17700; ce: -2.892805; steps/sec: 23.27;\nFastEstimator-Train: step: 17986; epoch: 46; epoch_time: 16.19 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 17986; epoch: 46; ce: -1.1846745; max_mcc: 0.4516928294301088; mcc: 0.4516928294301088; since_best_mcc: 0;\nFastEstimator-Train: step: 18000; ce: -1.9225727; steps/sec: 23.57;\nFastEstimator-Train: step: 18300; ce: -2.5122561; steps/sec: 26.8;\nFastEstimator-Train: step: 18377; epoch: 47; epoch_time: 16.16 sec;\nFastEstimator-Eval: step: 18377; epoch: 47; ce: -1.190774; max_mcc: 0.4516928294301088; mcc: 0.44820341989774154; since_best_mcc: 1;\nFastEstimator-Train: step: 18600; ce: -2.5353403; steps/sec: 23.46;\nFastEstimator-Train: step: 18768; epoch: 48; epoch_time: 16.36 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 18768; epoch: 48; ce: -1.1954826; max_mcc: 0.4535162464296696; mcc: 0.4535162464296696; since_best_mcc: 0;\nFastEstimator-Train: step: 18900; ce: -1.7238238; steps/sec: 22.65;\nFastEstimator-Train: step: 19159; epoch: 49; epoch_time: 16.18 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Eval: step: 19159; epoch: 49; ce: -1.1699214; max_mcc: 0.456133498289478; mcc: 0.456133498289478; since_best_mcc: 0;\nFastEstimator-Train: step: 19200; ce: -2.1122396; steps/sec: 23.94;\nFastEstimator-Train: step: 19500; ce: -2.927199; steps/sec: 27.23;\nFastEstimator-Train: step: 19550; epoch: 50; epoch_time: 16.02 sec;\nFastEstimator-Eval: step: 19550; epoch: 50; ce: -1.175966; max_mcc: 0.456133498289478; mcc: 0.43898991915944996; since_best_mcc: 1;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpzn39e8qh/model1_best_mcc.h5\nFastEstimator-Finish: step: 19550; model1_lr: 0.001; total_time: 890.63 sec;\n</pre> In\u00a0[7]: Copied! <pre>estimator_regular.test()\n</pre> estimator_regular.test() <pre>WARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Test: step: 19550; epoch: 50; ce: 2.5485935; mcc: 0.3998134494844641;\n</pre> Out[7]: <pre>&lt;fastestimator.summary.summary.Summary at 0x2ff5397f0&gt;</pre> In\u00a0[8]: Copied! <pre>estimator_super.test()\n</pre> estimator_super.test() <pre>WARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nWARNING:tensorflow:@custom_gradient grad_fn has 'variables' in signature, but no ResourceVariables were used on the forward pass.\nFastEstimator-Test: step: 19550; epoch: 50; ce: -1.0826309; mcc: 0.44526758544276457;\n</pre> Out[8]: <pre>&lt;fastestimator.summary.summary.Summary at 0x176d02250&gt;</pre> In\u00a0[9]: Copied! <pre>fe.summary.logs.visualize_logs([regular, superL], include_metrics={'mcc', 'ce', 'max_mcc'})\n</pre> fe.summary.logs.visualize_logs([regular, superL], include_metrics={'mcc', 'ce', 'max_mcc'}) <p>As we can see from the results above, a simple 1 line change to add SuperLoss into the training procedure can raise our model's mcc by a full 4 or 5 points in the presence of noisy input labels. Let's also take a look at the confidence scores generated by SuperLoss on the noisy vs clean data:</p> In\u00a0[10]: Copied! <pre>fe.summary.logs.visualize_logs(estimator_super.system.custom_graphs['label_confidence'])\n</pre> fe.summary.logs.visualize_logs(estimator_super.system.custom_graphs['label_confidence']) <p>As the graph above demonstrates, the corrupted samples have significantly lower average confidence scores than the clean samples. This is also true when we analyze the confidence scores during regular training, but the separation is not as strong:</p> In\u00a0[11]: Copied! <pre>fe.summary.logs.visualize_logs(estimator_regular.system.custom_graphs['label_confidence'])\n</pre> fe.summary.logs.visualize_logs(estimator_regular.system.custom_graphs['label_confidence'])"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#curriculum-learning-with-superloss-tensorflow-backend", "title": "Curriculum Learning with SuperLoss (Tensorflow Backend)\u00b6", "text": "<p>In this example, we are going to demonstrate how to easily add curriculum learning to any project using SuperLoss (paper available here). When humans learn something in school, we are first taught how to do easy versions of the task before graduating to more difficult problems. Curriculum learning seeks to emulate that process with neural networks. One way to do this would be to try and modify a data pipeline to change the order in which it presents examples, but an easier way is to simply modify your loss term to reduce the contribution of difficult examples until later on during training. Curriculum learning has been shown to be especially useful when you have label noise in your dataset, since noisy samples are essentially 'hard' and you want to put off trying to learn them.</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/curriculum_learning/superloss/superloss.html#step-1-data-preparation", "title": "Step 1 - Data preparation\u00b6", "text": "<p>In this step, we will load the ciFAIR100 training and validation datasets. We use a FastEstimator API to load the dataset and then get a test set by splitting 50% of the data off of the evaluation set. We are also going to corrupt the training data by adding 40% label noise, to simulate the fact that many real-world datasets may have low quality annotations.</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#step-2-build-some-estimators", "title": "Step 2 - Build some Estimators\u00b6", "text": "<p>We will define a function that builds relatively simple estimators given only a particular loss function as an input. We can then compare the effects of using a regular loss versus a SuperLoss on our artificially corrupted dataset.</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#step-3-train-a-baseline-model-with-a-regular-loss", "title": "Step 3 - Train a baseline model with a regular loss\u00b6", "text": "<p>Let's start by training a regular model using standard CrossEntropy and see what we get. We will also define a fake SuperLoss wrapper to get sample confidence estimates in order to visualize the differences between clean and corrupted data performance.</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#step-4-train-a-model-using-superloss", "title": "Step 4 - Train a model using SuperLoss\u00b6", "text": "<p>Now it's time to try using SuperLoss to see whether curriculum learning can help us overcome our label noise. Note how easy it is to add SuperLoss to any existing loss function:</p>"}, {"location": "apphub/curriculum_learning/superloss/superloss.html#step-5-performance-comparison", "title": "Step 5 - Performance Comparison\u00b6", "text": "<p>Let's take a look at how each of the final models compare:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n\nfrom fastestimator.util import BatchDisplay, GridDisplay\n</pre> import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile  from fastestimator.util import BatchDisplay, GridDisplay In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 24\nbatch_size = 512\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\nclass_names = [\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \"frogs\", \"horses\", \"ships\", \"trucks\"]\n</pre> #training parameters epochs = 24 batch_size = 512 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() class_names = [\"airplanes\", \"cars\", \"birds\", \"cats\", \"deer\", \"dogs\", \"frogs\", \"horses\", \"ships\", \"trucks\"] In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifair10\n\ntrain_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifair10  train_data, eval_data = cifair10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop import Delete\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n        CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n        ChannelTranspose(inputs=\"x\", outputs=\"x\"),\n        Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=10, label_smoothing=0.2)\n    ])\n</pre> from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop import Delete  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),         ChannelTranspose(inputs=\"x\", outputs=\"x\"),         Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=10, label_smoothing=0.2)     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_x = data[\"x\"]\ndata_y = data[\"y\"]\n\nprint(\"the pipeline output image size: {}\".format(data_x.numpy().shape))\nprint(\"the pipeline output label size: {}\".format(data_y.numpy().shape))\n</pre> data = pipeline.get_results() data_x = data[\"x\"] data_y = data[\"y\"]  print(\"the pipeline output image size: {}\".format(data_x.numpy().shape)) print(\"the pipeline output label size: {}\".format(data_y.numpy().shape)) <pre>the pipeline output image size: (512, 3, 32, 32)\nthe pipeline output label size: (512, 10)\n</pre> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nfig = GridDisplay([BatchDisplay(image=data_x[0:sample_num], title=\"Pipeline Output\"),\n                   BatchDisplay(text=np.argmax(data_y, axis=-1)[0:sample_num], title=\"Label\")])\nfig.show()\n</pre> sample_num = 5  fig = GridDisplay([BatchDisplay(image=data_x[0:sample_num], title=\"Pipeline Output\"),                    BatchDisplay(text=np.argmax(data_y, axis=-1)[0:sample_num], title=\"Label\")]) fig.show() In\u00a0[7]: Copied! <pre>from fastestimator.architecture.pytorch import ResNet9\n\nmodel = fe.build(model_fn=ResNet9, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.pytorch import ResNet9  model = fe.build(model_fn=ResNet9, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")     ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\ndef lr_schedule(step):\n    if step &lt;= 490:\n        lr = step / 490 * 0.4\n    else:\n        lr = (2352 - step) / 1862 * 0.4\n    return lr * 0.1\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lr_schedule)\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n\nestimator.fit() # start the training\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  def lr_schedule(step):     if step &lt;= 490:         lr = step / 490 * 0.4     else:         lr = (2352 - step) / 1862 * 0.4     return lr * 0.1  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lr_schedule) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch)  estimator.fit() # start the training  <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; ce: 2.8280644; model_lr: 8.163265e-05;\nFastEstimator-Train: step: 98; epoch: 1; epoch_time: 9.81 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 56.61;\nEval Progress: 6/9; steps/sec: 61.74;\nEval Progress: 9/9; steps/sec: 68.46;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 98; epoch: 1; accuracy: 0.5464; ce: 1.3504643; max_accuracy: 0.5464; since_best_accuracy: 0;\nFastEstimator-Train: step: 100; ce: 1.5739734; model_lr: 0.008163265; steps/sec: 10.35;\nFastEstimator-Train: step: 196; epoch: 2; epoch_time: 9.54 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 60.31;\nEval Progress: 6/9; steps/sec: 63.37;\nEval Progress: 9/9; steps/sec: 68.77;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 196; epoch: 2; accuracy: 0.6372; ce: 1.1597185; max_accuracy: 0.6372; since_best_accuracy: 0;\nFastEstimator-Train: step: 200; ce: 1.4725939; model_lr: 0.01632653; steps/sec: 10.56;\nFastEstimator-Train: step: 294; epoch: 3; epoch_time: 9.38 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 53.16;\nEval Progress: 6/9; steps/sec: 61.63;\nEval Progress: 9/9; steps/sec: 66.2;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 294; epoch: 3; accuracy: 0.7268; ce: 0.8682629; max_accuracy: 0.7268; since_best_accuracy: 0;\nFastEstimator-Train: step: 300; ce: 1.348976; model_lr: 0.024489796; steps/sec: 10.51;\nFastEstimator-Train: step: 392; epoch: 4; epoch_time: 9.4 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 58.85;\nEval Progress: 6/9; steps/sec: 65.5;\nEval Progress: 9/9; steps/sec: 65.74;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 392; epoch: 4; accuracy: 0.7662; ce: 0.7805661; max_accuracy: 0.7662; since_best_accuracy: 0;\nFastEstimator-Train: step: 400; ce: 1.3233813; model_lr: 0.03265306; steps/sec: 10.51;\nFastEstimator-Train: step: 490; epoch: 5; epoch_time: 9.48 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 42.64;\nEval Progress: 6/9; steps/sec: 65.71;\nEval Progress: 9/9; steps/sec: 67.01;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 490; epoch: 5; accuracy: 0.7768; ce: 0.75958633; max_accuracy: 0.7768; since_best_accuracy: 0;\nFastEstimator-Train: step: 500; ce: 1.2187204; model_lr: 0.039785177; steps/sec: 10.62;\nFastEstimator-Train: step: 588; epoch: 6; epoch_time: 9.33 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 57.03;\nEval Progress: 6/9; steps/sec: 63.87;\nEval Progress: 9/9; steps/sec: 64.77;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 588; epoch: 6; accuracy: 0.8296; ce: 0.6205404; max_accuracy: 0.8296; since_best_accuracy: 0;\nFastEstimator-Train: step: 600; ce: 1.1162705; model_lr: 0.03763695; steps/sec: 10.58;\nFastEstimator-Train: step: 686; epoch: 7; epoch_time: 9.42 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 55.44;\nEval Progress: 6/9; steps/sec: 64.01;\nEval Progress: 9/9; steps/sec: 64.72;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 686; epoch: 7; accuracy: 0.8496; ce: 0.59812355; max_accuracy: 0.8496; since_best_accuracy: 0;\nFastEstimator-Train: step: 700; ce: 1.1050198; model_lr: 0.03548872; steps/sec: 10.58;\nFastEstimator-Train: step: 784; epoch: 8; epoch_time: 9.39 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 47.2;\nEval Progress: 6/9; steps/sec: 65.16;\nEval Progress: 9/9; steps/sec: 64.75;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 784; epoch: 8; accuracy: 0.858; ce: 0.5833092; max_accuracy: 0.858; since_best_accuracy: 0;\nFastEstimator-Train: step: 800; ce: 1.1079816; model_lr: 0.033340495; steps/sec: 10.59;\nFastEstimator-Train: step: 882; epoch: 9; epoch_time: 9.34 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 47.24;\nEval Progress: 6/9; steps/sec: 61.69;\nEval Progress: 9/9; steps/sec: 67.45;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 882; epoch: 9; accuracy: 0.8608; ce: 0.5679447; max_accuracy: 0.8608; since_best_accuracy: 0;\nFastEstimator-Train: step: 900; ce: 1.1163852; model_lr: 0.031192265; steps/sec: 10.33;\nFastEstimator-Train: step: 980; epoch: 10; epoch_time: 9.58 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 38.38;\nEval Progress: 6/9; steps/sec: 59.85;\nEval Progress: 9/9; steps/sec: 63.56;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 980; epoch: 10; accuracy: 0.8716; ce: 0.55340326; max_accuracy: 0.8716; since_best_accuracy: 0;\nFastEstimator-Train: step: 1000; ce: 1.0618074; model_lr: 0.02904404; steps/sec: 10.63;\nFastEstimator-Train: step: 1078; epoch: 11; epoch_time: 9.39 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 49.09;\nEval Progress: 6/9; steps/sec: 68.61;\nEval Progress: 9/9; steps/sec: 65.94;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1078; epoch: 11; accuracy: 0.8834; ce: 0.49868757; max_accuracy: 0.8834; since_best_accuracy: 0;\nFastEstimator-Train: step: 1100; ce: 1.0652387; model_lr: 0.026895812; steps/sec: 10.55;\nFastEstimator-Train: step: 1176; epoch: 12; epoch_time: 9.4 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 47.16;\nEval Progress: 6/9; steps/sec: 59.78;\nEval Progress: 9/9; steps/sec: 62.87;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1176; epoch: 12; accuracy: 0.8902; ce: 0.50505483; max_accuracy: 0.8902; since_best_accuracy: 0;\nFastEstimator-Train: step: 1200; ce: 1.0391171; model_lr: 0.024747584; steps/sec: 10.5;\nFastEstimator-Train: step: 1274; epoch: 13; epoch_time: 9.48 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 56.42;\nEval Progress: 6/9; steps/sec: 59.49;\nEval Progress: 9/9; steps/sec: 66.38;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1274; epoch: 13; accuracy: 0.8958; ce: 0.4790287; max_accuracy: 0.8958; since_best_accuracy: 0;\nFastEstimator-Train: step: 1300; ce: 1.0158072; model_lr: 0.022599356; steps/sec: 10.57;\nFastEstimator-Train: step: 1372; epoch: 14; epoch_time: 9.38 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 32.5;\nEval Progress: 6/9; steps/sec: 63.85;\nEval Progress: 9/9; steps/sec: 63.61;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1372; epoch: 14; accuracy: 0.899; ce: 0.4504097; max_accuracy: 0.899; since_best_accuracy: 0;\nFastEstimator-Train: step: 1400; ce: 1.005058; model_lr: 0.020451128; steps/sec: 10.6;\nFastEstimator-Train: step: 1470; epoch: 15; epoch_time: 9.35 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 52.57;\nEval Progress: 6/9; steps/sec: 56.99;\nEval Progress: 9/9; steps/sec: 68.16;\nFastEstimator-Eval: step: 1470; epoch: 15; accuracy: 0.8934; ce: 0.47465906; max_accuracy: 0.899; since_best_accuracy: 1;\nFastEstimator-Train: step: 1500; ce: 0.99793816; model_lr: 0.0183029; steps/sec: 10.55;\nFastEstimator-Train: step: 1568; epoch: 16; epoch_time: 9.43 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 52.53;\nEval Progress: 6/9; steps/sec: 68.94;\nEval Progress: 9/9; steps/sec: 63.96;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1568; epoch: 16; accuracy: 0.9104; ce: 0.4402462; max_accuracy: 0.9104; since_best_accuracy: 0;\nFastEstimator-Train: step: 1600; ce: 0.9909789; model_lr: 0.016154673; steps/sec: 10.62;\nFastEstimator-Train: step: 1666; epoch: 17; epoch_time: 9.34 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 55.53;\nEval Progress: 6/9; steps/sec: 64.31;\nEval Progress: 9/9; steps/sec: 64.53;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1666; epoch: 17; accuracy: 0.9142; ce: 0.42767367; max_accuracy: 0.9142; since_best_accuracy: 0;\nFastEstimator-Train: step: 1700; ce: 0.97114897; model_lr: 0.014006444; steps/sec: 10.7;\nFastEstimator-Train: step: 1764; epoch: 18; epoch_time: 9.24 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 58.98;\nEval Progress: 6/9; steps/sec: 64.64;\nEval Progress: 9/9; steps/sec: 59.78;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1764; epoch: 18; accuracy: 0.9158; ce: 0.41351128; max_accuracy: 0.9158; since_best_accuracy: 0;\nFastEstimator-Train: step: 1800; ce: 0.9536685; model_lr: 0.011858217; steps/sec: 10.56;\nFastEstimator-Train: step: 1862; epoch: 19; epoch_time: 9.37 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 47.6;\nEval Progress: 6/9; steps/sec: 64.17;\nEval Progress: 9/9; steps/sec: 64.03;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1862; epoch: 19; accuracy: 0.9194; ce: 0.4258132; max_accuracy: 0.9194; since_best_accuracy: 0;\nFastEstimator-Train: step: 1900; ce: 0.94474137; model_lr: 0.00970999; steps/sec: 10.59;\nFastEstimator-Train: step: 1960; epoch: 20; epoch_time: 9.47 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 62.46;\nEval Progress: 6/9; steps/sec: 68.06;\nEval Progress: 9/9; steps/sec: 65.02;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 1960; epoch: 20; accuracy: 0.9234; ce: 0.39099675; max_accuracy: 0.9234; since_best_accuracy: 0;\nFastEstimator-Train: step: 2000; ce: 0.93317723; model_lr: 0.0075617614; steps/sec: 10.39;\nFastEstimator-Train: step: 2058; epoch: 21; epoch_time: 9.53 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 35.81;\nEval Progress: 6/9; steps/sec: 59.58;\nEval Progress: 9/9; steps/sec: 67.37;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 2058; epoch: 21; accuracy: 0.924; ce: 0.3927131; max_accuracy: 0.924; since_best_accuracy: 0;\nFastEstimator-Train: step: 2100; ce: 0.9343517; model_lr: 0.0054135337; steps/sec: 10.41;\nFastEstimator-Train: step: 2156; epoch: 22; epoch_time: 9.53 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 53.58;\nEval Progress: 6/9; steps/sec: 66.41;\nEval Progress: 9/9; steps/sec: 63.95;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 2156; epoch: 22; accuracy: 0.929; ce: 0.3977168; max_accuracy: 0.929; since_best_accuracy: 0;\nFastEstimator-Train: step: 2200; ce: 0.93351036; model_lr: 0.0032653061; steps/sec: 10.51;\nFastEstimator-Train: step: 2254; epoch: 23; epoch_time: 9.42 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 53.55;\nEval Progress: 6/9; steps/sec: 63.23;\nEval Progress: 9/9; steps/sec: 64.59;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpq5st5yqv/model_best_accuracy.pt\nFastEstimator-Eval: step: 2254; epoch: 23; accuracy: 0.9374; ce: 0.376044; max_accuracy: 0.9374; since_best_accuracy: 0;\nFastEstimator-Train: step: 2300; ce: 0.90236723; model_lr: 0.0011170784; steps/sec: 10.48;\nFastEstimator-Train: step: 2352; epoch: 24; epoch_time: 9.5 sec;\nEval Progress: 1/9;\nEval Progress: 3/9; steps/sec: 62.99;\nEval Progress: 6/9; steps/sec: 63.47;\nEval Progress: 9/9; steps/sec: 67.18;\nFastEstimator-Eval: step: 2352; epoch: 24; accuracy: 0.9348; ce: 0.37468904; max_accuracy: 0.9374; since_best_accuracy: 1;\nFastEstimator-Finish: step: 2352; model_lr: 0.0; total_time: 371.66 sec;\n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 2352; epoch: 24; accuracy: 0.9252; ce: 0.3898231;\n</pre> In\u00a0[11]: Copied! <pre>sample_num = 5\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    data = {\"x\": test_data[\"x\"][j]}\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predict = data[\"y_pred\"].numpy()\n    \n    fig = GridDisplay([BatchDisplay(image=data['x'], title=\"Pipeline Output\"),\n                       BatchDisplay(text=[class_names[np.argmax(predict)]], title=\"Predicted Class\")\n                      ])\n    fig.show()\n</pre> sample_num = 5  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     data = {\"x\": test_data[\"x\"][j]}          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")           # run the network     data = network.transform(data, mode=\"infer\")     predict = data[\"y_pred\"].numpy()          fig = GridDisplay([BatchDisplay(image=data['x'], title=\"Pipeline Output\"),                        BatchDisplay(text=[class_names[np.argmax(predict)]], title=\"Predicted Class\")                       ])     fig.show()"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#cifar-10-image-classification-using-resnet-pytorch-backend", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)\u00b6", "text": "<p>In this example we are going to demonstrate how to train a CIFAR-10 image classification model using a ResNet architecture on the PyTorch backend. All training details including model structure, data preprocessing, learning rate control, etc. come from https://github.com/davidcpage/cifar10-fast. Note that we will, however, be using the ciFAIR-10 dataset which fixes train/test duplicates found in the original CIFAR-10 dataset (https://cvjena.github.io/cifair/)</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load ciFAIR-10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the ciFAIR-10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#set-up-a-pre-processing-pipeline", "title": "Set up a pre-processing <code>Pipeline</code>\u00b6", "text": "<p>Here we set up the data pipeline. This will involve a variety of data augmentation including: random cropping, horizontal flipping, image obscuration, and smoothed one-hot label encoding. Beside all of this, the image channels need to be transposed from HWC to CHW format due to PyTorch conventions. We set up these processing steps using <code>Ops</code> and also bundle the data sources and batch_size together into our <code>Pipeline</code>.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the <code>Pipeline</code> works as expected, let's visualize the output and check its size. <code>Pipeline.get_results</code> will return a batch data of pipeline output for this purpose:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the PyTorch way in this example.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-construction", "title": "Model construction\u00b6", "text": "<p>The model definitions are implemented in PyTorch and instantiated by calling <code>fe.build</code> which also associates the model with a specific optimizer. Here we are going to directly import the model architecture from FastEstimator.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p><code>Ops</code> are the basic components of a network that include models, loss calculation units, and post-processing units. In this step we are going to combine those pieces together into a <code>Network</code>:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>), save our best model (<code>BestModelSaver</code>), and change the learning rate (<code>LRScheduler</code>) of our optimizer over time. We will then use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> will trigger model testing using all of the test data defined in the <code>Pipeline</code>. This will allow us to check our accuracy on previously unseen data.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>In this step we run image inference directly using the model that we just trained. We randomly select 5 images from testing dataset and infer them image by image using <code>Pipeline.transform</code> and <code>Network.transform</code>. Please be aware that the <code>Pipeline</code> is no longer the same as it was during training, because we don't want to use data augmentation during inference. This detail was already defined in the <code>Pipeline</code> (mode = \"!infer\").</p>"}, {"location": "apphub/image_classification/mnist/mnist.html", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n\nfrom fastestimator.util import BatchDisplay, GridDisplay\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile  from fastestimator.util import BatchDisplay, GridDisplay In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 2\nbatch_size = 32\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 2 batch_size = 32 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import mnist\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import mnist  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=batch_size,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"), \n                            Minmax(inputs=\"x_out\", outputs=\"x_out\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=batch_size,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"),                              Minmax(inputs=\"x_out\", outputs=\"x_out\")]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\n\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\nprint(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy())))\nprint(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy())))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"]  print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) print(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy()))) print(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy()))) <pre>the pipeline input data size: (32, 28, 28)\nthe pipeline output data size: (32, 28, 28, 1)\nthe maximum pixel value of output image: 1.0\nthe minimum pixel value of output image: 0.0\n</pre> In\u00a0[6]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\ninputs = tf.gather(data_xin.numpy(), indices)\noutputs = tf.gather(data_xout.numpy(), indices)\n\nfig = GridDisplay([BatchDisplay(image=inputs, title='Pipeline Input'),\n                   BatchDisplay(image=outputs, title='Pipeline Output')\n                  ])\nfig.show()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False) inputs = tf.gather(data_xin.numpy(), indices) outputs = tf.gather(data_xout.numpy(), indices)  fig = GridDisplay([BatchDisplay(image=inputs, title='Pipeline Input'),                    BatchDisplay(image=outputs, title='Pipeline Output')                   ]) fig.show() <pre>2022-05-17 18:39:52.069204: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-17 18:39:52.548658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n</pre> In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ]) <pre>2022-05-17 18:39:53.965393: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[9]: Copied! <pre>from fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy   traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'x' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>2022-05-17 18:39:58.639145: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n2022-05-17 18:39:59.830141: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; ce: 2.3435714; model_lr: 0.0009999998;\nFastEstimator-Train: step: 100; ce: 0.4765919; model_lr: 0.0009982482; steps/sec: 321.43;\nFastEstimator-Train: step: 200; ce: 0.20118365; model_lr: 0.000993005; steps/sec: 436.99;\nFastEstimator-Train: step: 300; ce: 0.099995025; model_lr: 0.0009843073; steps/sec: 427.64;\nFastEstimator-Train: step: 400; ce: 0.0135647105; model_lr: 0.000972216; steps/sec: 396.01;\nFastEstimator-Train: step: 500; ce: 0.21389233; model_lr: 0.00095681596; steps/sec: 437.99;\nFastEstimator-Train: step: 600; ce: 0.11859661; model_lr: 0.0009382152; steps/sec: 451.64;\nFastEstimator-Train: step: 700; ce: 0.0150666125; model_lr: 0.0009165442; steps/sec: 437.15;\nFastEstimator-Train: step: 800; ce: 0.19784231; model_lr: 0.00089195487; steps/sec: 411.11;\nFastEstimator-Train: step: 900; ce: 0.09315811; model_lr: 0.00086461985; steps/sec: 415.99;\nFastEstimator-Train: step: 1000; ce: 0.08798367; model_lr: 0.00083473074; steps/sec: 418.98;\nFastEstimator-Train: step: 1100; ce: 0.13316694; model_lr: 0.00080249726; steps/sec: 409.57;\nFastEstimator-Train: step: 1200; ce: 0.101819955; model_lr: 0.0007681455; steps/sec: 414.03;\nFastEstimator-Train: step: 1300; ce: 0.02795989; model_lr: 0.00073191634; steps/sec: 418.81;\nFastEstimator-Train: step: 1400; ce: 0.14486375; model_lr: 0.000694064; steps/sec: 419.0;\nFastEstimator-Train: step: 1500; ce: 0.00974; model_lr: 0.000654854; steps/sec: 412.56;\nFastEstimator-Train: step: 1600; ce: 0.011371499; model_lr: 0.00061456126; steps/sec: 423.26;\nFastEstimator-Train: step: 1700; ce: 0.2414889; model_lr: 0.00057346845; steps/sec: 428.78;\nFastEstimator-Train: step: 1800; ce: 0.052666433; model_lr: 0.0005318639; steps/sec: 423.53;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 8.27 sec;\nEval Progress: 1/156;\nEval Progress: 52/156; steps/sec: 466.18;\nEval Progress: 104/156; steps/sec: 601.23;\nEval Progress: 156/156; steps/sec: 572.91;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpx2c0kg_c/model_best_accuracy.h5\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9882; ce: 0.035068836; max_accuracy: 0.9882; since_best_accuracy: 0;\nFastEstimator-Train: step: 1900; ce: 0.074525386; model_lr: 0.00049003924; steps/sec: 35.29;\nFastEstimator-Train: step: 2000; ce: 0.14389601; model_lr: 0.00044828805; steps/sec: 396.1;\nFastEstimator-Train: step: 2100; ce: 0.016613875; model_lr: 0.00040690304; steps/sec: 414.19;\nFastEstimator-Train: step: 2200; ce: 0.021673243; model_lr: 0.00036617456; steps/sec: 413.71;\nFastEstimator-Train: step: 2300; ce: 0.006829162; model_lr: 0.00032638825; steps/sec: 415.69;\nFastEstimator-Train: step: 2400; ce: 0.0015307405; model_lr: 0.00028782323; steps/sec: 426.31;\nFastEstimator-Train: step: 2500; ce: 0.025499923; model_lr: 0.00025075; steps/sec: 412.58;\nFastEstimator-Train: step: 2600; ce: 0.0025987546; model_lr: 0.00021542858; steps/sec: 401.93;\nFastEstimator-Train: step: 2700; ce: 0.006895067; model_lr: 0.00018210671; steps/sec: 399.96;\nFastEstimator-Train: step: 2800; ce: 0.026727632; model_lr: 0.00015101816; steps/sec: 404.2;\nFastEstimator-Train: step: 2900; ce: 0.002839017; model_lr: 0.00012238097; steps/sec: 418.88;\nFastEstimator-Train: step: 3000; ce: 0.035146505; model_lr: 9.639601e-05; steps/sec: 431.53;\nFastEstimator-Train: step: 3100; ce: 0.0043295464; model_lr: 7.324555e-05; steps/sec: 430.43;\nFastEstimator-Train: step: 3200; ce: 0.17529601; model_lr: 5.3091975e-05; steps/sec: 402.93;\nFastEstimator-Train: step: 3300; ce: 0.0788301; model_lr: 3.6076646e-05; steps/sec: 412.71;\nFastEstimator-Train: step: 3400; ce: 0.031229565; model_lr: 2.231891e-05; steps/sec: 410.9;\nFastEstimator-Train: step: 3500; ce: 0.0007895004; model_lr: 1.1915274e-05; steps/sec: 406.93;\nFastEstimator-Train: step: 3600; ce: 0.014925261; model_lr: 4.9387068e-06; steps/sec: 411.33;\nFastEstimator-Train: step: 3700; ce: 0.13713205; model_lr: 1.4381463e-06; steps/sec: 375.61;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 7.16 sec;\nEval Progress: 1/156;\nEval Progress: 52/156; steps/sec: 428.12;\nEval Progress: 104/156; steps/sec: 560.12;\nEval Progress: 156/156; steps/sec: 585.27;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpx2c0kg_c/model_best_accuracy.h5\nFastEstimator-Eval: step: 3750; epoch: 2; accuracy: 0.9908; ce: 0.027614402; max_accuracy: 0.9908; since_best_accuracy: 0;\nFastEstimator-Finish: step: 3750; model_lr: 1e-06; total_time: 21.62 sec;\n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.991; ce: 0.02567655;\n</pre> In\u00a0[11]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\n\ninputs = []\noutputs = []\npredictions = []\n\nfor idx in indices:\n    inputs.append(test_data[\"x\"][idx])\n    data = {\"x\": inputs[-1]}\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))\n\nfig = GridDisplay([BatchDisplay(image=np.stack(inputs), title=\"Pipeline Input\"),\n                   BatchDisplay(image=np.stack(outputs), title=\"Pipeline Output\"),\n                   BatchDisplay(text=np.stack(predictions), title=\"Predictions\")\n                  ])\nfig.show()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False)  inputs = [] outputs = [] predictions = []  for idx in indices:     inputs.append(test_data[\"x\"][idx])     data = {\"x\": inputs[-1]}          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))          # run the network     data = network.transform(data, mode=\"infer\")     predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))  fig = GridDisplay([BatchDisplay(image=np.stack(inputs), title=\"Pipeline Input\"),                    BatchDisplay(image=np.stack(outputs), title=\"Pipeline Output\"),                    BatchDisplay(text=np.stack(predictions), title=\"Predictions\")                   ]) fig.show()"}, {"location": "apphub/image_classification/mnist/mnist.html#mnist-image-classification-using-lenet-tensorflow-backend", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)\u00b6", "text": "<p>In this example, we are going to demonstrate how to train an MNIST image classification model using a LeNet model architecture and TensorFlow backend.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/mnist/mnist.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the MNIST dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#set-up-a-preprocessing-pipeline", "title": "Set up a preprocessing pipeline\u00b6", "text": "<p>In this example, the data preprocessing steps include adding a channel to the images (since they are grey-scale) and normalizing the image pixel values to the range [0, 1]. We set up these processing steps using <code>Ops</code>. The <code>Pipeline</code> also takes our data sources and batch size as inputs.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch  of pipeline output to enable this:</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Here we are going to import one of FastEstimator's pre-defined model architectures, which was written in TensorFlow. We create a model instance by compiling our model definition function along with a specific model optimizer.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect the model and <code>Ops</code> together into a <code>Network</code>. <code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update rules, or even models themselves.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which compute accuracy (<code>Accuracy</code>), save the best model (<code>BestModelSaver</code>), and change the model learning rate over time (<code>LRScheduler</code>).</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing using the test dataset that was specified in <code>Pipeline</code>. We can evaluate the model's accuracy on this previously unseen data.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Now let's run inferencing on several images directly using the model that we just trained. We randomly select 5 images from the testing dataset and infer them image by image by leveraging <code>Pipeline.transform</code> and <code>Network.transform</code>:</p>"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html", "title": "Image Classification with Deep Pyramidal Residual Networks", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data.cifair10 import load_data\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import get_num_devices, BatchDisplay, GridDisplay\n</pre> import tempfile  import tensorflow as tf import tensorflow_addons as tfa from tensorflow.keras import layers  import fastestimator as fe from fastestimator.dataset.data.cifair10 import load_data from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import get_num_devices, BatchDisplay, GridDisplay In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 150\nbatch_size = 32\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 150 batch_size = 32 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>train_data, eval_data = load_data()\n</pre> train_data, eval_data = load_data() In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size * get_num_devices(),\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n        CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1)\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size * get_num_devices(),     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1)     ]) In\u00a0[5]: Copied! <pre>import numpy as np\nimages_to_display = min(4, batch_size)\ndata = pipeline.get_results()\nclass_dictionary = {\n    0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n}\ny = np.array([class_dictionary[clazz.item()] for clazz in fe.util.to_number(data[\"y\"])])\nfig = GridDisplay([BatchDisplay(image=data[\"x\"].numpy()[0:images_to_display], title=\"x\"),\n                   BatchDisplay(text=y[0:images_to_display], title=\"y\")\n                  ])\nfig.show()\n</pre> import numpy as np images_to_display = min(4, batch_size) data = pipeline.get_results() class_dictionary = {     0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\" } y = np.array([class_dictionary[clazz.item()] for clazz in fe.util.to_number(data[\"y\"])]) fig = GridDisplay([BatchDisplay(image=data[\"x\"].numpy()[0:images_to_display], title=\"x\"),                    BatchDisplay(text=y[0:images_to_display], title=\"y\")                   ]) fig.show() In\u00a0[6]: Copied! <pre>def basic_block(x, planes, stride=1, downsample=False):\n    out = layers.BatchNormalization()(x)\n    out = layers.Conv2D(filters=planes, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n    out = layers.ReLU()(out)\n    out = layers.Conv2D(filters=planes, kernel_size=3, padding=\"same\", use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n    shortcut = x\n    if downsample:\n        shortcut = layers.AveragePooling2D()(shortcut)\n    if shortcut.shape[-1] != out.shape[-1]:\n        shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0], [0, out.shape[-1] - shortcut.shape[-1]]])\n    return out + shortcut\n\n\ndef bottle_neck(x, planes, stride=1, downsample=False):\n    out = layers.BatchNormalization()(x)\n    out = layers.Conv2D(filters=planes, kernel_size=1, padding=\"same\", use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n    out = layers.ReLU()(out)\n    out = layers.Conv2D(filters=planes, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n    out = layers.ReLU()(out)\n    out = layers.Conv2D(filters=planes * 4, kernel_size=1, padding=\"same\", use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n    shortcut = x\n    if downsample:\n        shortcut = layers.AveragePooling2D()(shortcut)\n    if shortcut.shape[-1] != out.shape[-1]:\n        shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0], [0, out.shape[-1] - shortcut.shape[-1]]])\n    return out + shortcut\n\n\ndef make_group(x, featuremap_dim, addrate, block, block_depth, stride=1):\n    for i in range(0, block_depth):\n        featuremap_dim += addrate\n        if i == 0:\n            x = block(x=x, planes=int(round(featuremap_dim)), stride=stride, downsample=stride != 1)\n        else:\n            x = block(x=x, planes=int(round(featuremap_dim)), stride=1)\n    return x, featuremap_dim\n\n\ndef pyramidnet_cifar(inputs_shape, depth, alpha, num_classes, bottleneck=False):\n    if bottleneck:\n        n = int((depth - 2) / 9)\n        block = bottle_neck\n    else:\n        n = int((depth - 2) / 6)\n        block = basic_block\n    addrate = alpha / 3 / n\n    inputs = layers.Input(shape=inputs_shape)\n    x = layers.Conv2D(filters=16, kernel_size=3, padding=\"same\", use_bias=False)(inputs)\n    x = layers.BatchNormalization()(x)\n    x, featuremap_dim = make_group(x, 16, addrate=addrate, block=block, block_depth=n)\n    x, featuremap_dim = make_group(x, featuremap_dim, addrate=addrate, block=block, block_depth=n, stride=2)\n    x, featuremap_dim = make_group(x, featuremap_dim, addrate=addrate, block=block, block_depth=n, stride=2)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.AveragePooling2D(8)(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(num_classes)(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n</pre> def basic_block(x, planes, stride=1, downsample=False):     out = layers.BatchNormalization()(x)     out = layers.Conv2D(filters=planes, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)(out)     out = layers.BatchNormalization()(out)     out = layers.ReLU()(out)     out = layers.Conv2D(filters=planes, kernel_size=3, padding=\"same\", use_bias=False)(out)     out = layers.BatchNormalization()(out)     shortcut = x     if downsample:         shortcut = layers.AveragePooling2D()(shortcut)     if shortcut.shape[-1] != out.shape[-1]:         shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0], [0, out.shape[-1] - shortcut.shape[-1]]])     return out + shortcut   def bottle_neck(x, planes, stride=1, downsample=False):     out = layers.BatchNormalization()(x)     out = layers.Conv2D(filters=planes, kernel_size=1, padding=\"same\", use_bias=False)(out)     out = layers.BatchNormalization()(out)     out = layers.ReLU()(out)     out = layers.Conv2D(filters=planes, kernel_size=3, strides=stride, padding=\"same\", use_bias=False)(out)     out = layers.BatchNormalization()(out)     out = layers.ReLU()(out)     out = layers.Conv2D(filters=planes * 4, kernel_size=1, padding=\"same\", use_bias=False)(out)     out = layers.BatchNormalization()(out)     shortcut = x     if downsample:         shortcut = layers.AveragePooling2D()(shortcut)     if shortcut.shape[-1] != out.shape[-1]:         shortcut = tf.pad(shortcut, [[0, 0], [0, 0], [0, 0], [0, out.shape[-1] - shortcut.shape[-1]]])     return out + shortcut   def make_group(x, featuremap_dim, addrate, block, block_depth, stride=1):     for i in range(0, block_depth):         featuremap_dim += addrate         if i == 0:             x = block(x=x, planes=int(round(featuremap_dim)), stride=stride, downsample=stride != 1)         else:             x = block(x=x, planes=int(round(featuremap_dim)), stride=1)     return x, featuremap_dim   def pyramidnet_cifar(inputs_shape, depth, alpha, num_classes, bottleneck=False):     if bottleneck:         n = int((depth - 2) / 9)         block = bottle_neck     else:         n = int((depth - 2) / 6)         block = basic_block     addrate = alpha / 3 / n     inputs = layers.Input(shape=inputs_shape)     x = layers.Conv2D(filters=16, kernel_size=3, padding=\"same\", use_bias=False)(inputs)     x = layers.BatchNormalization()(x)     x, featuremap_dim = make_group(x, 16, addrate=addrate, block=block, block_depth=n)     x, featuremap_dim = make_group(x, featuremap_dim, addrate=addrate, block=block, block_depth=n, stride=2)     x, featuremap_dim = make_group(x, featuremap_dim, addrate=addrate, block=block, block_depth=n, stride=2)     x = layers.BatchNormalization()(x)     x = layers.ReLU()(x)     x = layers.AveragePooling2D(8)(x)     x = layers.Flatten()(x)     x = layers.Dense(num_classes)(x)     return tf.keras.Model(inputs=inputs, outputs=x) In\u00a0[7]: Copied! <pre>model = fe.build(\n    model_fn=lambda: pyramidnet_cifar(inputs_shape=(32, 32, 3), depth=272, alpha=200, num_classes=10, bottleneck=True),\n    optimizer_fn=lambda: tfa.optimizers.SGDW(weight_decay=0.0001, lr=0.1, momentum=0.9))\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> model = fe.build(     model_fn=lambda: pyramidnet_cifar(inputs_shape=(32, 32, 3), depth=272, alpha=200, num_classes=10, bottleneck=True),     optimizer_fn=lambda: tfa.optimizers.SGDW(weight_decay=0.0001, lr=0.1, momentum=0.9))  network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),     UpdateOp(model=model, loss_name=\"ce\") ]) In\u00a0[8]: Copied! <pre>def lr_schedule(epoch):\n    if epoch &lt; 75:\n        lr = 0.1\n    elif epoch &lt; 110:\n        lr = 0.01\n    else:\n        lr = 0.001\n    return lr\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lr_schedule),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> def lr_schedule(epoch):     if epoch &lt; 75:         lr = 0.1     elif epoch &lt; 110:         lr = 0.01     else:         lr = 0.001     return lr  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lr_schedule),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit()"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#image-classification-with-deep-pyramidal-residual-networks", "title": "Image Classification with Deep Pyramidal Residual Networks\u00b6", "text": "<p>In this application, We are going to implement the Deep Pyramidal Residual Network Proposed here in TensorFlow.</p> <p>The core idea of the paper is that building the network with number of feature channels increased by predefined linear function, combining with residual connection can help with learning process.</p>"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#dataset-loading", "title": "Dataset Loading\u00b6", "text": "<p>In this application, we apply the network on the ciFAIR10 dataset.</p>"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#preprocessing", "title": "Preprocessing\u00b6", "text": "<p>The image pixel values are first normalized by predefined constant, then we apply random padding and cropping, followed by horizontal flip and cut out during training.</p>"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#visualize-preprocessing-results", "title": "Visualize Preprocessing Results\u00b6", "text": ""}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#define-pyramidal-network", "title": "Define Pyramidal Network\u00b6", "text": ""}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#create-instance-of-the-model-and-put-it-in-network", "title": "Create instance of the model and put it in <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#define-traces-and-estimator", "title": "Define traces and <code>Estimator</code>\u00b6", "text": "<p>We schedule the learning rate based on epochs, and calculate accuracy to save the best model we have seen during evaluation.</p>"}, {"location": "apphub/image_classification/pyramidnet/pyramidnet.html#train-the-model", "title": "Train the model\u00b6", "text": "<p>The training takes ~18 hours using single V100 GPU, the best evaluation accuracy is ~ 96.5%.</p> <p>Here are the accuracy from 5 independent runs for reference: 0.9661, 0.9636, 0.9646, 0.9625, 0.9655</p>"}, {"location": "apphub/image_classification/vit/vit.html", "title": "Image Classification Using Vision Transformer", "text": "In\u00a0[1]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass ViTEmbeddings(nn.Module):\n    def __init__(self, image_size=224, patch_size=16, num_channels=3, em_dim=768, drop=0.1) -&gt; None:\n        super().__init__()\n        assert image_size % patch_size == 0, \"image size must be an integer multiply of patch size\"\n        self.patch_embedding = nn.Conv2d(num_channels, em_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n        self.position_embedding = nn.Parameter(torch.zeros(1, (image_size // patch_size)**2 + 1, em_dim))\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, em_dim))\n        self.dropout = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.patch_embedding(x).flatten(2).transpose(1, 2)  # [B,C, H, W] -&gt; [B, num_patches, em_dim]\n        x = torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], dim=1)  # [B, num_patches+1, em_dim]\n        x = x + self.position_embedding\n        x = self.dropout(x)\n        return x\n\n    \nclass ViTEncoder(nn.Module):\n    def __init__(self, num_layers, image_size, patch_size, num_channels, em_dim, drop, num_heads, ff_dim):\n        super().__init__()\n        self.embedding = ViTEmbeddings(image_size, patch_size, num_channels, em_dim, drop)\n        encoder_layer = TransformerEncoderLayer(em_dim,\n                                                nhead=num_heads,\n                                                dim_feedforward=ff_dim,\n                                                activation='gelu',\n                                                dropout=drop)\n        self.encoder = TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)\n        self.layernorm = nn.LayerNorm(em_dim, eps=1e-6)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.transpose(0, 1)  # Switch batch and sequence length dimension for pytorch convention\n        x = self.encoder(x)\n        x = self.layernorm(x[0])\n        return x\n    \n\nclass ViTModel(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 num_layers=12,\n                 image_size=224,\n                 patch_size=16,\n                 num_channels=3,\n                 em_dim=768,\n                 drop=0.1,\n                 num_heads=12,\n                 ff_dim=3072):\n        super().__init__()\n        self.vit_encoder = ViTEncoder(num_layers=num_layers,\n                                      image_size=image_size,\n                                      patch_size=patch_size,\n                                      num_channels=num_channels,\n                                      em_dim=em_dim,\n                                      drop=drop,\n                                      num_heads=num_heads,\n                                      ff_dim=ff_dim)\n        self.linear_classifier = nn.Linear(em_dim, num_classes)\n\n    def forward(self, x):\n        x = self.vit_encoder(x)\n        x = self.linear_classifier(x)\n        return x\n</pre> import torch import torch.nn as nn from torch.nn import TransformerEncoder, TransformerEncoderLayer  class ViTEmbeddings(nn.Module):     def __init__(self, image_size=224, patch_size=16, num_channels=3, em_dim=768, drop=0.1) -&gt; None:         super().__init__()         assert image_size % patch_size == 0, \"image size must be an integer multiply of patch size\"         self.patch_embedding = nn.Conv2d(num_channels, em_dim, kernel_size=patch_size, stride=patch_size, bias=False)         self.position_embedding = nn.Parameter(torch.zeros(1, (image_size // patch_size)**2 + 1, em_dim))         self.cls_token = nn.Parameter(torch.zeros(1, 1, em_dim))         self.dropout = nn.Dropout(drop)      def forward(self, x):         x = self.patch_embedding(x).flatten(2).transpose(1, 2)  # [B,C, H, W] -&gt; [B, num_patches, em_dim]         x = torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], dim=1)  # [B, num_patches+1, em_dim]         x = x + self.position_embedding         x = self.dropout(x)         return x       class ViTEncoder(nn.Module):     def __init__(self, num_layers, image_size, patch_size, num_channels, em_dim, drop, num_heads, ff_dim):         super().__init__()         self.embedding = ViTEmbeddings(image_size, patch_size, num_channels, em_dim, drop)         encoder_layer = TransformerEncoderLayer(em_dim,                                                 nhead=num_heads,                                                 dim_feedforward=ff_dim,                                                 activation='gelu',                                                 dropout=drop)         self.encoder = TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)         self.layernorm = nn.LayerNorm(em_dim, eps=1e-6)      def forward(self, x):         x = self.embedding(x)         x = x.transpose(0, 1)  # Switch batch and sequence length dimension for pytorch convention         x = self.encoder(x)         x = self.layernorm(x[0])         return x       class ViTModel(nn.Module):     def __init__(self,                  num_classes,                  num_layers=12,                  image_size=224,                  patch_size=16,                  num_channels=3,                  em_dim=768,                  drop=0.1,                  num_heads=12,                  ff_dim=3072):         super().__init__()         self.vit_encoder = ViTEncoder(num_layers=num_layers,                                       image_size=image_size,                                       patch_size=patch_size,                                       num_channels=num_channels,                                       em_dim=em_dim,                                       drop=drop,                                       num_heads=num_heads,                                       ff_dim=ff_dim)         self.linear_classifier = nn.Linear(em_dim, num_classes)      def forward(self, x):         x = self.vit_encoder(x)         x = self.linear_classifier(x)         return x <p>Now let's define some parameters that will be used later:</p> In\u00a0[2]: parameters Copied! <pre>batch_size=128\npretrain_epochs=100\nfinetune_epochs=1\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\n</pre> batch_size=128 pretrain_epochs=100 finetune_epochs=1 train_steps_per_epoch=None eval_steps_per_epoch=None In\u00a0[3]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import cifair10, cifair100\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.metric import Accuracy\n\n\ndef pretrain(batch_size,\n             epochs,\n             model_dir=tempfile.mkdtemp(),\n             train_steps_per_epoch=None,\n             eval_steps_per_epoch=None):\n    train_data, eval_data = cifair100.load_data()\n    pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        batch_size=batch_size,\n        ops=[\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n            PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n            CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n            ChannelTranspose(inputs=\"x\", outputs=\"x\")\n        ])\n    model = fe.build(\n        model_fn=lambda: ViTModel(num_classes=100,\n                                  image_size=32,\n                                  patch_size=4,\n                                  num_layers=6,\n                                  num_channels=3,\n                                  em_dim=256,\n                                  num_heads=8,\n                                  ff_dim=512),\n        optimizer_fn=lambda x: torch.optim.SGD(x, lr=0.01, momentum=0.9, weight_decay=1e-4))\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\")\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch,\n                             log_steps=0)\n    estimator.fit(warmup=False)\n    return model\n</pre> import tempfile  import fastestimator as fe from fastestimator.dataset.data import cifair10, cifair100 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.metric import Accuracy   def pretrain(batch_size,              epochs,              model_dir=tempfile.mkdtemp(),              train_steps_per_epoch=None,              eval_steps_per_epoch=None):     train_data, eval_data = cifair100.load_data()     pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         batch_size=batch_size,         ops=[             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),             PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),             RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),             Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),             CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),             ChannelTranspose(inputs=\"x\", outputs=\"x\")         ])     model = fe.build(         model_fn=lambda: ViTModel(num_classes=100,                                   image_size=32,                                   patch_size=4,                                   num_layers=6,                                   num_channels=3,                                   em_dim=256,                                   num_heads=8,                                   ff_dim=512),         optimizer_fn=lambda x: torch.optim.SGD(x, lr=0.01, momentum=0.9, weight_decay=1e-4))     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),         UpdateOp(model=model, loss_name=\"ce\")     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\")     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch,                              log_steps=0)     estimator.fit(warmup=False)     return model In\u00a0[4]: Copied! <pre>pretrained_model = pretrain(batch_size=batch_size,\n                            epochs=pretrain_epochs,\n                            train_steps_per_epoch=train_steps_per_epoch,\n                            eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> pretrained_model = pretrain(batch_size=batch_size,                             epochs=pretrain_epochs,                             train_steps_per_epoch=train_steps_per_epoch,                             eval_steps_per_epoch=eval_steps_per_epoch) <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 0; num_device: 1;\nFastEstimator-Eval: step: 391; epoch: 1; accuracy: 0.1088; ce: 3.8288884;\nFastEstimator-Eval: step: 782; epoch: 2; accuracy: 0.158; ce: 3.5546496;\nFastEstimator-Eval: step: 1173; epoch: 3; accuracy: 0.1876; ce: 3.3546138;\nFastEstimator-Eval: step: 1564; epoch: 4; accuracy: 0.225; ce: 3.1463547;\nFastEstimator-Eval: step: 1955; epoch: 5; accuracy: 0.2515; ce: 3.0236564;\nFastEstimator-Eval: step: 2346; epoch: 6; accuracy: 0.2781; ce: 2.8541021;\nFastEstimator-Eval: step: 2737; epoch: 7; accuracy: 0.2922; ce: 2.7996583;\nFastEstimator-Eval: step: 3128; epoch: 8; accuracy: 0.3128; ce: 2.6991034;\nFastEstimator-Eval: step: 3519; epoch: 9; accuracy: 0.3314; ce: 2.57633;\nFastEstimator-Eval: step: 3910; epoch: 10; accuracy: 0.3394; ce: 2.5583541;\nFastEstimator-Eval: step: 4301; epoch: 11; accuracy: 0.3635; ce: 2.4394403;\nFastEstimator-Eval: step: 4692; epoch: 12; accuracy: 0.3717; ce: 2.4280012;\nFastEstimator-Eval: step: 5083; epoch: 13; accuracy: 0.3845; ce: 2.3532598;\nFastEstimator-Eval: step: 5474; epoch: 14; accuracy: 0.3756; ce: 2.3746123;\nFastEstimator-Eval: step: 5865; epoch: 15; accuracy: 0.4079; ce: 2.2628024;\nFastEstimator-Eval: step: 6256; epoch: 16; accuracy: 0.4045; ce: 2.2397344;\nFastEstimator-Eval: step: 6647; epoch: 17; accuracy: 0.4175; ce: 2.183634;\nFastEstimator-Eval: step: 7038; epoch: 18; accuracy: 0.4167; ce: 2.209709;\nFastEstimator-Eval: step: 7429; epoch: 19; accuracy: 0.4339; ce: 2.1296408;\nFastEstimator-Eval: step: 7820; epoch: 20; accuracy: 0.4182; ce: 2.1953375;\nFastEstimator-Eval: step: 8211; epoch: 21; accuracy: 0.438; ce: 2.1236746;\nFastEstimator-Eval: step: 8602; epoch: 22; accuracy: 0.4438; ce: 2.092245;\nFastEstimator-Eval: step: 8993; epoch: 23; accuracy: 0.4559; ce: 2.0420241;\nFastEstimator-Eval: step: 9384; epoch: 24; accuracy: 0.461; ce: 2.021573;\nFastEstimator-Eval: step: 9775; epoch: 25; accuracy: 0.4577; ce: 2.0449996;\nFastEstimator-Eval: step: 10166; epoch: 26; accuracy: 0.4648; ce: 2.0265305;\nFastEstimator-Eval: step: 10557; epoch: 27; accuracy: 0.4609; ce: 2.0219545;\nFastEstimator-Eval: step: 10948; epoch: 28; accuracy: 0.4599; ce: 2.0249476;\nFastEstimator-Eval: step: 11339; epoch: 29; accuracy: 0.4799; ce: 1.958858;\nFastEstimator-Eval: step: 11730; epoch: 30; accuracy: 0.4651; ce: 2.0040777;\nFastEstimator-Eval: step: 12121; epoch: 31; accuracy: 0.4759; ce: 1.9787812;\nFastEstimator-Eval: step: 12512; epoch: 32; accuracy: 0.4815; ce: 1.9677575;\nFastEstimator-Eval: step: 12903; epoch: 33; accuracy: 0.4836; ce: 1.9488634;\nFastEstimator-Eval: step: 13294; epoch: 34; accuracy: 0.4698; ce: 2.0040216;\nFastEstimator-Eval: step: 13685; epoch: 35; accuracy: 0.4854; ce: 1.933885;\nFastEstimator-Eval: step: 14076; epoch: 36; accuracy: 0.4915; ce: 1.9364777;\nFastEstimator-Eval: step: 14467; epoch: 37; accuracy: 0.4872; ce: 1.9454862;\nFastEstimator-Eval: step: 14858; epoch: 38; accuracy: 0.4953; ce: 1.9281081;\nFastEstimator-Eval: step: 15249; epoch: 39; accuracy: 0.4987; ce: 1.8994861;\nFastEstimator-Eval: step: 15640; epoch: 40; accuracy: 0.4972; ce: 1.9311935;\nFastEstimator-Eval: step: 16031; epoch: 41; accuracy: 0.4999; ce: 1.9120353;\nFastEstimator-Eval: step: 16422; epoch: 42; accuracy: 0.4999; ce: 1.9262657;\nFastEstimator-Eval: step: 16813; epoch: 43; accuracy: 0.5003; ce: 1.9173524;\nFastEstimator-Eval: step: 17204; epoch: 44; accuracy: 0.5099; ce: 1.9153186;\nFastEstimator-Eval: step: 17595; epoch: 45; accuracy: 0.5064; ce: 1.9490457;\nFastEstimator-Eval: step: 17986; epoch: 46; accuracy: 0.4941; ce: 1.9536077;\nFastEstimator-Eval: step: 18377; epoch: 47; accuracy: 0.5109; ce: 1.9044245;\nFastEstimator-Eval: step: 18768; epoch: 48; accuracy: 0.5015; ce: 1.9598173;\nFastEstimator-Eval: step: 19159; epoch: 49; accuracy: 0.5036; ce: 1.9729359;\nFastEstimator-Eval: step: 19550; epoch: 50; accuracy: 0.5087; ce: 1.9391878;\nFastEstimator-Eval: step: 19941; epoch: 51; accuracy: 0.509; ce: 1.9359056;\nFastEstimator-Eval: step: 20332; epoch: 52; accuracy: 0.5055; ce: 1.9588828;\nFastEstimator-Eval: step: 20723; epoch: 53; accuracy: 0.5104; ce: 1.9606155;\nFastEstimator-Eval: step: 21114; epoch: 54; accuracy: 0.502; ce: 2.0006518;\nFastEstimator-Eval: step: 21505; epoch: 55; accuracy: 0.5102; ce: 1.9584115;\nFastEstimator-Eval: step: 21896; epoch: 56; accuracy: 0.5031; ce: 2.0014715;\nFastEstimator-Eval: step: 22287; epoch: 57; accuracy: 0.5135; ce: 1.9787788;\nFastEstimator-Eval: step: 22678; epoch: 58; accuracy: 0.5041; ce: 2.000715;\nFastEstimator-Eval: step: 23069; epoch: 59; accuracy: 0.509; ce: 2.0128548;\nFastEstimator-Eval: step: 23460; epoch: 60; accuracy: 0.5124; ce: 2.0124855;\nFastEstimator-Eval: step: 23851; epoch: 61; accuracy: 0.5126; ce: 2.0069928;\nFastEstimator-Eval: step: 24242; epoch: 62; accuracy: 0.5109; ce: 2.0440252;\nFastEstimator-Eval: step: 24633; epoch: 63; accuracy: 0.5176; ce: 2.0516953;\nFastEstimator-Eval: step: 25024; epoch: 64; accuracy: 0.513; ce: 2.0567915;\nFastEstimator-Eval: step: 25415; epoch: 65; accuracy: 0.5103; ce: 2.0795443;\nFastEstimator-Eval: step: 25806; epoch: 66; accuracy: 0.5038; ce: 2.1041098;\nFastEstimator-Eval: step: 26197; epoch: 67; accuracy: 0.5087; ce: 2.1095006;\nFastEstimator-Eval: step: 26588; epoch: 68; accuracy: 0.5121; ce: 2.1082811;\nFastEstimator-Eval: step: 26979; epoch: 69; accuracy: 0.5071; ce: 2.1289942;\nFastEstimator-Eval: step: 27370; epoch: 70; accuracy: 0.5192; ce: 2.1182418;\nFastEstimator-Eval: step: 27761; epoch: 71; accuracy: 0.5175; ce: 2.1154375;\nFastEstimator-Eval: step: 28152; epoch: 72; accuracy: 0.5139; ce: 2.1458533;\nFastEstimator-Eval: step: 28543; epoch: 73; accuracy: 0.5152; ce: 2.15533;\nFastEstimator-Eval: step: 28934; epoch: 74; accuracy: 0.5079; ce: 2.199765;\nFastEstimator-Eval: step: 29325; epoch: 75; accuracy: 0.5053; ce: 2.1914499;\nFastEstimator-Eval: step: 29716; epoch: 76; accuracy: 0.5072; ce: 2.2124186;\nFastEstimator-Eval: step: 30107; epoch: 77; accuracy: 0.5102; ce: 2.1962357;\nFastEstimator-Eval: step: 30498; epoch: 78; accuracy: 0.5134; ce: 2.2328248;\nFastEstimator-Eval: step: 30889; epoch: 79; accuracy: 0.5078; ce: 2.2428932;\nFastEstimator-Eval: step: 31280; epoch: 80; accuracy: 0.5101; ce: 2.278882;\nFastEstimator-Eval: step: 31671; epoch: 81; accuracy: 0.5121; ce: 2.2327974;\nFastEstimator-Eval: step: 32062; epoch: 82; accuracy: 0.5165; ce: 2.2351596;\nFastEstimator-Eval: step: 32453; epoch: 83; accuracy: 0.5146; ce: 2.252345;\nFastEstimator-Eval: step: 32844; epoch: 84; accuracy: 0.5176; ce: 2.265459;\nFastEstimator-Eval: step: 33235; epoch: 85; accuracy: 0.5035; ce: 2.3590689;\nFastEstimator-Eval: step: 33626; epoch: 86; accuracy: 0.5101; ce: 2.3089907;\nFastEstimator-Eval: step: 34017; epoch: 87; accuracy: 0.5125; ce: 2.3316317;\nFastEstimator-Eval: step: 34408; epoch: 88; accuracy: 0.5072; ce: 2.3564215;\nFastEstimator-Eval: step: 34799; epoch: 89; accuracy: 0.5128; ce: 2.310105;\nFastEstimator-Eval: step: 35190; epoch: 90; accuracy: 0.517; ce: 2.294423;\nFastEstimator-Eval: step: 35581; epoch: 91; accuracy: 0.5169; ce: 2.303369;\nFastEstimator-Eval: step: 35972; epoch: 92; accuracy: 0.5125; ce: 2.355053;\nFastEstimator-Eval: step: 36363; epoch: 93; accuracy: 0.5154; ce: 2.3520496;\nFastEstimator-Eval: step: 36754; epoch: 94; accuracy: 0.5081; ce: 2.377033;\nFastEstimator-Eval: step: 37145; epoch: 95; accuracy: 0.515; ce: 2.405619;\nFastEstimator-Eval: step: 37536; epoch: 96; accuracy: 0.5193; ce: 2.3753698;\nFastEstimator-Eval: step: 37927; epoch: 97; accuracy: 0.5184; ce: 2.3919399;\nFastEstimator-Eval: step: 38318; epoch: 98; accuracy: 0.5048; ce: 2.4691393;\nFastEstimator-Eval: step: 38709; epoch: 99; accuracy: 0.5226; ce: 2.3937373;\nFastEstimator-Eval: step: 39100; epoch: 100; accuracy: 0.5154; ce: 2.383983;\nFastEstimator-Finish: step: 39100; model_lr: 0.01; total_time: 2576.35 sec;\n</pre> In\u00a0[7]: Copied! <pre>def finetune(pretrained_model,\n             batch_size,\n             epochs,\n             model_dir=tempfile.mkdtemp(),\n             train_steps_per_epoch=None,\n             eval_steps_per_epoch=None):\n    train_data, eval_data = cifair10.load_data()\n    pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        batch_size=batch_size,\n        ops=[\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n            PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n            CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n            ChannelTranspose(inputs=\"x\", outputs=\"x\")\n        ])\n    model = fe.build(\n        model_fn=lambda: ViTModel(num_classes=100,\n                                  image_size=32,\n                                  patch_size=4,\n                                  num_layers=6,\n                                  num_channels=3,\n                                  em_dim=256,\n                                  num_heads=8,\n                                  ff_dim=512),\n        optimizer_fn=lambda x: torch.optim.SGD(x, lr=0.01, momentum=0.9, weight_decay=1e-4))\n    # load the encoder's weight\n    if hasattr(model, \"module\"):\n        model.module.vit_encoder.load_state_dict(pretrained_model.module.vit_encoder.state_dict())\n    else:\n        model.vit_encoder.load_state_dict(pretrained_model.vit_encoder.state_dict())\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\")\n    ]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             eval_steps_per_epoch=eval_steps_per_epoch)\n    estimator.fit(warmup=False)\n</pre> def finetune(pretrained_model,              batch_size,              epochs,              model_dir=tempfile.mkdtemp(),              train_steps_per_epoch=None,              eval_steps_per_epoch=None):     train_data, eval_data = cifair10.load_data()     pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         batch_size=batch_size,         ops=[             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),             PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),             RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),             Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),             CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),             ChannelTranspose(inputs=\"x\", outputs=\"x\")         ])     model = fe.build(         model_fn=lambda: ViTModel(num_classes=100,                                   image_size=32,                                   patch_size=4,                                   num_layers=6,                                   num_channels=3,                                   em_dim=256,                                   num_heads=8,                                   ff_dim=512),         optimizer_fn=lambda x: torch.optim.SGD(x, lr=0.01, momentum=0.9, weight_decay=1e-4))     # load the encoder's weight     if hasattr(model, \"module\"):         model.module.vit_encoder.load_state_dict(pretrained_model.module.vit_encoder.state_dict())     else:         model.vit_encoder.load_state_dict(pretrained_model.vit_encoder.state_dict())     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),         UpdateOp(model=model, loss_name=\"ce\")     ])     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\")     ]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              train_steps_per_epoch=train_steps_per_epoch,                              eval_steps_per_epoch=eval_steps_per_epoch)     estimator.fit(warmup=False) In\u00a0[8]: Copied! <pre>finetune(pretrained_model,\n         batch_size=batch_size,\n         epochs=finetune_epochs,\n         train_steps_per_epoch=train_steps_per_epoch,\n         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> finetune(pretrained_model,          batch_size=batch_size,          epochs=finetune_epochs,          train_steps_per_epoch=train_steps_per_epoch,          eval_steps_per_epoch=eval_steps_per_epoch) <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; ce: 4.801615;\nFastEstimator-Train: step: 100; ce: 1.0262994; steps/sec: 17.72;\nFastEstimator-Train: step: 200; ce: 0.74568576; steps/sec: 17.57;\nFastEstimator-Train: step: 300; ce: 0.7660386; steps/sec: 17.54;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 22.4 sec;\nFastEstimator-Eval: step: 391; epoch: 1; accuracy: 0.7426; ce: 0.7396317;\nFastEstimator-Finish: step: 391; model2_lr: 0.01; total_time: 25.4 sec;\n</pre> <p>With only one epoch of training, we are able to get 74% top-1 accuracy on the CIFAIR 10 test set.  Not bad huh?</p>"}, {"location": "apphub/image_classification/vit/vit.html#image-classification-using-vision-transformer", "title": "Image Classification Using Vision Transformer\u00b6", "text": "<p>Vision Transformer (ViT) is a new alternative to Convolution Neural Networks (CNNs) in the field of computer vision. The idea of ViT was inspired from the success of the Transformer and BERT architectures in NLP applications. In this example, we will implement a ViT in PyTorch and showcase how to pre-train a ViT and then fine-tune it on a downstream task for good results with minimal downstream training time.</p>"}, {"location": "apphub/image_classification/vit/vit.html#vit-model", "title": "ViT Model\u00b6", "text": "<p>The ViT model is almost the same as the original Transformer except for the following differences:</p> <ol> <li>Input image is broken down into small patches, which are used as sequences similar to language. The patching and embedding are implemented by a Convolution2D operation in the <code>patch_embedding</code>.</li> <li>Different from original Transformer, the positional embedding is now a trainable parameter.</li> <li>Similar to BERT, a <code>CLS</code> token is added before the patch sequence. But in contrast to BERT, the value of the <code>CLS</code> token is trainable.</li> <li>After the Transformer encoding, only the embedding corresponding to the <code>CLS</code> token will be used as feature for the classification layer.</li> </ol>"}, {"location": "apphub/image_classification/vit/vit.html#upstream-pre-training", "title": "Upstream Pre-training\u00b6", "text": "<p>We will use CIFAIR 100 as our upstream dataset. The data preprocessing and augmentation is the standard Padded Crop + Dropout used in this example.</p>"}, {"location": "apphub/image_classification/vit/vit.html#start-pre-training", "title": "Start Pre-training\u00b6", "text": "<p>Let's train the ViT model for 100 epochs, and get the pre-trained weight. This would take ~40 minutes on single GTX 1080 TI GPU.</p> <p>Here we are only training a mini version of the actual ViT model, and the CIFAR100 performance after 100 epochs is similar to the 55% top-1 performance reported in the community. However, training the official <code>ViTModel</code> model with its original parameters on the JFT-300M dataset would produce much better encoder weights at the cost of a much longer training time. The paper used this strategy to reach near 81% ImageNet downstream top-1 accuracy.</p>"}, {"location": "apphub/image_classification/vit/vit.html#downstream-fine-tuning", "title": "Downstream Fine-tuning\u00b6", "text": "<p>A general rule-of-thumb to ensure successful downstream fine-tuning is to choose a downstream task with less variety and complexity than the upstream training. In this example, given that we used CIFAIR100 as our upstream task, a good candidate for the downstream dataset is CIFAIR10.  The official implementation mapped this practice to a larger scale, using JFT-300M as their upstream task and then ImageNet as their downstream task.</p> <p>Given the similarity between our downstream and upstream datasets, the fine-tuning configuration is almost the same as before.</p>"}, {"location": "apphub/image_classification/vit/vit.html#start-the-fine-tuning", "title": "Start the Fine-tuning\u00b6", "text": "<p>The downstream ViT is re-using the ViT encoder pre-trained on the CIFAR100 dataset. To illustrate the effect of using the pre-trained encoder, we will only train the downstream task for a single epoch.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom typing import Any, Dict, Tuple\nfrom fastestimator.util import BatchDisplay, GridDisplay\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import tempfile import matplotlib.pyplot as plt from typing import Any, Dict, Tuple from fastestimator.util import BatchDisplay, GridDisplay In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 20\nbatch_size = 100\ntrain_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 20 batch_size = 100 train_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\n\ntrain_data, test_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data  train_data, test_data = load_data() In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1) \n        Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1] \n        Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value\n    ])\n</pre> from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax  pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1)          Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1]          Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"] print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) <pre>the pipeline input data size: (100, 28, 28)\nthe pipeline output data size: (100, 28, 28, 1)\n</pre> <p>Let's select 5 samples and visualize the differences between the <code>Pipeline</code> input and output.</p> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nGridDisplay([BatchDisplay(image=data_xin[0:sample_num], title=\"Pipeline Input\"), \n             BatchDisplay(image=data_xout[0:sample_num], title=\"Pipeline Output\")]).show()\n</pre> sample_num = 5  GridDisplay([BatchDisplay(image=data_xin[0:sample_num], title=\"Pipeline Input\"),               BatchDisplay(image=data_xout[0:sample_num], title=\"Pipeline Output\")]).show() In\u00a0[7]: Copied! <pre>LATENT_DIM = 2\n\ndef encoder_net():\n    infer_model = tf.keras.Sequential()\n    infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n    infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Flatten())\n    infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))\n    return infer_model\n\n\ndef decoder_net():\n    generative_model = tf.keras.Sequential()\n    generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))\n    generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))\n    generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))\n    return generative_model\n\nencode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\")\ndecode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\")\n</pre> LATENT_DIM = 2  def encoder_net():     infer_model = tf.keras.Sequential()     infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))     infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Flatten())     infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))     return infer_model   def decoder_net():     generative_model = tf.keras.Sequential()     generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))     generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))     generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))     return generative_model  encode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\") decode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\") <pre>2022-05-17 22:36:32.431886: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-17 22:36:33.263045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n</pre> In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass SplitOp(TensorOp):\n\"\"\"To split the infer net output into two \"\"\"\n    def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n        mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)\n        return mean, logvar\n</pre> from fastestimator.op.tensorop import TensorOp  class SplitOp(TensorOp):     \"\"\"To split the infer net output into two \"\"\"     def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:         mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)         return mean, logvar In\u00a0[9]: Copied! <pre>class ReparameterizeOp(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:\n        mean, logvar = data\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n</pre> class ReparameterizeOp(TensorOp):     def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:         mean, logvar = data         eps = tf.random.normal(shape=mean.shape)         return eps * tf.exp(logvar * .5) + mean In\u00a0[10]: Copied! <pre>import math\n\nclass CVAELoss(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:\n        cross_ent_mean, mean, logvar, z = data   \n        \n        cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches \n                                                         # make it total cross entropy over pixels \n        logpz = self._log_normal_pdf(z, 0., 0.)\n        logqz_x = self._log_normal_pdf(z, mean, logvar)\n        total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)\n\n        return total_loss\n    \n    @staticmethod\n    def _log_normal_pdf(sample, mean, logvar, raxis=1):\n        log2pi = tf.math.log(2. * tf.constant(math.pi))\n        return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n</pre> import math  class CVAELoss(TensorOp):     def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:         cross_ent_mean, mean, logvar, z = data                     cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches                                                           # make it total cross entropy over pixels          logpz = self._log_normal_pdf(z, 0., 0.)         logqz_x = self._log_normal_pdf(z, mean, logvar)         total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)          return total_loss          @staticmethod     def _log_normal_pdf(sample, mean, logvar, raxis=1):         log2pi = tf.math.log(2. * tf.constant(math.pi))         return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis) In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),\n    SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),\n    ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"), \n    ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),\n    CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"), \n    CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),\n    UpdateOp(model=encode_model, loss_name=\"loss\"),\n    UpdateOp(model=decode_model, loss_name=\"loss\"),\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),     SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),     ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"),      ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),     CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"),      CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),     UpdateOp(model=encode_model, loss_name=\"loss\"),     UpdateOp(model=decode_model, loss_name=\"loss\"), ]) <pre>2022-05-17 22:36:34.215892: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[12]: Copied! <pre>from fastestimator.trace.io import ModelSaver\n\ntraces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs), \n          ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         log_steps=600)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.trace.io import ModelSaver  traces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs),            ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          log_steps=600)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'x' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Warn: the key 'y' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>2022-05-17 22:36:42.024562: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n2022-05-17 22:36:43.959941: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Start: step: 1; logging_interval: 600; num_device: 1;\nFastEstimator-Train: step: 1; loss: 544.2965;\nFastEstimator-Train: step: 600; loss: 172.18253; steps/sec: 184.56;\nFastEstimator-Train: step: 600; epoch: 1; epoch_time: 9.7 sec;\nFastEstimator-Train: step: 1200; loss: 161.00087; steps/sec: 71.45;\nFastEstimator-Train: step: 1200; epoch: 2; epoch_time: 8.39 sec;\nFastEstimator-Train: step: 1800; loss: 152.92236; steps/sec: 72.24;\nFastEstimator-Train: step: 1800; epoch: 3; epoch_time: 8.36 sec;\nFastEstimator-Train: step: 2400; loss: 161.80312; steps/sec: 77.87;\nFastEstimator-Train: step: 2400; epoch: 4; epoch_time: 7.46 sec;\nFastEstimator-Train: step: 3000; loss: 153.76183; steps/sec: 101.51;\nFastEstimator-Train: step: 3000; epoch: 5; epoch_time: 5.93 sec;\nFastEstimator-Train: step: 3600; loss: 146.66841; steps/sec: 99.31;\nFastEstimator-Train: step: 3600; epoch: 6; epoch_time: 6.03 sec;\nFastEstimator-Train: step: 4200; loss: 166.19159; steps/sec: 104.04;\nFastEstimator-Train: step: 4200; epoch: 7; epoch_time: 5.76 sec;\nFastEstimator-Train: step: 4800; loss: 153.33478; steps/sec: 105.05;\nFastEstimator-Train: step: 4800; epoch: 8; epoch_time: 5.73 sec;\nFastEstimator-Train: step: 5400; loss: 158.04784; steps/sec: 103.01;\nFastEstimator-Train: step: 5400; epoch: 9; epoch_time: 5.82 sec;\nFastEstimator-Train: step: 6000; loss: 150.19344; steps/sec: 102.04;\nFastEstimator-Train: step: 6000; epoch: 10; epoch_time: 6.02 sec;\nFastEstimator-Train: step: 6600; loss: 151.40405; steps/sec: 98.36;\nFastEstimator-Train: step: 6600; epoch: 11; epoch_time: 5.95 sec;\nFastEstimator-Train: step: 7200; loss: 147.26006; steps/sec: 98.58;\nFastEstimator-Train: step: 7200; epoch: 12; epoch_time: 6.1 sec;\nFastEstimator-Train: step: 7800; loss: 156.42514; steps/sec: 96.8;\nFastEstimator-Train: step: 7800; epoch: 13; epoch_time: 6.21 sec;\nFastEstimator-Train: step: 8400; loss: 152.11465; steps/sec: 101.98;\nFastEstimator-Train: step: 8400; epoch: 14; epoch_time: 5.86 sec;\nFastEstimator-Train: step: 9000; loss: 155.98889; steps/sec: 102.13;\nFastEstimator-Train: step: 9000; epoch: 15; epoch_time: 5.9 sec;\nFastEstimator-Train: step: 9600; loss: 149.54395; steps/sec: 100.31;\nFastEstimator-Train: step: 9600; epoch: 16; epoch_time: 5.98 sec;\nFastEstimator-Train: step: 10200; loss: 143.47643; steps/sec: 97.57;\nFastEstimator-Train: step: 10200; epoch: 17; epoch_time: 6.15 sec;\nFastEstimator-Train: step: 10800; loss: 146.95685; steps/sec: 98.12;\nFastEstimator-Train: step: 10800; epoch: 18; epoch_time: 6.1 sec;\nFastEstimator-Train: step: 11400; loss: 142.988; steps/sec: 99.65;\nFastEstimator-Train: step: 11400; epoch: 19; epoch_time: 6.03 sec;\nFastEstimator-Train: step: 12000; loss: 151.13768; steps/sec: 99.97;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpln8yvy8_/encoder_epoch_20.h5\nFastEstimator-ModelSaver: Saved model to /tmp/tmpln8yvy8_/decoder_epoch_20.h5\nFastEstimator-Train: step: 12000; epoch: 20; epoch_time: 6.0 sec;\nFastEstimator-Finish: step: 12000; decoder_lr: 0.001; encoder_lr: 0.001; total_time: 129.78 sec;\n</pre> In\u00a0[13]: Copied! <pre>scale = 2.0 \nn = 20\nimg_size = 28\nfigure = np.zeros((img_size * n, img_size * n))\n\ngrid_x = np.linspace(-scale, scale, n)\ngrid_y = np.linspace(-scale, scale, n)\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),\n])\n\nfor i, xi in enumerate(grid_x):\n    for j, yi in enumerate(grid_y):\n        data = {\"z\": np.array([[xi, yi]])}\n        data = network.transform(data, mode=\"infer\")\n        figure[i * img_size : (i + 1) * img_size, \n               j * img_size : (j + 1) * img_size] = data[\"x_logit\"].numpy().squeeze(axis=(0,3))\n\nplt.figure(figsize=(14, 14))\npixel_range = np.arange(img_size//2, n * img_size + img_size//2, img_size)\nplt.xticks(pixel_range, np.round(grid_x, 1))\nplt.yticks(pixel_range, np.round(grid_y, 1))\nplt.xlabel(\"z [0]\")\nplt.ylabel(\"z [1]\")\nplt.imshow(figure, cmap=\"gray\")\nplt.show()\n</pre> scale = 2.0  n = 20 img_size = 28 figure = np.zeros((img_size * n, img_size * n))  grid_x = np.linspace(-scale, scale, n) grid_y = np.linspace(-scale, scale, n)  network = fe.Network(ops=[     ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"), ])  for i, xi in enumerate(grid_x):     for j, yi in enumerate(grid_y):         data = {\"z\": np.array([[xi, yi]])}         data = network.transform(data, mode=\"infer\")         figure[i * img_size : (i + 1) * img_size,                 j * img_size : (j + 1) * img_size] = data[\"x_logit\"].numpy().squeeze(axis=(0,3))  plt.figure(figsize=(14, 14)) pixel_range = np.arange(img_size//2, n * img_size + img_size//2, img_size) plt.xticks(pixel_range, np.round(grid_x, 1)) plt.yticks(pixel_range, np.round(grid_y, 1)) plt.xlabel(\"z [0]\") plt.ylabel(\"z [1]\") plt.imshow(figure, cmap=\"gray\") plt.show()"}, {"location": "apphub/image_generation/cvae/cvae.html#convolutional-variational-autoencoder-using-the-mnist-dataset-tensorflow-backend", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#introduction-to-cvae", "title": "Introduction to CVAE\u00b6", "text": "<p>CVAEs are Convolutional Variational Autoencoders. They are composed of two models using convolutions: an encoder to cast the input into a latent dimension, and a decoder that will move data from the latent dimension back to the input space. The figure below illustrates the main idea behind CVAEs.</p> <p>In this example, we will use a CVAE to generate data similar to the MNIST dataset using the TensorFlow backend. All training details including model structure, data preprocessing, loss calculation, etc. come from the TensorFlow CVAE tutorial </p>"}, {"location": "apphub/image_generation/cvae/cvae.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation dataset and prepare FastEstimator's data <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>Let's use a FastEstimator API to load the MNIST dataset:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#set-up-the-preprocessing-pipline", "title": "Set up the preprocessing <code>Pipline</code>\u00b6", "text": "<p>In this example, the data preprocessing steps include expanding image dimension, normalizing the image pixel values to the range [0, 1], and binarizing pixel values. We set up these processing steps using <code>Ops</code>, while also defining the data source and batch size for the <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch of data for this purpose:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Both of our models' definitions are implemented in TensorFlow and instantiated by calling <code>fe.build</code> (which also associates the model with specific optimizers).</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops", "title": "Customize <code>Ops</code>\u00b6", "text": "<p><code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update units, or even the model itself. Some <code>Ops</code> such as cross entropy are pre-defined in FastEstimator, but for any logic that is not there yet, users need to define their own <code>Ops</code>. Please keep all custom <code>Ops</code> backend-consistent with your model backend. In this case all <code>Ops</code> need to be implemented in TensorFlow since our model is built from Tensorflow.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-splitop", "title": "Customize Ops - SplitOp\u00b6", "text": "<p>Because the encoder output contains both mean and log of variance, we need to split them into two outputs:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-reparameterizeop", "title": "Customize Ops - ReparameterizeOp\u00b6", "text": "<p>In this example case, the input to the decoder is a random sample from a normal distribution whose mean and variation are the output of the encoder. We are going to build an <code>Op</code> called \"ReparameterizeOp\" to accomplish this:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-cvaeloss", "title": "Customize Ops - CVAELoss\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect all models and <code>Ops</code> together into a <code>Network</code></p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to compile the <code>Network</code> and <code>Pipeline</code> and indicate in <code>traces</code> that we want to save the best models. We can then use <code>estimator.fit()</code> to start the training process:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the model is trained, we will try to run our models to generate images by sampling from the latent space:</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html", "title": "Horse to Zebra Unpaired Image Translation with CycleGAN in FastEstimator", "text": "<p>This notebook demonstrates how to perform an unpaired image to image translation using CycleGAN in FastEstimator. The details of the method is found in Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. We will specifically look at the problem of translating horse images to zebra images.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn.init import normal_\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.util import ImageDisplay, GridDisplay\n</pre> import tempfile  import numpy as np import torch import torch.nn as nn from torch.nn.init import normal_  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.util import ImageDisplay, GridDisplay In\u00a0[2]: parameters Copied! <pre>#Parameters\nepochs = 200\nbatch_size = 1\ntrain_steps_per_epoch = None\nsave_dir=tempfile.mkdtemp()\nweight = 10.0\ndata_dir=None\n</pre> #Parameters epochs = 200 batch_size = 1 train_steps_per_epoch = None save_dir=tempfile.mkdtemp() weight = 10.0 data_dir=None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.horse2zebra import load_data\ntrain_data, test_data = load_data(batch_size=batch_size, root_dir=data_dir)\n</pre> from fastestimator.dataset.data.horse2zebra import load_data train_data, test_data = load_data(batch_size=batch_size, root_dir=data_dir) <p>Let's create the pipeline. As, batch_size must be <code>None</code> when BatchDataset is being used, we will not provide the batch_size argument.</p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, RandomCrop, Resize\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    ops=[\n        ReadImage(inputs=[\"A\", \"B\"], outputs=[\"A\", \"B\"]),\n        Normalize(inputs=[\"A\", \"B\"], outputs=[\"real_A\", \"real_B\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n        Resize(height=286, width=286, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),\n        RandomCrop(height=256, width=256, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),\n        Resize(height=286, width=286, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),\n        RandomCrop(height=256, width=256, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"real_A\", image_out=\"real_A\", mode=\"train\")),\n        Sometimes(HorizontalFlip(image_in=\"real_B\", image_out=\"real_B\", mode=\"train\")),\n        ChannelTranspose(inputs=[\"real_A\", \"real_B\"], outputs=[\"real_A\", \"real_B\"]),\n        Delete(keys=[\"A\", \"B\"])\n    ])\n</pre> from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, RandomCrop, Resize from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  pipeline = fe.Pipeline(     train_data=train_data,     ops=[         ReadImage(inputs=[\"A\", \"B\"], outputs=[\"A\", \"B\"]),         Normalize(inputs=[\"A\", \"B\"], outputs=[\"real_A\", \"real_B\"], mean=1.0, std=1.0, max_pixel_value=127.5),         Resize(height=286, width=286, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),         RandomCrop(height=256, width=256, image_in=\"real_A\", image_out=\"real_A\", mode=\"train\"),         Resize(height=286, width=286, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),         RandomCrop(height=256, width=256, image_in=\"real_B\", image_out=\"real_B\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"real_A\", image_out=\"real_A\", mode=\"train\")),         Sometimes(HorizontalFlip(image_in=\"real_B\", image_out=\"real_B\", mode=\"train\")),         ChannelTranspose(inputs=[\"real_A\", \"real_B\"], outputs=[\"real_A\", \"real_B\"]),         Delete(keys=[\"A\", \"B\"])     ]) <p>We can visualize sample images from the <code>pipeline</code> using <code>get_results</code> method.</p> In\u00a0[5]: Copied! <pre>def Minmax(img):\n    img_max = np.max(img)\n    img_min = np.min(img)\n    img = (img - img_min)/max((img_max - img_min), 1e-7)\n    img = (img*255).astype(np.uint8)\n    return img\n</pre> def Minmax(img):     img_max = np.max(img)     img_min = np.min(img)     img = (img - img_min)/max((img_max - img_min), 1e-7)     img = (img*255).astype(np.uint8)     return img In\u00a0[6]: Copied! <pre>sample_batch = pipeline.get_results()\nhorse_img = sample_batch[\"real_A\"][0]\nhorse_img = np.transpose(horse_img.numpy(), (1, 2, 0))\nhorse_img = np.expand_dims(Minmax(horse_img), 0)\n\nzebra_img = sample_batch[\"real_B\"][0]\nzebra_img = np.transpose(zebra_img.numpy(), (1, 2, 0))\nzebra_img = np.expand_dims(Minmax(zebra_img), 0)\n\nGridDisplay([ImageDisplay(image=horse_img[0], title=\"Horse\"),\n             ImageDisplay(image=zebra_img[0], title=\"Zebra\")\n            ]).show()\n</pre> sample_batch = pipeline.get_results() horse_img = sample_batch[\"real_A\"][0] horse_img = np.transpose(horse_img.numpy(), (1, 2, 0)) horse_img = np.expand_dims(Minmax(horse_img), 0)  zebra_img = sample_batch[\"real_B\"][0] zebra_img = np.transpose(zebra_img.numpy(), (1, 2, 0)) zebra_img = np.expand_dims(Minmax(zebra_img), 0)  GridDisplay([ImageDisplay(image=horse_img[0], title=\"Horse\"),              ImageDisplay(image=zebra_img[0], title=\"Zebra\")             ]).show() <p>In CycleGAN, there are 2 generators and 2 discriminators being trained.</p> <ul> <li>Generator <code>g_AtoB</code> learns to map horse images to zebra images</li> <li>Generator <code>g_BtoA</code> learns to map zebra images to horse images</li> <li>Discriminator <code>d_A</code> learns to differentiate between real hores images and fake horse images produced by <code>g_BtoA</code></li> <li>Discriminator <code>d_B</code> learns to differentiate between image zebra and fake zebra images produced by <code>g_AtoB</code></li> </ul> <p>The architecture of generator is a modified resnet, and the architecture of discriminator is a PatchGAN.</p> In\u00a0[7]: Copied! <pre>class ResidualBlock(nn.Module):\n\"\"\"Residual block architecture\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super().__init__()\n        self.layers = nn.Sequential(nn.ReflectionPad2d(1),\n                                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size),\n                                    nn.InstanceNorm2d(out_channels),\n                                    nn.ReLU(inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size),\n                                    nn.InstanceNorm2d(out_channels))\n\n        for layer in self.layers:\n            if isinstance(layer, nn.Conv2d):\n                normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x_out = self.layers(x)\n        x_out = x_out + x\n        return x_out\n\n\nclass Discriminator(nn.Module):\n\"\"\"Discriminator network architecture\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n                                    nn.InstanceNorm2d(128),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n                                    nn.InstanceNorm2d(256),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(256, 512, kernel_size=4, stride=1),\n                                    nn.InstanceNorm2d(512),\n                                    nn.LeakyReLU(0.2, inplace=True),\n                                    nn.ReflectionPad2d(1),\n                                    nn.Conv2d(512, 1, kernel_size=4, stride=1))\n\n        for layer in self.layers:\n            if isinstance(layer, nn.Conv2d):\n                normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\n\nclass Generator(nn.Module):\n\"\"\"Generator network architecture\"\"\"\n    def __init__(self, num_blocks=9):\n        super().__init__()\n        self.layers1 = nn.Sequential(nn.ReflectionPad2d(3),\n                                     nn.Conv2d(3, 64, kernel_size=7, stride=1),\n                                     nn.InstanceNorm2d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n                                     nn.InstanceNorm2d(128),\n                                     nn.ReLU(inplace=True),\n                                     nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n                                     nn.InstanceNorm2d(256),\n                                     nn.ReLU(inplace=True))\n        self.resblocks = nn.Sequential(*[ResidualBlock(256, 256) for i in range(num_blocks)])\n        self.layers2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n                                     nn.InstanceNorm2d(128),\n                                     nn.ReLU(inplace=True),\n                                     nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n                                     nn.InstanceNorm2d(64),\n                                     nn.ReLU(inplace=True),\n                                     nn.ReflectionPad2d(3),\n                                     nn.Conv2d(64, 3, kernel_size=7, stride=1))\n\n        for block in [self.layers1, self.layers2]:\n            for layer in block:\n                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n                    normal_(layer.weight.data, mean=0, std=0.02)\n\n    def forward(self, x):\n        x = self.layers1(x)\n        x = self.resblocks(x)\n        x = self.layers2(x)\n        x = torch.tanh(x)\n        return x\n</pre> class ResidualBlock(nn.Module):     \"\"\"Residual block architecture\"\"\"     def __init__(self, in_channels, out_channels, kernel_size=3):         super().__init__()         self.layers = nn.Sequential(nn.ReflectionPad2d(1),                                     nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size),                                     nn.InstanceNorm2d(out_channels),                                     nn.ReLU(inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size),                                     nn.InstanceNorm2d(out_channels))          for layer in self.layers:             if isinstance(layer, nn.Conv2d):                 normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x_out = self.layers(x)         x_out = x_out + x         return x_out   class Discriminator(nn.Module):     \"\"\"Discriminator network architecture\"\"\"     def __init__(self):         super().__init__()         self.layers = nn.Sequential(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),                                     nn.InstanceNorm2d(128),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),                                     nn.InstanceNorm2d(256),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(256, 512, kernel_size=4, stride=1),                                     nn.InstanceNorm2d(512),                                     nn.LeakyReLU(0.2, inplace=True),                                     nn.ReflectionPad2d(1),                                     nn.Conv2d(512, 1, kernel_size=4, stride=1))          for layer in self.layers:             if isinstance(layer, nn.Conv2d):                 normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x = self.layers(x)         return x   class Generator(nn.Module):     \"\"\"Generator network architecture\"\"\"     def __init__(self, num_blocks=9):         super().__init__()         self.layers1 = nn.Sequential(nn.ReflectionPad2d(3),                                      nn.Conv2d(3, 64, kernel_size=7, stride=1),                                      nn.InstanceNorm2d(64),                                      nn.ReLU(inplace=True),                                      nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),                                      nn.InstanceNorm2d(128),                                      nn.ReLU(inplace=True),                                      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),                                      nn.InstanceNorm2d(256),                                      nn.ReLU(inplace=True))         self.resblocks = nn.Sequential(*[ResidualBlock(256, 256) for i in range(num_blocks)])         self.layers2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),                                      nn.InstanceNorm2d(128),                                      nn.ReLU(inplace=True),                                      nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),                                      nn.InstanceNorm2d(64),                                      nn.ReLU(inplace=True),                                      nn.ReflectionPad2d(3),                                      nn.Conv2d(64, 3, kernel_size=7, stride=1))          for block in [self.layers1, self.layers2]:             for layer in block:                 if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):                     normal_(layer.weight.data, mean=0, std=0.02)      def forward(self, x):         x = self.layers1(x)         x = self.resblocks(x)         x = self.layers2(x)         x = torch.tanh(x)         return x In\u00a0[8]: Copied! <pre>g_AtoB = fe.build(model_fn=Generator,\n                  model_name=\"g_AtoB\",\n                  optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\ng_BtoA = fe.build(model_fn=Generator,\n                  model_name=\"g_BtoA\",\n                  optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\nd_A = fe.build(model_fn=Discriminator,\n               model_name=\"d_A\",\n               optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\nd_B = fe.build(model_fn=Discriminator,\n               model_name=\"d_B\",\n               optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999)))\n</pre> g_AtoB = fe.build(model_fn=Generator,                   model_name=\"g_AtoB\",                   optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) g_BtoA = fe.build(model_fn=Generator,                   model_name=\"g_BtoA\",                   optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) d_A = fe.build(model_fn=Discriminator,                model_name=\"d_A\",                optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) d_B = fe.build(model_fn=Discriminator,                model_name=\"d_B\",                optimizer_fn=lambda x: torch.optim.Adam(x, lr=2e-4, betas=(0.5, 0.999))) <p>Because horse images and zebra images are unpaired, the loss of generator is quite complex. The generator's loss is composed of three terms: * adversarial * cycle-consistency * identity. The cycle-consistency term and identity term are weighted by a parameter <code>LAMBDA</code>. In the paper the authors used 10 for <code>LAMBDA</code>.</p> <p>Let's consider computing the loss for <code>g_AtoB</code> which translates horses to zebras.</p> <ol> <li>Adversarial term that is computed as binary cross entropy between ones and <code>d_A</code>'s prediction on the translated images</li> <li>Cycle consistency term is computed with mean absolute error between original horse images and the cycled horse images that are translated forward by <code>g_AtoB</code> and then backward by <code>g_BtoA</code>.</li> <li>Identity term that is computed with the mean absolute error between original zebra and the output of <code>g_AtoB</code> on these images.</li> </ol> <p>The discriminator's loss is the standard adversarial loss that is computed as binary cross entropy between:</p> <ul> <li>Ones and real images</li> <li>Zeros and fake images</li> </ul> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass GLoss(TensorOp):\n\"\"\"TensorOp to compute generator loss\"\"\"\n    def __init__(self, inputs, weight, device, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_fn = nn.MSELoss(reduction=\"none\")\n        self.LAMBDA = weight\n        self.device = device\n        self.average_loss = average_loss\n\n    def _adversarial_loss(self, fake_img):\n        return torch.mean(self.loss_fn(fake_img, torch.ones_like(fake_img, device=self.device)), dim=(2, 3))\n\n    def _identity_loss(self, real_img, same_img):\n        return 0.5 * self.LAMBDA * torch.mean(torch.abs(real_img - same_img), dim=(1, 2, 3))\n\n    def _cycle_loss(self, real_img, cycled_img):\n        return self.LAMBDA * torch.mean(torch.abs(real_img - cycled_img), dim=(1, 2, 3))\n\n    def forward(self, data, state):\n        real_img, fake_img, cycled_img, same_img = data\n        total_loss = self._adversarial_loss(fake_img) + self._identity_loss(real_img, same_img) + self._cycle_loss(\n            real_img, cycled_img)\n\n        if self.average_loss:\n            total_loss = reduce_mean(total_loss)\n\n        return total_loss\n\n\nclass DLoss(TensorOp):\n\"\"\"TensorOp to compute discriminator loss\"\"\"\n    def __init__(self, inputs, device, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_fn = nn.MSELoss(reduction=\"none\")\n        self.device = device\n        self.average_loss = average_loss\n\n    def forward(self, data, state):\n        real_img, fake_img = data\n        real_img_loss = torch.mean(self.loss_fn(real_img, torch.ones_like(real_img, device=self.device)), dim=(2, 3))\n        fake_img_loss = torch.mean(self.loss_fn(fake_img, torch.zeros_like(real_img, device=self.device)), dim=(2, 3))\n        total_loss = real_img_loss + fake_img_loss\n\n        if self.average_loss:\n            total_loss = reduce_mean(total_loss)\n\n        return 0.5 * total_loss\n</pre> from fastestimator.op.tensorop import TensorOp  class GLoss(TensorOp):     \"\"\"TensorOp to compute generator loss\"\"\"     def __init__(self, inputs, weight, device, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_fn = nn.MSELoss(reduction=\"none\")         self.LAMBDA = weight         self.device = device         self.average_loss = average_loss      def _adversarial_loss(self, fake_img):         return torch.mean(self.loss_fn(fake_img, torch.ones_like(fake_img, device=self.device)), dim=(2, 3))      def _identity_loss(self, real_img, same_img):         return 0.5 * self.LAMBDA * torch.mean(torch.abs(real_img - same_img), dim=(1, 2, 3))      def _cycle_loss(self, real_img, cycled_img):         return self.LAMBDA * torch.mean(torch.abs(real_img - cycled_img), dim=(1, 2, 3))      def forward(self, data, state):         real_img, fake_img, cycled_img, same_img = data         total_loss = self._adversarial_loss(fake_img) + self._identity_loss(real_img, same_img) + self._cycle_loss(             real_img, cycled_img)          if self.average_loss:             total_loss = reduce_mean(total_loss)          return total_loss   class DLoss(TensorOp):     \"\"\"TensorOp to compute discriminator loss\"\"\"     def __init__(self, inputs, device, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_fn = nn.MSELoss(reduction=\"none\")         self.device = device         self.average_loss = average_loss      def forward(self, data, state):         real_img, fake_img = data         real_img_loss = torch.mean(self.loss_fn(real_img, torch.ones_like(real_img, device=self.device)), dim=(2, 3))         fake_img_loss = torch.mean(self.loss_fn(fake_img, torch.zeros_like(real_img, device=self.device)), dim=(2, 3))         total_loss = real_img_loss + fake_img_loss          if self.average_loss:             total_loss = reduce_mean(total_loss)          return 0.5 * total_loss <p>We implement an image buffer as a <code>TensorOp</code> which stores the previous images produced by the generators to updated the discriminators as outlined in Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.</p> In\u00a0[10]: Copied! <pre>class Buffer(TensorOp):\n    def __init__(self, image_in=None, image_out=None, mode=None, buffer_size=50):\n        super().__init__(inputs=image_in, outputs=image_out, mode=mode)\n        self.buffer_size = buffer_size\n        self.num_imgs = 0\n        self.image_buffer = []\n\n    def forward(self, data, state):\n        output = []\n        for image in data:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs &lt; self.buffer_size:\n                self.image_buffer.append(image)\n                output.append(image)\n                self.num_imgs += 1\n            else:\n                if np.random.uniform() &gt; 0.5:\n                    idx = np.random.randint(self.buffer_size)\n                    temp = self.image_buffer[idx].clone()\n                    self.image_buffer[idx] = image\n                    output.append(temp)\n                else:\n                    output.append(image)\n\n        output = torch.cat(output, 0)\n        return output\n</pre> class Buffer(TensorOp):     def __init__(self, image_in=None, image_out=None, mode=None, buffer_size=50):         super().__init__(inputs=image_in, outputs=image_out, mode=mode)         self.buffer_size = buffer_size         self.num_imgs = 0         self.image_buffer = []      def forward(self, data, state):         output = []         for image in data:             image = torch.unsqueeze(image.data, 0)             if self.num_imgs &lt; self.buffer_size:                 self.image_buffer.append(image)                 output.append(image)                 self.num_imgs += 1             else:                 if np.random.uniform() &gt; 0.5:                     idx = np.random.randint(self.buffer_size)                     temp = self.image_buffer[idx].clone()                     self.image_buffer[idx] = image                     output.append(temp)                 else:                     output.append(image)          output = torch.cat(output, 0)         return output <p>Once associated losses are defined, we can now define the <code>Network</code> object.</p> In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),\n    ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),\n    Buffer(image_in=\"fake_A\", image_out=\"buffer_fake_A\"),\n    Buffer(image_in=\"fake_B\", image_out=\"buffer_fake_B\"),\n    ModelOp(inputs=\"real_A\", model=d_A, outputs=\"d_real_A\"),\n    ModelOp(inputs=\"fake_A\", model=d_A, outputs=\"d_fake_A\"),\n    ModelOp(inputs=\"buffer_fake_A\", model=d_A, outputs=\"buffer_d_fake_A\"),\n    ModelOp(inputs=\"real_B\", model=d_B, outputs=\"d_real_B\"),\n    ModelOp(inputs=\"fake_B\", model=d_B, outputs=\"d_fake_B\"),\n    ModelOp(inputs=\"buffer_fake_B\", model=d_B, outputs=\"buffer_d_fake_B\"),\n    ModelOp(inputs=\"real_A\", model=g_BtoA, outputs=\"same_A\"),\n    ModelOp(inputs=\"fake_B\", model=g_BtoA, outputs=\"cycled_A\"),\n    ModelOp(inputs=\"real_B\", model=g_AtoB, outputs=\"same_B\"),\n    ModelOp(inputs=\"fake_A\", model=g_AtoB, outputs=\"cycled_B\"),\n    GLoss(inputs=(\"real_A\", \"d_fake_B\", \"cycled_A\", \"same_A\"), weight=weight, device=device, outputs=\"g_AtoB_loss\"),\n    GLoss(inputs=(\"real_B\", \"d_fake_A\", \"cycled_B\", \"same_B\"), weight=weight, device=device, outputs=\"g_BtoA_loss\"),\n    DLoss(inputs=(\"d_real_A\", \"buffer_d_fake_A\"), outputs=\"d_A_loss\", device=device),\n    DLoss(inputs=(\"d_real_B\", \"buffer_d_fake_B\"), outputs=\"d_B_loss\", device=device),\n    UpdateOp(model=g_AtoB, loss_name=\"g_AtoB_loss\"),\n    UpdateOp(model=g_BtoA, loss_name=\"g_BtoA_loss\"),\n    UpdateOp(model=d_A, loss_name=\"d_A_loss\"),\n    UpdateOp(model=d_B, loss_name=\"d_B_loss\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  network = fe.Network(ops=[     ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),     ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),     Buffer(image_in=\"fake_A\", image_out=\"buffer_fake_A\"),     Buffer(image_in=\"fake_B\", image_out=\"buffer_fake_B\"),     ModelOp(inputs=\"real_A\", model=d_A, outputs=\"d_real_A\"),     ModelOp(inputs=\"fake_A\", model=d_A, outputs=\"d_fake_A\"),     ModelOp(inputs=\"buffer_fake_A\", model=d_A, outputs=\"buffer_d_fake_A\"),     ModelOp(inputs=\"real_B\", model=d_B, outputs=\"d_real_B\"),     ModelOp(inputs=\"fake_B\", model=d_B, outputs=\"d_fake_B\"),     ModelOp(inputs=\"buffer_fake_B\", model=d_B, outputs=\"buffer_d_fake_B\"),     ModelOp(inputs=\"real_A\", model=g_BtoA, outputs=\"same_A\"),     ModelOp(inputs=\"fake_B\", model=g_BtoA, outputs=\"cycled_A\"),     ModelOp(inputs=\"real_B\", model=g_AtoB, outputs=\"same_B\"),     ModelOp(inputs=\"fake_A\", model=g_AtoB, outputs=\"cycled_B\"),     GLoss(inputs=(\"real_A\", \"d_fake_B\", \"cycled_A\", \"same_A\"), weight=weight, device=device, outputs=\"g_AtoB_loss\"),     GLoss(inputs=(\"real_B\", \"d_fake_A\", \"cycled_B\", \"same_B\"), weight=weight, device=device, outputs=\"g_BtoA_loss\"),     DLoss(inputs=(\"d_real_A\", \"buffer_d_fake_A\"), outputs=\"d_A_loss\", device=device),     DLoss(inputs=(\"d_real_B\", \"buffer_d_fake_B\"), outputs=\"d_B_loss\", device=device),     UpdateOp(model=g_AtoB, loss_name=\"g_AtoB_loss\"),     UpdateOp(model=g_BtoA, loss_name=\"g_BtoA_loss\"),     UpdateOp(model=d_A, loss_name=\"d_A_loss\"),     UpdateOp(model=d_B, loss_name=\"d_B_loss\") ]) <p>Here, we use a linear learning rate decay for training.</p> In\u00a0[12]: Copied! <pre>def lr_schedule(epoch):\n    if epoch&lt;=100:\n        lr = 2e-4\n    else:\n        lr = 2e-4*(200 - epoch)/100\n    return lr\n</pre> def lr_schedule(epoch):     if epoch&lt;=100:         lr = 2e-4     else:         lr = 2e-4*(200 - epoch)/100     return lr <p>In this example we will use <code>ModelSaver</code> traces to save the two generators <code>g_AtoB</code> and <code>g_BtoA</code> throughout training and <code>LRScheduler</code> traces to update the learning rate.</p> In\u00a0[13]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import ModelSaver\n\ntraces = [\n    ModelSaver(model=g_AtoB, save_dir=save_dir, frequency=10),\n    ModelSaver(model=g_BtoA, save_dir=save_dir, frequency=10),\n    LRScheduler(model=g_AtoB, lr_fn=lr_schedule),\n    LRScheduler(model=g_BtoA, lr_fn=lr_schedule),\n    LRScheduler(model=d_A, lr_fn=lr_schedule),\n    LRScheduler(model=d_B, lr_fn=lr_schedule)\n]\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import ModelSaver  traces = [     ModelSaver(model=g_AtoB, save_dir=save_dir, frequency=10),     ModelSaver(model=g_BtoA, save_dir=save_dir, frequency=10),     LRScheduler(model=g_AtoB, lr_fn=lr_schedule),     LRScheduler(model=g_BtoA, lr_fn=lr_schedule),     LRScheduler(model=d_A, lr_fn=lr_schedule),     LRScheduler(model=d_B, lr_fn=lr_schedule) ] In\u00a0[14]: Copied! <pre>estimator = fe.Estimator(network=network, \n                         pipeline=pipeline, \n                         epochs=epochs, \n                         traces=traces,\n                         log_steps=1000, \n                         train_steps_per_epoch=train_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                           pipeline=pipeline,                           epochs=epochs,                           traces=traces,                          log_steps=1000,                           train_steps_per_epoch=train_steps_per_epoch) In\u00a0[15]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 1000; num_device: 1;\nFastEstimator-Train: step: 1; d_A_loss: 3.2108502; d_A_lr: 0.0002; d_B_loss: 3.1976771; d_B_lr: 0.0002; g_AtoB_loss: 13.665354; g_AtoB_lr: 0.0002; g_BtoA_loss: 13.892114; g_BtoA_lr: 0.0002;\nFastEstimator-Train: step: 1000; d_A_loss: 0.010945682; d_A_lr: 0.0002; d_B_loss: 0.3114647; d_B_lr: 0.0002; g_AtoB_loss: 15.73657; g_AtoB_lr: 0.0002; g_BtoA_loss: 9.770332; g_BtoA_lr: 0.0002; steps/sec: 10.26;\nFastEstimator-Train: step: 1334; epoch: 1; epoch_time: 138.9 sec;\nFastEstimator-Train: step: 2000; d_A_loss: 0.21845323; d_A_lr: 0.0002; d_B_loss: 0.3194074; d_B_lr: 0.0002; g_AtoB_loss: 7.026337; g_AtoB_lr: 0.0002; g_BtoA_loss: 9.131756; g_BtoA_lr: 0.0002; steps/sec: 9.05;\nFastEstimator-Train: step: 2668; epoch: 2; epoch_time: 143.35 sec;\nFastEstimator-Train: step: 3000; d_A_loss: 0.2580701; d_A_lr: 0.0002; d_B_loss: 0.43598607; d_B_lr: 0.0002; g_AtoB_loss: 4.68789; g_AtoB_lr: 0.0002; g_BtoA_loss: 8.580699; g_BtoA_lr: 0.0002; steps/sec: 8.18;\nFastEstimator-Train: step: 4000; d_A_loss: 0.12693632; d_A_lr: 0.0002; d_B_loss: 0.04536821; d_B_lr: 0.0002; g_AtoB_loss: 5.3422995; g_AtoB_lr: 0.0002; g_BtoA_loss: 7.5210814; g_BtoA_lr: 0.0002; steps/sec: 8.7;\nFastEstimator-Train: step: 4002; epoch: 3; epoch_time: 169.88 sec;\nFastEstimator-Train: step: 5000; d_A_loss: 0.24568222; d_A_lr: 0.0002; d_B_loss: 0.04645998; d_B_lr: 0.0002; g_AtoB_loss: 6.6831093; g_AtoB_lr: 0.0002; g_BtoA_loss: 7.0778866; g_BtoA_lr: 0.0002; steps/sec: 8.15;\nFastEstimator-Train: step: 5336; epoch: 4; epoch_time: 159.61 sec;\nFastEstimator-Train: step: 6000; d_A_loss: 0.16322158; d_A_lr: 0.0002; d_B_loss: 0.019150853; d_B_lr: 0.0002; g_AtoB_loss: 7.7576237; g_AtoB_lr: 0.0002; g_BtoA_loss: 10.1092415; g_BtoA_lr: 0.0002; steps/sec: 9.25;\nFastEstimator-Train: step: 6670; epoch: 5; epoch_time: 135.78 sec;\nFastEstimator-Train: step: 7000; d_A_loss: 0.10134612; d_A_lr: 0.0002; d_B_loss: 0.17742005; d_B_lr: 0.0002; g_AtoB_loss: 7.486726; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.1879535; g_BtoA_lr: 0.0002; steps/sec: 8.96;\nFastEstimator-Train: step: 8000; d_A_loss: 0.13598159; d_A_lr: 0.0002; d_B_loss: 0.06359689; d_B_lr: 0.0002; g_AtoB_loss: 5.198177; g_AtoB_lr: 0.0002; g_BtoA_loss: 5.1237717; g_BtoA_lr: 0.0002; steps/sec: 10.15;\nFastEstimator-Train: step: 8004; epoch: 6; epoch_time: 146.04 sec;\nFastEstimator-Train: step: 9000; d_A_loss: 0.24748227; d_A_lr: 0.0002; d_B_loss: 0.08985389; d_B_lr: 0.0002; g_AtoB_loss: 7.138328; g_AtoB_lr: 0.0002; g_BtoA_loss: 12.837671; g_BtoA_lr: 0.0002; steps/sec: 9.73;\nFastEstimator-Train: step: 9338; epoch: 7; epoch_time: 132.51 sec;\nFastEstimator-Train: step: 10000; d_A_loss: 0.28274047; d_A_lr: 0.0002; d_B_loss: 0.23293617; d_B_lr: 0.0002; g_AtoB_loss: 3.688202; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.485837; g_BtoA_lr: 0.0002; steps/sec: 9.29;\nFastEstimator-Train: step: 10672; epoch: 8; epoch_time: 155.15 sec;\nFastEstimator-Train: step: 11000; d_A_loss: 0.03592102; d_A_lr: 0.0002; d_B_loss: 0.2748941; d_B_lr: 0.0002; g_AtoB_loss: 4.3695536; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.2612844; g_BtoA_lr: 0.0002; steps/sec: 8.29;\nFastEstimator-Train: step: 12000; d_A_loss: 0.05218474; d_A_lr: 0.0002; d_B_loss: 0.017800268; d_B_lr: 0.0002; g_AtoB_loss: 19.132637; g_AtoB_lr: 0.0002; g_BtoA_loss: 17.503262; g_BtoA_lr: 0.0002; steps/sec: 11.89;\nFastEstimator-Train: step: 12006; epoch: 9; epoch_time: 126.77 sec;\nFastEstimator-Train: step: 13000; d_A_loss: 0.13947828; d_A_lr: 0.0002; d_B_loss: 0.17353384; d_B_lr: 0.0002; g_AtoB_loss: 3.1774468; g_AtoB_lr: 0.0002; g_BtoA_loss: 6.9188337; g_BtoA_lr: 0.0002; steps/sec: 11.05;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_AtoB_epoch_10.pt\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_BtoA_epoch_10.pt\nFastEstimator-Train: step: 13340; epoch: 10; epoch_time: 118.57 sec;\nFastEstimator-Train: step: 14000; d_A_loss: 0.18080026; d_A_lr: 0.0002; d_B_loss: 0.10328345; d_B_lr: 0.0002; g_AtoB_loss: 4.9533176; g_AtoB_lr: 0.0002; g_BtoA_loss: 7.5026536; g_BtoA_lr: 0.0002; steps/sec: 11.06;\nFastEstimator-Train: step: 14674; epoch: 11; epoch_time: 119.07 sec;\nFastEstimator-Train: step: 15000; d_A_loss: 0.0076176217; d_A_lr: 0.0002; d_B_loss: 0.10207303; d_B_lr: 0.0002; g_AtoB_loss: 16.60973; g_AtoB_lr: 0.0002; g_BtoA_loss: 13.892546; g_BtoA_lr: 0.0002; steps/sec: 11.05;\nFastEstimator-Train: step: 16000; d_A_loss: 0.18173808; d_A_lr: 0.0002; d_B_loss: 0.052283786; d_B_lr: 0.0002; g_AtoB_loss: 6.226133; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.731463; g_BtoA_lr: 0.0002; steps/sec: 12.0;\nFastEstimator-Train: step: 16008; epoch: 12; epoch_time: 117.27 sec;\nFastEstimator-Train: step: 17000; d_A_loss: 0.11598459; d_A_lr: 0.0002; d_B_loss: 0.121301845; d_B_lr: 0.0002; g_AtoB_loss: 4.3975754; g_AtoB_lr: 0.0002; g_BtoA_loss: 6.546061; g_BtoA_lr: 0.0002; steps/sec: 11.02;\nFastEstimator-Train: step: 17342; epoch: 13; epoch_time: 118.98 sec;\nFastEstimator-Train: step: 18000; d_A_loss: 0.15181227; d_A_lr: 0.0002; d_B_loss: 0.16069743; d_B_lr: 0.0002; g_AtoB_loss: 6.0471025; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.4856486; g_BtoA_lr: 0.0002; steps/sec: 11.05;\nFastEstimator-Train: step: 18676; epoch: 14; epoch_time: 118.49 sec;\nFastEstimator-Train: step: 19000; d_A_loss: 0.20692636; d_A_lr: 0.0002; d_B_loss: 0.07573674; d_B_lr: 0.0002; g_AtoB_loss: 4.6534934; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.594749; g_BtoA_lr: 0.0002; steps/sec: 11.02;\nFastEstimator-Train: step: 20000; d_A_loss: 0.21789747; d_A_lr: 0.0002; d_B_loss: 0.21549332; d_B_lr: 0.0002; g_AtoB_loss: 4.447349; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.937718; g_BtoA_lr: 0.0002; steps/sec: 11.97;\nFastEstimator-Train: step: 20010; epoch: 15; epoch_time: 118.25 sec;\nFastEstimator-Train: step: 21000; d_A_loss: 0.28535625; d_A_lr: 0.0002; d_B_loss: 0.22555062; d_B_lr: 0.0002; g_AtoB_loss: 2.8892808; g_AtoB_lr: 0.0002; g_BtoA_loss: 5.4860334; g_BtoA_lr: 0.0002; steps/sec: 11.08;\nFastEstimator-Train: step: 21344; epoch: 16; epoch_time: 118.57 sec;\nFastEstimator-Train: step: 22000; d_A_loss: 0.19136402; d_A_lr: 0.0002; d_B_loss: 0.109166086; d_B_lr: 0.0002; g_AtoB_loss: 3.6108365; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.6259403; g_BtoA_lr: 0.0002; steps/sec: 11.03;\nFastEstimator-Train: step: 22678; epoch: 17; epoch_time: 118.98 sec;\nFastEstimator-Train: step: 23000; d_A_loss: 0.15124202; d_A_lr: 0.0002; d_B_loss: 0.1616896; d_B_lr: 0.0002; g_AtoB_loss: 3.598625; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.349188; g_BtoA_lr: 0.0002; steps/sec: 11.0;\nFastEstimator-Train: step: 24000; d_A_loss: 0.16977733; d_A_lr: 0.0002; d_B_loss: 0.06550271; d_B_lr: 0.0002; g_AtoB_loss: 6.7928667; g_AtoB_lr: 0.0002; g_BtoA_loss: 5.1823797; g_BtoA_lr: 0.0002; steps/sec: 11.98;\nFastEstimator-Train: step: 24012; epoch: 18; epoch_time: 117.98 sec;\nFastEstimator-Train: step: 25000; d_A_loss: 0.15378422; d_A_lr: 0.0002; d_B_loss: 0.08358267; d_B_lr: 0.0002; g_AtoB_loss: 6.564144; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.2897534; g_BtoA_lr: 0.0002; steps/sec: 11.06;\nFastEstimator-Train: step: 25346; epoch: 19; epoch_time: 118.51 sec;\nFastEstimator-Train: step: 26000; d_A_loss: 0.0070417607; d_A_lr: 0.0002; d_B_loss: 0.12145958; d_B_lr: 0.0002; g_AtoB_loss: 15.752372; g_AtoB_lr: 0.0002; g_BtoA_loss: 6.0002174; g_BtoA_lr: 0.0002; steps/sec: 11.06;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_AtoB_epoch_20.pt\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_BtoA_epoch_20.pt\nFastEstimator-Train: step: 26680; epoch: 20; epoch_time: 118.79 sec;\nFastEstimator-Train: step: 27000; d_A_loss: 0.0622226; d_A_lr: 0.0002; d_B_loss: 0.04732838; d_B_lr: 0.0002; g_AtoB_loss: 10.552514; g_AtoB_lr: 0.0002; g_BtoA_loss: 9.402086; g_BtoA_lr: 0.0002; steps/sec: 11.01;\nFastEstimator-Train: step: 28000; d_A_loss: 0.028189775; d_A_lr: 0.0002; d_B_loss: 0.13978265; d_B_lr: 0.0002; g_AtoB_loss: 10.884317; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.892234; g_BtoA_lr: 0.0002; steps/sec: 11.87;\nFastEstimator-Train: step: 28014; epoch: 21; epoch_time: 118.8 sec;\nFastEstimator-Train: step: 29000; d_A_loss: 0.09849219; d_A_lr: 0.0002; d_B_loss: 0.124093324; d_B_lr: 0.0002; g_AtoB_loss: 5.1369505; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.663924; g_BtoA_lr: 0.0002; steps/sec: 11.02;\nFastEstimator-Train: step: 29348; epoch: 22; epoch_time: 118.78 sec;\nFastEstimator-Train: step: 30000; d_A_loss: 0.12628208; d_A_lr: 0.0002; d_B_loss: 0.10195969; d_B_lr: 0.0002; g_AtoB_loss: 2.6797104; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.2230177; g_BtoA_lr: 0.0002; steps/sec: 11.14;\nFastEstimator-Train: step: 30682; epoch: 23; epoch_time: 123.55 sec;\nFastEstimator-Train: step: 31000; d_A_loss: 0.15682167; d_A_lr: 0.0002; d_B_loss: 0.32548973; d_B_lr: 0.0002; g_AtoB_loss: 4.840435; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.4905968; g_BtoA_lr: 0.0002; steps/sec: 4.6;\nFastEstimator-Train: step: 32000; d_A_loss: 0.11451645; d_A_lr: 0.0002; d_B_loss: 0.06042458; d_B_lr: 0.0002; g_AtoB_loss: 3.4695773; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.142742; g_BtoA_lr: 0.0002; steps/sec: 3.78;\nFastEstimator-Train: step: 32016; epoch: 24; epoch_time: 430.36 sec;\nFastEstimator-Train: step: 33000; d_A_loss: 0.29871425; d_A_lr: 0.0002; d_B_loss: 0.16944641; d_B_lr: 0.0002; g_AtoB_loss: 4.16166; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.8225899; g_BtoA_lr: 0.0002; steps/sec: 2.83;\nFastEstimator-Train: step: 33350; epoch: 25; epoch_time: 499.05 sec;\nFastEstimator-Train: step: 34000; d_A_loss: 0.114286; d_A_lr: 0.0002; d_B_loss: 0.109651625; d_B_lr: 0.0002; g_AtoB_loss: 4.5099387; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.6966114; g_BtoA_lr: 0.0002; steps/sec: 2.77;\nFastEstimator-Train: step: 34684; epoch: 26; epoch_time: 396.64 sec;\nFastEstimator-Train: step: 35000; d_A_loss: 0.07977181; d_A_lr: 0.0002; d_B_loss: 0.21154696; d_B_lr: 0.0002; g_AtoB_loss: 4.3349495; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.3471603; g_BtoA_lr: 0.0002; steps/sec: 3.37;\nFastEstimator-Train: step: 36000; d_A_loss: 0.16729036; d_A_lr: 0.0002; d_B_loss: 0.062212124; d_B_lr: 0.0002; g_AtoB_loss: 4.9130187; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.848304; g_BtoA_lr: 0.0002; steps/sec: 3.76;\nFastEstimator-Train: step: 36018; epoch: 27; epoch_time: 372.0 sec;\nFastEstimator-Train: step: 37000; d_A_loss: 0.2021332; d_A_lr: 0.0002; d_B_loss: 0.13457865; d_B_lr: 0.0002; g_AtoB_loss: 2.7795298; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.057055; g_BtoA_lr: 0.0002; steps/sec: 3.65;\nFastEstimator-Train: step: 37352; epoch: 28; epoch_time: 385.47 sec;\nFastEstimator-Train: step: 38000; d_A_loss: 0.41481462; d_A_lr: 0.0002; d_B_loss: 0.0745297; d_B_lr: 0.0002; g_AtoB_loss: 4.2751684; g_AtoB_lr: 0.0002; g_BtoA_loss: 6.6050787; g_BtoA_lr: 0.0002; steps/sec: 3.17;\nFastEstimator-Train: step: 38686; epoch: 29; epoch_time: 389.7 sec;\nFastEstimator-Train: step: 39000; d_A_loss: 0.2179724; d_A_lr: 0.0002; d_B_loss: 0.07892369; d_B_lr: 0.0002; g_AtoB_loss: 2.9368668; g_AtoB_lr: 0.0002; g_BtoA_loss: 5.7667227; g_BtoA_lr: 0.0002; steps/sec: 3.14;\nFastEstimator-Train: step: 40000; d_A_loss: 0.38974488; d_A_lr: 0.0002; d_B_loss: 0.10798988; d_B_lr: 0.0002; g_AtoB_loss: 4.255815; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.5183225; g_BtoA_lr: 0.0002; steps/sec: 3.84;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_AtoB_epoch_30.pt\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_BtoA_epoch_30.pt\nFastEstimator-Train: step: 40020; epoch: 30; epoch_time: 402.99 sec;\nFastEstimator-Train: step: 41000; d_A_loss: 0.37121627; d_A_lr: 0.0002; d_B_loss: 0.11034091; d_B_lr: 0.0002; g_AtoB_loss: 2.4258504; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.009646; g_BtoA_lr: 0.0002; steps/sec: 2.78;\nFastEstimator-Train: step: 41354; epoch: 31; epoch_time: 414.77 sec;\nFastEstimator-Train: step: 42000; d_A_loss: 0.114548594; d_A_lr: 0.0002; d_B_loss: 0.248036; d_B_lr: 0.0002; g_AtoB_loss: 5.9243593; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.291023; g_BtoA_lr: 0.0002; steps/sec: 3.64;\nFastEstimator-Train: step: 42688; epoch: 32; epoch_time: 440.97 sec;\nFastEstimator-Train: step: 43000; d_A_loss: 0.08480996; d_A_lr: 0.0002; d_B_loss: 0.018437251; d_B_lr: 0.0002; g_AtoB_loss: 6.847295; g_AtoB_lr: 0.0002; g_BtoA_loss: 9.564554; g_BtoA_lr: 0.0002; steps/sec: 3.18;\nFastEstimator-Train: step: 44000; d_A_loss: 0.118619055; d_A_lr: 0.0002; d_B_loss: 0.13077882; d_B_lr: 0.0002; g_AtoB_loss: 2.9808536; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.238474; g_BtoA_lr: 0.0002; steps/sec: 2.77;\nFastEstimator-Train: step: 44022; epoch: 33; epoch_time: 444.05 sec;\nFastEstimator-Train: step: 45000; d_A_loss: 0.16026257; d_A_lr: 0.0002; d_B_loss: 0.07697069; d_B_lr: 0.0002; g_AtoB_loss: 3.919404; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.2792253; g_BtoA_lr: 0.0002; steps/sec: 3.35;\nFastEstimator-Train: step: 45356; epoch: 34; epoch_time: 396.14 sec;\nFastEstimator-Train: step: 46000; d_A_loss: 0.045636296; d_A_lr: 0.0002; d_B_loss: 0.07664771; d_B_lr: 0.0002; g_AtoB_loss: 3.9265654; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.7000437; g_BtoA_lr: 0.0002; steps/sec: 3.41;\nFastEstimator-Train: step: 46690; epoch: 35; epoch_time: 379.74 sec;\nFastEstimator-Train: step: 47000; d_A_loss: 0.16862245; d_A_lr: 0.0002; d_B_loss: 0.05204377; d_B_lr: 0.0002; g_AtoB_loss: 4.0016527; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.7975125; g_BtoA_lr: 0.0002; steps/sec: 4.13;\nFastEstimator-Train: step: 48000; d_A_loss: 0.17546852; d_A_lr: 0.0002; d_B_loss: 0.27352092; d_B_lr: 0.0002; g_AtoB_loss: 2.6668086; g_AtoB_lr: 0.0002; g_BtoA_loss: 2.9398174; g_BtoA_lr: 0.0002; steps/sec: 3.32;\nFastEstimator-Train: step: 48024; epoch: 36; epoch_time: 359.12 sec;\nFastEstimator-Train: step: 49000; d_A_loss: 0.09111771; d_A_lr: 0.0002; d_B_loss: 0.09417858; d_B_lr: 0.0002; g_AtoB_loss: 3.3456988; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.4651456; g_BtoA_lr: 0.0002; steps/sec: 2.64;\nFastEstimator-Train: step: 49358; epoch: 37; epoch_time: 447.61 sec;\nFastEstimator-Train: step: 50000; d_A_loss: 0.11524596; d_A_lr: 0.0002; d_B_loss: 0.12104061; d_B_lr: 0.0002; g_AtoB_loss: 9.68929; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.233226; g_BtoA_lr: 0.0002; steps/sec: 2.92;\nFastEstimator-Train: step: 50692; epoch: 38; epoch_time: 529.96 sec;\nFastEstimator-Train: step: 51000; d_A_loss: 0.105448775; d_A_lr: 0.0002; d_B_loss: 0.07446474; d_B_lr: 0.0002; g_AtoB_loss: 2.8403997; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.0699873; g_BtoA_lr: 0.0002; steps/sec: 3.09;\nFastEstimator-Train: step: 52000; d_A_loss: 0.32795072; d_A_lr: 0.0002; d_B_loss: 0.2948647; d_B_lr: 0.0002; g_AtoB_loss: 2.2968879; g_AtoB_lr: 0.0002; g_BtoA_loss: 2.3472998; g_BtoA_lr: 0.0002; steps/sec: 2.89;\nFastEstimator-Train: step: 52026; epoch: 39; epoch_time: 414.88 sec;\nFastEstimator-Train: step: 53000; d_A_loss: 0.14258908; d_A_lr: 0.0002; d_B_loss: 0.09975105; d_B_lr: 0.0002; g_AtoB_loss: 4.417849; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.8894544; g_BtoA_lr: 0.0002; steps/sec: 2.64;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_AtoB_epoch_40.pt\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_BtoA_epoch_40.pt\nFastEstimator-Train: step: 53360; epoch: 40; epoch_time: 474.55 sec;\nFastEstimator-Train: step: 54000; d_A_loss: 0.40623146; d_A_lr: 0.0002; d_B_loss: 0.13360925; d_B_lr: 0.0002; g_AtoB_loss: 3.5933185; g_AtoB_lr: 0.0002; g_BtoA_loss: 8.165068; g_BtoA_lr: 0.0002; steps/sec: 2.67;\nFastEstimator-Train: step: 54694; epoch: 41; epoch_time: 488.97 sec;\nFastEstimator-Train: step: 55000; d_A_loss: 0.32001668; d_A_lr: 0.0002; d_B_loss: 0.19170062; d_B_lr: 0.0002; g_AtoB_loss: 5.8548794; g_AtoB_lr: 0.0002; g_BtoA_loss: 2.9984937; g_BtoA_lr: 0.0002; steps/sec: 2.69;\nFastEstimator-Train: step: 56000; d_A_loss: 0.18761483; d_A_lr: 0.0002; d_B_loss: 0.1781841; d_B_lr: 0.0002; g_AtoB_loss: 6.030718; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.817857; g_BtoA_lr: 0.0002; steps/sec: 3.07;\nFastEstimator-Train: step: 56028; epoch: 42; epoch_time: 507.62 sec;\nFastEstimator-Train: step: 57000; d_A_loss: 0.21374495; d_A_lr: 0.0002; d_B_loss: 0.030690651; d_B_lr: 0.0002; g_AtoB_loss: 3.0272703; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.9826207; g_BtoA_lr: 0.0002; steps/sec: 2.84;\nFastEstimator-Train: step: 57362; epoch: 43; epoch_time: 469.08 sec;\nFastEstimator-Train: step: 58000; d_A_loss: 0.015637912; d_A_lr: 0.0002; d_B_loss: 0.01957808; d_B_lr: 0.0002; g_AtoB_loss: 11.956296; g_AtoB_lr: 0.0002; g_BtoA_loss: 7.9149184; g_BtoA_lr: 0.0002; steps/sec: 2.76;\nFastEstimator-Train: step: 58696; epoch: 44; epoch_time: 461.95 sec;\nFastEstimator-Train: step: 59000; d_A_loss: 0.028864238; d_A_lr: 0.0002; d_B_loss: 0.0040443847; d_B_lr: 0.0002; g_AtoB_loss: 12.938608; g_AtoB_lr: 0.0002; g_BtoA_loss: 9.135632; g_BtoA_lr: 0.0002; steps/sec: 2.78;\nFastEstimator-Train: step: 60000; d_A_loss: 0.05312334; d_A_lr: 0.0002; d_B_loss: 0.037270255; d_B_lr: 0.0002; g_AtoB_loss: 7.9331784; g_AtoB_lr: 0.0002; g_BtoA_loss: 5.792024; g_BtoA_lr: 0.0002; steps/sec: 2.62;\nFastEstimator-Train: step: 60030; epoch: 45; epoch_time: 503.9 sec;\nFastEstimator-Train: step: 61000; d_A_loss: 0.089201994; d_A_lr: 0.0002; d_B_loss: 0.17658553; d_B_lr: 0.0002; g_AtoB_loss: 2.7340398; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.7218227; g_BtoA_lr: 0.0002; steps/sec: 3.43;\nFastEstimator-Train: step: 61364; epoch: 46; epoch_time: 403.93 sec;\nFastEstimator-Train: step: 62000; d_A_loss: 0.16056217; d_A_lr: 0.0002; d_B_loss: 0.036287144; d_B_lr: 0.0002; g_AtoB_loss: 3.8993127; g_AtoB_lr: 0.0002; g_BtoA_loss: 4.52674; g_BtoA_lr: 0.0002; steps/sec: 3.23;\nFastEstimator-Train: step: 62698; epoch: 47; epoch_time: 378.64 sec;\nFastEstimator-Train: step: 63000; d_A_loss: 0.1105344; d_A_lr: 0.0002; d_B_loss: 0.07777613; d_B_lr: 0.0002; g_AtoB_loss: 5.8097258; g_AtoB_lr: 0.0002; g_BtoA_loss: 6.6205196; g_BtoA_lr: 0.0002; steps/sec: 3.01;\nFastEstimator-Train: step: 64000; d_A_loss: 0.36758858; d_A_lr: 0.0002; d_B_loss: 0.15424584; d_B_lr: 0.0002; g_AtoB_loss: 3.3357177; g_AtoB_lr: 0.0002; g_BtoA_loss: 7.3640885; g_BtoA_lr: 0.0002; steps/sec: 3.56;\nFastEstimator-Train: step: 64032; epoch: 48; epoch_time: 439.95 sec;\nFastEstimator-Train: step: 65000; d_A_loss: 0.048484586; d_A_lr: 0.0002; d_B_loss: 0.20226882; d_B_lr: 0.0002; g_AtoB_loss: 4.131755; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.4538093; g_BtoA_lr: 0.0002; steps/sec: 2.99;\nFastEstimator-Train: step: 65366; epoch: 49; epoch_time: 406.73 sec;\nFastEstimator-Train: step: 66000; d_A_loss: 0.054230925; d_A_lr: 0.0002; d_B_loss: 0.15945143; d_B_lr: 0.0002; g_AtoB_loss: 3.2555401; g_AtoB_lr: 0.0002; g_BtoA_loss: 3.5932798; g_BtoA_lr: 0.0002; steps/sec: 3.46;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_AtoB_epoch_50.pt\nFastEstimator-ModelSaver: Saved model to /tmp/tmpn34de7gf/g_BtoA_epoch_50.pt\nFastEstimator-Train: step: 66700; epoch: 50; epoch_time: 476.56 sec;\nFastEstimator-Finish: step: 66700; d_A_lr: 0.0002; d_B_lr: 0.0002; g_AtoB_lr: 0.0002; g_BtoA_lr: 0.0002; total_time: 14691.95 sec;\n</pre> In\u00a0[16]: Copied! <pre>idx = np.random.randint(len(test_data))\ndata = test_data[idx][0]\nresult = pipeline.transform(data, mode=\"infer\")\n</pre> idx = np.random.randint(len(test_data)) data = test_data[idx][0] result = pipeline.transform(data, mode=\"infer\") In\u00a0[17]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),\n    ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"),\n])\n\npredictions = network.transform(result, mode=\"infer\")\nhorse_img = np.transpose(predictions[\"real_A\"].numpy(), (0, 2, 3, 1))\nzebra_img = np.transpose(predictions[\"real_B\"].numpy(), (0, 2, 3, 1))\nfake_zebra = np.transpose(predictions[\"fake_B\"].numpy(), (0, 2, 3, 1))\nfake_horse = np.transpose(predictions[\"fake_A\"].numpy(), (0, 2, 3, 1))\n</pre> network = fe.Network(ops=[     ModelOp(inputs=\"real_A\", model=g_AtoB, outputs=\"fake_B\"),     ModelOp(inputs=\"real_B\", model=g_BtoA, outputs=\"fake_A\"), ])  predictions = network.transform(result, mode=\"infer\") horse_img = np.transpose(predictions[\"real_A\"].numpy(), (0, 2, 3, 1)) zebra_img = np.transpose(predictions[\"real_B\"].numpy(), (0, 2, 3, 1)) fake_zebra = np.transpose(predictions[\"fake_B\"].numpy(), (0, 2, 3, 1)) fake_horse = np.transpose(predictions[\"fake_A\"].numpy(), (0, 2, 3, 1)) In\u00a0[19]: Copied! <pre>GridDisplay([ImageDisplay(image=horse_img[0], title=\"Real Horse\"),\n             ImageDisplay(image=fake_zebra[0], title=\"Fake Zebra\")\n            ]).show()\n\nGridDisplay([ImageDisplay(image=zebra_img[0], title=\"Real Zebra\"),\n             ImageDisplay(image=fake_horse[0], title=\"Fake Horse\")\n            ]).show()\n</pre> GridDisplay([ImageDisplay(image=horse_img[0], title=\"Real Horse\"),              ImageDisplay(image=fake_zebra[0], title=\"Fake Zebra\")             ]).show()  GridDisplay([ImageDisplay(image=zebra_img[0], title=\"Real Zebra\"),              ImageDisplay(image=fake_horse[0], title=\"Fake Horse\")             ]).show() <p>Note the addition of zebra-like stripe texture on top of horses when translating from horses to zebras. When translating zebras to horses, we can observe that generator removes the stripe texture from zebras.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#horse-to-zebra-unpaired-image-translation-with-cyclegan-in-fastestimator", "title": "Horse to Zebra Unpaired Image Translation with CycleGAN in FastEstimator\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download the dataset of horses and zebras via our dataset API. The images will be first downloaded from here. As this task requires an unpaired datasets of horse and zebra images, horse2zebra dataset is implemented using <code>BatchDataset</code> in FastEstimator. Hence, we need to specify the batch size while loading the dataset.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-1-create-pipeline", "title": "Step 1: Create pipeline\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-2-create-network", "title": "Step 2: Create Network\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#defining-loss-functions", "title": "Defining Loss functions\u00b6", "text": "<p>For each network, we need to define associated losses.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#step-3-estimator", "title": "Step 3: Estimator\u00b6", "text": "<p>Finally, we are ready to define <code>Estimator</code> object and then call <code>fit</code> method to start the training. Just for the sake of demo purpose, we would only run 50 epochs.</p>"}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/image_generation/cyclegan/cyclegan.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Below are infering results of the two generators.</p>"}, {"location": "apphub/image_generation/dcgan/dcgan.html", "title": "DCGAN on the MNIST Dataset", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom matplotlib import pyplot as plt\nimport fastestimator as fe\nfrom fastestimator.backend import binary_crossentropy, feed_forward\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Normalize\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import ModelSaver\n</pre> import tempfile import os import numpy as np import tensorflow as tf from tensorflow.keras import layers from matplotlib import pyplot as plt import fastestimator as fe from fastestimator.backend import binary_crossentropy, feed_forward from fastestimator.dataset.data import mnist from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.univariate import ExpandDims, Normalize from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import ModelSaver In\u00a0[2]: parameters Copied! <pre>batch_size = 256\nepochs = 50\ntrain_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\nmodel_name = 'model_epoch_50.h5'\n</pre> batch_size = 256 epochs = 50 train_steps_per_epoch = None save_dir = tempfile.mkdtemp() model_name = 'model_epoch_50.h5' Building components <p>We are loading data from tf.keras.datasets.mnist and defining a series of operations to perform on the data before the training:</p> In\u00a0[3]: Copied! <pre>train_data, _ = mnist.load_data()\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x\"),\n        Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        LambdaOp(fn=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")\n    ])\n</pre> train_data, _ = mnist.load_data() pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x\"),         Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),         LambdaOp(fn=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")     ]) <p>First, we have to define the network architecture for both our Generator and Discriminator. After defining the architecture, users are expected to feed the architecture definition, along with associated model names and optimizers, to fe.build.</p> In\u00a0[4]: Copied! <pre>def generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((7, 7, 256)))\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    return model\n</pre> def generator():     model = tf.keras.Sequential()     model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Reshape((7, 7, 256)))     model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))     return model In\u00a0[5]: Copied! <pre>def discriminator():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n</pre> def discriminator():     model = tf.keras.Sequential()     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Flatten())     model.add(layers.Dense(1))     return model In\u00a0[6]: Copied! <pre>gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\ndisc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) disc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <pre>2022-05-17 22:24:12.679107: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-17 22:24:13.365903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 32253 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n</pre> <p>We define the generator and discriminator losses. These can have multiple inputs and outputs.</p> In\u00a0[7]: Copied! <pre>class GLoss(TensorOp):\n\"\"\"Compute generator loss.\"\"\"\n    def forward(self, data, state):\n        return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True)\n</pre> class GLoss(TensorOp):     \"\"\"Compute generator loss.\"\"\"     def forward(self, data, state):         return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True) In\u00a0[8]: Copied! <pre>class DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def forward(self, data, state):\n        true_score, fake_score = data\n        real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)\n        fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)\n        total_loss = real_loss + fake_loss\n        return total_loss\n</pre> class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def forward(self, data, state):         true_score, fake_score = data         real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)         fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)         total_loss = real_loss + fake_loss         return total_loss <p><code>fe.Network</code> takes series of operators. Here we pass our models wrapped into <code>ModelOps</code> along with our loss functions and some update rules:</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),\n        ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),\n        GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n        UpdateOp(model=gen_model, loss_name=\"gloss\"),\n        ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),\n        DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),\n        UpdateOp(model=disc_model, loss_name=\"dloss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),         ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),         GLoss(inputs=\"fake_score\", outputs=\"gloss\"),         UpdateOp(model=gen_model, loss_name=\"gloss\"),         ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),         DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),         UpdateOp(model=disc_model, loss_name=\"dloss\")     ]) <pre>2022-05-17 22:24:13.980469: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> <p>We will define an <code>Estimator</code> that has four notable arguments: network, pipeline, epochs and traces. Our <code>Network</code> and <code>Pipeline</code> objects are passed here as an argument along with the number of epochs and a <code>Trace</code>, in this case one designed to save our model every 5 epochs.</p> In\u00a0[10]: Copied! <pre>traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5)\n</pre> traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5) In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch) Training In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'y' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>2022-05-17 22:24:19.148391: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n2022-05-17 22:24:19.882754: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; dloss: 1.4074755; gloss: 0.6938505;\nFastEstimator-Train: step: 100; dloss: 1.071872; gloss: 0.6625423; steps/sec: 96.76;\nFastEstimator-Train: step: 200; dloss: 1.1390224; gloss: 0.7711286; steps/sec: 153.13;\nFastEstimator-Train: step: 235; epoch: 1; epoch_time: 6.54 sec;\nFastEstimator-Train: step: 300; dloss: 1.1058098; gloss: 0.8283564; steps/sec: 21.38;\nFastEstimator-Train: step: 400; dloss: 1.2330828; gloss: 0.8234701; steps/sec: 137.98;\nFastEstimator-Train: step: 470; epoch: 2; epoch_time: 4.88 sec;\nFastEstimator-Train: step: 500; dloss: 1.0860771; gloss: 0.8769846; steps/sec: 26.53;\nFastEstimator-Train: step: 600; dloss: 1.3000612; gloss: 0.76368; steps/sec: 139.42;\nFastEstimator-Train: step: 700; dloss: 1.3079492; gloss: 0.7894113; steps/sec: 136.25;\nFastEstimator-Train: step: 705; epoch: 3; epoch_time: 4.73 sec;\nFastEstimator-Train: step: 800; dloss: 0.95769334; gloss: 1.0632424; steps/sec: 24.9;\nFastEstimator-Train: step: 900; dloss: 1.0605512; gloss: 1.0276053; steps/sec: 118.47;\nFastEstimator-Train: step: 940; epoch: 4; epoch_time: 5.15 sec;\nFastEstimator-Train: step: 1000; dloss: 1.1515687; gloss: 1.0443377; steps/sec: 21.87;\nFastEstimator-Train: step: 1100; dloss: 1.312542; gloss: 0.8080373; steps/sec: 143.04;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_5.h5\nFastEstimator-Train: step: 1175; epoch: 5; epoch_time: 5.49 sec;\nFastEstimator-Train: step: 1200; dloss: 1.5896232; gloss: 0.69061863; steps/sec: 24.5;\nFastEstimator-Train: step: 1300; dloss: 1.0532413; gloss: 1.063014; steps/sec: 139.64;\nFastEstimator-Train: step: 1400; dloss: 1.1432295; gloss: 0.9709608; steps/sec: 118.93;\nFastEstimator-Train: step: 1410; epoch: 6; epoch_time: 5.16 sec;\nFastEstimator-Train: step: 1500; dloss: 1.1589987; gloss: 0.88277376; steps/sec: 23.83;\nFastEstimator-Train: step: 1600; dloss: 1.4223424; gloss: 0.82864374; steps/sec: 136.68;\nFastEstimator-Train: step: 1645; epoch: 7; epoch_time: 5.19 sec;\nFastEstimator-Train: step: 1700; dloss: 1.0954866; gloss: 1.0265985; steps/sec: 24.66;\nFastEstimator-Train: step: 1800; dloss: 1.2621644; gloss: 0.8546214; steps/sec: 137.29;\nFastEstimator-Train: step: 1880; epoch: 8; epoch_time: 5.04 sec;\nFastEstimator-Train: step: 1900; dloss: 1.23395; gloss: 0.9362403; steps/sec: 24.59;\nFastEstimator-Train: step: 2000; dloss: 1.1891618; gloss: 0.93046916; steps/sec: 141.07;\nFastEstimator-Train: step: 2100; dloss: 1.1270559; gloss: 0.83571935; steps/sec: 135.63;\nFastEstimator-Train: step: 2115; epoch: 9; epoch_time: 5.04 sec;\nFastEstimator-Train: step: 2200; dloss: 1.0217198; gloss: 1.0431002; steps/sec: 24.59;\nFastEstimator-Train: step: 2300; dloss: 1.0043646; gloss: 0.987643; steps/sec: 134.45;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_10.h5\nFastEstimator-Train: step: 2350; epoch: 10; epoch_time: 5.05 sec;\nFastEstimator-Train: step: 2400; dloss: 1.1894727; gloss: 0.957337; steps/sec: 26.01;\nFastEstimator-Train: step: 2500; dloss: 1.5908065; gloss: 0.67834544; steps/sec: 144.04;\nFastEstimator-Train: step: 2585; epoch: 11; epoch_time: 4.8 sec;\nFastEstimator-Train: step: 2600; dloss: 1.1317401; gloss: 1.0065825; steps/sec: 24.88;\nFastEstimator-Train: step: 2700; dloss: 1.0598496; gloss: 1.0172932; steps/sec: 131.43;\nFastEstimator-Train: step: 2800; dloss: 1.2017224; gloss: 0.9210052; steps/sec: 133.42;\nFastEstimator-Train: step: 2820; epoch: 12; epoch_time: 5.07 sec;\nFastEstimator-Train: step: 2900; dloss: 1.1818032; gloss: 0.8824554; steps/sec: 25.56;\nFastEstimator-Train: step: 3000; dloss: 1.0128953; gloss: 1.0633328; steps/sec: 123.48;\nFastEstimator-Train: step: 3055; epoch: 13; epoch_time: 4.97 sec;\nFastEstimator-Train: step: 3100; dloss: 1.1521642; gloss: 0.98313975; steps/sec: 24.99;\nFastEstimator-Train: step: 3200; dloss: 1.2207543; gloss: 1.0411373; steps/sec: 130.05;\nFastEstimator-Train: step: 3290; epoch: 14; epoch_time: 5.11 sec;\nFastEstimator-Train: step: 3300; dloss: 1.0686741; gloss: 1.04258; steps/sec: 24.05;\nFastEstimator-Train: step: 3400; dloss: 1.1607264; gloss: 0.9313967; steps/sec: 125.47;\nFastEstimator-Train: step: 3500; dloss: 1.108815; gloss: 1.0197322; steps/sec: 128.05;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_15.h5\nFastEstimator-Train: step: 3525; epoch: 15; epoch_time: 5.18 sec;\nFastEstimator-Train: step: 3600; dloss: 0.84284836; gloss: 1.3197017; steps/sec: 25.89;\nFastEstimator-Train: step: 3700; dloss: 1.3472433; gloss: 0.9266864; steps/sec: 131.6;\nFastEstimator-Train: step: 3760; epoch: 16; epoch_time: 4.87 sec;\nFastEstimator-Train: step: 3800; dloss: 1.1090488; gloss: 1.148634; steps/sec: 24.4;\nFastEstimator-Train: step: 3900; dloss: 1.0149467; gloss: 1.171385; steps/sec: 134.44;\nFastEstimator-Train: step: 3995; epoch: 17; epoch_time: 5.12 sec;\nFastEstimator-Train: step: 4000; dloss: 1.0201095; gloss: 1.3736193; steps/sec: 24.89;\nFastEstimator-Train: step: 4100; dloss: 1.1828756; gloss: 1.1176487; steps/sec: 127.13;\nFastEstimator-Train: step: 4200; dloss: 0.9339646; gloss: 1.2892692; steps/sec: 129.5;\nFastEstimator-Train: step: 4230; epoch: 18; epoch_time: 5.09 sec;\nFastEstimator-Train: step: 4300; dloss: 1.031354; gloss: 1.1992577; steps/sec: 22.92;\nFastEstimator-Train: step: 4400; dloss: 0.59360105; gloss: 1.7209172; steps/sec: 131.32;\nFastEstimator-Train: step: 4465; epoch: 19; epoch_time: 5.35 sec;\nFastEstimator-Train: step: 4500; dloss: 0.8413017; gloss: 1.3761427; steps/sec: 22.88;\nFastEstimator-Train: step: 4600; dloss: 1.0306518; gloss: 1.3534794; steps/sec: 133.22;\nFastEstimator-Train: step: 4700; dloss: 1.1800865; gloss: 1.1182059; steps/sec: 128.35;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_20.h5\nFastEstimator-Train: step: 4700; epoch: 20; epoch_time: 5.41 sec;\nFastEstimator-Train: step: 4800; dloss: 1.0419474; gloss: 1.090816; steps/sec: 24.75;\nFastEstimator-Train: step: 4900; dloss: 0.9564374; gloss: 1.2522755; steps/sec: 136.4;\nFastEstimator-Train: step: 4935; epoch: 21; epoch_time: 5.05 sec;\nFastEstimator-Train: step: 5000; dloss: 1.1178182; gloss: 1.0265534; steps/sec: 26.88;\nFastEstimator-Train: step: 5100; dloss: 0.9095393; gloss: 1.2775141; steps/sec: 139.1;\nFastEstimator-Train: step: 5170; epoch: 22; epoch_time: 4.95 sec;\nFastEstimator-Train: step: 5200; dloss: 1.1132319; gloss: 1.1454469; steps/sec: 20.47;\nFastEstimator-Train: step: 5300; dloss: 1.1913971; gloss: 1.2246889; steps/sec: 73.91;\nFastEstimator-Train: step: 5400; dloss: 0.94440126; gloss: 1.2502146; steps/sec: 84.12;\nFastEstimator-Train: step: 5405; epoch: 23; epoch_time: 6.99 sec;\nFastEstimator-Train: step: 5500; dloss: 1.0678749; gloss: 1.2804728; steps/sec: 17.85;\nFastEstimator-Train: step: 5600; dloss: 1.0255647; gloss: 1.2616677; steps/sec: 80.99;\nFastEstimator-Train: step: 5640; epoch: 24; epoch_time: 7.2 sec;\nFastEstimator-Train: step: 5700; dloss: 0.9666496; gloss: 1.2296994; steps/sec: 19.36;\nFastEstimator-Train: step: 5800; dloss: 1.116164; gloss: 0.96720326; steps/sec: 87.0;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_25.h5\nFastEstimator-Train: step: 5875; epoch: 25; epoch_time: 6.74 sec;\nFastEstimator-Train: step: 5900; dloss: 1.034753; gloss: 1.1733758; steps/sec: 19.73;\nFastEstimator-Train: step: 6000; dloss: 1.0251997; gloss: 1.2077578; steps/sec: 80.12;\nFastEstimator-Train: step: 6100; dloss: 0.9085286; gloss: 1.4258803; steps/sec: 82.07;\nFastEstimator-Train: step: 6110; epoch: 26; epoch_time: 6.79 sec;\nFastEstimator-Train: step: 6200; dloss: 0.91629565; gloss: 1.2635107; steps/sec: 18.95;\nFastEstimator-Train: step: 6300; dloss: 0.9700071; gloss: 1.2341801; steps/sec: 81.38;\nFastEstimator-Train: step: 6345; epoch: 27; epoch_time: 6.91 sec;\nFastEstimator-Train: step: 6400; dloss: 1.0925424; gloss: 1.1383421; steps/sec: 19.67;\nFastEstimator-Train: step: 6500; dloss: 0.886595; gloss: 1.2264949; steps/sec: 88.04;\nFastEstimator-Train: step: 6580; epoch: 28; epoch_time: 6.63 sec;\nFastEstimator-Train: step: 6600; dloss: 0.9399621; gloss: 1.2295549; steps/sec: 18.91;\nFastEstimator-Train: step: 6700; dloss: 1.1239285; gloss: 1.0375732; steps/sec: 85.2;\nFastEstimator-Train: step: 6800; dloss: 1.115287; gloss: 1.074056; steps/sec: 87.68;\nFastEstimator-Train: step: 6815; epoch: 29; epoch_time: 6.77 sec;\nFastEstimator-Train: step: 6900; dloss: 0.99310243; gloss: 1.0726111; steps/sec: 19.46;\nFastEstimator-Train: step: 7000; dloss: 1.3016744; gloss: 1.1302423; steps/sec: 80.95;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_30.h5\nFastEstimator-Train: step: 7050; epoch: 30; epoch_time: 6.59 sec;\nFastEstimator-Train: step: 7100; dloss: 1.1211019; gloss: 1.0915082; steps/sec: 22.69;\nFastEstimator-Train: step: 7200; dloss: 1.0433137; gloss: 1.3584124; steps/sec: 74.73;\nFastEstimator-Train: step: 7285; epoch: 31; epoch_time: 6.6 sec;\nFastEstimator-Train: step: 7300; dloss: 1.3104564; gloss: 1.133613; steps/sec: 18.21;\nFastEstimator-Train: step: 7400; dloss: 1.195024; gloss: 1.1311966; steps/sec: 82.59;\nFastEstimator-Train: step: 7500; dloss: 1.1094584; gloss: 1.1424885; steps/sec: 84.76;\nFastEstimator-Train: step: 7520; epoch: 32; epoch_time: 6.99 sec;\nFastEstimator-Train: step: 7600; dloss: 0.9371455; gloss: 1.1962738; steps/sec: 18.9;\nFastEstimator-Train: step: 7700; dloss: 1.039571; gloss: 1.158539; steps/sec: 84.59;\nFastEstimator-Train: step: 7755; epoch: 33; epoch_time: 6.9 sec;\nFastEstimator-Train: step: 7800; dloss: 1.2328132; gloss: 1.0920696; steps/sec: 18.49;\nFastEstimator-Train: step: 7900; dloss: 1.263283; gloss: 1.057182; steps/sec: 75.0;\nFastEstimator-Train: step: 7990; epoch: 34; epoch_time: 7.16 sec;\nFastEstimator-Train: step: 8000; dloss: 1.1744511; gloss: 1.0851499; steps/sec: 18.84;\nFastEstimator-Train: step: 8100; dloss: 1.2414798; gloss: 0.9666091; steps/sec: 68.94;\nFastEstimator-Train: step: 8200; dloss: 1.101659; gloss: 1.0953238; steps/sec: 84.85;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_35.h5\nFastEstimator-Train: step: 8225; epoch: 35; epoch_time: 7.23 sec;\nFastEstimator-Train: step: 8300; dloss: 1.1480453; gloss: 1.0929885; steps/sec: 18.74;\nFastEstimator-Train: step: 8400; dloss: 1.0150359; gloss: 1.2104923; steps/sec: 75.67;\nFastEstimator-Train: step: 8460; epoch: 36; epoch_time: 6.93 sec;\nFastEstimator-Train: step: 8500; dloss: 1.2448437; gloss: 0.95777893; steps/sec: 19.26;\nFastEstimator-Train: step: 8600; dloss: 1.128643; gloss: 0.9374757; steps/sec: 84.76;\nFastEstimator-Train: step: 8695; epoch: 37; epoch_time: 6.76 sec;\nFastEstimator-Train: step: 8700; dloss: 1.263104; gloss: 1.0629486; steps/sec: 18.54;\nFastEstimator-Train: step: 8800; dloss: 1.0654749; gloss: 1.1919184; steps/sec: 81.95;\nFastEstimator-Train: step: 8900; dloss: 1.0990744; gloss: 1.0925766; steps/sec: 80.36;\nFastEstimator-Train: step: 8930; epoch: 38; epoch_time: 7.05 sec;\nFastEstimator-Train: step: 9000; dloss: 1.3174253; gloss: 0.9213707; steps/sec: 21.57;\nFastEstimator-Train: step: 9100; dloss: 1.3086524; gloss: 0.89782405; steps/sec: 87.45;\nFastEstimator-Train: step: 9165; epoch: 39; epoch_time: 6.25 sec;\nFastEstimator-Train: step: 9200; dloss: 1.2255262; gloss: 0.9562658; steps/sec: 18.78;\nFastEstimator-Train: step: 9300; dloss: 1.2224264; gloss: 1.0891576; steps/sec: 80.81;\nFastEstimator-Train: step: 9400; dloss: 1.2472556; gloss: 1.0490451; steps/sec: 76.39;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_40.h5\nFastEstimator-Train: step: 9400; epoch: 40; epoch_time: 7.04 sec;\nFastEstimator-Train: step: 9500; dloss: 1.2820866; gloss: 0.8918587; steps/sec: 19.16;\nFastEstimator-Train: step: 9600; dloss: 1.266947; gloss: 0.9083342; steps/sec: 80.53;\nFastEstimator-Train: step: 9635; epoch: 41; epoch_time: 6.89 sec;\nFastEstimator-Train: step: 9700; dloss: 1.2590028; gloss: 0.91593623; steps/sec: 18.61;\nFastEstimator-Train: step: 9800; dloss: 1.2626295; gloss: 0.8595179; steps/sec: 85.58;\nFastEstimator-Train: step: 9870; epoch: 42; epoch_time: 6.98 sec;\nFastEstimator-Train: step: 9900; dloss: 1.2414654; gloss: 0.9379455; steps/sec: 19.63;\nFastEstimator-Train: step: 10000; dloss: 1.209197; gloss: 0.8486848; steps/sec: 80.11;\nFastEstimator-Train: step: 10100; dloss: 1.221647; gloss: 1.0560616; steps/sec: 82.71;\nFastEstimator-Train: step: 10105; epoch: 43; epoch_time: 6.79 sec;\nFastEstimator-Train: step: 10200; dloss: 1.2570242; gloss: 0.9568354; steps/sec: 18.62;\nFastEstimator-Train: step: 10300; dloss: 1.1466038; gloss: 1.1030976; steps/sec: 86.59;\nFastEstimator-Train: step: 10340; epoch: 44; epoch_time: 6.91 sec;\nFastEstimator-Train: step: 10400; dloss: 1.3359842; gloss: 0.815323; steps/sec: 18.28;\nFastEstimator-Train: step: 10500; dloss: 1.436327; gloss: 0.82999104; steps/sec: 79.86;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_45.h5\nFastEstimator-Train: step: 10575; epoch: 45; epoch_time: 7.27 sec;\nFastEstimator-Train: step: 10600; dloss: 1.3078375; gloss: 0.9893749; steps/sec: 17.96;\nFastEstimator-Train: step: 10700; dloss: 1.50738; gloss: 0.88688844; steps/sec: 79.93;\nFastEstimator-Train: step: 10800; dloss: 1.2391019; gloss: 0.94254756; steps/sec: 82.19;\nFastEstimator-Train: step: 10810; epoch: 46; epoch_time: 7.2 sec;\nFastEstimator-Train: step: 10900; dloss: 1.2033031; gloss: 0.9017648; steps/sec: 20.95;\nFastEstimator-Train: step: 11000; dloss: 1.1743715; gloss: 0.9873478; steps/sec: 80.95;\nFastEstimator-Train: step: 11045; epoch: 47; epoch_time: 6.37 sec;\nFastEstimator-Train: step: 11100; dloss: 1.2350852; gloss: 0.8709105; steps/sec: 19.28;\nFastEstimator-Train: step: 11200; dloss: 1.3278979; gloss: 0.8681942; steps/sec: 81.55;\nFastEstimator-Train: step: 11280; epoch: 48; epoch_time: 7.01 sec;\nFastEstimator-Train: step: 11300; dloss: 1.2809682; gloss: 0.87823415; steps/sec: 18.17;\nFastEstimator-Train: step: 11400; dloss: 1.3148129; gloss: 0.9447537; steps/sec: 80.3;\nFastEstimator-Train: step: 11500; dloss: 1.2640177; gloss: 0.9070884; steps/sec: 82.19;\nFastEstimator-Train: step: 11515; epoch: 49; epoch_time: 7.03 sec;\nFastEstimator-Train: step: 11600; dloss: 1.209223; gloss: 0.99375534; steps/sec: 20.29;\nFastEstimator-Train: step: 11700; dloss: 1.1119881; gloss: 0.991818; steps/sec: 83.43;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp7hg1f143/model_epoch_50.h5\nFastEstimator-Train: step: 11750; epoch: 50; epoch_time: 6.52 sec;\nFastEstimator-Finish: step: 11750; model1_lr: 1e-04; model_lr: 1e-04; total_time: 307.39 sec;\n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We will load the trained generator weights using fe.build</p> In\u00a0[13]: Copied! <pre>model_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <p>We will the generate some images from random noise:</p> In\u00a0[14]: Copied! <pre>images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False)\n</pre> images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False) In\u00a0[15]: Copied! <pre>fe.util.BatchDisplay(image=images).show()\n</pre> fe.util.BatchDisplay(image=images).show()"}, {"location": "apphub/image_generation/dcgan/dcgan.html#dcgan-on-the-mnist-dataset", "title": "DCGAN on the MNIST Dataset\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-1-prepare-training-and-define-a-pipeline", "title": "Step 1: Prepare training and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html", "title": "Progressive Growing GAN (PGGAN)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nimport fastestimator as fe\nfrom fastestimator.schedule import EpochScheduler\nfrom fastestimator.util import get_num_devices\n</pre> import tempfile  import numpy as np import torch import matplotlib.pyplot as plt  import fastestimator as fe from fastestimator.schedule import EpochScheduler from fastestimator.util import get_num_devices In\u00a0[2]: parameters Copied! <pre>target_size=128\nepochs=55\nsave_dir=tempfile.mkdtemp()\ntrain_steps_per_epoch=None\ndata_dir=None\n</pre> target_size=128 epochs=55 save_dir=tempfile.mkdtemp() train_steps_per_epoch=None data_dir=None In\u00a0[3]: Copied! <pre>num_grow = np.log2(target_size) - 2\nassert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\"\nnum_phases = int(2 * num_grow + 1)\nassert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size)\nnum_grow, phase_length = int(num_grow), int(epochs / num_phases)\nevent_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)]\nevent_size = [4] + [2**(i + 3) for i in range(num_grow)]\n</pre> num_grow = np.log2(target_size) - 2 assert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\" num_phases = int(2 * num_grow + 1) assert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size) num_grow, phase_length = int(num_grow), int(epochs / num_phases) event_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)] event_size = [4] + [2**(i + 3) for i in range(num_grow)] In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import nih_chestxray\n\ndataset = nih_chestxray.load_data(root_dir=data_dir)\n</pre> from fastestimator.dataset.data import nih_chestxray  dataset = nih_chestxray.load_data(root_dir=data_dir) In\u00a0[5]: Copied! <pre>from fastestimator.op.numpyop import Batch, LambdaOp\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\nresize_map = {\n    epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map1 = {\n    epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map2 = {\n    epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_size_map = {\n    epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_scheduler = EpochScheduler(epoch_dict=batch_size_map)\npipeline = fe.Pipeline(\n    batch_size=batch_scheduler,\n    train_data=dataset,\n    ops=[\n        ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),\n        EpochScheduler(epoch_dict=resize_map),\n        EpochScheduler(epoch_dict=resize_low_res_map1),\n        EpochScheduler(epoch_dict=resize_low_res_map2),\n        Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n        ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),\n        LambdaOp(fn=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\"),\n        Batch(drop_last=True)\n    ])\n</pre> from fastestimator.op.numpyop import Batch, LambdaOp from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  resize_map = {     epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map1 = {     epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map2 = {     epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } batch_size_map = {     epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()     for (epoch, size) in zip(event_epoch, event_size) } batch_scheduler = EpochScheduler(epoch_dict=batch_size_map) pipeline = fe.Pipeline(     batch_size=batch_scheduler,     train_data=dataset,     ops=[         ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),         EpochScheduler(epoch_dict=resize_map),         EpochScheduler(epoch_dict=resize_low_res_map1),         EpochScheduler(epoch_dict=resize_low_res_map2),         Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),         ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),         LambdaOp(fn=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\"),         Batch(drop_last=True)     ]) <p>Let's visualize how our <code>Pipeline</code> changes image resolution at the different epochs we specified using <code>Schedulers</code>. FastEstimator as a <code>get_results</code> method to aid in this. In order to correctly visualize the output of the <code>Pipeline</code>, we need to provide epoch numbers to the <code>get_results</code> method:</p> In\u00a0[6]: Copied! <pre>plt.figure(figsize=(50,50))\nfor i, epoch in enumerate(event_epoch):\n    batch_data = pipeline.get_results(epoch=epoch)\n    img = np.squeeze(batch_data[\"x\"][0] + 0.5)\n    plt.subplot(1, 9, i+1)\n    plt.imshow(img, cmap='gray')\n</pre> plt.figure(figsize=(50,50)) for i, epoch in enumerate(event_epoch):     batch_data = pipeline.get_results(epoch=epoch)     img = np.squeeze(batch_data[\"x\"][0] + 0.5)     plt.subplot(1, 9, i+1)     plt.imshow(img, cmap='gray')  In\u00a0[7]: Copied! <pre>from torch.optim import Adam\n\ndef _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):\n    return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)\n\n\nclass EqualizedLRDense(torch.nn.Linear):\n    def __init__(self, in_features, out_features, gain=np.sqrt(2)):\n        super().__init__(in_features, out_features, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        self.wscale = np.float32(gain / np.sqrt(in_features))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\nclass ApplyBias(torch.nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.bias = torch.nn.Parameter(torch.Tensor(in_features))\n        torch.nn.init.constant_(self.bias.data, val=0.0)\n\n    def forward(self, x):\n        if len(x.shape) == 4:\n            x = x + self.bias.view(1, -1, 1, 1).expand_as(x)\n        else:\n            x = x + self.bias\n        return x\n\n\nclass EqualizedLRConv2D(torch.nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):\n        super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        fan_in = np.float32(np.prod(self.weight.data.shape[1:]))\n        self.wscale = np.float32(gain / np.sqrt(fan_in))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\ndef pixel_normalization(x, eps=1e-8):\n    return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)\n\n\ndef mini_batch_std(x, group_size=4, eps=1e-8):\n    b, c, h, w = x.shape\n    group_size = min(group_size, b)\n    y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]\n    y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]\n    y = torch.mean(y**2, axis=0)  # [M, C, H, W]\n    y = torch.sqrt(y + eps)  # [M, C, H, W]\n    y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]\n    y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]\n    return torch.cat((x, y), 1)\n\n\ndef fade_in(x, y, alpha):\n    return (1.0 - alpha) * x + alpha * y\n\n\nclass ToRGB(torch.nn.Module):\n    def __init__(self, in_channels, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)\n        self.bias = ApplyBias(in_features=num_channels)\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        return x\n\n\nclass FromRGB(torch.nn.Module):\n    def __init__(self, res, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)\n        self.bias = ApplyBias(in_features=_nf(res - 1))\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        return x\n\n\nclass BlockG1D(torch.nn.Module):\n    def __init__(self, res=2, latent_dim=512):\n        super().__init__()\n        self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512]\n        x = pixel_normalization(x)  # [batch, 512]\n        x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]\n        x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)\n        return x\n\n\nclass BlockG2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]\n        x = self.upsample(x)\n        x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        return x\n\n\ndef _block_G(res, latent_dim=512, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockG1D(res=res, latent_dim=latent_dim)\n    else:\n        model = BlockG2D(res=res)\n    return model\n\n\nclass Gen(torch.nn.Module):\n    def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.g_blocks = torch.nn.ModuleList(g_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        for g in self.g_blocks[:-1]:\n            x = g(x)\n        previous_img = self.rgb_blocks[0](x)\n        previous_img = self.upsample(previous_img)\n        x = self.g_blocks[-1](x)\n        new_img = self.rgb_blocks[1](x)\n        return fade_in(previous_img, new_img, self.fade_in_alpha)\n\n\ndef build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):\n    g_blocks = [\n        _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)\n    ]\n    rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]\n    for idx in range(2, len(g_blocks) + 1):\n        generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    final_model_list = g_blocks + [rgb_blocks[-1]]\n    generators.append(torch.nn.Sequential(*final_model_list))\n    return generators\n\n\nclass BlockD1D(torch.nn.Module):\n    def __init__(self, res=2):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)\n        self.bias3 = ApplyBias(in_features=1)\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512, 4, 4]\n        x = mini_batch_std(x)  # [batch, 513, 4, 4]\n        x = self.elr_conv2d(x)  # [batch, 512, 4, 4]\n        x = self.bias1(x)  # [batch, 512, 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]\n        x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]\n        x = self.elr_dense1(x)  # [batch, 512]\n        x = self.bias2(x)  # [batch, 512]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]\n        x = self.elr_dense2(x)  # [batch, 1]\n        x = self.bias3(x)  # [batch, 1]\n        return x\n\n\nclass BlockD2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.elr_conv2d1(x)\n        x = self.bias1(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.elr_conv2d2(x)\n        x = self.bias2(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.pool(x)\n        return x\n\n\ndef _block_D(res, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockD1D(res)\n    else:\n        model = BlockD2D(res)\n    return model\n\n\nclass Disc(torch.nn.Module):\n    def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.d_blocks = torch.nn.ModuleList(d_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        new_x = self.rgb_blocks[1](x)\n        new_x = self.d_blocks[-1](new_x)\n        downscale_x = self.pool(x)\n        downscale_x = self.rgb_blocks[0](downscale_x)\n        x = fade_in(downscale_x, new_x, self.fade_in_alpha)\n        for d in self.d_blocks[:-1][::-1]:\n            x = d(x)\n        return x\n\n\ndef build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):\n    d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]\n    rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]\n    for idx in range(2, len(d_blocks) + 1):\n        discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    return discriminators\n\n\n\nfade_in_alpha = torch.tensor(1.0)\nd_models = fe.build(\n    model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),\n    model_name=[\"d_{}\".format(size) for size in event_size])\n\ng_models = fe.build(\n    model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],\n    model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"])\n</pre> from torch.optim import Adam  def _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):     return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)   class EqualizedLRDense(torch.nn.Linear):     def __init__(self, in_features, out_features, gain=np.sqrt(2)):         super().__init__(in_features, out_features, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         self.wscale = np.float32(gain / np.sqrt(in_features))      def forward(self, x):         return super().forward(x) * self.wscale   class ApplyBias(torch.nn.Module):     def __init__(self, in_features):         super().__init__()         self.in_features = in_features         self.bias = torch.nn.Parameter(torch.Tensor(in_features))         torch.nn.init.constant_(self.bias.data, val=0.0)      def forward(self, x):         if len(x.shape) == 4:             x = x + self.bias.view(1, -1, 1, 1).expand_as(x)         else:             x = x + self.bias         return x   class EqualizedLRConv2D(torch.nn.Conv2d):     def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):         super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         fan_in = np.float32(np.prod(self.weight.data.shape[1:]))         self.wscale = np.float32(gain / np.sqrt(fan_in))      def forward(self, x):         return super().forward(x) * self.wscale   def pixel_normalization(x, eps=1e-8):     return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)   def mini_batch_std(x, group_size=4, eps=1e-8):     b, c, h, w = x.shape     group_size = min(group_size, b)     y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]     y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]     y = torch.mean(y**2, axis=0)  # [M, C, H, W]     y = torch.sqrt(y + eps)  # [M, C, H, W]     y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]     y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]     return torch.cat((x, y), 1)   def fade_in(x, y, alpha):     return (1.0 - alpha) * x + alpha * y   class ToRGB(torch.nn.Module):     def __init__(self, in_channels, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)         self.bias = ApplyBias(in_features=num_channels)      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         return x   class FromRGB(torch.nn.Module):     def __init__(self, res, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)         self.bias = ApplyBias(in_features=_nf(res - 1))      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         return x   class BlockG1D(torch.nn.Module):     def __init__(self, res=2, latent_dim=512):         super().__init__()         self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.res = res      def forward(self, x):         # x: [batch, 512]         x = pixel_normalization(x)  # [batch, 512]         x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]         x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]         x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]         x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]         x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)         return x   class BlockG2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]         x = self.upsample(x)         x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         return x   def _block_G(res, latent_dim=512, initial_resolution=2):     if res == initial_resolution:         model = BlockG1D(res=res, latent_dim=latent_dim)     else:         model = BlockG2D(res=res)     return model   class Gen(torch.nn.Module):     def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.g_blocks = torch.nn.ModuleList(g_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         for g in self.g_blocks[:-1]:             x = g(x)         previous_img = self.rgb_blocks[0](x)         previous_img = self.upsample(previous_img)         x = self.g_blocks[-1](x)         new_img = self.rgb_blocks[1](x)         return fade_in(previous_img, new_img, self.fade_in_alpha)   def build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):     g_blocks = [         _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)     ]     rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]     generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]     for idx in range(2, len(g_blocks) + 1):         generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     final_model_list = g_blocks + [rgb_blocks[-1]]     generators.append(torch.nn.Sequential(*final_model_list))     return generators   class BlockD1D(torch.nn.Module):     def __init__(self, res=2):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)         self.bias3 = ApplyBias(in_features=1)         self.res = res      def forward(self, x):         # x: [batch, 512, 4, 4]         x = mini_batch_std(x)  # [batch, 513, 4, 4]         x = self.elr_conv2d(x)  # [batch, 512, 4, 4]         x = self.bias1(x)  # [batch, 512, 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]         x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]         x = self.elr_dense1(x)  # [batch, 512]         x = self.bias2(x)  # [batch, 512]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]         x = self.elr_dense2(x)  # [batch, 1]         x = self.bias3(x)  # [batch, 1]         return x   class BlockD2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         x = self.elr_conv2d1(x)         x = self.bias1(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.elr_conv2d2(x)         x = self.bias2(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.pool(x)         return x   def _block_D(res, initial_resolution=2):     if res == initial_resolution:         model = BlockD1D(res)     else:         model = BlockD2D(res)     return model   class Disc(torch.nn.Module):     def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.d_blocks = torch.nn.ModuleList(d_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         new_x = self.rgb_blocks[1](x)         new_x = self.d_blocks[-1](new_x)         downscale_x = self.pool(x)         downscale_x = self.rgb_blocks[0](downscale_x)         x = fade_in(downscale_x, new_x, self.fade_in_alpha)         for d in self.d_blocks[:-1][::-1]:             x = d(x)         return x   def build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):     d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]     rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]     discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]     for idx in range(2, len(d_blocks) + 1):         discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     return discriminators    fade_in_alpha = torch.tensor(1.0) d_models = fe.build(     model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),     model_name=[\"d_{}\".format(size) for size in event_size])  g_models = fe.build(     model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],     model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"]) In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.backend import feed_forward, get_gradient\n\nclass ImageBlender(TensorOp):\n    def __init__(self, alpha, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.alpha = alpha\n\n    def forward(self, data, state):\n        image, image_lowres = data\n        new_img = self.alpha * image + (1 - self.alpha) * image_lowres\n        return new_img\n\n\nclass Interpolate(TensorOp):\n    def forward(self, data, state):\n        fake, real = data\n        batch_size = real.shape[0]\n        coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)\n        return real + (fake - real) * coeff\n\n\nclass GradientPenalty(TensorOp):\n    def __init__(self, inputs, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n    def forward(self, data, state):\n        x_interp, interp_score = data\n        gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)\n        grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))\n        gp = (grad_l2 - 1.0)**2\n        return gp\n\n\nclass GLoss(TensorOp):\n    def forward(self, data, state):\n        return -torch.mean(data)\n\n\nclass DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.wgan_lambda = wgan_lambda\n        self.wgan_epsilon = wgan_epsilon\n\n    def forward(self, data, state):\n        real_score, fake_score, gp = data\n        loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon\n        return torch.mean(loss)\n\n\nfake_img_map = {\n    epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nfake_score_map = {\n    epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\nreal_score_map = {\n    epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ninterp_score_map = {\n    epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ng_update_map = {\n    epoch: UpdateOp(loss_name=\"gloss\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nd_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)}\nnetwork = fe.Network(ops=[\n    EpochScheduler(fake_img_map),\n    EpochScheduler(fake_score_map),\n    ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),\n    EpochScheduler(real_score_map),\n    Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),\n    EpochScheduler(interp_score_map),\n    GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),\n    GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n    DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),\n    EpochScheduler(g_update_map),\n    EpochScheduler(d_update_map)\n])\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.backend import feed_forward, get_gradient  class ImageBlender(TensorOp):     def __init__(self, alpha, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.alpha = alpha      def forward(self, data, state):         image, image_lowres = data         new_img = self.alpha * image + (1 - self.alpha) * image_lowres         return new_img   class Interpolate(TensorOp):     def forward(self, data, state):         fake, real = data         batch_size = real.shape[0]         coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)         return real + (fake - real) * coeff   class GradientPenalty(TensorOp):     def __init__(self, inputs, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)      def forward(self, data, state):         x_interp, interp_score = data         gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)         grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))         gp = (grad_l2 - 1.0)**2         return gp   class GLoss(TensorOp):     def forward(self, data, state):         return -torch.mean(data)   class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.wgan_lambda = wgan_lambda         self.wgan_epsilon = wgan_epsilon      def forward(self, data, state):         real_score, fake_score, gp = data         loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon         return torch.mean(loss)   fake_img_map = {     epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } fake_score_map = {     epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } real_score_map = {     epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } interp_score_map = {     epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } g_update_map = {     epoch: UpdateOp(loss_name=\"gloss\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } d_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)} network = fe.Network(ops=[     EpochScheduler(fake_img_map),     EpochScheduler(fake_score_map),     ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),     EpochScheduler(real_score_map),     Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),     EpochScheduler(interp_score_map),     GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),     GLoss(inputs=\"fake_score\", outputs=\"gloss\"),     DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),     EpochScheduler(g_update_map),     EpochScheduler(d_update_map) ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace import Trace\nfrom fastestimator.trace.io import ModelSaver\n\nclass AlphaController(Trace):\n    def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):\n        super().__init__(inputs=None, outputs=None, mode=\"train\")\n        self.alpha = alpha\n        self.fade_start_epochs = fade_start_epochs\n        self.duration = duration\n        self.batch_scheduler = batch_scheduler\n        self.num_examples = num_examples\n        self.change_alpha = False\n        self.nimg_total = self.duration * self.num_examples\n        self._idx = 0\n        self.nimg_so_far = 0\n        self.current_batch_size = None\n\n    def on_epoch_begin(self, state):\n        # check whetehr the current epoch is in smooth transition of resolutions\n        fade_epoch = self.fade_start_epochs[self._idx]\n        if self.system.epoch_idx == fade_epoch:\n            self.change_alpha = True\n            self.nimg_so_far = 0\n            self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)\n            print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))\n        elif self.system.epoch_idx == fade_epoch + self.duration:\n            print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))\n            self.change_alpha = False\n            if self._idx + 1 &lt; len(self.fade_start_epochs):\n                self._idx += 1\n            self.alpha.data = torch.tensor(1.0)\n\n    def on_batch_begin(self, state):\n        # if in resolution transition, smoothly change the alpha from 0 to 1\n        if self.change_alpha:\n            self.nimg_so_far += self.current_batch_size\n            self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)\n            \ntraces = [\n    AlphaController(alpha=fade_in_alpha,\n                    fade_start_epochs=event_epoch[1:],\n                    duration=phase_length,\n                    batch_scheduler=batch_scheduler,\n                    num_examples=len(dataset)),\n    ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]\n            \n            \nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch)\n</pre> from fastestimator.trace import Trace from fastestimator.trace.io import ModelSaver  class AlphaController(Trace):     def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):         super().__init__(inputs=None, outputs=None, mode=\"train\")         self.alpha = alpha         self.fade_start_epochs = fade_start_epochs         self.duration = duration         self.batch_scheduler = batch_scheduler         self.num_examples = num_examples         self.change_alpha = False         self.nimg_total = self.duration * self.num_examples         self._idx = 0         self.nimg_so_far = 0         self.current_batch_size = None      def on_epoch_begin(self, state):         # check whetehr the current epoch is in smooth transition of resolutions         fade_epoch = self.fade_start_epochs[self._idx]         if self.system.epoch_idx == fade_epoch:             self.change_alpha = True             self.nimg_so_far = 0             self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)             print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))         elif self.system.epoch_idx == fade_epoch + self.duration:             print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))             self.change_alpha = False             if self._idx + 1 &lt; len(self.fade_start_epochs):                 self._idx += 1             self.alpha.data = torch.tensor(1.0)      def on_batch_begin(self, state):         # if in resolution transition, smoothly change the alpha from 0 to 1         if self.change_alpha:             self.nimg_so_far += self.current_batch_size             self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)              traces = [     AlphaController(alpha=fade_in_alpha,                     fade_start_epochs=event_epoch[1:],                     duration=phase_length,                     batch_scheduler=batch_scheduler,                     num_examples=len(dataset)),     ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]                           estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit()"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-gan-pggan", "title": "Progressive Growing GAN (PGGAN)\u00b6", "text": "<p>In this notebook, we will demonstrate the functionality of <code>Scheduler</code> which enables advanced training schemes such as the progressive training method described in Karras et al.. We will train a PGGAN to produce synthetic frontal chest X-ray images where both the generator and the discriminator grow from $4\\times4$ to $128\\times128$.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-strategy", "title": "Progressive Growing Strategy\u00b6", "text": "<p>Karras et al. propose a training scheme in which both the generator and the discriminator progressively grow from a low resolution to a high resolution. Both networks begin their training based on $4\\times4$ images as illustrated below.  Then, both networks progress from $4\\times4$ to $8\\times8$ by an adding an additional block that contains a couple of convolutional layers.  Both the generator and the discriminator progressively grow until reaching the desired resolution of $1024\\times 1024$.  Image Credit: Presentation slide</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#smooth-transition-between-resolutions", "title": "Smooth Transition between Resolutions\u00b6", "text": "<p>However, when growing the networks, the new blocks must be slowly faded into the networks in order to smoothly transition between different resolutions. For example, when growing the generator from $16\\times16$ to $32\\times32$, the newly added block of $32\\times32$ is slowly faded into the already well trained $16\\times16$ network by linearly increasing a fade-factor $\\alpha$ from $0$ to $1$. Once the network is fully transitioned to $32\\times32$, the network is trained a bit further to stabilize before growing to $64\\times64$.  Image Credit: PGGAN Paper</p> <p>With this progressive training strategy, PGGAN has achieved the state-of-the-art results in producing high fidelity synthetic images.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#problem-setting", "title": "Problem Setting\u00b6", "text": "<p>In this PGGAN example, we decided the following:</p> <ul> <li>560K images will be used when transitioning from a lower resolution to a higher resolution.</li> <li>560K images will be used when stabilizing the fully transitioned network.</li> <li>Initial resolution will be $4\\times4$.</li> <li>Final resolution will be $128\\times128$.</li> </ul> <p>The number of images for both transitioning and stabilizing is equivalent to 5 epochs; the networks would smoothly grow over 5 epochs and would stabilize for 5 epochs. This yields the following schedule for growing both networks:</p> <ul> <li>From $1^{st}$ epoch to $5^{th}$ epoch: train $4\\times4$ resolution</li> <li>From $6^{th}$ epoch to $10^{th}$ epoch: transition from $4\\times4$ to $8\\times8$</li> <li>From $11^{th}$ epoch to $15^{th}$ epoch: stabilize $8\\times8$</li> <li>From $16^{th}$ epoch to $20^{th}$ epoch: transition from $8\\times8$ to $16\\times16$</li> <li>From $21^{st}$ epoch to $25^{th}$ epoch: stabilize $16\\times16$</li> <li>From $26^{th}$ epoch to $30^{th}$ epoch: transition from $16\\times16$ to $32\\times32$</li> <li>From $31^{st}$ epoch to $35^{th}$ epoch: stabilize $32\\times32$</li> </ul> <p>$\\cdots$</p> <ul> <li>From $51^{th}$ epoch to $55^{th}$ epoch: stabilize $128\\times128$</li> </ul>"}, {"location": "apphub/image_generation/pggan/pggan.html#configure-growing-parameters", "title": "Configure growing parameters\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-input-pipeline", "title": "Defining Input <code>Pipeline</code>\u00b6", "text": "<p>First, we need to download the chest frontal X-ray dataset from the National Institute of Health (NIH); the dataset has over 112,000 images with resolution $1024\\times1024$. We use the pre-built <code>fastestimator.dataset.nih_chestxray</code> API to download these images. A detailed description of the dataset is available here.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#note-please-make-sure-to-have-a-stable-internet-connection-when-downloading-the-dataset-for-the-first-time-since-the-size-of-the-dataset-is-over-40gb", "title": "Note: Please make sure to have a stable internet connection when downloading the dataset for the first time since the size of the dataset is over 40GB.\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#given-the-images-we-need-the-following-preprocessing-operations-to-execute-dynamically-for-every-batch", "title": "Given the images, we need the following preprocessing operations to execute dynamically for every batch:\u00b6", "text": "<ol> <li>Read the image.</li> <li>Resize the image to the correct size based on the current epoch.</li> <li>Create a lower resolution of the image, which is accomplished by downsampling by a factor of 2 then upsampling by a factor of 2.</li> <li>Rescale the pixels of both the original image and lower resolution image to the range [-1, 1]</li> <li>Convert both the original image and lower resolution image from channel last to channel first</li> <li>Create the latent vector used by the generator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-network", "title": "Defining <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-the-generator-and-the-discriminator", "title": "Defining the generator and the discriminator\u00b6", "text": "<p>To express the progressive growing of networks, we return a list of models that progressively grow from $4 \\times 4$ to $1024 \\times 1024$ such that $i^{th}$ model in the list is a superset of the previous models. We define a <code>fade_in_alpha</code> to control the smoothness of growth. <code>fe.build</code> then bundles each model, optimizer, and model name together for use.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#the-following-operations-will-happen-in-our-network", "title": "The Following operations will happen in our <code>Network</code>:\u00b6", "text": "<ol> <li>random vector -&gt; generator -&gt; fake images</li> <li>fake images -&gt; discriminator -&gt; fake scores</li> <li>real image, low resolution real image -&gt; blender -&gt; blended real images</li> <li>blended real images -&gt; discriminator -&gt; real scores</li> <li>fake images, real images -&gt; interpolater -&gt; interpolated images</li> <li>interpolated images -&gt; discriminator -&gt; interpolated scores</li> <li>interpolated scores, interpolated image -&gt; get_gradient -&gt; gradient penalty</li> <li>fake_score -&gt; GLoss -&gt; generator loss</li> <li>real score, fake score, gradient penalty -&gt; DLoss -&gt; discriminator loss</li> <li>update generator</li> <li>update discriminator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-estimator", "title": "Defining Estimator\u00b6", "text": "<p>Given that <code>Pipeline</code> and <code>Network</code> are properly defined, we need to define an <code>AlphaController</code> <code>Trace</code> to help both the generator and the discriminator smoothly grow by controlling the value of the <code>fade_in_alpha</code> tensor created previously.  We will also use <code>ModelSaver</code> to save our model during every training phase.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#note-for-128x128-resolution-it-takes-about-24-hours-on-single-v100-gpu-for-1024x1024-resolution-it-takes-25-days-on-4-v100-gpus", "title": "Note: for 128x128 resolution, it takes about 24 hours on single V100 GPU.    for 1024x1024 resolution, it takes ~ 2.5 days on 4 V100 GPUs.\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html", "title": "Instance Detection with RetinaNet", "text": "<p>We are going to implement RetinaNet by Lin et al., 2017 for COCO dataset instance detection.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom matplotlib import pyplot as plt\n\nimport fastestimator as fe\n</pre> import tempfile  import cv2 import numpy as np import torch import torch.nn as nn import torchvision from matplotlib import pyplot as plt  import fastestimator as fe In\u00a0[2]: parameters Copied! <pre>#training parameters\ndata_dir=None\nbatch_size = 16\nepochs = 13\nimage_size=512\nnum_classes=90\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nmodel_dir=tempfile.mkdtemp()\nclass_json_path = 'class.json'\n</pre> #training parameters data_dir=None batch_size = 16 epochs = 13 image_size=512 num_classes=90 train_steps_per_epoch = None eval_steps_per_epoch = None model_dir=tempfile.mkdtemp() class_json_path = 'class.json' <p>The <code>class.json</code> includes a map for the class number and what object the number corresponds to.</p> In\u00a0[3]: Copied! <pre>import json\n\nwith open(class_json_path, 'r') as f:\n    class_map = json.load(f)\n</pre> import json  with open(class_json_path, 'r') as f:     class_map = json.load(f) <p>We call our <code>mscoco</code> data API to obtain the training and validation set:</p> <ul> <li>118287 images for training</li> <li>5000 images for validation</li> </ul> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\n\ntrain_ds, eval_ds = mscoco.load_data(root_dir=data_dir)\nprint(len(train_ds)) # 118287\nprint(len(eval_ds)) # 5000\n</pre> from fastestimator.dataset.data import mscoco  train_ds, eval_ds = mscoco.load_data(root_dir=data_dir) print(len(train_ds)) # 118287 print(len(eval_ds)) # 5000 <pre>118287\n5000\n</pre> <p>Anchors are predefined for each pixel in the feature map. In this apphub our backbone is ResNet-50, so we can precompute all the anchors we need for training.</p> In\u00a0[5]: Copied! <pre>def _get_fpn_anchor_box(width, height):\n    assert height % 32 == 0 and width % 32 == 0\n    shapes = [(int(height / 8), int(width / 8))]  # P3\n    num_pixel = [np.prod(shapes)]\n    anchor_lengths = [32, 64, 128, 256, 512]\n    for _ in range(4):  # P4 through P7\n        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n        num_pixel.append(np.prod(shapes[-1]))\n    total_num_pixels = np.sum(num_pixel)\n    anchorbox = np.zeros((9 * total_num_pixels, 4))\n    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n    anchor_idx = 0\n    for shape, anchor_length in zip(shapes, anchor_lengths):\n        p_h, p_w = shape\n        base_y = 2**np.ceil(np.log2(height / p_h))\n        base_x = 2**np.ceil(np.log2(width / p_w))\n        for i in range(p_h):\n            center_y = (i + 1 / 2) * base_y\n            for j in range(p_w):\n                center_x = (j + 1 / 2) * base_x\n                for anchor_length_multiplier in anchor_length_multipliers:\n                    area = (anchor_length * anchor_length_multiplier)**2\n                    for aspect_ratio in aspect_ratios:\n                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n                        anchorbox[anchor_idx, 0] = x1\n                        anchorbox[anchor_idx, 1] = y1\n                        anchorbox[anchor_idx, 2] = x2 - x1\n                        anchorbox[anchor_idx, 3] = y2 - y1\n                        anchor_idx += 1\n        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n            break\n    return np.float32(anchorbox), np.int32(num_pixel) * 9\n</pre> def _get_fpn_anchor_box(width, height):     assert height % 32 == 0 and width % 32 == 0     shapes = [(int(height / 8), int(width / 8))]  # P3     num_pixel = [np.prod(shapes)]     anchor_lengths = [32, 64, 128, 256, 512]     for _ in range(4):  # P4 through P7         shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))         num_pixel.append(np.prod(shapes[-1]))     total_num_pixels = np.sum(num_pixel)     anchorbox = np.zeros((9 * total_num_pixels, 4))     anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]     aspect_ratios = [1.0, 2.0, 0.5]  #x:y     anchor_idx = 0     for shape, anchor_length in zip(shapes, anchor_lengths):         p_h, p_w = shape         base_y = 2**np.ceil(np.log2(height / p_h))         base_x = 2**np.ceil(np.log2(width / p_w))         for i in range(p_h):             center_y = (i + 1 / 2) * base_y             for j in range(p_w):                 center_x = (j + 1 / 2) * base_x                 for anchor_length_multiplier in anchor_length_multipliers:                     area = (anchor_length * anchor_length_multiplier)**2                     for aspect_ratio in aspect_ratios:                         x1 = center_x - np.sqrt(area * aspect_ratio) / 2                         y1 = center_y - np.sqrt(area / aspect_ratio) / 2                         x2 = center_x + np.sqrt(area * aspect_ratio) / 2                         y2 = center_y + np.sqrt(area / aspect_ratio) / 2                         anchorbox[anchor_idx, 0] = x1                         anchorbox[anchor_idx, 1] = y1                         anchorbox[anchor_idx, 2] = x2 - x1                         anchorbox[anchor_idx, 3] = y2 - y1                         anchor_idx += 1         if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore             break     return np.float32(anchorbox), np.int32(num_pixel) * 9 In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass ShiftLabel(NumpyOp):\n    def forward(self, data, state):\n        # the label of COCO dataset starts from 1, shifting the start to 0\n        bbox = np.array(data, dtype=np.float32)\n        bbox[:, -1] = bbox[:, -1] - 1\n        return bbox\n\n\nclass AnchorBox(NumpyOp):\n    def __init__(self, width, height, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n\n    def forward(self, data, state):\n        target = self._generate_target(data)  # bbox is #obj x 5\n        return np.float32(target)\n\n    def _generate_target(self, bbox):\n        object_boxes = bbox[:, :-1]  # num_obj x 4\n        label = bbox[:, -1]  # num_obj x 1\n        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n        #now for each object in image, assign the anchor box with highest iou to them\n        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n        num_obj = ious.shape[0]\n        for row in range(num_obj):\n            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n        #next, begin the anchor box assignment based on iou\n        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n        cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class\n        cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples\n        #finally, calculate localization target\n        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n        dwidth = np.squeeze(np.log(gt_width / ac_width))\n        dheight = np.squeeze(np.log(gt_height / ac_height))\n        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n\n    @staticmethod\n    def _get_iou(boxes1, boxes2):\n\"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n        Args:\n            box1 (array): first boxes in N x 4\n            box2 (array): second box in M x 4\n        Returns:\n            float: IoU value in N x M\n        \"\"\"\n        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n        x12 = x11 + w1\n        y12 = y11 + h1\n        x22 = x21 + w2\n        y22 = y21 + h2\n        xmin = np.maximum(x11, np.transpose(x21))\n        ymin = np.maximum(y11, np.transpose(y21))\n        xmax = np.minimum(x12, np.transpose(x22))\n        ymax = np.minimum(y12, np.transpose(y22))\n        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n        area1 = (w1 + 1) * (h1 + 1)\n        area2 = (w2 + 1) * (h2 + 1)\n        iou = inter_area / (area1 + area2.T - inter_area)\n        return iou\n</pre> from fastestimator.op.numpyop import NumpyOp  class ShiftLabel(NumpyOp):     def forward(self, data, state):         # the label of COCO dataset starts from 1, shifting the start to 0         bbox = np.array(data, dtype=np.float32)         bbox[:, -1] = bbox[:, -1] - 1         return bbox   class AnchorBox(NumpyOp):     def __init__(self, width, height, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.anchorbox, _ = _get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4      def forward(self, data, state):         target = self._generate_target(data)  # bbox is #obj x 5         return np.float32(target)      def _generate_target(self, bbox):         object_boxes = bbox[:, :-1]  # num_obj x 4         label = bbox[:, -1]  # num_obj x 1         ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor         #now for each object in image, assign the anchor box with highest iou to them         anchorbox_best_iou_idx = np.argmax(ious, axis=1)         num_obj = ious.shape[0]         for row in range(num_obj):             ious[row, anchorbox_best_iou_idx[row]] = 0.99         #next, begin the anchor box assignment based on iou         anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1         anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1         cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1         cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class         cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples         #finally, calculate localization target         single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4         gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)         ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)         dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)         dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)         dwidth = np.squeeze(np.log(gt_width / ac_width))         dheight = np.squeeze(np.log(gt_height / ac_height))         return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5      @staticmethod     def _get_iou(boxes1, boxes2):         \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.         Args:             box1 (array): first boxes in N x 4             box2 (array): second box in M x 4         Returns:             float: IoU value in N x M         \"\"\"         x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)         x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)         x12 = x11 + w1         y12 = y11 + h1         x22 = x21 + w2         y22 = y21 + h2         xmin = np.maximum(x11, np.transpose(x21))         ymin = np.maximum(y11, np.transpose(y21))         xmax = np.minimum(x12, np.transpose(x22))         ymax = np.minimum(y12, np.transpose(y22))         inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)         area1 = (w1 + 1) * (h1 + 1)         area2 = (w2 + 1) * (h2 + 1)         iou = inter_area / (area1 + area2.T - inter_area)         return iou <p>For our data pipeline, we resize the images such that the longer side is 512 pixels. We keep the aspect ratio the same as original image, so on the shorter side we pad zeros. The resized image is 512 by 512. For data augmentation we only flip the image horizontally.</p> In\u00a0[7]: Copied! <pre>from albumentations import BboxParams\n\nfrom fastestimator.op.numpyop import Batch\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=eval_ds,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        LongestMaxSize(image_size, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params=BboxParams(\"coco\", min_area=1.0)),\n        PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\",bbox_params=BboxParams(\"coco\", min_area=1.0)),\n        Sometimes(HorizontalFlip(mode=\"train\", image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params='coco')),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        ShiftLabel(inputs=\"bbox\", outputs=\"bbox\"),\n        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=image_size, height=image_size),\n        ChannelTranspose(inputs=\"image\", outputs=\"image\"),\n        Batch(batch_size=batch_size, pad_value=0)\n    ])\n</pre> from albumentations import BboxParams  from fastestimator.op.numpyop import Batch from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=eval_ds,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         LongestMaxSize(image_size, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params=BboxParams(\"coco\", min_area=1.0)),         PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_CONSTANT, image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\",bbox_params=BboxParams(\"coco\", min_area=1.0)),         Sometimes(HorizontalFlip(mode=\"train\", image_in=\"image\", image_out=\"image\", bbox_in=\"bbox\", bbox_out=\"bbox\", bbox_params='coco')),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         ShiftLabel(inputs=\"bbox\", outputs=\"bbox\"),         AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=image_size, height=image_size),         ChannelTranspose(inputs=\"image\", outputs=\"image\"),         Batch(batch_size=batch_size, pad_value=0)     ]) In\u00a0[8]: Copied! <pre>batch_data = pipeline.get_results(mode='eval', num_steps=2)\n</pre> batch_data = pipeline.get_results(mode='eval', num_steps=2) In\u00a0[9]: Copied! <pre>import matplotlib.patches as patches\n\nstep_index = 0\nbatch_index = 7\n\nimg = batch_data[step_index]['image'][batch_index].numpy()\nimg = ((img + 1)/2 * 255).astype(np.uint8)\nimg = np.transpose(img, [1, 2, 0])\n\nkeep = ~np.all(batch_data[step_index]['bbox'][batch_index].numpy() == 0, axis=1)\nx1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\n\nfig, ax = plt.subplots(figsize=(10, 10))\nax.imshow(img)\nfor j in range(len(x1)):\n    rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\n    ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]+1))], color=(1, 0, 0), fontsize=14)\n\n\nprint(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))\n</pre> import matplotlib.patches as patches  step_index = 0 batch_index = 7  img = batch_data[step_index]['image'][batch_index].numpy() img = ((img + 1)/2 * 255).astype(np.uint8) img = np.transpose(img, [1, 2, 0])  keep = ~np.all(batch_data[step_index]['bbox'][batch_index].numpy() == 0, axis=1) x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T  fig, ax = plt.subplots(figsize=(10, 10)) ax.imshow(img) for j in range(len(x1)):     rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='r',facecolor='none')     ax.add_patch(rect)     ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]+1))], color=(1, 0, 0), fontsize=14)   print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy())) <pre>id = 872\n</pre> <p>We define the classification (class) subnet and regression (box) subnet. See Fig. 3 of the original paper.</p> In\u00a0[10]: Copied! <pre>class ClassificationSubNet(nn.Module):\n    def __init__(self, in_channels, num_classes, num_anchors=9):\n        super().__init__()\n        self.num_classes = num_classes\n        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_1.bias.data)\n        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_2.bias.data)\n        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_3.bias.data)\n        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_4.bias.data)\n        self.conv2d_5 = nn.Conv2d(256, num_classes * num_anchors, 3, padding=1)\n        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n        nn.init.constant_(self.conv2d_5.bias.data, val=np.log(1 / 99))\n\n    def forward(self, x):\n        x = self.conv2d_1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_2(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_3(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_4(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_5(x)\n        x = torch.sigmoid(x)\n        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position\n        return x.reshape(x.size(0), -1, self.num_classes)  # the output dimension is [batch, #anchor, #classes]\n\n\nclass RegressionSubNet(nn.Module):\n    def __init__(self, in_channels, num_anchors=9):\n        super().__init__()\n        self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_1.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_1.bias.data)\n        self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_2.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_2.bias.data)\n        self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_3.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_3.bias.data)\n        self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)\n        nn.init.normal_(self.conv2d_4.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_4.bias.data)\n        self.conv2d_5 = nn.Conv2d(256, 4 * num_anchors, 3, padding=1)\n        nn.init.normal_(self.conv2d_5.weight.data, std=0.01)\n        nn.init.zeros_(self.conv2d_5.bias.data)\n\n    def forward(self, x):\n        x = self.conv2d_1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_2(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_3(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_4(x)\n        x = nn.functional.relu(x)\n        x = self.conv2d_5(x)\n        x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position\n        return x.reshape(x.size(0), -1, 4)  # the output dimension is [batch, #anchor, 4]\n</pre> class ClassificationSubNet(nn.Module):     def __init__(self, in_channels, num_classes, num_anchors=9):         super().__init__()         self.num_classes = num_classes         self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)         nn.init.normal_(self.conv2d_1.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_1.bias.data)         self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_2.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_2.bias.data)         self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_3.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_3.bias.data)         self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_4.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_4.bias.data)         self.conv2d_5 = nn.Conv2d(256, num_classes * num_anchors, 3, padding=1)         nn.init.normal_(self.conv2d_5.weight.data, std=0.01)         nn.init.constant_(self.conv2d_5.bias.data, val=np.log(1 / 99))      def forward(self, x):         x = self.conv2d_1(x)         x = nn.functional.relu(x)         x = self.conv2d_2(x)         x = nn.functional.relu(x)         x = self.conv2d_3(x)         x = nn.functional.relu(x)         x = self.conv2d_4(x)         x = nn.functional.relu(x)         x = self.conv2d_5(x)         x = torch.sigmoid(x)         x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position         return x.reshape(x.size(0), -1, self.num_classes)  # the output dimension is [batch, #anchor, #classes]   class RegressionSubNet(nn.Module):     def __init__(self, in_channels, num_anchors=9):         super().__init__()         self.conv2d_1 = nn.Conv2d(in_channels, 256, 3, padding=1)         nn.init.normal_(self.conv2d_1.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_1.bias.data)         self.conv2d_2 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_2.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_2.bias.data)         self.conv2d_3 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_3.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_3.bias.data)         self.conv2d_4 = nn.Conv2d(256, 256, 3, padding=1)         nn.init.normal_(self.conv2d_4.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_4.bias.data)         self.conv2d_5 = nn.Conv2d(256, 4 * num_anchors, 3, padding=1)         nn.init.normal_(self.conv2d_5.weight.data, std=0.01)         nn.init.zeros_(self.conv2d_5.bias.data)      def forward(self, x):         x = self.conv2d_1(x)         x = nn.functional.relu(x)         x = self.conv2d_2(x)         x = nn.functional.relu(x)         x = self.conv2d_3(x)         x = nn.functional.relu(x)         x = self.conv2d_4(x)         x = nn.functional.relu(x)         x = self.conv2d_5(x)         x = x.permute(0, 2, 3, 1)  # [8, c, h, w] -&gt; [8, h, w, c] , to make reshape meaningful on position         return x.reshape(x.size(0), -1, 4)  # the output dimension is [batch, #anchor, 4] <p>We use ResNet-50 as our backbone.</p> In\u00a0[11]: Copied! <pre>class RetinaNet(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        res50_layers = list(torchvision.models.resnet50(pretrained=True).children())\n        self.res50_in_C3 = nn.Sequential(*(res50_layers[:6]))\n        self.res50_C3_C4 = nn.Sequential(*(res50_layers[6]))\n        self.res50_C4_C5 = nn.Sequential(*(res50_layers[7]))\n        self.conv2d_C5 = nn.Conv2d(2048, 256, 1)\n        self.conv2d_C4 = nn.Conv2d(1024, 256, 1)\n        self.conv2d_C3 = nn.Conv2d(512, 256, 1)\n        self.conv2d_P6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)\n        self.conv2d_P7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n        self.conv2d_P5 = nn.Conv2d(256, 256, 3, padding=1)\n        self.conv2d_P4 = nn.Conv2d(256, 256, 3, padding=1)\n        self.conv2d_P3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.cls_net = ClassificationSubNet(in_channels=256, num_classes=num_classes)\n        self.reg_net = RegressionSubNet(in_channels=256)\n\n    def forward(self, x):\n        C3 = self.res50_in_C3(x)\n        C4 = self.res50_C3_C4(C3)\n        C5 = self.res50_C4_C5(C4)\n        P5 = self.conv2d_C5(C5)\n        P5_upsampling = nn.functional.interpolate(P5, scale_factor=2)\n        P4 = self.conv2d_C4(C4)\n        P4 = P5_upsampling + P4\n        P4_upsampling = nn.functional.interpolate(P4, scale_factor=2)\n        P3 = self.conv2d_C3(C3)\n        P3 = P4_upsampling + P3\n        P6 = self.conv2d_P6(C5)\n        P7 = nn.functional.relu(P6)\n        P7 = self.conv2d_P7(P7)\n        P5 = self.conv2d_P5(P5)\n        P4 = self.conv2d_P4(P4)\n        P3 = self.conv2d_P3(P3)\n        pyramid = [P3, P4, P5, P6, P7]\n        cls_output = torch.cat([self.cls_net(x) for x in pyramid], dim=-2)\n        loc_output = torch.cat([self.reg_net(x) for x in pyramid], dim=-2)\n        return cls_output, loc_output\n</pre> class RetinaNet(nn.Module):     def __init__(self, num_classes):         super().__init__()         res50_layers = list(torchvision.models.resnet50(pretrained=True).children())         self.res50_in_C3 = nn.Sequential(*(res50_layers[:6]))         self.res50_C3_C4 = nn.Sequential(*(res50_layers[6]))         self.res50_C4_C5 = nn.Sequential(*(res50_layers[7]))         self.conv2d_C5 = nn.Conv2d(2048, 256, 1)         self.conv2d_C4 = nn.Conv2d(1024, 256, 1)         self.conv2d_C3 = nn.Conv2d(512, 256, 1)         self.conv2d_P6 = nn.Conv2d(2048, 256, 3, stride=2, padding=1)         self.conv2d_P7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)         self.conv2d_P5 = nn.Conv2d(256, 256, 3, padding=1)         self.conv2d_P4 = nn.Conv2d(256, 256, 3, padding=1)         self.conv2d_P3 = nn.Conv2d(256, 256, 3, padding=1)         self.cls_net = ClassificationSubNet(in_channels=256, num_classes=num_classes)         self.reg_net = RegressionSubNet(in_channels=256)      def forward(self, x):         C3 = self.res50_in_C3(x)         C4 = self.res50_C3_C4(C3)         C5 = self.res50_C4_C5(C4)         P5 = self.conv2d_C5(C5)         P5_upsampling = nn.functional.interpolate(P5, scale_factor=2)         P4 = self.conv2d_C4(C4)         P4 = P5_upsampling + P4         P4_upsampling = nn.functional.interpolate(P4, scale_factor=2)         P3 = self.conv2d_C3(C3)         P3 = P4_upsampling + P3         P6 = self.conv2d_P6(C5)         P7 = nn.functional.relu(P6)         P7 = self.conv2d_P7(P7)         P5 = self.conv2d_P5(P5)         P4 = self.conv2d_P4(P4)         P3 = self.conv2d_P3(P3)         pyramid = [P3, P4, P5, P6, P7]         cls_output = torch.cat([self.cls_net(x) for x in pyramid], dim=-2)         loc_output = torch.cat([self.reg_net(x) for x in pyramid], dim=-2)         return cls_output, loc_output <p>We use focal loss for classification and smooth L1 for regression loss.</p> In\u00a0[12]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass RetinaLoss(TensorOp):\n    def forward(self, data, state):\n        anchorbox, cls_pred, loc_pred = data\n        batch_size = anchorbox.size(0)\n        focal_loss, l1_loss = 0.0, 0.0\n        for idx in range(batch_size):\n            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], anchorbox[idx][:, -1].long()\n            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n            single_focal_loss, anchor_obj_bool = self.focal_loss(single_cls_gt, single_cls_pred)\n            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_bool)\n            focal_loss += single_focal_loss\n            l1_loss += single_l1_loss\n        focal_loss = focal_loss / batch_size\n        l1_loss = l1_loss / batch_size\n        total_loss = focal_loss + l1_loss\n        return total_loss, focal_loss, l1_loss\n\n    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n        num_classes = single_cls_pred.size(-1)\n        # gather the objects and background, discard the rest\n        anchor_obj_bool = single_cls_gt &gt;= 0\n        anchor_background_obj_bool = single_cls_gt &gt;= -1\n        anchor_background_bool = single_cls_gt == -1\n        # create one hot encoder, make -1 (background) and -2 (ignore) encoded as 0 in ground truth\n        single_cls_gt[single_cls_gt &lt; 0] = 0\n        single_cls_gt = nn.functional.one_hot(single_cls_gt, num_classes=num_classes)\n        single_cls_gt[anchor_background_bool] = 0\n        single_cls_gt = single_cls_gt[anchor_background_obj_bool]  # remove all ignore cases\n        single_cls_gt = single_cls_gt.view(-1)\n        single_cls_pred = single_cls_pred[anchor_background_obj_bool]\n        single_cls_pred = single_cls_pred.view(-1)\n        # compute the focal weight on each selected anchor box\n        alpha_factor = torch.ones_like(single_cls_gt) * alpha\n        alpha_factor = torch.where(single_cls_gt == 1, alpha_factor, 1 - alpha_factor)\n        focal_weight = torch.where(single_cls_gt == 1, 1 - single_cls_pred, single_cls_pred)\n        focal_weight = alpha_factor * focal_weight**gamma / torch.sum(anchor_obj_bool)\n        focal_loss = nn.functional.binary_cross_entropy(input=single_cls_pred,\n                                                        target=single_cls_gt.float(),\n                                                        weight=focal_weight.detach(),\n                                                        reduction=\"sum\")\n        return focal_loss, anchor_obj_bool\n\n    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_bool, beta=0.1):\n        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n        single_loc_pred = single_loc_pred[anchor_obj_bool]  # anchor_obj_count x 4\n        single_loc_gt = single_loc_gt[anchor_obj_bool]  # anchor_obj_count x 4\n        single_loc_pred = single_loc_pred.view(-1)\n        single_loc_gt = single_loc_gt.view(-1)\n        loc_diff = torch.abs(single_loc_gt - single_loc_pred)\n        loc_loss = torch.where(loc_diff &lt; beta, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n        loc_loss = torch.sum(loc_loss) / torch.sum(anchor_obj_bool)\n        return loc_loss\n</pre> from fastestimator.op.tensorop import TensorOp  class RetinaLoss(TensorOp):     def forward(self, data, state):         anchorbox, cls_pred, loc_pred = data         batch_size = anchorbox.size(0)         focal_loss, l1_loss = 0.0, 0.0         for idx in range(batch_size):             single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], anchorbox[idx][:, -1].long()             single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]             single_focal_loss, anchor_obj_bool = self.focal_loss(single_cls_gt, single_cls_pred)             single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_bool)             focal_loss += single_focal_loss             l1_loss += single_l1_loss         focal_loss = focal_loss / batch_size         l1_loss = l1_loss / batch_size         total_loss = focal_loss + l1_loss         return total_loss, focal_loss, l1_loss      def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):         # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]         num_classes = single_cls_pred.size(-1)         # gather the objects and background, discard the rest         anchor_obj_bool = single_cls_gt &gt;= 0         anchor_background_obj_bool = single_cls_gt &gt;= -1         anchor_background_bool = single_cls_gt == -1         # create one hot encoder, make -1 (background) and -2 (ignore) encoded as 0 in ground truth         single_cls_gt[single_cls_gt &lt; 0] = 0         single_cls_gt = nn.functional.one_hot(single_cls_gt, num_classes=num_classes)         single_cls_gt[anchor_background_bool] = 0         single_cls_gt = single_cls_gt[anchor_background_obj_bool]  # remove all ignore cases         single_cls_gt = single_cls_gt.view(-1)         single_cls_pred = single_cls_pred[anchor_background_obj_bool]         single_cls_pred = single_cls_pred.view(-1)         # compute the focal weight on each selected anchor box         alpha_factor = torch.ones_like(single_cls_gt) * alpha         alpha_factor = torch.where(single_cls_gt == 1, alpha_factor, 1 - alpha_factor)         focal_weight = torch.where(single_cls_gt == 1, 1 - single_cls_pred, single_cls_pred)         focal_weight = alpha_factor * focal_weight**gamma / torch.sum(anchor_obj_bool)         focal_loss = nn.functional.binary_cross_entropy(input=single_cls_pred,                                                         target=single_cls_gt.float(),                                                         weight=focal_weight.detach(),                                                         reduction=\"sum\")         return focal_loss, anchor_obj_bool      def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_bool, beta=0.1):         # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]         single_loc_pred = single_loc_pred[anchor_obj_bool]  # anchor_obj_count x 4         single_loc_gt = single_loc_gt[anchor_obj_bool]  # anchor_obj_count x 4         single_loc_pred = single_loc_pred.view(-1)         single_loc_gt = single_loc_gt.view(-1)         loc_diff = torch.abs(single_loc_gt - single_loc_pred)         loc_loss = torch.where(loc_diff &lt; beta, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)         loc_loss = torch.sum(loc_loss) / torch.sum(anchor_obj_bool)         return loc_loss <p>The learning rate has a warm up phase when step number is &lt; 1000. After that, we reduce the learning rate by 10 at 60k and 80k respectively. The original batch size in the paper is 16 for 8 GPUs. Here we are using 1GPU, with batch size 16 due to a smaller image size.</p> In\u00a0[13]: Copied! <pre>def lr_fn(step):\n    if step &lt; 1000:\n        lr = (0.01 - 0.0002) / 1000 * step + 0.0002\n    elif step &lt; 60000:\n        lr = 0.01\n    elif step &lt; 80000:\n        lr = 0.001\n    else:\n        lr = 0.0001\n    return lr\n</pre> def lr_fn(step):     if step &lt; 1000:         lr = (0.01 - 0.0002) / 1000 * step + 0.0002     elif step &lt; 60000:         lr = 0.01     elif step &lt; 80000:         lr = 0.001     else:         lr = 0.0001     return lr   In\u00a0[14]: Copied! <pre>model = fe.build(model_fn=lambda: RetinaNet(num_classes=num_classes), optimizer_fn=lambda x: torch.optim.SGD(x, lr=2e-4, momentum=0.9, weight_decay=0.0001))\n</pre> model = fe.build(model_fn=lambda: RetinaNet(num_classes=num_classes), optimizer_fn=lambda x: torch.optim.SGD(x, lr=2e-4, momentum=0.9, weight_decay=0.0001)) <p>During evaluation, testing and inferencing, some additional postprocessing stes are required, for example, filter out lower scores and perform Non-maximal suppression on bounding box predictions. These postprocessing steps are implemented as <code>TensorOp</code> named <code>PredictBox</code>.</p> In\u00a0[15]: Copied! <pre>class PredictBox(TensorOp):\n\"\"\"Convert network output to bounding boxes.\n        \"\"\"\n    def __init__(self,\n                 inputs=None,\n                 outputs=None,\n                 mode=None,\n                 input_shape=(512, 512, 3),\n                 select_top_k=1000,\n                 nms_max_outputs=100,\n                 score_threshold=0.05):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.input_shape = input_shape\n        self.select_top_k = select_top_k\n        self.nms_max_outputs = nms_max_outputs\n        self.score_threshold = score_threshold\n        self.all_anchors, self.num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n        self.all_anchors = torch.Tensor(self.all_anchors)\n        if torch.cuda.is_available():\n            self.all_anchors = self.all_anchors.to(\"cuda:0\")\n\n    def forward(self, data, state):\n        cls_pred, loc_pred = data  # [Batch, #anchor, #num_classes], [Batch, #anchor, 4]\n        batch_size = cls_pred.size(0)\n        scores_pred, labels_pred = torch.max(cls_pred, dim=-1)\n        # loc_pred -&gt; loc_abs\n        x1_abs = loc_pred[..., 0] * self.all_anchors[..., 2] + self.all_anchors[..., 0]\n        y1_abs = loc_pred[..., 1] * self.all_anchors[..., 3] + self.all_anchors[..., 1]\n        w_abs = torch.exp(loc_pred[..., 2]) * self.all_anchors[..., 2]\n        h_abs = torch.exp(loc_pred[..., 3]) * self.all_anchors[..., 3]\n        x2_abs, y2_abs = x1_abs + w_abs, y1_abs + h_abs\n        # iterate over images\n        final_results = []\n        for idx in range(batch_size):\n            scores_pred_single = scores_pred[idx]\n            boxes_pred_single = torch.stack([x1_abs[idx], y1_abs[idx], x2_abs[idx], y2_abs[idx]], dim=-1)\n            # iterate over each pyramid to select top 1000 anchor boxes\n            start = 0\n            top_idx = []\n            for num_anchors_fpn_level in self.num_anchors_per_level:\n                fpn_scores = scores_pred_single[start:start + num_anchors_fpn_level]\n                _, selected_index = torch.topk(fpn_scores, min(self.select_top_k, int(num_anchors_fpn_level)))\n                top_idx.append(selected_index + start)\n                start += num_anchors_fpn_level\n            top_idx = torch.cat([x.long() for x in top_idx])\n            # perform nms\n            nms_keep = torchvision.ops.nms(boxes_pred_single[top_idx], scores_pred_single[top_idx], iou_threshold=0.5)\n            nms_keep = nms_keep[:self.nms_max_outputs]  # select the top nms outputs\n            top_idx = top_idx[nms_keep]  # narrow the keep index\n            results_single = [\n                x1_abs[idx][top_idx],\n                y1_abs[idx][top_idx],\n                w_abs[idx][top_idx],\n                h_abs[idx][top_idx],\n                labels_pred[idx][top_idx].float(),\n                scores_pred[idx][top_idx],\n                torch.ones_like(x1_abs[idx][top_idx])\n            ]\n            # clip bounding boxes to image size\n            results_single[0] = torch.clamp(results_single[0], min=0, max=self.input_shape[1])\n            results_single[1] = torch.clamp(results_single[1], min=0, max=self.input_shape[0])\n            results_single[2] = torch.clamp(results_single[2], min=0)\n            results_single[2] = torch.where(results_single[2] &gt; self.input_shape[1] - results_single[0],\n                                            self.input_shape[1] - results_single[0],\n                                            results_single[2])\n            results_single[3] = torch.clamp(results_single[3], min=0)\n            results_single[3] = torch.where(results_single[3] &gt; self.input_shape[0] - results_single[1],\n                                            self.input_shape[0] - results_single[1],\n                                            results_single[3])\n            # mark the select as 0 for any anchorbox with score lower than threshold\n            results_single[-1] = torch.where(results_single[-2] &gt; self.score_threshold,\n                                             results_single[-1],\n                                             torch.zeros_like(results_single[-1]))\n            final_results.append(torch.stack(results_single, dim=-1))\n        return torch.stack(final_results)\n</pre> class PredictBox(TensorOp):     \"\"\"Convert network output to bounding boxes.         \"\"\"     def __init__(self,                  inputs=None,                  outputs=None,                  mode=None,                  input_shape=(512, 512, 3),                  select_top_k=1000,                  nms_max_outputs=100,                  score_threshold=0.05):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.input_shape = input_shape         self.select_top_k = select_top_k         self.nms_max_outputs = nms_max_outputs         self.score_threshold = score_threshold         self.all_anchors, self.num_anchors_per_level = _get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])         self.all_anchors = torch.Tensor(self.all_anchors)         if torch.cuda.is_available():             self.all_anchors = self.all_anchors.to(\"cuda:0\")      def forward(self, data, state):         cls_pred, loc_pred = data  # [Batch, #anchor, #num_classes], [Batch, #anchor, 4]         batch_size = cls_pred.size(0)         scores_pred, labels_pred = torch.max(cls_pred, dim=-1)         # loc_pred -&gt; loc_abs         x1_abs = loc_pred[..., 0] * self.all_anchors[..., 2] + self.all_anchors[..., 0]         y1_abs = loc_pred[..., 1] * self.all_anchors[..., 3] + self.all_anchors[..., 1]         w_abs = torch.exp(loc_pred[..., 2]) * self.all_anchors[..., 2]         h_abs = torch.exp(loc_pred[..., 3]) * self.all_anchors[..., 3]         x2_abs, y2_abs = x1_abs + w_abs, y1_abs + h_abs         # iterate over images         final_results = []         for idx in range(batch_size):             scores_pred_single = scores_pred[idx]             boxes_pred_single = torch.stack([x1_abs[idx], y1_abs[idx], x2_abs[idx], y2_abs[idx]], dim=-1)             # iterate over each pyramid to select top 1000 anchor boxes             start = 0             top_idx = []             for num_anchors_fpn_level in self.num_anchors_per_level:                 fpn_scores = scores_pred_single[start:start + num_anchors_fpn_level]                 _, selected_index = torch.topk(fpn_scores, min(self.select_top_k, int(num_anchors_fpn_level)))                 top_idx.append(selected_index + start)                 start += num_anchors_fpn_level             top_idx = torch.cat([x.long() for x in top_idx])             # perform nms             nms_keep = torchvision.ops.nms(boxes_pred_single[top_idx], scores_pred_single[top_idx], iou_threshold=0.5)             nms_keep = nms_keep[:self.nms_max_outputs]  # select the top nms outputs             top_idx = top_idx[nms_keep]  # narrow the keep index             results_single = [                 x1_abs[idx][top_idx],                 y1_abs[idx][top_idx],                 w_abs[idx][top_idx],                 h_abs[idx][top_idx],                 labels_pred[idx][top_idx].float(),                 scores_pred[idx][top_idx],                 torch.ones_like(x1_abs[idx][top_idx])             ]             # clip bounding boxes to image size             results_single[0] = torch.clamp(results_single[0], min=0, max=self.input_shape[1])             results_single[1] = torch.clamp(results_single[1], min=0, max=self.input_shape[0])             results_single[2] = torch.clamp(results_single[2], min=0)             results_single[2] = torch.where(results_single[2] &gt; self.input_shape[1] - results_single[0],                                             self.input_shape[1] - results_single[0],                                             results_single[2])             results_single[3] = torch.clamp(results_single[3], min=0)             results_single[3] = torch.where(results_single[3] &gt; self.input_shape[0] - results_single[1],                                             self.input_shape[0] - results_single[1],                                             results_single[3])             # mark the select as 0 for any anchorbox with score lower than threshold             results_single[-1] = torch.where(results_single[-2] &gt; self.score_threshold,                                              results_single[-1],                                              torch.zeros_like(results_single[-1]))             final_results.append(torch.stack(results_single, dim=-1))         return torch.stack(final_results) In\u00a0[16]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n    UpdateOp(model=model, loss_name=\"total_loss\"),\n    PredictBox(input_shape=(image_size, image_size, 3), inputs=[\"cls_pred\", \"loc_pred\"], outputs=\"pred\", mode=\"!train\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),     RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),     UpdateOp(model=model, loss_name=\"total_loss\"),     PredictBox(input_shape=(image_size, image_size, 3), inputs=[\"cls_pred\", \"loc_pred\"], outputs=\"pred\", mode=\"!train\") ]) In\u00a0[17]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import MeanAveragePrecision\n\ntraces = [\n    LRScheduler(model=model, lr_fn=lr_fn),\n    BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\"),\n    MeanAveragePrecision(num_classes=num_classes, true_key='bbox', pred_key='pred', mode=\"eval\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch,\n                         monitor_names=[\"l1_loss\", \"focal_loss\"])\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import MeanAveragePrecision  traces = [     LRScheduler(model=model, lr_fn=lr_fn),     BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\"),     MeanAveragePrecision(num_classes=num_classes, true_key='bbox', pred_key='pred', mode=\"eval\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch,                          monitor_names=[\"l1_loss\", \"focal_loss\"]) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <p>We select 4 images and visualize the model predictions.</p> In\u00a0[\u00a0]: Copied! <pre>num_images = 4\nbatch_idx = 1\nsample_data = pipeline.get_results(mode=\"eval\", num_steps=batch_idx+1)[batch_idx]\nnetwork_out = network.transform(data=sample_data, mode=\"eval\")\n</pre> num_images = 4 batch_idx = 1 sample_data = pipeline.get_results(mode=\"eval\", num_steps=batch_idx+1)[batch_idx] network_out = network.transform(data=sample_data, mode=\"eval\") In\u00a0[41]: Copied! <pre>fig, ax = plt.subplots(num_images, 2, figsize=(20, 30))\nfor batch_index in range(num_images):\n    img = network_out['image'].numpy()[batch_index, ...]\n    img = ((img + 1) / 2 * 255).astype(np.uint8)\n    img = np.transpose(img, [1, 2, 0])\n\n    img2 = img.copy()\n    keep = ~np.all(network_out['bbox'][batch_index].numpy() == 0, axis=1)\n    gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n\n    scores = network_out['pred'][batch_index].numpy()[..., -2]\n    labels = network_out['pred'][batch_index].numpy()[..., -3]\n    keep = scores &gt; 0.5\n    x1, y1, w, h, label, _, _ = network_out['pred'][batch_index].numpy()[keep].T\n\n    for i in range(len(gt_x1)):\n        rect = patches.Rectangle((gt_x1[i], gt_y1[i]),gt_w[i],gt_h[i],linewidth=1,edgecolor='r',facecolor='none')\n        ax[batch_index, 0].add_patch(rect)\n        ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n        ax[batch_index, 0].text(gt_x1[i] + 3,\n                   gt_y1[i] + 12,\n                   class_map[str(int(gt_label[i])+1)],\n                   color=(1, 0, 0),\n                   fontsize=14,\n                   fontweight='bold')\n    for j in range(len(x1)):\n        rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='b',facecolor='none')\n        ax[batch_index, 1].add_patch(rect)\n        ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n        ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]) + 1)], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n\n    ax[batch_index, 0].imshow(img)\n    ax[batch_index, 1].imshow(img2)\n\nplt.tight_layout()\n</pre> fig, ax = plt.subplots(num_images, 2, figsize=(20, 30)) for batch_index in range(num_images):     img = network_out['image'].numpy()[batch_index, ...]     img = ((img + 1) / 2 * 255).astype(np.uint8)     img = np.transpose(img, [1, 2, 0])      img2 = img.copy()     keep = ~np.all(network_out['bbox'][batch_index].numpy() == 0, axis=1)     gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T      scores = network_out['pred'][batch_index].numpy()[..., -2]     labels = network_out['pred'][batch_index].numpy()[..., -3]     keep = scores &gt; 0.5     x1, y1, w, h, label, _, _ = network_out['pred'][batch_index].numpy()[keep].T      for i in range(len(gt_x1)):         rect = patches.Rectangle((gt_x1[i], gt_y1[i]),gt_w[i],gt_h[i],linewidth=1,edgecolor='r',facecolor='none')         ax[batch_index, 0].add_patch(rect)         ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')         ax[batch_index, 0].text(gt_x1[i] + 3,                    gt_y1[i] + 12,                    class_map[str(int(gt_label[i])+1)],                    color=(1, 0, 0),                    fontsize=14,                    fontweight='bold')     for j in range(len(x1)):         rect = patches.Rectangle((x1[j], y1[j]),w[j],h[j],linewidth=1,edgecolor='b',facecolor='none')         ax[batch_index, 1].add_patch(rect)         ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')         ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]) + 1)], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')      ax[batch_index, 0].imshow(img)     ax[batch_index, 1].imshow(img2)  plt.tight_layout()"}, {"location": "apphub/instance_detection/retinanet/retinanet.html#instance-detection-with-retinanet", "title": "Instance Detection with RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#load-data", "title": "Load data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#generate-anchors", "title": "Generate Anchors\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#pipeline", "title": "<code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualization-of-batch-data", "title": "Visualization of batch data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#class-and-box-subnets", "title": "Class and box subnets\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#retinanet", "title": "RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#loss-functions", "title": "Loss functions\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#learning-rate", "title": "Learning rate\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#predict-box", "title": "Predict Box\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#start-training", "title": "Start Training\u00b6", "text": "<p>The training will take ~14 hours on single V100 GPU.</p>"}, {"location": "apphub/instance_detection/retinanet/retinanet.html#inference", "title": "Inference\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualize-predictions", "title": "Visualize predictions\u00b6", "text": ""}, {"location": "apphub/instance_detection/yolov5/yolov5.html", "title": "YOLOv5", "text": "In\u00a0[1]: Copied! <pre>import math\nimport random\nimport tempfile\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom albumentations import BboxParams\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.initializers import Constant\nfrom torch.utils.data import Dataset\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import mscoco\nfrom fastestimator.op.numpyop import Batch, Delete, NumpyOp\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import CenterCrop, HorizontalFlip, LongestMaxSize, PadIfNeeded\nfrom fastestimator.op.numpyop.univariate import ReadImage, ToArray\nfrom fastestimator.op.tensorop import Average, TensorOp\nfrom fastestimator.op.tensorop.loss import LossOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import EpochScheduler, cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import MeanAveragePrecision\nfrom fastestimator.util import get_num_devices\n</pre> import math import random import tempfile  import cv2 import numpy as np import tensorflow as tf from albumentations import BboxParams from tensorflow.keras import layers from tensorflow.keras.initializers import Constant from torch.utils.data import Dataset  import fastestimator as fe from fastestimator.dataset.data import mscoco from fastestimator.op.numpyop import Batch, Delete, NumpyOp from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import CenterCrop, HorizontalFlip, LongestMaxSize, PadIfNeeded from fastestimator.op.numpyop.univariate import ReadImage, ToArray from fastestimator.op.tensorop import Average, TensorOp from fastestimator.op.tensorop.loss import LossOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import EpochScheduler, cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import MeanAveragePrecision from fastestimator.util import get_num_devices In\u00a0[2]: parameters Copied! <pre>data_dir = None\nmodel_dir = tempfile.mkdtemp()\nepochs=200\nbatch_size_per_gpu=16\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\n</pre> data_dir = None model_dir = tempfile.mkdtemp() epochs=200 batch_size_per_gpu=16 train_steps_per_epoch=None eval_steps_per_epoch=None In\u00a0[3]: Copied! <pre># This dataset selects 4 images and their bboxes\nclass PreMosaicDataset(Dataset):\n    def __init__(self, mscoco_ds):\n        self.mscoco_ds = mscoco_ds\n\n    def __len__(self):\n        return len(self.mscoco_ds)\n\n    def __getitem__(self, idx):\n        indices = [idx] + [random.randint(0, len(self) - 1) for _ in range(3)]\n        samples = [self.mscoco_ds[i] for i in indices]\n        return {\n            \"image1\": samples[0][\"image\"],\n            \"bbox1\": samples[0][\"bbox\"],\n            \"image2\": samples[1][\"image\"],\n            \"bbox2\": samples[1][\"bbox\"],\n            \"image3\": samples[2][\"image\"],\n            \"bbox3\": samples[2][\"bbox\"],\n            \"image4\": samples[3][\"image\"],\n            \"bbox4\": samples[3][\"bbox\"]\n        }\n\n\nclass CombineMosaic(NumpyOp):\n    def forward(self, data, state):\n        image1, image2, image3, image4, bbox1, bbox2, bbox3, bbox4 = data\n        images = [image1, image2, image3, image4]\n        bboxes = [bbox1, bbox2, bbox3, bbox4]\n        images_new, boxes_new = self._combine_images_boxes(images, bboxes)\n        return images_new, boxes_new\n\n    def _combine_images_boxes(self, images, bboxes):\n        s = 640\n        yc, xc = int(random.uniform(320, 960)), int(random.uniform(320, 960))\n        images_new = np.full((1280, 1280, 3), fill_value=114, dtype=np.uint8)\n        bboxes_new = []\n        for idx, (image, bbox) in enumerate(zip(images, bboxes)):\n            h, w = image.shape[0], image.shape[1]\n            # place img in img4\n            if idx == 0:  # top left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif idx == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif idx == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n            elif idx == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n            images_new[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            padw, padh = x1a - x1b, y1a - y1b\n            for x1, y1, bw, bh, label in bbox:\n                x1_new = np.clip(x1 + padw, x1a, x2a)\n                y1_new = np.clip(y1 + padh, y1a, y2a)\n                x2_new = np.clip(x1 + padw + bw, x1a, x2a)\n                y2_new = np.clip(y1 + padh + bh, y1a, y2a)\n                bw_new = x2_new - x1_new\n                bh_new = y2_new - y1_new\n                if bw_new * bh_new &gt; 1:\n                    bboxes_new.append((x1_new, y1_new, bw_new, bh_new, label))\n        return images_new, bboxes_new\n\n\nclass HSVAugment(NumpyOp):\n    def __init__(self, inputs, outputs, mode=\"train\", hsv_h=0.015, hsv_s=0.7, hsv_v=0.4):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.hsv_h = hsv_h\n        self.hsv_s = hsv_s\n        self.hsv_v = hsv_v\n\n    def forward(self, data, state):\n        img = data\n        r = np.random.uniform(-1, 1, 3) * [self.hsv_h, self.hsv_s, self.hsv_v] + 1  # random gains\n        hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_RGB2HSV))\n        dtype = img.dtype  # uint8\n        x = np.arange(0, 256, dtype=np.int16)\n        lut_hue = ((x * r[0]) % 180).astype(dtype)\n        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n        img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n        img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n        return img\n\n\nclass CategoryID2ClassID(NumpyOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]\n        category = [x for x in range(1, 91) if not x in missing_category]\n        self.mapping = {k: v for k, v in zip(category, list(range(80)))}\n\n    def forward(self, data, state):\n        if data.size &gt; 0:\n            classes = np.array([self.mapping[int(x)] for x in data[:, -1]], dtype=\"float32\")\n            data[:, -1] = classes\n        else:\n            data = np.zeros(shape=(1, 5), dtype=\"float32\")\n        return data\n\n\nclass GTBox(NumpyOp):\n    def __init__(self, inputs, outputs, image_size, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.image_size = image_size\n        self.anchor_s = [(10, 13), (16, 30), (33, 23)]\n        self.anchor_m = [(30, 61), (62, 45), (59, 119)]\n        self.anchor_l = [(116, 90), (156, 198), (373, 326)]\n\n    def forward(self, data, state):\n        bbox = data[np.sum(data, 1) &gt; 0]\n        if bbox.size &gt; 0:\n            gt_sbbox = self._generate_target(data, anchors=self.anchor_s, feature_size=80)\n            gt_mbbox = self._generate_target(data, anchors=self.anchor_m, feature_size=40)\n            gt_lbbox = self._generate_target(data, anchors=self.anchor_l, feature_size=20)\n        else:\n            gt_sbbox = np.zeros((80, 80, 3, 6), dtype=\"float32\")\n            gt_mbbox = np.zeros((40, 40, 3, 6), dtype=\"float32\")\n            gt_lbbox = np.zeros((20, 20, 3, 6), dtype=\"float32\")\n        return gt_sbbox, gt_mbbox, gt_lbbox\n\n    def _generate_target(self, bbox, anchors, feature_size, wh_threshold=4.0):\n        object_boxes, label = bbox[:, :-1], bbox[:, -1]\n        gt_bbox = np.zeros((feature_size, feature_size, 3, 6), dtype=\"float32\")\n        for object_idx, object_box in enumerate(object_boxes):\n            for anchor_idx, anchor in enumerate(anchors):\n                ratio = object_box[2:] / np.array(anchor, dtype=\"float32\")\n                match = np.max(np.maximum(ratio, 1 / ratio)) &lt; wh_threshold\n                if match:\n                    center_feature_map = (object_box[:2] + object_box[2:] / 2) / self.image_size * feature_size\n                    candidate_coords = self._get_candidate_coords(center_feature_map, feature_size)\n                    for xc, yc in candidate_coords:\n                        gt_bbox[yc, xc, anchor_idx][:4] = object_box  # use absoulte x1,y1,w,h\n                        gt_bbox[yc, xc, anchor_idx][4] = 1.0\n                        gt_bbox[yc, xc, anchor_idx][5] = label[object_idx]\n        return gt_bbox\n\n    @staticmethod\n    def _get_candidate_coords(center_feature_map, feature_size):\n        xc, yc = center_feature_map\n        candidate_coords = [(int(xc), int(yc))]\n        if xc % 1 &lt; 0.5 and xc &gt; 1:\n            candidate_coords.append((int(xc) - 1, int(yc)))\n        if xc % 1 &gt;= 0.5 and xc &lt; feature_size - 1:\n            candidate_coords.append((int(xc) + 1, int(yc)))\n        if yc % 1 &lt; 0.5 and yc &gt; 1:\n            candidate_coords.append((int(xc), int(yc) - 1))\n        if yc % 1 &gt;= 0.5 and yc &lt; feature_size - 1:\n            candidate_coords.append((int(xc), int(yc) + 1))\n        return candidate_coords\n</pre> # This dataset selects 4 images and their bboxes class PreMosaicDataset(Dataset):     def __init__(self, mscoco_ds):         self.mscoco_ds = mscoco_ds      def __len__(self):         return len(self.mscoco_ds)      def __getitem__(self, idx):         indices = [idx] + [random.randint(0, len(self) - 1) for _ in range(3)]         samples = [self.mscoco_ds[i] for i in indices]         return {             \"image1\": samples[0][\"image\"],             \"bbox1\": samples[0][\"bbox\"],             \"image2\": samples[1][\"image\"],             \"bbox2\": samples[1][\"bbox\"],             \"image3\": samples[2][\"image\"],             \"bbox3\": samples[2][\"bbox\"],             \"image4\": samples[3][\"image\"],             \"bbox4\": samples[3][\"bbox\"]         }   class CombineMosaic(NumpyOp):     def forward(self, data, state):         image1, image2, image3, image4, bbox1, bbox2, bbox3, bbox4 = data         images = [image1, image2, image3, image4]         bboxes = [bbox1, bbox2, bbox3, bbox4]         images_new, boxes_new = self._combine_images_boxes(images, bboxes)         return images_new, boxes_new      def _combine_images_boxes(self, images, bboxes):         s = 640         yc, xc = int(random.uniform(320, 960)), int(random.uniform(320, 960))         images_new = np.full((1280, 1280, 3), fill_value=114, dtype=np.uint8)         bboxes_new = []         for idx, (image, bbox) in enumerate(zip(images, bboxes)):             h, w = image.shape[0], image.shape[1]             # place img in img4             if idx == 0:  # top left                 x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)                 x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)             elif idx == 1:  # top right                 x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc                 x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h             elif idx == 2:  # bottom left                 x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)                 x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)             elif idx == 3:  # bottom right                 x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)                 x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)             images_new[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]             padw, padh = x1a - x1b, y1a - y1b             for x1, y1, bw, bh, label in bbox:                 x1_new = np.clip(x1 + padw, x1a, x2a)                 y1_new = np.clip(y1 + padh, y1a, y2a)                 x2_new = np.clip(x1 + padw + bw, x1a, x2a)                 y2_new = np.clip(y1 + padh + bh, y1a, y2a)                 bw_new = x2_new - x1_new                 bh_new = y2_new - y1_new                 if bw_new * bh_new &gt; 1:                     bboxes_new.append((x1_new, y1_new, bw_new, bh_new, label))         return images_new, bboxes_new   class HSVAugment(NumpyOp):     def __init__(self, inputs, outputs, mode=\"train\", hsv_h=0.015, hsv_s=0.7, hsv_v=0.4):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.hsv_h = hsv_h         self.hsv_s = hsv_s         self.hsv_v = hsv_v      def forward(self, data, state):         img = data         r = np.random.uniform(-1, 1, 3) * [self.hsv_h, self.hsv_s, self.hsv_v] + 1  # random gains         hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_RGB2HSV))         dtype = img.dtype  # uint8         x = np.arange(0, 256, dtype=np.int16)         lut_hue = ((x * r[0]) % 180).astype(dtype)         lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)         lut_val = np.clip(x * r[2], 0, 255).astype(dtype)         img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)         img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)         return img   class CategoryID2ClassID(NumpyOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]         category = [x for x in range(1, 91) if not x in missing_category]         self.mapping = {k: v for k, v in zip(category, list(range(80)))}      def forward(self, data, state):         if data.size &gt; 0:             classes = np.array([self.mapping[int(x)] for x in data[:, -1]], dtype=\"float32\")             data[:, -1] = classes         else:             data = np.zeros(shape=(1, 5), dtype=\"float32\")         return data   class GTBox(NumpyOp):     def __init__(self, inputs, outputs, image_size, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.image_size = image_size         self.anchor_s = [(10, 13), (16, 30), (33, 23)]         self.anchor_m = [(30, 61), (62, 45), (59, 119)]         self.anchor_l = [(116, 90), (156, 198), (373, 326)]      def forward(self, data, state):         bbox = data[np.sum(data, 1) &gt; 0]         if bbox.size &gt; 0:             gt_sbbox = self._generate_target(data, anchors=self.anchor_s, feature_size=80)             gt_mbbox = self._generate_target(data, anchors=self.anchor_m, feature_size=40)             gt_lbbox = self._generate_target(data, anchors=self.anchor_l, feature_size=20)         else:             gt_sbbox = np.zeros((80, 80, 3, 6), dtype=\"float32\")             gt_mbbox = np.zeros((40, 40, 3, 6), dtype=\"float32\")             gt_lbbox = np.zeros((20, 20, 3, 6), dtype=\"float32\")         return gt_sbbox, gt_mbbox, gt_lbbox      def _generate_target(self, bbox, anchors, feature_size, wh_threshold=4.0):         object_boxes, label = bbox[:, :-1], bbox[:, -1]         gt_bbox = np.zeros((feature_size, feature_size, 3, 6), dtype=\"float32\")         for object_idx, object_box in enumerate(object_boxes):             for anchor_idx, anchor in enumerate(anchors):                 ratio = object_box[2:] / np.array(anchor, dtype=\"float32\")                 match = np.max(np.maximum(ratio, 1 / ratio)) &lt; wh_threshold                 if match:                     center_feature_map = (object_box[:2] + object_box[2:] / 2) / self.image_size * feature_size                     candidate_coords = self._get_candidate_coords(center_feature_map, feature_size)                     for xc, yc in candidate_coords:                         gt_bbox[yc, xc, anchor_idx][:4] = object_box  # use absoulte x1,y1,w,h                         gt_bbox[yc, xc, anchor_idx][4] = 1.0                         gt_bbox[yc, xc, anchor_idx][5] = label[object_idx]         return gt_bbox      @staticmethod     def _get_candidate_coords(center_feature_map, feature_size):         xc, yc = center_feature_map         candidate_coords = [(int(xc), int(yc))]         if xc % 1 &lt; 0.5 and xc &gt; 1:             candidate_coords.append((int(xc) - 1, int(yc)))         if xc % 1 &gt;= 0.5 and xc &lt; feature_size - 1:             candidate_coords.append((int(xc) + 1, int(yc)))         if yc % 1 &lt; 0.5 and yc &gt; 1:             candidate_coords.append((int(xc), int(yc) - 1))         if yc % 1 &gt;= 0.5 and yc &lt; feature_size - 1:             candidate_coords.append((int(xc), int(yc) + 1))         return candidate_coords In\u00a0[4]: Copied! <pre>num_device = get_num_devices()\ntrain_ds, val_ds = mscoco.load_data(root_dir=data_dir)\ntrain_ds = PreMosaicDataset(mscoco_ds=train_ds)\nbatch_size = num_device * batch_size_per_gpu\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=val_ds,\n    ops=[\n        ReadImage(inputs=(\"image1\", \"image2\", \"image3\", \"image4\"),\n                  outputs=(\"image1\", \"image2\", \"image3\", \"image4\"),\n                  mode=\"train\"),\n        ReadImage(inputs=\"image\", outputs=\"image\", mode=\"eval\"),\n        LongestMaxSize(max_size=640,\n                       image_in=\"image1\",\n                       bbox_in=\"bbox1\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0),\n                       mode=\"train\"),\n        LongestMaxSize(max_size=640,\n                       image_in=\"image2\",\n                       bbox_in=\"bbox2\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0),\n                       mode=\"train\"),\n        LongestMaxSize(max_size=640,\n                       image_in=\"image3\",\n                       bbox_in=\"bbox3\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0),\n                       mode=\"train\"),\n        LongestMaxSize(max_size=640,\n                       image_in=\"image4\",\n                       bbox_in=\"bbox4\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0),\n                       mode=\"train\"),\n        LongestMaxSize(max_size=640,\n                       image_in=\"image\",\n                       bbox_in=\"bbox\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0),\n                       mode=\"eval\"),\n        PadIfNeeded(min_height=640,\n                    min_width=640,\n                    image_in=\"image\",\n                    bbox_in=\"bbox\",\n                    bbox_params=BboxParams(\"coco\", min_area=1.0),\n                    mode=\"eval\",\n                    border_mode=cv2.BORDER_CONSTANT,\n                    value=(114, 114, 114)),\n        CombineMosaic(inputs=(\"image1\", \"image2\", \"image3\", \"image4\", \"bbox1\", \"bbox2\", \"bbox3\", \"bbox4\"),\n                      outputs=(\"image\", \"bbox\"),\n                      mode=\"train\"),\n        CenterCrop(height=640,\n                   width=640,\n                   image_in=\"image\",\n                   bbox_in=\"bbox\",\n                   bbox_params=BboxParams(\"coco\", min_area=1.0),\n                   mode=\"train\"),\n        Sometimes(\n            HorizontalFlip(image_in=\"image\",\n                           bbox_in=\"bbox\",\n                           bbox_params=BboxParams(\"coco\", min_area=1.0),\n                           mode=\"train\")),\n        HSVAugment(inputs=\"image\", outputs=\"image\", mode=\"train\"),\n        ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),\n        CategoryID2ClassID(inputs=\"bbox\", outputs=\"bbox\"),\n        GTBox(inputs=\"bbox\", outputs=(\"gt_sbbox\", \"gt_mbbox\", \"gt_lbbox\"), image_size=640),\n        Delete(keys=(\"image1\", \"image2\", \"image3\", \"image4\", \"bbox1\", \"bbox2\", \"bbox3\", \"bbox4\"),\n               mode=\"train\"),\n        Delete(keys=\"image_id\", mode=\"eval\"),\n        Batch(batch_size=batch_size, pad_value=0)\n    ])\n</pre> num_device = get_num_devices() train_ds, val_ds = mscoco.load_data(root_dir=data_dir) train_ds = PreMosaicDataset(mscoco_ds=train_ds) batch_size = num_device * batch_size_per_gpu pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=val_ds,     ops=[         ReadImage(inputs=(\"image1\", \"image2\", \"image3\", \"image4\"),                   outputs=(\"image1\", \"image2\", \"image3\", \"image4\"),                   mode=\"train\"),         ReadImage(inputs=\"image\", outputs=\"image\", mode=\"eval\"),         LongestMaxSize(max_size=640,                        image_in=\"image1\",                        bbox_in=\"bbox1\",                        bbox_params=BboxParams(\"coco\", min_area=1.0),                        mode=\"train\"),         LongestMaxSize(max_size=640,                        image_in=\"image2\",                        bbox_in=\"bbox2\",                        bbox_params=BboxParams(\"coco\", min_area=1.0),                        mode=\"train\"),         LongestMaxSize(max_size=640,                        image_in=\"image3\",                        bbox_in=\"bbox3\",                        bbox_params=BboxParams(\"coco\", min_area=1.0),                        mode=\"train\"),         LongestMaxSize(max_size=640,                        image_in=\"image4\",                        bbox_in=\"bbox4\",                        bbox_params=BboxParams(\"coco\", min_area=1.0),                        mode=\"train\"),         LongestMaxSize(max_size=640,                        image_in=\"image\",                        bbox_in=\"bbox\",                        bbox_params=BboxParams(\"coco\", min_area=1.0),                        mode=\"eval\"),         PadIfNeeded(min_height=640,                     min_width=640,                     image_in=\"image\",                     bbox_in=\"bbox\",                     bbox_params=BboxParams(\"coco\", min_area=1.0),                     mode=\"eval\",                     border_mode=cv2.BORDER_CONSTANT,                     value=(114, 114, 114)),         CombineMosaic(inputs=(\"image1\", \"image2\", \"image3\", \"image4\", \"bbox1\", \"bbox2\", \"bbox3\", \"bbox4\"),                       outputs=(\"image\", \"bbox\"),                       mode=\"train\"),         CenterCrop(height=640,                    width=640,                    image_in=\"image\",                    bbox_in=\"bbox\",                    bbox_params=BboxParams(\"coco\", min_area=1.0),                    mode=\"train\"),         Sometimes(             HorizontalFlip(image_in=\"image\",                            bbox_in=\"bbox\",                            bbox_params=BboxParams(\"coco\", min_area=1.0),                            mode=\"train\")),         HSVAugment(inputs=\"image\", outputs=\"image\", mode=\"train\"),         ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),         CategoryID2ClassID(inputs=\"bbox\", outputs=\"bbox\"),         GTBox(inputs=\"bbox\", outputs=(\"gt_sbbox\", \"gt_mbbox\", \"gt_lbbox\"), image_size=640),         Delete(keys=(\"image1\", \"image2\", \"image3\", \"image4\", \"bbox1\", \"bbox2\", \"bbox3\", \"bbox4\"),                mode=\"train\"),         Delete(keys=\"image_id\", mode=\"eval\"),         Batch(batch_size=batch_size, pad_value=0)     ]) In\u00a0[5]: Copied! <pre>import random\nfrom matplotlib import pyplot as plt\n\n\nBOX_COLOR = (255, 0, 0)  # Red\nTEXT_COLOR = (255, 255, 255)  # White\n\ndef visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n\"\"\"Visualizes a single bounding box on the image\"\"\"\n    x_min, y_min, w, h = bbox\n    x_min, x_max, y_min, y_max = int(x_min), int(x_min +\n                                                 w), int(y_min), int(y_min + h)\n\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max),\n                  color=color,\n                  thickness=thickness)\n\n    ((text_width, text_height), _) = cv2.getTextSize(class_name,\n                                                     cv2.FONT_HERSHEY_SIMPLEX,\n                                                     0.35, 1)\n    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)),\n                  (x_min + text_width, y_min), color, -1)\n    cv2.putText(\n        img,\n        text=class_name,\n        org=(x_min, y_min - int(0.3 * text_height)),\n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=0.35,\n        color=(255, 255, 255),\n        lineType=cv2.LINE_AA,\n    )\n    return img\n\n\ndef visualize(image, bboxes, category_ids, category_id_to_name):\n    img = image.copy()\n    for bbox, category_id in zip(bboxes, category_ids):\n        class_name = category_id_to_name[category_id]\n        img = visualize_bbox(img, bbox, class_name)\n        # print(\"{}: {}\".format(class_name, bbox))\n    plt.figure(figsize=(15, 15))\n    plt.axis('off')\n    plt.imshow(img)\n    plt.show()\n\n\ndef visualize_image_with_box(images, bboxes):\n    category_names = [\n        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n        'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n        'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n        'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n        'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n        'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n        'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n        'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n        'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n        'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n        'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n    ]\n    category_id_to_name = {\n        key: name\n        for key, name in zip(range(80), category_names)\n    }\n    images = np.uint8(images)\n    num_images = images.shape[0]\n    index = np.random.randint(0, num_images)\n    image = images[index]\n    bbox = bboxes[index]\n    bbox = bbox[np.sum(bbox, axis=1) != 0]\n    bbox_coco = [(x[0], x[1], x[2], x[3]) for x in bbox]\n    category_ids = [int(x[4]) for x in bbox]\n    visualize(image, bbox_coco, category_ids, category_id_to_name)\n</pre> import random from matplotlib import pyplot as plt   BOX_COLOR = (255, 0, 0)  # Red TEXT_COLOR = (255, 255, 255)  # White  def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):     \"\"\"Visualizes a single bounding box on the image\"\"\"     x_min, y_min, w, h = bbox     x_min, x_max, y_min, y_max = int(x_min), int(x_min +                                                  w), int(y_min), int(y_min + h)      cv2.rectangle(img, (x_min, y_min), (x_max, y_max),                   color=color,                   thickness=thickness)      ((text_width, text_height), _) = cv2.getTextSize(class_name,                                                      cv2.FONT_HERSHEY_SIMPLEX,                                                      0.35, 1)     cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)),                   (x_min + text_width, y_min), color, -1)     cv2.putText(         img,         text=class_name,         org=(x_min, y_min - int(0.3 * text_height)),         fontFace=cv2.FONT_HERSHEY_SIMPLEX,         fontScale=0.35,         color=(255, 255, 255),         lineType=cv2.LINE_AA,     )     return img   def visualize(image, bboxes, category_ids, category_id_to_name):     img = image.copy()     for bbox, category_id in zip(bboxes, category_ids):         class_name = category_id_to_name[category_id]         img = visualize_bbox(img, bbox, class_name)         # print(\"{}: {}\".format(class_name, bbox))     plt.figure(figsize=(15, 15))     plt.axis('off')     plt.imshow(img)     plt.show()   def visualize_image_with_box(images, bboxes):     category_names = [         'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',         'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',         'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',         'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',         'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',         'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',         'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',         'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',         'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',         'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',         'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',         'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',         'scissors', 'teddy bear', 'hair drier', 'toothbrush'     ]     category_id_to_name = {         key: name         for key, name in zip(range(80), category_names)     }     images = np.uint8(images)     num_images = images.shape[0]     index = np.random.randint(0, num_images)     image = images[index]     bbox = bboxes[index]     bbox = bbox[np.sum(bbox, axis=1) != 0]     bbox_coco = [(x[0], x[1], x[2], x[3]) for x in bbox]     category_ids = [int(x[4]) for x in bbox]     visualize(image, bbox_coco, category_ids, category_id_to_name) In\u00a0[6]: Copied! <pre>batch_data = pipeline.get_results(mode=\"train\")\nimages = batch_data[\"image\"].numpy()\nbboxes = batch_data[\"bbox\"].numpy()\nvisualize_image_with_box(images, bboxes)\n</pre> batch_data = pipeline.get_results(mode=\"train\") images = batch_data[\"image\"].numpy() bboxes = batch_data[\"bbox\"].numpy() visualize_image_with_box(images, bboxes) In\u00a0[7]: Copied! <pre>def conv_block(x, c, k=1, s=1):\n    x = layers.Conv2D(filters=c,\n                      kernel_size=k,\n                      strides=s,\n                      padding='same',\n                      use_bias=False,\n                      kernel_regularizer=tf.keras.regularizers.L2(0.0005))(x)\n    x = layers.BatchNormalization(momentum=0.97)(x)\n    x = tf.nn.silu(x)\n    return x\n\n\ndef bottleneck(x, c, k=1, shortcut=True):\n    out = conv_block(x, c=c, k=1)\n    out = conv_block(out, c=c, k=3)\n    if shortcut and c == x.shape[-1]:\n        out = out + x\n    return out\n\n\ndef csp_bottleneck_conv3(x, c, n=1, shortcut=True):\n    out1 = conv_block(x, c=c // 2)\n    for _ in range(n):\n        out1 = bottleneck(out1, c=c // 2, shortcut=shortcut)\n    out2 = conv_block(x, c=c // 2)\n    out = tf.concat([out1, out2], axis=-1)\n    out = conv_block(out, c=c)\n    return out\n\n\ndef spatial_pyramid_pooling(x, c, k=(5, 9, 13)):\n    input_c = x.shape[-1]\n    x = conv_block(x, c=input_c // 2)\n    x = tf.concat([x] + [layers.MaxPool2D(pool_size=p, strides=1, padding='same')(x) for p in k], axis=-1)\n    x = conv_block(x, c=c)\n    return x\n\n\ndef yolov5(input_shape, num_classes, strides=(8, 16, 32)):\n    inp = layers.Input(shape=input_shape)\n    x = tf.concat([inp[:, ::2, ::2, :], inp[:, 1::2, ::2, :], inp[:, ::2, 1::2, :], inp[:, 1::2, 1::2, :]], axis=-1)\n    x = conv_block(x, c=32, k=3)\n    x = conv_block(x, c=64, k=3, s=2)\n    x = csp_bottleneck_conv3(x, c=64)\n    x = conv_block(x, c=128, k=3, s=2)\n    x_4 = csp_bottleneck_conv3(x, c=128, n=3)\n    x = conv_block(x_4, c=256, k=3, s=2)\n    x_6 = csp_bottleneck_conv3(x, c=256, n=3)\n    x = conv_block(x_6, c=512, k=3, s=2)\n    x = spatial_pyramid_pooling(x, c=512)\n    x = csp_bottleneck_conv3(x, 512, shortcut=False)\n    x_10 = conv_block(x, 256)\n    x = layers.UpSampling2D()(x_10)\n    x = tf.concat([x, x_6], axis=-1)\n    x = csp_bottleneck_conv3(x, 256, shortcut=False)\n    x_14 = conv_block(x, 128)\n    x = layers.UpSampling2D()(x_14)\n    x = tf.concat([x, x_4], axis=-1)\n    x_17 = csp_bottleneck_conv3(x, 128, shortcut=False)\n    x = conv_block(x_17, 128, 3, 2)\n    x = tf.concat([x, x_14], axis=-1)\n    x_20 = csp_bottleneck_conv3(x, 256, shortcut=False)\n    x = conv_block(x_20, 256, 3, 2)\n    x = tf.concat([x, x_10], axis=-1)\n    x_23 = csp_bottleneck_conv3(x, 512, shortcut=False)\n    # initialize the bias for the final layer\n    biases = []\n    for stride, in_channel in zip(strides, (128, 256, 512)):\n        bias = np.random.uniform(low=-(1 / in_channel)**0.5, high=(1 / in_channel)**0.5, size=(3, num_classes + 5))\n        bias[:, 4] += math.log(8 / (640 / stride)**2)  # obj (8 objects per 640 image)\n        bias[:, 5:] += math.log(0.6 / (num_classes - 0.99))  # cls\n        biases.append(bias.flatten())\n    out_17 = layers.Conv2D((num_classes + 5) * 3,\n                           1,\n                           bias_initializer=Constant(biases[0]),\n                           kernel_regularizer=tf.keras.regularizers.L2(0.0005),\n                           bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_17)\n    out_17 = layers.Reshape((out_17.shape[1], out_17.shape[2], 3, num_classes + 5))(out_17)\n    out_20 = layers.Conv2D((num_classes + 5) * 3,\n                           1,\n                           bias_initializer=Constant(biases[1]),\n                           kernel_regularizer=tf.keras.regularizers.L2(0.0005),\n                           bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_20)\n    out_20 = layers.Reshape((out_20.shape[1], out_20.shape[2], 3, num_classes + 5))(out_20)\n    out_23 = layers.Conv2D((num_classes + 5) * 3,\n                           1,\n                           bias_initializer=Constant(biases[2]),\n                           kernel_regularizer=tf.keras.regularizers.L2(0.0005),\n                           bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_23)\n    out_23 = layers.Reshape((out_23.shape[1], out_23.shape[2], 3, num_classes + 5))(out_23)  # B, h/32, w/32, 3, 85\n    return tf.keras.Model(inputs=inp, outputs=[out_17, out_20, out_23])\n</pre> def conv_block(x, c, k=1, s=1):     x = layers.Conv2D(filters=c,                       kernel_size=k,                       strides=s,                       padding='same',                       use_bias=False,                       kernel_regularizer=tf.keras.regularizers.L2(0.0005))(x)     x = layers.BatchNormalization(momentum=0.97)(x)     x = tf.nn.silu(x)     return x   def bottleneck(x, c, k=1, shortcut=True):     out = conv_block(x, c=c, k=1)     out = conv_block(out, c=c, k=3)     if shortcut and c == x.shape[-1]:         out = out + x     return out   def csp_bottleneck_conv3(x, c, n=1, shortcut=True):     out1 = conv_block(x, c=c // 2)     for _ in range(n):         out1 = bottleneck(out1, c=c // 2, shortcut=shortcut)     out2 = conv_block(x, c=c // 2)     out = tf.concat([out1, out2], axis=-1)     out = conv_block(out, c=c)     return out   def spatial_pyramid_pooling(x, c, k=(5, 9, 13)):     input_c = x.shape[-1]     x = conv_block(x, c=input_c // 2)     x = tf.concat([x] + [layers.MaxPool2D(pool_size=p, strides=1, padding='same')(x) for p in k], axis=-1)     x = conv_block(x, c=c)     return x   def yolov5(input_shape, num_classes, strides=(8, 16, 32)):     inp = layers.Input(shape=input_shape)     x = tf.concat([inp[:, ::2, ::2, :], inp[:, 1::2, ::2, :], inp[:, ::2, 1::2, :], inp[:, 1::2, 1::2, :]], axis=-1)     x = conv_block(x, c=32, k=3)     x = conv_block(x, c=64, k=3, s=2)     x = csp_bottleneck_conv3(x, c=64)     x = conv_block(x, c=128, k=3, s=2)     x_4 = csp_bottleneck_conv3(x, c=128, n=3)     x = conv_block(x_4, c=256, k=3, s=2)     x_6 = csp_bottleneck_conv3(x, c=256, n=3)     x = conv_block(x_6, c=512, k=3, s=2)     x = spatial_pyramid_pooling(x, c=512)     x = csp_bottleneck_conv3(x, 512, shortcut=False)     x_10 = conv_block(x, 256)     x = layers.UpSampling2D()(x_10)     x = tf.concat([x, x_6], axis=-1)     x = csp_bottleneck_conv3(x, 256, shortcut=False)     x_14 = conv_block(x, 128)     x = layers.UpSampling2D()(x_14)     x = tf.concat([x, x_4], axis=-1)     x_17 = csp_bottleneck_conv3(x, 128, shortcut=False)     x = conv_block(x_17, 128, 3, 2)     x = tf.concat([x, x_14], axis=-1)     x_20 = csp_bottleneck_conv3(x, 256, shortcut=False)     x = conv_block(x_20, 256, 3, 2)     x = tf.concat([x, x_10], axis=-1)     x_23 = csp_bottleneck_conv3(x, 512, shortcut=False)     # initialize the bias for the final layer     biases = []     for stride, in_channel in zip(strides, (128, 256, 512)):         bias = np.random.uniform(low=-(1 / in_channel)**0.5, high=(1 / in_channel)**0.5, size=(3, num_classes + 5))         bias[:, 4] += math.log(8 / (640 / stride)**2)  # obj (8 objects per 640 image)         bias[:, 5:] += math.log(0.6 / (num_classes - 0.99))  # cls         biases.append(bias.flatten())     out_17 = layers.Conv2D((num_classes + 5) * 3,                            1,                            bias_initializer=Constant(biases[0]),                            kernel_regularizer=tf.keras.regularizers.L2(0.0005),                            bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_17)     out_17 = layers.Reshape((out_17.shape[1], out_17.shape[2], 3, num_classes + 5))(out_17)     out_20 = layers.Conv2D((num_classes + 5) * 3,                            1,                            bias_initializer=Constant(biases[1]),                            kernel_regularizer=tf.keras.regularizers.L2(0.0005),                            bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_20)     out_20 = layers.Reshape((out_20.shape[1], out_20.shape[2], 3, num_classes + 5))(out_20)     out_23 = layers.Conv2D((num_classes + 5) * 3,                            1,                            bias_initializer=Constant(biases[2]),                            kernel_regularizer=tf.keras.regularizers.L2(0.0005),                            bias_regularizer=tf.keras.regularizers.L2(0.0005))(x_23)     out_23 = layers.Reshape((out_23.shape[1], out_23.shape[2], 3, num_classes + 5))(out_23)  # B, h/32, w/32, 3, 85     return tf.keras.Model(inputs=inp, outputs=[out_17, out_20, out_23]) In\u00a0[8]: Copied! <pre>init_lr = 1e-2 / 64 * batch_size\nmodel = fe.build(lambda: yolov5(input_shape=(640, 640, 3), num_classes=80),\n                 optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.937, learning_rate=init_lr, nesterov=True))\n</pre> init_lr = 1e-2 / 64 * batch_size model = fe.build(lambda: yolov5(input_shape=(640, 640, 3), num_classes=80),                  optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.937, learning_rate=init_lr, nesterov=True)) In\u00a0[9]: Copied! <pre>class ComputeLoss(LossOp):\n    def __init__(self, inputs, outputs, img_size=640, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.loss_conf = tf.losses.BinaryCrossentropy(reduction=\"none\")\n        self.loss_cls = tf.losses.BinaryCrossentropy(reduction=\"none\")\n        self.img_size = img_size\n\n    def forward(self, data, state):\n        pred, true = data\n        true_box, true_obj, true_class = tf.split(true, (4, 1, -1), axis=-1)\n        pred_box, pred_obj, pred_class = tf.split(pred, (4, 1, -1), axis=-1)\n        num_classes = pred_class.shape[-1]\n        true_class = tf.squeeze(tf.one_hot(tf.cast(true_class, tf.int32), depth=num_classes, axis=-1), -2)\n        box_scale = 2 - 1.0 * true_box[..., 2] * true_box[..., 3] / (self.img_size**2)\n        obj_mask = tf.squeeze(true_obj, -1)\n        conf_focal = tf.squeeze(tf.math.pow(true_obj - pred_obj, 2), -1)\n        iou = self.bbox_iou(pred_box, true_box, giou=True)\n        iou_loss = (1 - iou) * obj_mask * box_scale\n        conf_loss = conf_focal * self.loss_conf(true_obj, pred_obj)\n        class_loss = obj_mask * self.loss_cls(true_class, pred_class)\n        iou_loss = tf.reduce_mean(tf.reduce_sum(iou_loss, axis=[1, 2, 3]))\n        conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3]))\n        class_loss = tf.reduce_mean(tf.reduce_sum(class_loss, axis=[1, 2, 3])) * num_classes\n        return iou_loss, conf_loss, class_loss\n\n    @staticmethod\n    def bbox_iou(bbox1, bbox2, giou=False, diou=False, ciou=False, epsilon=1e-7):\n        b1x1, b1x2, b1y1, b1y2 = bbox1[..., 0], bbox1[..., 0] + bbox1[..., 2], bbox1[..., 1], bbox1[..., 1] + bbox1[..., 3]\n        b2x1, b2x2, b2y1, b2y2  = bbox2[..., 0], bbox2[..., 0] + bbox2[..., 2], bbox2[..., 1], bbox2[..., 1] + bbox2[..., 3]\n        # intersection area\n        inter = tf.maximum(tf.minimum(b1x2, b2x2) - tf.maximum(b1x1, b2x1), 0) * tf.maximum(\n            tf.minimum(b1y2, b2y2) - tf.maximum(b1y1, b2y1), 0)\n        # union area\n        w1, h1 = b1x2 - b1x1 + epsilon, b1y2 - b1y1 + epsilon\n        w2, h2 = b2x2 - b2x1 + epsilon, b2y2 - b2y1 + epsilon\n        union = w1 * h1 + w2 * h2 - inter + epsilon\n        # iou\n        iou = inter / union\n        if giou or diou or ciou:\n            # enclosing box\n            cw = tf.maximum(b1x2, b2x2) - tf.minimum(b1x1, b2x1)\n            ch = tf.maximum(b1y2, b2y2) - tf.minimum(b1y1, b2y1)\n            if giou:\n                enclose_area = cw * ch + epsilon\n                return iou - (enclose_area - union) / enclose_area\n            if diou or ciou:\n                c2 = cw**2 + ch**2 + epsilon\n                rho2 = ((b2x1 + b2x2) - (b1x1 + b1x2))**2 / 4 + ((b2y1 + b2y2) - (b1y1 + b1y2))**2 / 4\n                if diou:\n                    return iou - rho2 / c2\n                elif ciou:\n                    v = (4 / math.pi**2) * tf.pow(tf.atan(w2 / h2) - tf.atan(w1 / h1), 2)\n                    alpha = v / (1 - iou + v)\n                    return iou - (rho2 / c2 + v * alpha)\n        return tf.clip_by_value(iou, 0, 1)\n\n\nclass Rescale(TensorOp):\n    def forward(self, data, state):\n        return data / 255\n\n\nclass DecodePred(TensorOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.strides = [8, 16, 32]\n        self.num_anchor = 3\n        self.width, self.height = 640, 640\n        self.grids = self.create_grid(self.strides, self.num_anchor)\n        anchor_s = [(10, 13), (16, 30), (33, 23)]\n        anchor_m = [(30, 61), (62, 45), (59, 119)]\n        anchor_l = [(116, 90), (156, 198), (373, 326)]\n        self.anchors = self.create_anchor(anchor_s, anchor_m, anchor_l, self.strides)\n\n    def create_grid(self, strides, num_anchor):\n        grids = []\n        for stride in strides:\n            x_coor = [stride * i for i in range(self.width // stride)]\n            y_coor = [stride * i for i in range(self.height // stride)]\n            xx, yy = np.meshgrid(x_coor, y_coor)\n            xx, yy = np.float32(xx), np.float32(yy)\n            xx, yy = np.stack([xx] * num_anchor, axis=-1), np.stack([yy] * num_anchor, axis=-1)\n            grids.append(tf.convert_to_tensor(np.stack([xx, yy], axis=-1)))\n        return grids\n\n    def create_anchor(self, anchor_s, anchor_m, anchor_l, strides):\n        anchors = []\n        for anchor, stride in zip([anchor_s, anchor_m, anchor_l], strides):\n            feature_size_x, feature_size_y = self.width // stride, self.height // stride\n            anchor = np.array(anchor, dtype=\"float32\").reshape((1, 1, 3, 2))\n            anchor = np.tile(anchor, [feature_size_y, feature_size_x, 1, 1])\n            anchors.append(tf.convert_to_tensor(anchor))\n        return anchors\n\n    def forward(self, data, state):\n        conv_sbbox = self.decode(data[0], self.grids[0], self.anchors[0], self.strides[0])\n        conv_mbbox = self.decode(data[1], self.grids[1], self.anchors[1], self.strides[1])\n        conv_lbbox = self.decode(data[2], self.grids[2], self.anchors[2], self.strides[2])\n        return conv_sbbox, conv_mbbox, conv_lbbox\n\n    def decode(self, conv_bbox, grid, anchor, stride):\n        batch_size = conv_bbox.shape[0]\n        grid, anchor = tf.expand_dims(grid, 0), tf.expand_dims(anchor, 0)\n        grid, anchor = tf.tile(grid, [batch_size, 1, 1, 1, 1]), tf.tile(anchor, [batch_size, 1, 1, 1, 1])\n        conv_bbox = tf.sigmoid(conv_bbox)\n        bbox_pred, conf_pred, cls_pred = conv_bbox[..., 0:4], conv_bbox[..., 4:5], conv_bbox[..., 5:]\n        xcyc_pred, wh_pred = bbox_pred[..., 0:2], bbox_pred[..., 2:4]\n        xcyc_pred = (xcyc_pred * 2 - 0.5) * stride + grid\n        wh_pred = (wh_pred * 2)**2 * anchor\n        x1y1_pred = xcyc_pred - wh_pred / 2\n        result = tf.concat([x1y1_pred, wh_pred, conf_pred, cls_pred], axis=-1)\n        return result\n\n\nclass PredictBox(TensorOp):\n    def __init__(self, inputs, outputs, mode, width, height, max_outputs=500, conf_threshold=0.4):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.width = width\n        self.height = height\n        self.max_outputs = max_outputs\n        self.conf_threshold = conf_threshold\n\n    def forward(self, data, state):\n        conv_sbbox, conv_mbbox, conv_lbbox = data\n        batch_size = conv_sbbox.shape[0]\n        final_results = []\n        for idx in range(batch_size):\n            pred_s, pred_m, pred_l = conv_sbbox[idx], conv_mbbox[idx], conv_lbbox[idx]\n            pred_s, pred_m, pred_l = tf.reshape(pred_s, (-1, 85)), tf.reshape(pred_m, (-1, 85)), tf.reshape(pred_l, (-1, 85))\n            preds = tf.concat([pred_s, pred_m, pred_l], axis=0)\n            preds = preds[preds[:, 4] &gt; self.conf_threshold]  # filter by confidence\n            classes = tf.argmax(preds[:, 5:], axis=-1)\n            unique_classes = tf.unique(classes)[0]\n            selected_boxes_all_classes = tf.zeros(shape=[0, 6], dtype=tf.float32)\n            for clss in unique_classes:\n                tf.autograph.experimental.set_loop_options(shape_invariants=[(selected_boxes_all_classes,\n                                                                              tf.TensorShape([None, 6]))])\n                mask = tf.math.equal(classes, clss)\n                preds_cls = tf.boolean_mask(preds, mask)\n                x1, y1, w, h = preds_cls[:, 0], preds_cls[:, 1], preds_cls[:, 2], preds_cls[:, 3]\n                x2, y2 = x1 + w, y1 + h\n                conf_score, label = preds_cls[:, 4], tf.boolean_mask(classes, mask)\n                selected_bboxes = tf.stack([y1, x1, y2, x2, conf_score, tf.cast(label, tf.float32)], axis=-1)\n                # nms for every class\n                nms_keep = tf.image.non_max_suppression(selected_bboxes[:, :4],\n                                                        selected_bboxes[:, 4],\n                                                        max_output_size=50,\n                                                        iou_threshold=0.35)\n                selected_bboxes = tf.gather(selected_bboxes, nms_keep)\n                selected_boxes_all_classes = tf.concat([selected_boxes_all_classes, selected_bboxes], axis=0)\n            # clip bounding boxes to image size\n            y1_abs = tf.clip_by_value(selected_boxes_all_classes[:, 0], 0, self.height)\n            x1_abs = tf.clip_by_value(selected_boxes_all_classes[:, 1], 0, self.width)\n            height_abs = tf.clip_by_value(selected_boxes_all_classes[:, 2] - y1_abs, 0, self.height - y1_abs)\n            width_abs = tf.clip_by_value(selected_boxes_all_classes[:, 3] - x1_abs, 0, self.width - x1_abs)\n            labels_score, labels = selected_boxes_all_classes[:, 4], selected_boxes_all_classes[:, 5]\n            # final output: [x1, y1, w, h, label, label_score, select_or_not]\n            results_single = [x1_abs, y1_abs, width_abs, height_abs, labels, labels_score, tf.ones_like(x1_abs)]\n            results_single = tf.stack(results_single, axis=-1)\n            # pad 0 to other rows to improve performance\n            results_single = tf.pad(results_single, [(0, self.max_outputs - tf.shape(results_single)[0]), (0, 0)])\n            final_results.append(results_single)\n        final_results = tf.stack(final_results)\n        return final_results\n</pre> class ComputeLoss(LossOp):     def __init__(self, inputs, outputs, img_size=640, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.loss_conf = tf.losses.BinaryCrossentropy(reduction=\"none\")         self.loss_cls = tf.losses.BinaryCrossentropy(reduction=\"none\")         self.img_size = img_size      def forward(self, data, state):         pred, true = data         true_box, true_obj, true_class = tf.split(true, (4, 1, -1), axis=-1)         pred_box, pred_obj, pred_class = tf.split(pred, (4, 1, -1), axis=-1)         num_classes = pred_class.shape[-1]         true_class = tf.squeeze(tf.one_hot(tf.cast(true_class, tf.int32), depth=num_classes, axis=-1), -2)         box_scale = 2 - 1.0 * true_box[..., 2] * true_box[..., 3] / (self.img_size**2)         obj_mask = tf.squeeze(true_obj, -1)         conf_focal = tf.squeeze(tf.math.pow(true_obj - pred_obj, 2), -1)         iou = self.bbox_iou(pred_box, true_box, giou=True)         iou_loss = (1 - iou) * obj_mask * box_scale         conf_loss = conf_focal * self.loss_conf(true_obj, pred_obj)         class_loss = obj_mask * self.loss_cls(true_class, pred_class)         iou_loss = tf.reduce_mean(tf.reduce_sum(iou_loss, axis=[1, 2, 3]))         conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3]))         class_loss = tf.reduce_mean(tf.reduce_sum(class_loss, axis=[1, 2, 3])) * num_classes         return iou_loss, conf_loss, class_loss      @staticmethod     def bbox_iou(bbox1, bbox2, giou=False, diou=False, ciou=False, epsilon=1e-7):         b1x1, b1x2, b1y1, b1y2 = bbox1[..., 0], bbox1[..., 0] + bbox1[..., 2], bbox1[..., 1], bbox1[..., 1] + bbox1[..., 3]         b2x1, b2x2, b2y1, b2y2  = bbox2[..., 0], bbox2[..., 0] + bbox2[..., 2], bbox2[..., 1], bbox2[..., 1] + bbox2[..., 3]         # intersection area         inter = tf.maximum(tf.minimum(b1x2, b2x2) - tf.maximum(b1x1, b2x1), 0) * tf.maximum(             tf.minimum(b1y2, b2y2) - tf.maximum(b1y1, b2y1), 0)         # union area         w1, h1 = b1x2 - b1x1 + epsilon, b1y2 - b1y1 + epsilon         w2, h2 = b2x2 - b2x1 + epsilon, b2y2 - b2y1 + epsilon         union = w1 * h1 + w2 * h2 - inter + epsilon         # iou         iou = inter / union         if giou or diou or ciou:             # enclosing box             cw = tf.maximum(b1x2, b2x2) - tf.minimum(b1x1, b2x1)             ch = tf.maximum(b1y2, b2y2) - tf.minimum(b1y1, b2y1)             if giou:                 enclose_area = cw * ch + epsilon                 return iou - (enclose_area - union) / enclose_area             if diou or ciou:                 c2 = cw**2 + ch**2 + epsilon                 rho2 = ((b2x1 + b2x2) - (b1x1 + b1x2))**2 / 4 + ((b2y1 + b2y2) - (b1y1 + b1y2))**2 / 4                 if diou:                     return iou - rho2 / c2                 elif ciou:                     v = (4 / math.pi**2) * tf.pow(tf.atan(w2 / h2) - tf.atan(w1 / h1), 2)                     alpha = v / (1 - iou + v)                     return iou - (rho2 / c2 + v * alpha)         return tf.clip_by_value(iou, 0, 1)   class Rescale(TensorOp):     def forward(self, data, state):         return data / 255   class DecodePred(TensorOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.strides = [8, 16, 32]         self.num_anchor = 3         self.width, self.height = 640, 640         self.grids = self.create_grid(self.strides, self.num_anchor)         anchor_s = [(10, 13), (16, 30), (33, 23)]         anchor_m = [(30, 61), (62, 45), (59, 119)]         anchor_l = [(116, 90), (156, 198), (373, 326)]         self.anchors = self.create_anchor(anchor_s, anchor_m, anchor_l, self.strides)      def create_grid(self, strides, num_anchor):         grids = []         for stride in strides:             x_coor = [stride * i for i in range(self.width // stride)]             y_coor = [stride * i for i in range(self.height // stride)]             xx, yy = np.meshgrid(x_coor, y_coor)             xx, yy = np.float32(xx), np.float32(yy)             xx, yy = np.stack([xx] * num_anchor, axis=-1), np.stack([yy] * num_anchor, axis=-1)             grids.append(tf.convert_to_tensor(np.stack([xx, yy], axis=-1)))         return grids      def create_anchor(self, anchor_s, anchor_m, anchor_l, strides):         anchors = []         for anchor, stride in zip([anchor_s, anchor_m, anchor_l], strides):             feature_size_x, feature_size_y = self.width // stride, self.height // stride             anchor = np.array(anchor, dtype=\"float32\").reshape((1, 1, 3, 2))             anchor = np.tile(anchor, [feature_size_y, feature_size_x, 1, 1])             anchors.append(tf.convert_to_tensor(anchor))         return anchors      def forward(self, data, state):         conv_sbbox = self.decode(data[0], self.grids[0], self.anchors[0], self.strides[0])         conv_mbbox = self.decode(data[1], self.grids[1], self.anchors[1], self.strides[1])         conv_lbbox = self.decode(data[2], self.grids[2], self.anchors[2], self.strides[2])         return conv_sbbox, conv_mbbox, conv_lbbox      def decode(self, conv_bbox, grid, anchor, stride):         batch_size = conv_bbox.shape[0]         grid, anchor = tf.expand_dims(grid, 0), tf.expand_dims(anchor, 0)         grid, anchor = tf.tile(grid, [batch_size, 1, 1, 1, 1]), tf.tile(anchor, [batch_size, 1, 1, 1, 1])         conv_bbox = tf.sigmoid(conv_bbox)         bbox_pred, conf_pred, cls_pred = conv_bbox[..., 0:4], conv_bbox[..., 4:5], conv_bbox[..., 5:]         xcyc_pred, wh_pred = bbox_pred[..., 0:2], bbox_pred[..., 2:4]         xcyc_pred = (xcyc_pred * 2 - 0.5) * stride + grid         wh_pred = (wh_pred * 2)**2 * anchor         x1y1_pred = xcyc_pred - wh_pred / 2         result = tf.concat([x1y1_pred, wh_pred, conf_pred, cls_pred], axis=-1)         return result   class PredictBox(TensorOp):     def __init__(self, inputs, outputs, mode, width, height, max_outputs=500, conf_threshold=0.4):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.width = width         self.height = height         self.max_outputs = max_outputs         self.conf_threshold = conf_threshold      def forward(self, data, state):         conv_sbbox, conv_mbbox, conv_lbbox = data         batch_size = conv_sbbox.shape[0]         final_results = []         for idx in range(batch_size):             pred_s, pred_m, pred_l = conv_sbbox[idx], conv_mbbox[idx], conv_lbbox[idx]             pred_s, pred_m, pred_l = tf.reshape(pred_s, (-1, 85)), tf.reshape(pred_m, (-1, 85)), tf.reshape(pred_l, (-1, 85))             preds = tf.concat([pred_s, pred_m, pred_l], axis=0)             preds = preds[preds[:, 4] &gt; self.conf_threshold]  # filter by confidence             classes = tf.argmax(preds[:, 5:], axis=-1)             unique_classes = tf.unique(classes)[0]             selected_boxes_all_classes = tf.zeros(shape=[0, 6], dtype=tf.float32)             for clss in unique_classes:                 tf.autograph.experimental.set_loop_options(shape_invariants=[(selected_boxes_all_classes,                                                                               tf.TensorShape([None, 6]))])                 mask = tf.math.equal(classes, clss)                 preds_cls = tf.boolean_mask(preds, mask)                 x1, y1, w, h = preds_cls[:, 0], preds_cls[:, 1], preds_cls[:, 2], preds_cls[:, 3]                 x2, y2 = x1 + w, y1 + h                 conf_score, label = preds_cls[:, 4], tf.boolean_mask(classes, mask)                 selected_bboxes = tf.stack([y1, x1, y2, x2, conf_score, tf.cast(label, tf.float32)], axis=-1)                 # nms for every class                 nms_keep = tf.image.non_max_suppression(selected_bboxes[:, :4],                                                         selected_bboxes[:, 4],                                                         max_output_size=50,                                                         iou_threshold=0.35)                 selected_bboxes = tf.gather(selected_bboxes, nms_keep)                 selected_boxes_all_classes = tf.concat([selected_boxes_all_classes, selected_bboxes], axis=0)             # clip bounding boxes to image size             y1_abs = tf.clip_by_value(selected_boxes_all_classes[:, 0], 0, self.height)             x1_abs = tf.clip_by_value(selected_boxes_all_classes[:, 1], 0, self.width)             height_abs = tf.clip_by_value(selected_boxes_all_classes[:, 2] - y1_abs, 0, self.height - y1_abs)             width_abs = tf.clip_by_value(selected_boxes_all_classes[:, 3] - x1_abs, 0, self.width - x1_abs)             labels_score, labels = selected_boxes_all_classes[:, 4], selected_boxes_all_classes[:, 5]             # final output: [x1, y1, w, h, label, label_score, select_or_not]             results_single = [x1_abs, y1_abs, width_abs, height_abs, labels, labels_score, tf.ones_like(x1_abs)]             results_single = tf.stack(results_single, axis=-1)             # pad 0 to other rows to improve performance             results_single = tf.pad(results_single, [(0, self.max_outputs - tf.shape(results_single)[0]), (0, 0)])             final_results.append(results_single)         final_results = tf.stack(final_results)         return final_results In\u00a0[10]: Copied! <pre>network = fe.Network(ops=[\n    Rescale(inputs=\"image\", outputs=\"image\"),\n    ModelOp(model=model, inputs=\"image\", outputs=(\"pred_s\", \"pred_m\", \"pred_l\")),\n    DecodePred(inputs=(\"pred_s\", \"pred_m\", \"pred_l\"), outputs=(\"pred_s\", \"pred_m\", \"pred_l\")),\n    ComputeLoss(inputs=(\"pred_s\", \"gt_sbbox\"), outputs=(\"sbbox_loss\", \"sconf_loss\", \"scls_loss\")),\n    ComputeLoss(inputs=(\"pred_m\", \"gt_mbbox\"), outputs=(\"mbbox_loss\", \"mconf_loss\", \"mcls_loss\")),\n    ComputeLoss(inputs=(\"pred_l\", \"gt_lbbox\"), outputs=(\"lbbox_loss\", \"lconf_loss\", \"lcls_loss\")),\n    Average(inputs=(\"sbbox_loss\", \"mbbox_loss\", \"lbbox_loss\"), outputs=\"bbox_loss\"),\n    Average(inputs=(\"sconf_loss\", \"mconf_loss\", \"lconf_loss\"), outputs=\"conf_loss\"),\n    Average(inputs=(\"scls_loss\", \"mcls_loss\", \"lcls_loss\"), outputs=\"cls_loss\"),\n    Average(inputs=(\"bbox_loss\", \"conf_loss\", \"cls_loss\"), outputs=\"total_loss\"),\n    PredictBox(width=640, height=640, inputs=(\"pred_s\", \"pred_m\", \"pred_l\"), outputs=\"box_pred\", mode=\"eval\"),\n    UpdateOp(model=model, loss_name=\"total_loss\")\n])\n</pre> network = fe.Network(ops=[     Rescale(inputs=\"image\", outputs=\"image\"),     ModelOp(model=model, inputs=\"image\", outputs=(\"pred_s\", \"pred_m\", \"pred_l\")),     DecodePred(inputs=(\"pred_s\", \"pred_m\", \"pred_l\"), outputs=(\"pred_s\", \"pred_m\", \"pred_l\")),     ComputeLoss(inputs=(\"pred_s\", \"gt_sbbox\"), outputs=(\"sbbox_loss\", \"sconf_loss\", \"scls_loss\")),     ComputeLoss(inputs=(\"pred_m\", \"gt_mbbox\"), outputs=(\"mbbox_loss\", \"mconf_loss\", \"mcls_loss\")),     ComputeLoss(inputs=(\"pred_l\", \"gt_lbbox\"), outputs=(\"lbbox_loss\", \"lconf_loss\", \"lcls_loss\")),     Average(inputs=(\"sbbox_loss\", \"mbbox_loss\", \"lbbox_loss\"), outputs=\"bbox_loss\"),     Average(inputs=(\"sconf_loss\", \"mconf_loss\", \"lconf_loss\"), outputs=\"conf_loss\"),     Average(inputs=(\"scls_loss\", \"mcls_loss\", \"lcls_loss\"), outputs=\"cls_loss\"),     Average(inputs=(\"bbox_loss\", \"conf_loss\", \"cls_loss\"), outputs=\"total_loss\"),     PredictBox(width=640, height=640, inputs=(\"pred_s\", \"pred_m\", \"pred_l\"), outputs=\"box_pred\", mode=\"eval\"),     UpdateOp(model=model, loss_name=\"total_loss\") ]) In\u00a0[11]: Copied! <pre>def lr_schedule_warmup(step, train_steps_epoch, init_lr):\n    warmup_steps = train_steps_epoch * 3\n    if step &lt; warmup_steps:\n        lr = init_lr / warmup_steps * step\n    else:\n        lr = init_lr\n    return lr\n\ntraces = [\n    MeanAveragePrecision(num_classes=80, true_key='bbox', pred_key='box_pred', mode=\"eval\"),\n    BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\")\n]\nlr_schedule = {\n    1:\n    LRScheduler(\n        model=model,\n        lr_fn=lambda step: lr_schedule_warmup(\n            step, train_steps_epoch=np.ceil(len(train_ds) / batch_size), init_lr=init_lr)),\n    4:\n    LRScheduler(\n        model=model,\n        lr_fn=lambda epoch: cosine_decay(\n            epoch, cycle_length=epochs - 3, init_lr=init_lr, min_lr=init_lr / 100, start=4))\n}\ntraces.append(EpochScheduler(lr_schedule))\n</pre> def lr_schedule_warmup(step, train_steps_epoch, init_lr):     warmup_steps = train_steps_epoch * 3     if step &lt; warmup_steps:         lr = init_lr / warmup_steps * step     else:         lr = init_lr     return lr  traces = [     MeanAveragePrecision(num_classes=80, true_key='bbox', pred_key='box_pred', mode=\"eval\"),     BestModelSaver(model=model, save_dir=model_dir, metric='mAP', save_best_mode=\"max\") ] lr_schedule = {     1:     LRScheduler(         model=model,         lr_fn=lambda step: lr_schedule_warmup(             step, train_steps_epoch=np.ceil(len(train_ds) / batch_size), init_lr=init_lr)),     4:     LRScheduler(         model=model,         lr_fn=lambda epoch: cosine_decay(             epoch, cycle_length=epochs - 3, init_lr=init_lr, min_lr=init_lr / 100, start=4)) } traces.append(EpochScheduler(lr_schedule)) In\u00a0[\u00a0]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         monitor_names=[\"bbox_loss\", \"conf_loss\", \"cls_loss\"],\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\nestimator.fit()\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          monitor_names=[\"bbox_loss\", \"conf_loss\", \"cls_loss\"],                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) estimator.fit() In\u00a0[13]: Copied! <pre>eval_data = pipeline.get_results(mode=\"eval\")\nimages_eval = eval_data[\"image\"].numpy()\neval_data = network.transform(eval_data, mode=\"eval\")\nbboxes_eval = eval_data[\"box_pred\"][..., :5]\n</pre> eval_data = pipeline.get_results(mode=\"eval\") images_eval = eval_data[\"image\"].numpy() eval_data = network.transform(eval_data, mode=\"eval\") bboxes_eval = eval_data[\"box_pred\"][..., :5] In\u00a0[21]: Copied! <pre>visualize_image_with_box(images_eval, bboxes_eval)\n</pre> visualize_image_with_box(images_eval, bboxes_eval) In\u00a0[24]: Copied! <pre>visualize_image_with_box(images_eval, bboxes_eval)\n</pre> visualize_image_with_box(images_eval, bboxes_eval)"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#yolov5", "title": "YOLOv5\u00b6", "text": "<p>You Only Look Once(YOLO) is a family of object detection models known for its fast inferencing speed. From the first YOLO idea to the current YOLOv5, different authors have proposed various improvements on object detection accuracy and inferencing speed.</p> <p>What makes YOLO standout is its core idea of combining region proposal and classification together so the network can be trained together. Specifically, YOLO splits the image into N x N grid, and each object is \"assigned\" to the corresponding grid.</p> <p>Comparing YOLOv5 against initial YOLO implementation, YOLOv5 utilizes many new ideas that others proposed throughout the year. Here is a summary of technical details of Yolov5:</p> <ul> <li>Mosaic data augmentation: combining 4 images together to increase the diversity and number of objects of each sample.</li> <li>HSV augmentation: changes the pixel saturation to further increase input image diversity</li> <li>Assigning object using ratio between object size and anchor box size: traditionally an object is assigned using IOU calculation, the drawback of using IOU assignment is that it often results in limited number of matches.</li> <li>Double assignment of object: To improve the number of matches further, once an object \"matches\" to an anchor box, then the next spatially closest anchor box is also matched.</li> <li>SILU activation: The SILU activation is x * sigmoid(x). It combines the advantage of ReLu and Sigmoid together. It is continuous, at the same time, robust against vanishing gradient.</li> <li>Convolution Block: The convolution block is Conv + Batchnorm + SILU (Activation) combination. The bias is only applied in the last prediction conv layer to prevent over-fitting.</li> <li>BottleNeck: Bottleneck consists of multiple convolution blocks, usually in forms of skip connection: output = conv2(conv1(x)) + x</li> <li>Focus Block: In the beginning of the network, it halves the spatial size by sampling every other pixel and later cocatenate them in the channel dimension. As a result, the output dimension becomes [h/2, w/2, c * 4], it translates width-height information into the channel space.</li> <li>Spatial Pyramid Pooling layer: Spatial pyramid pooling applies different kernel size of pooling (with stride 1) so that the final outcome can represent information at different smoothing range. Formula: output =  conv -&gt; concat[identity, pool(5), pool(9), pool(13)]) -&gt; conv2</li> <li>IOU loss for coordinates: Using IOU loss rather than traditional coordinate regression loss can better represent how close between the prediction box and ground truth box.</li> <li>Per-class NMS: Different from many other detection models, YOLO performs NMS separately for each class to increase the precision.</li> </ul> <p>Now let's implement this beautiful algorithm together in TensorFlow. First import necessary functions:</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#dataset", "title": "Dataset\u00b6", "text": "<p>During training, it uses mosaic dataset augmentation, then pad to the longest size, followed by horizontal flip and HSV augmentation. In the meantime, we will also create the ground truth vector to help calculate loss later.</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#pipeline", "title": "Pipeline\u00b6", "text": "<p>Now that we have the necessary components, we are ready to create the data pipeline. In TensorFlow, the batch size is 16 for 640 x 640 image on 11Gb VRAM GPU. The batch size can be scaled up by adding more number of GPUs.</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#visualize-pipeline-preprocessing-results", "title": "Visualize Pipeline Preprocessing Results\u00b6", "text": ""}, {"location": "apphub/instance_detection/yolov5/yolov5.html#define-yolov5-network", "title": "Define YOLOv5 Network\u00b6", "text": "<p>As mentioned earlier, YOLOv5 uses several special blocks to improve the detection accuracy, this section will illustrate its implementation in tensorflow.</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#training-and-evaluation-network-operations", "title": "Training and Evaluation Network Operations\u00b6", "text": "<p>Image pixel values will be rescaled to [0, 1], then used as input to the network.  The network output will go through an extra step of decoding to translate the prediction into actual bounding box coordinates. Finally, the coordinates and classes will be used to compute the loss and update the model.  During evaluation, per-class Non-maximal suppression will be performed to filter out similar boxes.</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#metrics-and-learning-rate-scheduling", "title": "Metrics and Learning Rate Scheduling\u00b6", "text": "<p>We will use mean Average Precision (mAP) as object detection metric. The learning rate schedule is a combination of two schedulers:</p> <ol> <li>During the first 3 epochs, learning rate will start from 0 and increase linearly per step until it reaches initial learning rate (0.01 when batch size is 64)</li> <li>For the rest of epochs, apply cosine-decay to the learning rate every epoch until it reaches 1% of initial learning rate in the end.</li> </ol>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#train-the-yolov5", "title": "Train the YOLOv5\u00b6", "text": "<p>We will train the model for 200 epochs, the training takes ~2 days on 4 Nvidia V100 GPU</p>"}, {"location": "apphub/instance_detection/yolov5/yolov5.html#run-inferencing", "title": "Run Inferencing\u00b6", "text": "<p>After training the model, we are ready to run the inferencing. For illustration purpose, we use evaluation data.</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html", "title": "SOLOv2", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom scipy.ndimage.measurements import center_of_mass\nfrom tensorflow.keras import layers\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import mscoco\nfrom fastestimator.op.numpyop import Batch, Delete, NumpyOp, RemoveIf\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, Resize\nfrom fastestimator.op.numpyop.univariate import ReadImage\nfrom fastestimator.op.tensorop.loss import L2Regularizaton\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop.tensorop import LambdaOp, TensorOp\nfrom fastestimator.op.tensorop.normalize import Normalize\nfrom fastestimator.schedule import EpochScheduler, cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.trace import Trace\nfrom fastestimator.util import Suppressor, get_num_devices\n</pre> import os import tempfile  import cv2 import numpy as np import pycocotools.mask as mask_util import tensorflow as tf import tensorflow_addons as tfa from pycocotools.coco import COCO from pycocotools.cocoeval import COCOeval from scipy.ndimage.measurements import center_of_mass from tensorflow.keras import layers  import fastestimator as fe from fastestimator.dataset.data import mscoco from fastestimator.op.numpyop import Batch, Delete, NumpyOp, RemoveIf from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, Resize from fastestimator.op.numpyop.univariate import ReadImage from fastestimator.op.tensorop.loss import L2Regularizaton from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop.tensorop import LambdaOp, TensorOp from fastestimator.op.tensorop.normalize import Normalize from fastestimator.schedule import EpochScheduler, cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.trace import Trace from fastestimator.util import Suppressor, get_num_devices In\u00a0[2]: parameters Copied! <pre>data_dir = None\nmodel_dir = tempfile.mkdtemp()\nepochs=12\nbatch_size_per_gpu=4\ntrain_steps_per_epoch=None\neval_steps_per_epoch=None\nim_size=1344\n</pre> data_dir = None model_dir = tempfile.mkdtemp() epochs=12 batch_size_per_gpu=4 train_steps_per_epoch=None eval_steps_per_epoch=None im_size=1344 In\u00a0[3]: Copied! <pre>class MergeMask(NumpyOp):\n    def forward(self, data, state):\n        data = np.stack(data, axis=-1)\n        return data\n\n\nclass GetImageSize(NumpyOp):\n    def forward(self, data, state):\n        height, width, _ = data.shape\n        return np.array([height, width], dtype=\"int32\")\n\n\nclass Gt2Target(NumpyOp):\n    def __init__(self,\n                 inputs,\n                 outputs,\n                 mode=None,\n                 num_grids=[40, 36, 24, 16, 12],\n                 scale_ranges=[[1, 96], [48, 192], [96, 384], [192, 768], [384, 2048]],\n                 coord_sigma=0.05):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.num_grids = num_grids\n        self.scale_ranges = scale_ranges\n        self.coord_sigma = coord_sigma\n        missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]\n        category = [x for x in range(1, 91) if not x in missing_category]\n        self.mapping = {k: v for k, v in zip(category, list(range(80)))}\n\n    def forward(self, data, state):\n        masks, bboxes = data\n        bboxes = np.array(bboxes, dtype=\"float32\")\n        masks = np.transpose(masks, [2, 0, 1])  # (H, W, #objects) -&gt; (#objects, H, W)\n        masks, bboxes = self.remove_empty_gt(masks, bboxes)\n        # 91 classes -&gt; 80 classes that starts from 1\n        classes = np.array([self.mapping[int(x[-1])] + 1 for x in bboxes], dtype=np.int32)\n        widths, heights = bboxes[:, 2], bboxes[:, 3]\n        gt_match = []  # number of objects x (grid_idx, height_idx, width_idx, exist)\n        for width, height, mask in zip(widths, heights, masks):\n            object_match = []\n            object_scale = np.sqrt(width * height)\n            center_h, center_w = center_of_mass(mask)\n            for grid_idx, ((lower_scale, upper_scale), num_grid) in enumerate(zip(self.scale_ranges, self.num_grids)):\n                grid_matched = (object_scale &gt;= lower_scale) &amp; (object_scale &lt;= upper_scale)\n                if grid_matched:\n                    w_delta, h_delta = 0.5 * width * self.coord_sigma, 0.5 * height * self.coord_sigma\n                    coord_h, coord_w = int(center_h / mask.shape[0] * num_grid), int(center_w / mask.shape[1] * num_grid)\n                    # each object will have some additional area of effect\n                    top_box_extend = max(0, int((center_h - h_delta) / mask.shape[0] * num_grid))\n                    down_box_extend = min(num_grid - 1, int((center_h + h_delta) / mask.shape[0] * num_grid))\n                    left_box_extend = max(0, int((center_w - w_delta) / mask.shape[1] * num_grid))\n                    right_box_extend = min(num_grid - 1, int((center_w + w_delta) / mask.shape[1] * num_grid))\n                    # make sure the additional area of effect is at most 1 grid more\n                    top_box_extend = max(top_box_extend, coord_h - 1)\n                    down_box_extend = min(down_box_extend, coord_h + 1)\n                    left_box_extend = max(left_box_extend, coord_w - 1)\n                    right_box_extend = min(right_box_extend, coord_w + 1)\n                    object_match.extend([(grid_idx, y, x, 1) for y in range(top_box_extend, down_box_extend + 1)\n                                         for x in range(left_box_extend, right_box_extend + 1)])\n            gt_match.append(object_match)\n        gt_match = self.pad_match(gt_match)  #num_object x num_matches x [grid_idx, heihght_idx, width_idx, exist]\n        return gt_match, masks, classes\n\n    def pad_match(self, gt_match):\n        max_num_matches = max([len(match) for match in gt_match])\n        for match in gt_match:\n            match.extend([(0, 0, 0, 0) for _ in range(max_num_matches - len(match))])\n        return np.array(gt_match, dtype=\"int32\")\n\n    def remove_empty_gt(self, masks, bboxes):\n        num_objects = masks.shape[0]\n        non_empty_mask = np.sum(masks.reshape(num_objects, -1), axis=1) &gt; 0\n        return masks[non_empty_mask], bboxes[non_empty_mask]\n\n\nassert im_size % 32 == 0, \"im_size must be a multiple of 32\"\nnum_device = get_num_devices()\ntrain_ds, val_ds = mscoco.load_data(root_dir=data_dir, load_masks=True)\nbatch_size = num_device * batch_size_per_gpu\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=val_ds,\n    test_data=val_ds,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        MergeMask(inputs=\"mask\", outputs=\"mask\"),\n        GetImageSize(inputs=\"image\", outputs=\"imsize\", mode=\"test\"),\n        LongestMaxSize(max_size=im_size, image_in=\"image\", mask_in=\"mask\", bbox_in=\"bbox\", bbox_params=\"coco\"),\n        RemoveIf(fn=lambda x: len(x) == 0, inputs=\"bbox\"),\n        PadIfNeeded(min_height=im_size,\n                    min_width=im_size,\n                    image_in=\"image\",\n                    mask_in=\"mask\",\n                    bbox_in=\"bbox\",\n                    bbox_params=\"coco\",\n                    border_mode=cv2.BORDER_CONSTANT,\n                    value=0),\n        Sometimes(HorizontalFlip(image_in=\"image\", mask_in=\"mask\", bbox_in=\"bbox\", bbox_params=\"coco\",\n                                 mode=\"train\")),\n        Resize(height=im_size // 4, width=im_size // 4, image_in='mask'),  # downscale mask for memory efficiency\n        Gt2Target(inputs=(\"mask\", \"bbox\"), outputs=(\"gt_match\", \"mask\", \"classes\")),\n        Delete(keys=\"bbox\"),\n        Delete(keys=\"image_id\", mode=\"!test\"),\n        Batch(batch_size=batch_size, pad_value=0)\n    ],\n    num_process=8 * num_device)\n</pre> class MergeMask(NumpyOp):     def forward(self, data, state):         data = np.stack(data, axis=-1)         return data   class GetImageSize(NumpyOp):     def forward(self, data, state):         height, width, _ = data.shape         return np.array([height, width], dtype=\"int32\")   class Gt2Target(NumpyOp):     def __init__(self,                  inputs,                  outputs,                  mode=None,                  num_grids=[40, 36, 24, 16, 12],                  scale_ranges=[[1, 96], [48, 192], [96, 384], [192, 768], [384, 2048]],                  coord_sigma=0.05):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.num_grids = num_grids         self.scale_ranges = scale_ranges         self.coord_sigma = coord_sigma         missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]         category = [x for x in range(1, 91) if not x in missing_category]         self.mapping = {k: v for k, v in zip(category, list(range(80)))}      def forward(self, data, state):         masks, bboxes = data         bboxes = np.array(bboxes, dtype=\"float32\")         masks = np.transpose(masks, [2, 0, 1])  # (H, W, #objects) -&gt; (#objects, H, W)         masks, bboxes = self.remove_empty_gt(masks, bboxes)         # 91 classes -&gt; 80 classes that starts from 1         classes = np.array([self.mapping[int(x[-1])] + 1 for x in bboxes], dtype=np.int32)         widths, heights = bboxes[:, 2], bboxes[:, 3]         gt_match = []  # number of objects x (grid_idx, height_idx, width_idx, exist)         for width, height, mask in zip(widths, heights, masks):             object_match = []             object_scale = np.sqrt(width * height)             center_h, center_w = center_of_mass(mask)             for grid_idx, ((lower_scale, upper_scale), num_grid) in enumerate(zip(self.scale_ranges, self.num_grids)):                 grid_matched = (object_scale &gt;= lower_scale) &amp; (object_scale &lt;= upper_scale)                 if grid_matched:                     w_delta, h_delta = 0.5 * width * self.coord_sigma, 0.5 * height * self.coord_sigma                     coord_h, coord_w = int(center_h / mask.shape[0] * num_grid), int(center_w / mask.shape[1] * num_grid)                     # each object will have some additional area of effect                     top_box_extend = max(0, int((center_h - h_delta) / mask.shape[0] * num_grid))                     down_box_extend = min(num_grid - 1, int((center_h + h_delta) / mask.shape[0] * num_grid))                     left_box_extend = max(0, int((center_w - w_delta) / mask.shape[1] * num_grid))                     right_box_extend = min(num_grid - 1, int((center_w + w_delta) / mask.shape[1] * num_grid))                     # make sure the additional area of effect is at most 1 grid more                     top_box_extend = max(top_box_extend, coord_h - 1)                     down_box_extend = min(down_box_extend, coord_h + 1)                     left_box_extend = max(left_box_extend, coord_w - 1)                     right_box_extend = min(right_box_extend, coord_w + 1)                     object_match.extend([(grid_idx, y, x, 1) for y in range(top_box_extend, down_box_extend + 1)                                          for x in range(left_box_extend, right_box_extend + 1)])             gt_match.append(object_match)         gt_match = self.pad_match(gt_match)  #num_object x num_matches x [grid_idx, heihght_idx, width_idx, exist]         return gt_match, masks, classes      def pad_match(self, gt_match):         max_num_matches = max([len(match) for match in gt_match])         for match in gt_match:             match.extend([(0, 0, 0, 0) for _ in range(max_num_matches - len(match))])         return np.array(gt_match, dtype=\"int32\")      def remove_empty_gt(self, masks, bboxes):         num_objects = masks.shape[0]         non_empty_mask = np.sum(masks.reshape(num_objects, -1), axis=1) &gt; 0         return masks[non_empty_mask], bboxes[non_empty_mask]   assert im_size % 32 == 0, \"im_size must be a multiple of 32\" num_device = get_num_devices() train_ds, val_ds = mscoco.load_data(root_dir=data_dir, load_masks=True) batch_size = num_device * batch_size_per_gpu pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=val_ds,     test_data=val_ds,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         MergeMask(inputs=\"mask\", outputs=\"mask\"),         GetImageSize(inputs=\"image\", outputs=\"imsize\", mode=\"test\"),         LongestMaxSize(max_size=im_size, image_in=\"image\", mask_in=\"mask\", bbox_in=\"bbox\", bbox_params=\"coco\"),         RemoveIf(fn=lambda x: len(x) == 0, inputs=\"bbox\"),         PadIfNeeded(min_height=im_size,                     min_width=im_size,                     image_in=\"image\",                     mask_in=\"mask\",                     bbox_in=\"bbox\",                     bbox_params=\"coco\",                     border_mode=cv2.BORDER_CONSTANT,                     value=0),         Sometimes(HorizontalFlip(image_in=\"image\", mask_in=\"mask\", bbox_in=\"bbox\", bbox_params=\"coco\",                                  mode=\"train\")),         Resize(height=im_size // 4, width=im_size // 4, image_in='mask'),  # downscale mask for memory efficiency         Gt2Target(inputs=(\"mask\", \"bbox\"), outputs=(\"gt_match\", \"mask\", \"classes\")),         Delete(keys=\"bbox\"),         Delete(keys=\"image_id\", mode=\"!test\"),         Batch(batch_size=batch_size, pad_value=0)     ],     num_process=8 * num_device) In\u00a0[4]: Copied! <pre>from matplotlib import pyplot as plt\nimport matplotlib.ticker as plticker\n\ndef plot_overlay(image_resized, masks, match, num_grid, axe, grid_idx, mask_size):\n    if num_grid is None:\n        overall_mask = np.max(masks, axis=0)\n    else:\n        overall_mask = np.zeros_like(masks)\n        for idx, (mask, mat) in enumerate(zip(masks, match)):\n            mat = mat[np.sum(mat, -1) &gt; 0]\n            matched_grids = set(mat[:, 0])\n            if grid_idx in matched_grids:\n                overall_mask[idx] = mask\n                matched_current_grid = mat[mat[:, 0] == grid_idx]\n                for point in matched_current_grid:\n                    y_coordinate = (point[1] + 0.5) * mask_size / num_grid\n                    x_coordinate = (point[2] + 0.5) * mask_size / num_grid\n                    axe.plot(x_coordinate, y_coordinate, 'bo')\n        overall_mask = np.max(overall_mask, axis=0)\n    overall_mask = np.uint8(np.where(overall_mask &lt; 0.5, 0, 255))\n    mask_rgb = cv2.cvtColor(overall_mask, cv2.COLOR_GRAY2RGB)\n    mask_overlay = np.where(mask_rgb != [0, 0, 0], [255, 0, 0], [0, 0, 0])\n    mask_overlay = mask_overlay.astype(np.uint8)\n    img_with_mask = cv2.addWeighted(image_resized, 0.7, mask_overlay, 0.3, 0)\n    if num_grid:\n        axe.set_title(\"{} x {}\".format(num_grid, num_grid))\n        loc = plticker.MultipleLocator(base= mask_size / num_grid)\n        axe.xaxis.set_major_locator(loc)\n        axe.yaxis.set_major_locator(loc)\n        axe.grid(which='major', axis='both', linestyle='-')\n    axe.xaxis.set_ticklabels([])\n    axe.yaxis.set_ticklabels([])\n    axe.imshow(img_with_mask)\n\ndef display_pipeline(idx, image, mask, match):\n    image, mask, match = image[idx], mask[idx], match[idx]\n    num_grids = [None, 40, 36, 24, 16, 12]\n    mask_size = mask.shape[-1]\n    image_resized = cv2.resize(image, (mask_size, mask_size))\n    mask_exist = [np.sum(m) &gt; 0 for m in mask]\n    mask = mask[mask_exist]\n    match = match[mask_exist]\n    _, axes = plt.subplots(1, 6, figsize=(25, 5))\n    for idx, (axe, num_grid) in enumerate(zip(axes, num_grids)):\n        plot_overlay(image_resized, mask, match, num_grid, axe, idx - 1, mask_size)\n    plt.show()\n    \nsample_batch = pipeline.get_results()\nimage = sample_batch['image'].numpy()\nmask = sample_batch['mask'].numpy()\ngt_match = sample_batch['gt_match'].numpy()\n</pre> from matplotlib import pyplot as plt import matplotlib.ticker as plticker  def plot_overlay(image_resized, masks, match, num_grid, axe, grid_idx, mask_size):     if num_grid is None:         overall_mask = np.max(masks, axis=0)     else:         overall_mask = np.zeros_like(masks)         for idx, (mask, mat) in enumerate(zip(masks, match)):             mat = mat[np.sum(mat, -1) &gt; 0]             matched_grids = set(mat[:, 0])             if grid_idx in matched_grids:                 overall_mask[idx] = mask                 matched_current_grid = mat[mat[:, 0] == grid_idx]                 for point in matched_current_grid:                     y_coordinate = (point[1] + 0.5) * mask_size / num_grid                     x_coordinate = (point[2] + 0.5) * mask_size / num_grid                     axe.plot(x_coordinate, y_coordinate, 'bo')         overall_mask = np.max(overall_mask, axis=0)     overall_mask = np.uint8(np.where(overall_mask &lt; 0.5, 0, 255))     mask_rgb = cv2.cvtColor(overall_mask, cv2.COLOR_GRAY2RGB)     mask_overlay = np.where(mask_rgb != [0, 0, 0], [255, 0, 0], [0, 0, 0])     mask_overlay = mask_overlay.astype(np.uint8)     img_with_mask = cv2.addWeighted(image_resized, 0.7, mask_overlay, 0.3, 0)     if num_grid:         axe.set_title(\"{} x {}\".format(num_grid, num_grid))         loc = plticker.MultipleLocator(base= mask_size / num_grid)         axe.xaxis.set_major_locator(loc)         axe.yaxis.set_major_locator(loc)         axe.grid(which='major', axis='both', linestyle='-')     axe.xaxis.set_ticklabels([])     axe.yaxis.set_ticklabels([])     axe.imshow(img_with_mask)  def display_pipeline(idx, image, mask, match):     image, mask, match = image[idx], mask[idx], match[idx]     num_grids = [None, 40, 36, 24, 16, 12]     mask_size = mask.shape[-1]     image_resized = cv2.resize(image, (mask_size, mask_size))     mask_exist = [np.sum(m) &gt; 0 for m in mask]     mask = mask[mask_exist]     match = match[mask_exist]     _, axes = plt.subplots(1, 6, figsize=(25, 5))     for idx, (axe, num_grid) in enumerate(zip(axes, num_grids)):         plot_overlay(image_resized, mask, match, num_grid, axe, idx - 1, mask_size)     plt.show()      sample_batch = pipeline.get_results() image = sample_batch['image'].numpy() mask = sample_batch['mask'].numpy() gt_match = sample_batch['gt_match'].numpy() In\u00a0[5]: Copied! <pre>display_pipeline(1, image, mask, gt_match)\n</pre> display_pipeline(1, image, mask, gt_match) In\u00a0[6]: Copied! <pre>display_pipeline(2, image, mask, gt_match)\n</pre> display_pipeline(2, image, mask, gt_match) In\u00a0[7]: Copied! <pre>display_pipeline(3, image, mask, gt_match)\n</pre> display_pipeline(3, image, mask, gt_match) <p>In figures above, the presence of blue dot represents the object being assigned to the grid location. To increase success, one object is often assigned to multiple grids at once. In addition, smaller grid cells are responsible for smaller objects, and large cells for larger objects. That's why zebras are only highlighted in 16x16 and 12x12 grids in the third figure.</p> In\u00a0[8]: Copied! <pre>def fpn(C2, C3, C4, C5):\n    # lateral conv\n    P5 = layers.Conv2D(256, kernel_size=1)(C5)\n    P5_up = layers.UpSampling2D()(P5)\n    P4 = layers.Conv2D(256, kernel_size=1)(C4)\n    P4 = P4 + P5_up\n    P4_up = layers.UpSampling2D()(P4)\n    P3 = layers.Conv2D(256, kernel_size=1)(C3)\n    P3 = P3 + P4_up\n    P3_up = layers.UpSampling2D()(P3)\n    P2 = layers.Conv2D(256, kernel_size=1)(C2)\n    P2 = P2 + P3_up\n    # fpn conv\n    P5 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P5)\n    P4 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P4)\n    P3 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P3)\n    P2 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P2)\n    return P2, P3, P4, P5\n\n\ndef pad_with_coord(data):\n    data_shape = tf.shape(data)\n    batch_size, height, width = data_shape[0], data_shape[1], data_shape[2]\n    x = tf.cast(tf.linspace(-1, 1, num=width), data.dtype)\n    x = tf.tile(x[tf.newaxis, tf.newaxis, ..., tf.newaxis], [batch_size, height, 1, 1])\n    y = tf.cast(tf.linspace(-1, 1, num=height), data.dtype)\n    y = tf.tile(y[tf.newaxis, ..., tf.newaxis, tf.newaxis], [batch_size, 1, width, 1])\n    data = tf.concat([data, x, y], axis=-1)\n    return data\n\n\ndef conv_norm(x, filters, kernel_size=3, groups=32):\n    x = layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', use_bias=False)(x)\n    x = tfa.layers.GroupNormalization(groups=groups, epsilon=1e-5)(x)\n    return x\n\n\ndef solov2_head_model(stacked_convs=4, ch_in=258, ch_feature=512, ch_kernel_out=256, num_classes=80):\n    inputs = layers.Input(shape=(None, None, ch_in))\n    feature_kernel = inputs\n    feature_cls = inputs[..., :-2]\n    for _ in range(stacked_convs):\n        feature_kernel = tf.nn.relu(conv_norm(feature_kernel, filters=ch_feature))\n        feature_cls = tf.nn.relu(conv_norm(feature_cls, filters=ch_feature))\n    feature_kernel = layers.Conv2D(filters=ch_kernel_out,\n                                   kernel_size=3,\n                                   padding='same',\n                                   kernel_initializer=tf.random_normal_initializer(stddev=0.01))(feature_kernel)\n    feature_cls = layers.Conv2D(filters=num_classes,\n                                kernel_size=3,\n                                padding='same',\n                                kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n                                bias_initializer=tf.initializers.constant(np.log(1 / 99)))(feature_cls)\n    return tf.keras.Model(inputs=inputs, outputs=[feature_kernel, feature_cls])\n\n\ndef solov2_head(P2, P3, P4, P5, num_classes=80):\n    head_model = solov2_head_model(num_classes=num_classes)\n    # applying maxpool first for P2\n    P2 = layers.MaxPool2D()(P2)\n    features = [P2, P3, P4, P5, P5]\n    grid_sizes = [40, 36, 24, 16, 12]\n    feat_kernel_list, feat_cls_list = [], []\n    for feature, grid_size in zip(features, grid_sizes):\n        feature = pad_with_coord(feature)\n        feature = tf.image.resize(feature, size=(grid_size, grid_size))\n        feat_kernel, feat_cls = head_model(feature)\n        feat_kernel_list.append(feat_kernel)\n        feat_cls_list.append(tf.sigmoid(feat_cls))\n    return feat_cls_list, feat_kernel_list\n\n\ndef solov2_maskhead(P2, P3, P4, P5, mid_ch=128, out_ch=256):\n    # first level\n    P2 = tf.nn.relu(conv_norm(P2, filters=mid_ch))\n    # second level\n    P3 = tf.nn.relu(conv_norm(P3, filters=mid_ch))\n    P3 = layers.UpSampling2D()(P3)\n    # third level\n    P4 = tf.nn.relu(conv_norm(P4, filters=mid_ch))\n    P4 = layers.UpSampling2D()(P4)\n    P4 = tf.nn.relu(conv_norm(P4, filters=mid_ch))\n    P4 = layers.UpSampling2D()(P4)\n    # top level, add coordinate\n    P5 = tf.nn.relu(conv_norm(pad_with_coord(P5), filters=mid_ch))\n    P5 = layers.UpSampling2D()(P5)\n    P5 = tf.nn.relu(conv_norm(P5, filters=mid_ch))\n    P5 = layers.UpSampling2D()(P5)\n    P5 = tf.nn.relu(conv_norm(P5, filters=mid_ch))\n    P5 = layers.UpSampling2D()(P5)\n    seg_outputs = tf.nn.relu(conv_norm(P2 + P3 + P4 + P5, filters=out_ch, kernel_size=1))\n    return seg_outputs\n\n\ndef solov2(input_shape=(None, None, 3), num_classes=80):\n    inputs = tf.keras.Input(shape=input_shape)\n    resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n    assert resnet50.layers[38].name == \"conv2_block3_out\"\n    C2 = resnet50.layers[38].output\n    assert resnet50.layers[80].name == \"conv3_block4_out\"\n    C3 = resnet50.layers[80].output\n    assert resnet50.layers[142].name == \"conv4_block6_out\"\n    C4 = resnet50.layers[142].output\n    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n    C5 = resnet50.layers[-1].output\n    P2, P3, P4, P5 = fpn(C2, C3, C4, C5)\n    feat_seg = solov2_maskhead(P2, P3, P4, P5)  # [B, h/4, w/4, 256]\n    feat_cls_list, feat_kernel_list = solov2_head(P2, P3, P4, P5, num_classes=num_classes)  # [B, grid, grid, 80], [B, grid, grid, 256]\n    model = tf.keras.Model(inputs=inputs, outputs=[feat_seg, feat_cls_list, feat_kernel_list])\n    return model\n\ninit_lr = 1e-2 / 16 * batch_size\nmodel = fe.build(model_fn=lambda: solov2(input_shape=(im_size, im_size, 3)),\n                 optimizer_fn=lambda: tf.optimizers.SGD(learning_rate=init_lr, momentum=0.9))\n</pre> def fpn(C2, C3, C4, C5):     # lateral conv     P5 = layers.Conv2D(256, kernel_size=1)(C5)     P5_up = layers.UpSampling2D()(P5)     P4 = layers.Conv2D(256, kernel_size=1)(C4)     P4 = P4 + P5_up     P4_up = layers.UpSampling2D()(P4)     P3 = layers.Conv2D(256, kernel_size=1)(C3)     P3 = P3 + P4_up     P3_up = layers.UpSampling2D()(P3)     P2 = layers.Conv2D(256, kernel_size=1)(C2)     P2 = P2 + P3_up     # fpn conv     P5 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P5)     P4 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P4)     P3 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P3)     P2 = layers.Conv2D(256, kernel_size=3, padding=\"same\")(P2)     return P2, P3, P4, P5   def pad_with_coord(data):     data_shape = tf.shape(data)     batch_size, height, width = data_shape[0], data_shape[1], data_shape[2]     x = tf.cast(tf.linspace(-1, 1, num=width), data.dtype)     x = tf.tile(x[tf.newaxis, tf.newaxis, ..., tf.newaxis], [batch_size, height, 1, 1])     y = tf.cast(tf.linspace(-1, 1, num=height), data.dtype)     y = tf.tile(y[tf.newaxis, ..., tf.newaxis, tf.newaxis], [batch_size, 1, width, 1])     data = tf.concat([data, x, y], axis=-1)     return data   def conv_norm(x, filters, kernel_size=3, groups=32):     x = layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', use_bias=False)(x)     x = tfa.layers.GroupNormalization(groups=groups, epsilon=1e-5)(x)     return x   def solov2_head_model(stacked_convs=4, ch_in=258, ch_feature=512, ch_kernel_out=256, num_classes=80):     inputs = layers.Input(shape=(None, None, ch_in))     feature_kernel = inputs     feature_cls = inputs[..., :-2]     for _ in range(stacked_convs):         feature_kernel = tf.nn.relu(conv_norm(feature_kernel, filters=ch_feature))         feature_cls = tf.nn.relu(conv_norm(feature_cls, filters=ch_feature))     feature_kernel = layers.Conv2D(filters=ch_kernel_out,                                    kernel_size=3,                                    padding='same',                                    kernel_initializer=tf.random_normal_initializer(stddev=0.01))(feature_kernel)     feature_cls = layers.Conv2D(filters=num_classes,                                 kernel_size=3,                                 padding='same',                                 kernel_initializer=tf.random_normal_initializer(stddev=0.01),                                 bias_initializer=tf.initializers.constant(np.log(1 / 99)))(feature_cls)     return tf.keras.Model(inputs=inputs, outputs=[feature_kernel, feature_cls])   def solov2_head(P2, P3, P4, P5, num_classes=80):     head_model = solov2_head_model(num_classes=num_classes)     # applying maxpool first for P2     P2 = layers.MaxPool2D()(P2)     features = [P2, P3, P4, P5, P5]     grid_sizes = [40, 36, 24, 16, 12]     feat_kernel_list, feat_cls_list = [], []     for feature, grid_size in zip(features, grid_sizes):         feature = pad_with_coord(feature)         feature = tf.image.resize(feature, size=(grid_size, grid_size))         feat_kernel, feat_cls = head_model(feature)         feat_kernel_list.append(feat_kernel)         feat_cls_list.append(tf.sigmoid(feat_cls))     return feat_cls_list, feat_kernel_list   def solov2_maskhead(P2, P3, P4, P5, mid_ch=128, out_ch=256):     # first level     P2 = tf.nn.relu(conv_norm(P2, filters=mid_ch))     # second level     P3 = tf.nn.relu(conv_norm(P3, filters=mid_ch))     P3 = layers.UpSampling2D()(P3)     # third level     P4 = tf.nn.relu(conv_norm(P4, filters=mid_ch))     P4 = layers.UpSampling2D()(P4)     P4 = tf.nn.relu(conv_norm(P4, filters=mid_ch))     P4 = layers.UpSampling2D()(P4)     # top level, add coordinate     P5 = tf.nn.relu(conv_norm(pad_with_coord(P5), filters=mid_ch))     P5 = layers.UpSampling2D()(P5)     P5 = tf.nn.relu(conv_norm(P5, filters=mid_ch))     P5 = layers.UpSampling2D()(P5)     P5 = tf.nn.relu(conv_norm(P5, filters=mid_ch))     P5 = layers.UpSampling2D()(P5)     seg_outputs = tf.nn.relu(conv_norm(P2 + P3 + P4 + P5, filters=out_ch, kernel_size=1))     return seg_outputs   def solov2(input_shape=(None, None, 3), num_classes=80):     inputs = tf.keras.Input(shape=input_shape)     resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)     assert resnet50.layers[38].name == \"conv2_block3_out\"     C2 = resnet50.layers[38].output     assert resnet50.layers[80].name == \"conv3_block4_out\"     C3 = resnet50.layers[80].output     assert resnet50.layers[142].name == \"conv4_block6_out\"     C4 = resnet50.layers[142].output     assert resnet50.layers[-1].name == \"conv5_block3_out\"     C5 = resnet50.layers[-1].output     P2, P3, P4, P5 = fpn(C2, C3, C4, C5)     feat_seg = solov2_maskhead(P2, P3, P4, P5)  # [B, h/4, w/4, 256]     feat_cls_list, feat_kernel_list = solov2_head(P2, P3, P4, P5, num_classes=num_classes)  # [B, grid, grid, 80], [B, grid, grid, 256]     model = tf.keras.Model(inputs=inputs, outputs=[feat_seg, feat_cls_list, feat_kernel_list])     return model  init_lr = 1e-2 / 16 * batch_size model = fe.build(model_fn=lambda: solov2(input_shape=(im_size, im_size, 3)),                  optimizer_fn=lambda: tf.optimizers.SGD(learning_rate=init_lr, momentum=0.9)) In\u00a0[9]: Copied! <pre>class Solov2Loss(TensorOp):\n    def __init__(self, level, grid_dim, inputs, outputs, mode=None, num_class=80):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.level = level\n        self.grid_dim = grid_dim\n        self.num_class = num_class\n\n    def forward(self, data, state):\n        masks, classes, gt_match, feat_segs, feat_clss, kernels = data\n        cls_loss, grid_object_maps = tf.map_fn(fn=lambda x: self.get_cls_loss(x[0], x[1], x[2]),\n                             elems=(classes, feat_clss, gt_match),\n                             fn_output_signature=(tf.float32, tf.float32))\n        seg_loss = tf.map_fn(fn=lambda x: self.get_seg_loss(x[0], x[1], x[2], x[3]),\n                             elems=(masks, feat_segs, kernels, grid_object_maps),\n                             fn_output_signature=tf.float32)\n        return cls_loss, seg_loss\n\n    def get_seg_loss(self, mask, feat_seg, kernel, grid_object_map):\n        indices = tf.where(grid_object_map[..., 0] &gt; 0)\n        object_indices = tf.cast(tf.gather_nd(grid_object_map, indices)[:, 1], tf.int32)\n        mask_gt = tf.cast(tf.gather(mask, object_indices), kernel.dtype)\n        active_kernel = tf.gather_nd(kernel, indices)\n        feat_seg = tf.reshape(tf.transpose(feat_seg, perm=[2, 0, 1]),\n                              (tf.shape(kernel)[-1], -1))  # H/4,W/4,C-&gt;C,H/4,W/4\n        seg_preds = tf.reshape(tf.matmul(active_kernel, feat_seg), tf.shape(mask_gt))\n        loss = self.dice_loss(seg_preds, mask_gt)\n        return loss\n\n    def dice_loss(self, pred, gt):\n        pred = tf.sigmoid(pred)\n        a = tf.reduce_sum(pred * gt)\n        b = tf.reduce_sum(pred * pred) + 0.001\n        c = tf.reduce_sum(gt * gt) + 0.001\n        dice = (2 * a) / (b + c)\n        return 1 - tf.where(dice &gt; 0, dice, 1)\n\n    def get_cls_loss(self, cls_gt, feat_cls, match):\n        cls_gt = tf.cast(cls_gt, feat_cls.dtype)\n        match, cls_gt = match[cls_gt &gt; 0], cls_gt[cls_gt &gt; 0]  # remove the padded object\n        feat_cls_gts_raw = tf.map_fn(fn=lambda x: self.assign_cls_feat(x[0], x[1]),\n                                     elems=(match, cls_gt),\n                                     fn_output_signature=tf.float32)\n        grid_object_map = self.reduce_to_single_grid(feat_cls_gts_raw)\n        feat_cls_gts = tf.one_hot(tf.cast(grid_object_map[..., 0], tf.int32), depth=self.num_class + 1)[..., 1:]\n        cls_loss = self.focal_loss(feat_cls, feat_cls_gts)\n        return cls_loss, grid_object_map\n\n    def reduce_to_single_grid(self, feat_cls_gts_raw):\n        feat_cls_gts = tf.zeros((self.grid_dim, self.grid_dim), dtype=feat_cls_gts_raw.dtype)\n        object_idx = tf.zeros((self.grid_dim, self.grid_dim), dtype=feat_cls_gts_raw.dtype)\n        num_obj = tf.shape(feat_cls_gts_raw)[0]\n        for idx in range(num_obj):\n            classes = feat_cls_gts_raw[idx]\n            indexes = tf.cast(tf.where(classes &gt; 0, idx, 0), classes.dtype)\n            object_idx = object_idx + tf.where(feat_cls_gts == 0, indexes, tf.zeros_like(indexes))\n            feat_cls_gts = feat_cls_gts + tf.where(feat_cls_gts == 0, classes, tf.zeros_like(classes))\n        grid_object_map = tf.stack([feat_cls_gts, object_idx], axis=-1)\n        return grid_object_map\n\n    def focal_loss(self, pred, gt, alpha=0.25, gamma=2.0):\n        pred, gt = tf.reshape(pred, (-1, 1)), tf.reshape(gt, (-1, 1))\n        anchor_obj_count = tf.cast(tf.math.count_nonzero(gt), pred.dtype)\n        alpha_factor = tf.ones_like(gt) * alpha\n        alpha_factor = tf.where(tf.equal(gt, 1), alpha_factor, 1 - alpha_factor)\n        focal_weight = tf.where(tf.equal(gt, 1), 1 - pred, pred)\n        focal_weight = alpha_factor * focal_weight**gamma / (anchor_obj_count + 1)\n        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(gt, pred, sample_weight=focal_weight)\n        return cls_loss\n\n    def assign_cls_feat(self, grid_match_info, cls_gt_obj):\n        match_bool = tf.logical_and(tf.reduce_sum(grid_match_info, axis=-1) &gt; 0, grid_match_info[:, 0] == self.level)\n        grid_match_info = grid_match_info[match_bool]\n        grid_indices = grid_match_info[:, 1:3]\n        num_indices = tf.shape(grid_indices)[0]\n        feat_cls_gt = tf.scatter_nd(grid_indices, tf.fill([num_indices], cls_gt_obj), (self.grid_dim, self.grid_dim))\n        return feat_cls_gt\n\n\nclass CombineLoss(TensorOp):\n    def forward(self, data, state):\n        l_c1, l_s1, l_c2, l_s2, l_c3, l_s3, l_c4, l_s4, l_c5, l_s5 = data\n        cls_losses = tf.reduce_sum(tf.stack([l_c1, l_c2, l_c3, l_c4, l_c5], axis=-1), axis=-1)\n        seg_losses = tf.reduce_sum(tf.stack([l_s1, l_s2, l_s3, l_s4, l_s5], axis=-1), axis=-1)\n        mean_cls_loss, mean_seg_loss = tf.reduce_mean(cls_losses), tf.reduce_mean(seg_losses) * 3\n        return mean_cls_loss + mean_seg_loss, mean_cls_loss, mean_seg_loss\n\n\nclass PointsNMS(TensorOp):\n    def forward(self, data, state):\n        feat_cls_list = [self.points_nms(x) for x in data]\n        return feat_cls_list\n\n    def points_nms(self, x):\n        x_max_pool = tf.nn.max_pool2d(x, ksize=2, strides=1, padding=[[0, 0], [1, 1], [1, 1], [0, 0]])[:, :-1, :-1, :]\n        x = tf.where(tf.equal(x, x_max_pool), x, 0)\n        return x\n\n\nclass Predict(TensorOp):\n    def __init__(self, inputs, outputs, mode=None, score_threshold=0.1, segm_strides=[8.0, 8.0, 16.0, 32.0, 32.0]):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.score_threshold = score_threshold\n        self.segm_strides = segm_strides\n\n    def forward(self, data, state):\n        feat_seg, feat_cls_list, feat_kernel_list = data\n        strides = [tf.fill((tf.shape(x)[1] * tf.shape(x)[2], ), s) for s, x in zip(self.segm_strides, feat_cls_list)]\n        batch_size, num_class = tf.shape(feat_cls_list[0])[0], tf.shape(feat_cls_list[0])[3]\n        kernel_dim = tf.shape(feat_kernel_list[0])[-1]\n        feat_cls = tf.concat([tf.reshape(x, (batch_size, -1, num_class)) for x in feat_cls_list], axis=1)\n        feat_kernel = tf.concat([tf.reshape(x, (batch_size, -1, kernel_dim)) for x in feat_kernel_list], axis=1)\n        strides = tf.concat(strides, axis=0)\n        seg_preds, cate_scores, cate_labels  = tf.map_fn(fn=lambda x: self.predict_sample(x[0], x[1], x[2], strides),\n                             elems=(feat_cls, feat_seg, feat_kernel),\n                             fn_output_signature=(tf.float32, tf.float32, tf.int32))\n        return seg_preds, cate_scores, cate_labels\n\n    def predict_sample(self, cate_preds, seg_preds, kernel_preds, strides):\n        # first filter class prediction by score_threshold\n        select_indices = tf.where(cate_preds &gt; self.score_threshold)\n        cate_labels = tf.cast(select_indices[:, 1], tf.int32)\n        kernel_preds = tf.gather(kernel_preds, select_indices[:, 0])\n        cate_scores = tf.gather_nd(cate_preds, select_indices)\n        strides = tf.gather(strides, select_indices[:, 0])\n        # next calculate the mask\n        kernel_preds = tf.transpose(kernel_preds)[tf.newaxis, tf.newaxis, ...]  # [k_h, k_w, c_in, c_out]\n        seg_preds = tf.sigmoid(tf.nn.conv2d(seg_preds[tf.newaxis, ...], kernel_preds, strides=1, padding=\"VALID\"))[0]\n        seg_preds = tf.transpose(seg_preds, perm=[2, 0, 1])  # [C, H, W]\n        seg_masks = tf.where(seg_preds &gt; 0.5, 1.0, 0.0)\n        # then filter masks based on strides\n        mask_sum = tf.reduce_sum(seg_masks, axis=[1, 2])\n        select_indices = tf.where(mask_sum &gt; strides)[:, 0]\n        seg_preds, seg_masks = tf.gather(seg_preds, select_indices), tf.gather(seg_masks, select_indices)\n        mask_sum = tf.gather(mask_sum, select_indices)\n        cate_labels, cate_scores = tf.gather(cate_labels, select_indices), tf.gather(cate_scores, select_indices)\n        # scale the category score by mask confidence then matrix nms\n        mask_scores = tf.reduce_sum(seg_preds * seg_masks, axis=[1, 2]) / mask_sum\n        cate_scores = cate_scores * mask_scores\n        seg_preds, cate_scores, cate_labels = self.matrix_nms(seg_preds, seg_masks, cate_labels, cate_scores, mask_sum)\n        return seg_preds, cate_scores, cate_labels\n\n    def matrix_nms(self, seg_preds, seg_masks, cate_labels, cate_scores, mask_sum, pre_nms_k=500, post_nms_k=100):\n        # first select top k category scores\n        num_selected = tf.minimum(pre_nms_k, tf.shape(cate_scores)[0])\n        indices = tf.argsort(cate_scores, direction='DESCENDING')[:num_selected]\n        seg_preds, seg_masks = tf.gather(seg_preds, indices), tf.gather(seg_masks, indices)\n        cate_labels, cate_scores = tf.gather(cate_labels, indices), tf.gather(cate_scores, indices)\n        mask_sum = tf.gather(mask_sum, indices)\n        # calculate iou between different masks\n        seg_masks = tf.reshape(seg_masks, shape=(num_selected, -1))\n        intersection = tf.matmul(seg_masks, seg_masks, transpose_b=True)\n        mask_sum = tf.tile(mask_sum[tf.newaxis, ...], multiples=[num_selected, 1])\n        union = mask_sum + tf.transpose(mask_sum) - intersection\n        iou = intersection / union\n        iou = tf.linalg.band_part(iou, 0, -1) - tf.linalg.band_part(iou, 0, 0)  # equivalent of np.triu(diagonal=1)\n        # iou decay and compensation\n        labels_match = tf.tile(cate_labels[tf.newaxis, ...], multiples=[num_selected, 1])\n        labels_match = tf.where(labels_match == tf.transpose(labels_match), 1.0, 0.0)\n        labels_match = tf.linalg.band_part(labels_match, 0, -1) - tf.linalg.band_part(labels_match, 0, 0)\n        decay_iou = iou * labels_match  # iou with any object from same class\n        compensate_iou = tf.reduce_max(decay_iou, axis=0)\n        compensate_iou = tf.tile(compensate_iou[..., tf.newaxis], multiples=[1, num_selected])\n        # matrix nms\n        decay_coefficient = tf.reduce_min(tf.exp(-2 * decay_iou**2) / tf.exp(-2 * compensate_iou**2), axis=0)\n        cate_scores = cate_scores * decay_coefficient\n        cate_scores = tf.where(cate_scores &gt;= 0.05, cate_scores, 0)\n        num_selected = tf.minimum(post_nms_k, tf.shape(cate_scores)[0])\n        # select the final predictions and pad output for batch shape consistency\n        indices = tf.argsort(cate_scores, direction='DESCENDING')[:num_selected]\n        seg_preds = tf.pad(tf.gather(seg_preds, indices), paddings=[[0, post_nms_k - num_selected], [0, 0], [0, 0]])\n        cate_scores = tf.pad(tf.gather(cate_scores, indices), paddings=[[0, post_nms_k - num_selected]])\n        cate_labels = tf.pad(tf.gather(cate_labels, indices), paddings=[[0, post_nms_k - num_selected]])\n        return seg_preds, cate_scores, cate_labels\n\nnetwork = fe.Network(ops=[\n    Normalize(inputs=\"image\", outputs=\"image\", mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ModelOp(model=model, inputs=\"image\", outputs=(\"feat_seg\", \"feat_cls_list\", \"feat_kernel_list\")),\n    LambdaOp(fn=lambda x: x, inputs=\"feat_cls_list\", outputs=(\"cls1\", \"cls2\", \"cls3\", \"cls4\", \"cls5\")),\n    LambdaOp(fn=lambda x: x, inputs=\"feat_kernel_list\", outputs=(\"k1\", \"k2\", \"k3\", \"k4\", \"k5\")),\n    Solov2Loss(0, 40, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls1\", \"k1\"), outputs=(\"l_c1\", \"l_s1\")),\n    Solov2Loss(1, 36, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls2\", \"k2\"), outputs=(\"l_c2\", \"l_s2\")),\n    Solov2Loss(2, 24, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls3\", \"k3\"), outputs=(\"l_c3\", \"l_s3\")),\n    Solov2Loss(3, 16, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls4\", \"k4\"), outputs=(\"l_c4\", \"l_s4\")),\n    Solov2Loss(4, 12, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls5\", \"k5\"), outputs=(\"l_c5\", \"l_s5\")),\n    CombineLoss(inputs=(\"l_c1\", \"l_s1\", \"l_c2\", \"l_s2\", \"l_c3\", \"l_s3\", \"l_c4\", \"l_s4\", \"l_c5\", \"l_s5\"),\n                outputs=(\"total_loss\", \"cls_loss\", \"seg_loss\")),\n    L2Regularizaton(inputs=\"total_loss\", outputs=\"total_loss_l2\", model=model, beta=1e-5, mode=\"train\"),\n    UpdateOp(model=model, loss_name=\"total_loss_l2\"),\n    PointsNMS(inputs=\"feat_cls_list\", outputs=\"feat_cls_list\", mode=\"test\"),\n    Predict(inputs=(\"feat_seg\", \"feat_cls_list\", \"feat_kernel_list\"),\n            outputs=(\"seg_preds\", \"cate_scores\", \"cate_labels\"),\n            mode=\"test\")\n])\n</pre> class Solov2Loss(TensorOp):     def __init__(self, level, grid_dim, inputs, outputs, mode=None, num_class=80):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.level = level         self.grid_dim = grid_dim         self.num_class = num_class      def forward(self, data, state):         masks, classes, gt_match, feat_segs, feat_clss, kernels = data         cls_loss, grid_object_maps = tf.map_fn(fn=lambda x: self.get_cls_loss(x[0], x[1], x[2]),                              elems=(classes, feat_clss, gt_match),                              fn_output_signature=(tf.float32, tf.float32))         seg_loss = tf.map_fn(fn=lambda x: self.get_seg_loss(x[0], x[1], x[2], x[3]),                              elems=(masks, feat_segs, kernels, grid_object_maps),                              fn_output_signature=tf.float32)         return cls_loss, seg_loss      def get_seg_loss(self, mask, feat_seg, kernel, grid_object_map):         indices = tf.where(grid_object_map[..., 0] &gt; 0)         object_indices = tf.cast(tf.gather_nd(grid_object_map, indices)[:, 1], tf.int32)         mask_gt = tf.cast(tf.gather(mask, object_indices), kernel.dtype)         active_kernel = tf.gather_nd(kernel, indices)         feat_seg = tf.reshape(tf.transpose(feat_seg, perm=[2, 0, 1]),                               (tf.shape(kernel)[-1], -1))  # H/4,W/4,C-&gt;C,H/4,W/4         seg_preds = tf.reshape(tf.matmul(active_kernel, feat_seg), tf.shape(mask_gt))         loss = self.dice_loss(seg_preds, mask_gt)         return loss      def dice_loss(self, pred, gt):         pred = tf.sigmoid(pred)         a = tf.reduce_sum(pred * gt)         b = tf.reduce_sum(pred * pred) + 0.001         c = tf.reduce_sum(gt * gt) + 0.001         dice = (2 * a) / (b + c)         return 1 - tf.where(dice &gt; 0, dice, 1)      def get_cls_loss(self, cls_gt, feat_cls, match):         cls_gt = tf.cast(cls_gt, feat_cls.dtype)         match, cls_gt = match[cls_gt &gt; 0], cls_gt[cls_gt &gt; 0]  # remove the padded object         feat_cls_gts_raw = tf.map_fn(fn=lambda x: self.assign_cls_feat(x[0], x[1]),                                      elems=(match, cls_gt),                                      fn_output_signature=tf.float32)         grid_object_map = self.reduce_to_single_grid(feat_cls_gts_raw)         feat_cls_gts = tf.one_hot(tf.cast(grid_object_map[..., 0], tf.int32), depth=self.num_class + 1)[..., 1:]         cls_loss = self.focal_loss(feat_cls, feat_cls_gts)         return cls_loss, grid_object_map      def reduce_to_single_grid(self, feat_cls_gts_raw):         feat_cls_gts = tf.zeros((self.grid_dim, self.grid_dim), dtype=feat_cls_gts_raw.dtype)         object_idx = tf.zeros((self.grid_dim, self.grid_dim), dtype=feat_cls_gts_raw.dtype)         num_obj = tf.shape(feat_cls_gts_raw)[0]         for idx in range(num_obj):             classes = feat_cls_gts_raw[idx]             indexes = tf.cast(tf.where(classes &gt; 0, idx, 0), classes.dtype)             object_idx = object_idx + tf.where(feat_cls_gts == 0, indexes, tf.zeros_like(indexes))             feat_cls_gts = feat_cls_gts + tf.where(feat_cls_gts == 0, classes, tf.zeros_like(classes))         grid_object_map = tf.stack([feat_cls_gts, object_idx], axis=-1)         return grid_object_map      def focal_loss(self, pred, gt, alpha=0.25, gamma=2.0):         pred, gt = tf.reshape(pred, (-1, 1)), tf.reshape(gt, (-1, 1))         anchor_obj_count = tf.cast(tf.math.count_nonzero(gt), pred.dtype)         alpha_factor = tf.ones_like(gt) * alpha         alpha_factor = tf.where(tf.equal(gt, 1), alpha_factor, 1 - alpha_factor)         focal_weight = tf.where(tf.equal(gt, 1), 1 - pred, pred)         focal_weight = alpha_factor * focal_weight**gamma / (anchor_obj_count + 1)         cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(gt, pred, sample_weight=focal_weight)         return cls_loss      def assign_cls_feat(self, grid_match_info, cls_gt_obj):         match_bool = tf.logical_and(tf.reduce_sum(grid_match_info, axis=-1) &gt; 0, grid_match_info[:, 0] == self.level)         grid_match_info = grid_match_info[match_bool]         grid_indices = grid_match_info[:, 1:3]         num_indices = tf.shape(grid_indices)[0]         feat_cls_gt = tf.scatter_nd(grid_indices, tf.fill([num_indices], cls_gt_obj), (self.grid_dim, self.grid_dim))         return feat_cls_gt   class CombineLoss(TensorOp):     def forward(self, data, state):         l_c1, l_s1, l_c2, l_s2, l_c3, l_s3, l_c4, l_s4, l_c5, l_s5 = data         cls_losses = tf.reduce_sum(tf.stack([l_c1, l_c2, l_c3, l_c4, l_c5], axis=-1), axis=-1)         seg_losses = tf.reduce_sum(tf.stack([l_s1, l_s2, l_s3, l_s4, l_s5], axis=-1), axis=-1)         mean_cls_loss, mean_seg_loss = tf.reduce_mean(cls_losses), tf.reduce_mean(seg_losses) * 3         return mean_cls_loss + mean_seg_loss, mean_cls_loss, mean_seg_loss   class PointsNMS(TensorOp):     def forward(self, data, state):         feat_cls_list = [self.points_nms(x) for x in data]         return feat_cls_list      def points_nms(self, x):         x_max_pool = tf.nn.max_pool2d(x, ksize=2, strides=1, padding=[[0, 0], [1, 1], [1, 1], [0, 0]])[:, :-1, :-1, :]         x = tf.where(tf.equal(x, x_max_pool), x, 0)         return x   class Predict(TensorOp):     def __init__(self, inputs, outputs, mode=None, score_threshold=0.1, segm_strides=[8.0, 8.0, 16.0, 32.0, 32.0]):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.score_threshold = score_threshold         self.segm_strides = segm_strides      def forward(self, data, state):         feat_seg, feat_cls_list, feat_kernel_list = data         strides = [tf.fill((tf.shape(x)[1] * tf.shape(x)[2], ), s) for s, x in zip(self.segm_strides, feat_cls_list)]         batch_size, num_class = tf.shape(feat_cls_list[0])[0], tf.shape(feat_cls_list[0])[3]         kernel_dim = tf.shape(feat_kernel_list[0])[-1]         feat_cls = tf.concat([tf.reshape(x, (batch_size, -1, num_class)) for x in feat_cls_list], axis=1)         feat_kernel = tf.concat([tf.reshape(x, (batch_size, -1, kernel_dim)) for x in feat_kernel_list], axis=1)         strides = tf.concat(strides, axis=0)         seg_preds, cate_scores, cate_labels  = tf.map_fn(fn=lambda x: self.predict_sample(x[0], x[1], x[2], strides),                              elems=(feat_cls, feat_seg, feat_kernel),                              fn_output_signature=(tf.float32, tf.float32, tf.int32))         return seg_preds, cate_scores, cate_labels      def predict_sample(self, cate_preds, seg_preds, kernel_preds, strides):         # first filter class prediction by score_threshold         select_indices = tf.where(cate_preds &gt; self.score_threshold)         cate_labels = tf.cast(select_indices[:, 1], tf.int32)         kernel_preds = tf.gather(kernel_preds, select_indices[:, 0])         cate_scores = tf.gather_nd(cate_preds, select_indices)         strides = tf.gather(strides, select_indices[:, 0])         # next calculate the mask         kernel_preds = tf.transpose(kernel_preds)[tf.newaxis, tf.newaxis, ...]  # [k_h, k_w, c_in, c_out]         seg_preds = tf.sigmoid(tf.nn.conv2d(seg_preds[tf.newaxis, ...], kernel_preds, strides=1, padding=\"VALID\"))[0]         seg_preds = tf.transpose(seg_preds, perm=[2, 0, 1])  # [C, H, W]         seg_masks = tf.where(seg_preds &gt; 0.5, 1.0, 0.0)         # then filter masks based on strides         mask_sum = tf.reduce_sum(seg_masks, axis=[1, 2])         select_indices = tf.where(mask_sum &gt; strides)[:, 0]         seg_preds, seg_masks = tf.gather(seg_preds, select_indices), tf.gather(seg_masks, select_indices)         mask_sum = tf.gather(mask_sum, select_indices)         cate_labels, cate_scores = tf.gather(cate_labels, select_indices), tf.gather(cate_scores, select_indices)         # scale the category score by mask confidence then matrix nms         mask_scores = tf.reduce_sum(seg_preds * seg_masks, axis=[1, 2]) / mask_sum         cate_scores = cate_scores * mask_scores         seg_preds, cate_scores, cate_labels = self.matrix_nms(seg_preds, seg_masks, cate_labels, cate_scores, mask_sum)         return seg_preds, cate_scores, cate_labels      def matrix_nms(self, seg_preds, seg_masks, cate_labels, cate_scores, mask_sum, pre_nms_k=500, post_nms_k=100):         # first select top k category scores         num_selected = tf.minimum(pre_nms_k, tf.shape(cate_scores)[0])         indices = tf.argsort(cate_scores, direction='DESCENDING')[:num_selected]         seg_preds, seg_masks = tf.gather(seg_preds, indices), tf.gather(seg_masks, indices)         cate_labels, cate_scores = tf.gather(cate_labels, indices), tf.gather(cate_scores, indices)         mask_sum = tf.gather(mask_sum, indices)         # calculate iou between different masks         seg_masks = tf.reshape(seg_masks, shape=(num_selected, -1))         intersection = tf.matmul(seg_masks, seg_masks, transpose_b=True)         mask_sum = tf.tile(mask_sum[tf.newaxis, ...], multiples=[num_selected, 1])         union = mask_sum + tf.transpose(mask_sum) - intersection         iou = intersection / union         iou = tf.linalg.band_part(iou, 0, -1) - tf.linalg.band_part(iou, 0, 0)  # equivalent of np.triu(diagonal=1)         # iou decay and compensation         labels_match = tf.tile(cate_labels[tf.newaxis, ...], multiples=[num_selected, 1])         labels_match = tf.where(labels_match == tf.transpose(labels_match), 1.0, 0.0)         labels_match = tf.linalg.band_part(labels_match, 0, -1) - tf.linalg.band_part(labels_match, 0, 0)         decay_iou = iou * labels_match  # iou with any object from same class         compensate_iou = tf.reduce_max(decay_iou, axis=0)         compensate_iou = tf.tile(compensate_iou[..., tf.newaxis], multiples=[1, num_selected])         # matrix nms         decay_coefficient = tf.reduce_min(tf.exp(-2 * decay_iou**2) / tf.exp(-2 * compensate_iou**2), axis=0)         cate_scores = cate_scores * decay_coefficient         cate_scores = tf.where(cate_scores &gt;= 0.05, cate_scores, 0)         num_selected = tf.minimum(post_nms_k, tf.shape(cate_scores)[0])         # select the final predictions and pad output for batch shape consistency         indices = tf.argsort(cate_scores, direction='DESCENDING')[:num_selected]         seg_preds = tf.pad(tf.gather(seg_preds, indices), paddings=[[0, post_nms_k - num_selected], [0, 0], [0, 0]])         cate_scores = tf.pad(tf.gather(cate_scores, indices), paddings=[[0, post_nms_k - num_selected]])         cate_labels = tf.pad(tf.gather(cate_labels, indices), paddings=[[0, post_nms_k - num_selected]])         return seg_preds, cate_scores, cate_labels  network = fe.Network(ops=[     Normalize(inputs=\"image\", outputs=\"image\", mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),     ModelOp(model=model, inputs=\"image\", outputs=(\"feat_seg\", \"feat_cls_list\", \"feat_kernel_list\")),     LambdaOp(fn=lambda x: x, inputs=\"feat_cls_list\", outputs=(\"cls1\", \"cls2\", \"cls3\", \"cls4\", \"cls5\")),     LambdaOp(fn=lambda x: x, inputs=\"feat_kernel_list\", outputs=(\"k1\", \"k2\", \"k3\", \"k4\", \"k5\")),     Solov2Loss(0, 40, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls1\", \"k1\"), outputs=(\"l_c1\", \"l_s1\")),     Solov2Loss(1, 36, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls2\", \"k2\"), outputs=(\"l_c2\", \"l_s2\")),     Solov2Loss(2, 24, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls3\", \"k3\"), outputs=(\"l_c3\", \"l_s3\")),     Solov2Loss(3, 16, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls4\", \"k4\"), outputs=(\"l_c4\", \"l_s4\")),     Solov2Loss(4, 12, inputs=(\"mask\", \"classes\", \"gt_match\", \"feat_seg\", \"cls5\", \"k5\"), outputs=(\"l_c5\", \"l_s5\")),     CombineLoss(inputs=(\"l_c1\", \"l_s1\", \"l_c2\", \"l_s2\", \"l_c3\", \"l_s3\", \"l_c4\", \"l_s4\", \"l_c5\", \"l_s5\"),                 outputs=(\"total_loss\", \"cls_loss\", \"seg_loss\")),     L2Regularizaton(inputs=\"total_loss\", outputs=\"total_loss_l2\", model=model, beta=1e-5, mode=\"train\"),     UpdateOp(model=model, loss_name=\"total_loss_l2\"),     PointsNMS(inputs=\"feat_cls_list\", outputs=\"feat_cls_list\", mode=\"test\"),     Predict(inputs=(\"feat_seg\", \"feat_cls_list\", \"feat_kernel_list\"),             outputs=(\"seg_preds\", \"cate_scores\", \"cate_labels\"),             mode=\"test\") ]) In\u00a0[10]: Copied! <pre>def lr_schedule_warmup(step, init_lr):\n    if step &lt; 1000:\n        lr = init_lr / 1000 * step\n    else:\n        lr = init_lr\n    return lr\n\n\nclass COCOMaskmAP(Trace):\n    def __init__(self, data_dir, inputs=None, outputs=\"mAP\", mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        with Suppressor():\n            self.coco_gt = COCO(os.path.join(data_dir.replace('val2017', 'annotations'), \"instances_val2017.json\"))\n        missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]\n        category = [x for x in range(1, 91) if not x in missing_category]\n        self.mapping = {k: v for k, v in zip(list(range(80)), category)}\n\n    def on_epoch_begin(self, data):\n        self.results = []\n\n    def on_batch_end(self, data):\n        seg_preds, = data['seg_preds'].numpy(),\n        cate_scores, cate_labels = data['cate_scores'].numpy(), data['cate_labels'].numpy()\n        image_ids, imsizes = data['image_id'].numpy(), data['imsize'].numpy()\n        for seg_pred, cate_score, cate_label, image_id, imsize in zip(seg_preds, cate_scores, cate_labels, image_ids, imsizes):\n            # remove the padded data due to batching\n            indices = cate_score &gt; 0.01\n            seg_pred, cate_score, cate_label = seg_pred[indices], cate_score[indices], cate_label[indices]\n            if seg_pred.shape[0] == 0:\n                continue\n            seg_pred = np.transpose(seg_pred, axes=(1, 2, 0))  # [H, W, #objects]\n            # remove the padded data due to image resize\n            mask_h, mask_w, num_obj = seg_pred.shape\n            image_h, image_w = 4 * mask_h, 4 * mask_w\n            seg_pred = cv2.resize(seg_pred, (image_w, image_h))\n            if num_obj == 1:\n                seg_pred = seg_pred[..., np.newaxis]  # when there's only single object, resize will remove the channel\n            ori_h, ori_w = imsize\n            scale_ratio = min(image_h / ori_h, image_w / ori_w)\n            pad_h, pad_w = image_h - scale_ratio * ori_h, image_w - scale_ratio * ori_w\n            h_start, h_end = round(pad_h / 2), image_h - round(pad_h / 2)\n            w_start, w_end = round(pad_w / 2), image_w - round(pad_w / 2)\n            seg_pred = seg_pred[h_start:h_end, w_start:w_end, :]\n            # now reshape to original shape\n            seg_pred = cv2.resize(seg_pred, (ori_w, ori_h))\n            if num_obj == 1:\n                seg_pred = seg_pred[..., np.newaxis]  # when there's only single object, resize will remove the channel\n            seg_pred = np.transpose(seg_pred, [2, 0, 1])  # [#objects, H, W]\n            seg_pred = np.uint8(np.where(seg_pred &gt; 0.5, 1, 0))\n            for seg, score, label in zip(seg_pred, cate_score, cate_label):\n                result = {\n                    \"image_id\": image_id,\n                    \"category_id\": self.mapping[label],\n                    \"score\": score,\n                    \"segmentation\": mask_util.encode(np.array(seg[..., np.newaxis], order='F'))[0]\n                }\n                self.results.append(result)\n        return data\n\n    def on_epoch_end(self, data):\n        mAP = 0.0\n        if self.results:\n            with Suppressor():\n                coco_results = self.coco_gt.loadRes(self.results)\n                cocoEval = COCOeval(self.coco_gt, coco_results, 'segm')\n                cocoEval.evaluate()\n                cocoEval.accumulate()\n                cocoEval.summarize()\n                mAP = cocoEval.stats[0]\n        data.write_with_log(self.outputs[0], mAP)\n\n\ntrain_steps_epoch = int(np.ceil(len(train_ds) / batch_size))\n\nlr_schedule = {\n    1:\n    LRScheduler(model=model, lr_fn=lambda step: lr_schedule_warmup(step, init_lr=init_lr)),\n    2:\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step,\n                                        cycle_length=train_steps_epoch * (epochs - 1),\n                                        init_lr=init_lr,\n                                        min_lr=init_lr / 100,\n                                        start=train_steps_epoch + 1))\n}\n\ntraces = [\n    EpochScheduler(lr_schedule),\n    COCOMaskmAP(data_dir=val_ds.root_dir,\n                inputs=(\"seg_preds\", \"cate_scores\", \"cate_labels\", \"image_id\", \"imsize\"),\n                mode=\"test\"),\n    BestModelSaver(model=model, save_dir=model_dir, metric=\"total_loss\")\n]\n</pre> def lr_schedule_warmup(step, init_lr):     if step &lt; 1000:         lr = init_lr / 1000 * step     else:         lr = init_lr     return lr   class COCOMaskmAP(Trace):     def __init__(self, data_dir, inputs=None, outputs=\"mAP\", mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         with Suppressor():             self.coco_gt = COCO(os.path.join(data_dir.replace('val2017', 'annotations'), \"instances_val2017.json\"))         missing_category = [66, 68, 69, 71, 12, 45, 83, 26, 29, 30]         category = [x for x in range(1, 91) if not x in missing_category]         self.mapping = {k: v for k, v in zip(list(range(80)), category)}      def on_epoch_begin(self, data):         self.results = []      def on_batch_end(self, data):         seg_preds, = data['seg_preds'].numpy(),         cate_scores, cate_labels = data['cate_scores'].numpy(), data['cate_labels'].numpy()         image_ids, imsizes = data['image_id'].numpy(), data['imsize'].numpy()         for seg_pred, cate_score, cate_label, image_id, imsize in zip(seg_preds, cate_scores, cate_labels, image_ids, imsizes):             # remove the padded data due to batching             indices = cate_score &gt; 0.01             seg_pred, cate_score, cate_label = seg_pred[indices], cate_score[indices], cate_label[indices]             if seg_pred.shape[0] == 0:                 continue             seg_pred = np.transpose(seg_pred, axes=(1, 2, 0))  # [H, W, #objects]             # remove the padded data due to image resize             mask_h, mask_w, num_obj = seg_pred.shape             image_h, image_w = 4 * mask_h, 4 * mask_w             seg_pred = cv2.resize(seg_pred, (image_w, image_h))             if num_obj == 1:                 seg_pred = seg_pred[..., np.newaxis]  # when there's only single object, resize will remove the channel             ori_h, ori_w = imsize             scale_ratio = min(image_h / ori_h, image_w / ori_w)             pad_h, pad_w = image_h - scale_ratio * ori_h, image_w - scale_ratio * ori_w             h_start, h_end = round(pad_h / 2), image_h - round(pad_h / 2)             w_start, w_end = round(pad_w / 2), image_w - round(pad_w / 2)             seg_pred = seg_pred[h_start:h_end, w_start:w_end, :]             # now reshape to original shape             seg_pred = cv2.resize(seg_pred, (ori_w, ori_h))             if num_obj == 1:                 seg_pred = seg_pred[..., np.newaxis]  # when there's only single object, resize will remove the channel             seg_pred = np.transpose(seg_pred, [2, 0, 1])  # [#objects, H, W]             seg_pred = np.uint8(np.where(seg_pred &gt; 0.5, 1, 0))             for seg, score, label in zip(seg_pred, cate_score, cate_label):                 result = {                     \"image_id\": image_id,                     \"category_id\": self.mapping[label],                     \"score\": score,                     \"segmentation\": mask_util.encode(np.array(seg[..., np.newaxis], order='F'))[0]                 }                 self.results.append(result)         return data      def on_epoch_end(self, data):         mAP = 0.0         if self.results:             with Suppressor():                 coco_results = self.coco_gt.loadRes(self.results)                 cocoEval = COCOeval(self.coco_gt, coco_results, 'segm')                 cocoEval.evaluate()                 cocoEval.accumulate()                 cocoEval.summarize()                 mAP = cocoEval.stats[0]         data.write_with_log(self.outputs[0], mAP)   train_steps_epoch = int(np.ceil(len(train_ds) / batch_size))  lr_schedule = {     1:     LRScheduler(model=model, lr_fn=lambda step: lr_schedule_warmup(step, init_lr=init_lr)),     2:     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step,                                         cycle_length=train_steps_epoch * (epochs - 1),                                         init_lr=init_lr,                                         min_lr=init_lr / 100,                                         start=train_steps_epoch + 1)) }  traces = [     EpochScheduler(lr_schedule),     COCOMaskmAP(data_dir=val_ds.root_dir,                 inputs=(\"seg_preds\", \"cate_scores\", \"cate_labels\", \"image_id\", \"imsize\"),                 mode=\"test\"),     BestModelSaver(model=model, save_dir=model_dir, metric=\"total_loss\") ] In\u00a0[\u00a0]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         monitor_names=(\"cls_loss\", \"seg_loss\", \"total_loss\"),\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\nestimator.fit()\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          monitor_names=(\"cls_loss\", \"seg_loss\", \"total_loss\"),                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) estimator.fit() In\u00a0[12]: Copied! <pre>test_data = pipeline.get_results(mode=\"test\")\nimages = test_data[\"image\"].numpy()\ntest_data = network.transform(test_data, mode=\"test\")\nseg_preds = test_data['seg_preds'].numpy()\ncate_scores = test_data['cate_scores'].numpy()\ncate_labels = test_data['cate_labels'].numpy()\n</pre> test_data = pipeline.get_results(mode=\"test\") images = test_data[\"image\"].numpy() test_data = network.transform(test_data, mode=\"test\") seg_preds = test_data['seg_preds'].numpy() cate_scores = test_data['cate_scores'].numpy() cate_labels = test_data['cate_labels'].numpy() In\u00a0[13]: Copied! <pre>import random\n\ndef auto_color():\n    rgbl=[255,0,0]\n    random.shuffle(rgbl)\n    return tuple(rgbl)\n\n\ndef visualize_test_results(idx, images, seg_preds, cate_scores, cate_labels, num_select=10):\n    category_names = [\n        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n        'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n        'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n        'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n        'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n        'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n        'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n        'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n        'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n        'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n        'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n        'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n    ]\n    category_id_to_name = {\n        key: name\n        for key, name in zip(range(80), category_names)\n    }\n    image, seg_preds = images[idx], seg_preds[idx][:num_select]\n    cate_scores, cate_labels = cate_scores[idx][:num_select], cate_labels[idx][:num_select]\n    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n    img_with_mask = image\n    for seg_pred, cate_score, cate_label in zip(seg_preds, cate_scores, cate_labels):\n        seg_pred = cv2.resize(seg_pred, (image.shape[1], image.shape[0]))\n        mask = np.where(seg_pred &gt; 0.5, 255, 0)\n        mask = np.stack([mask, mask, mask], axis=-1)\n        mask = np.uint8(np.where(mask != [0, 0, 0], auto_color(), img_with_mask))\n        img_with_mask = cv2.addWeighted(img_with_mask, 0.5, mask, 0.5, 0)\n        label_name = category_id_to_name[cate_label]\n        center_y, center_x = center_of_mass(seg_pred)\n        if not np.isnan(center_y) and not np.isnan(center_x):\n            ax[1].text(int(center_x), int(center_y), label_name, color='white', fontsize=11)\n    ax[0].imshow(image)\n    ax[0].axis('off')\n    ax[0].set_title('Original')\n    ax[1].imshow(img_with_mask)\n    ax[1].axis('off')\n    ax[1].set_title('Prediction')\n    plt.show()\n</pre> import random  def auto_color():     rgbl=[255,0,0]     random.shuffle(rgbl)     return tuple(rgbl)   def visualize_test_results(idx, images, seg_preds, cate_scores, cate_labels, num_select=10):     category_names = [         'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',         'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',         'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',         'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',         'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',         'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',         'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',         'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',         'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',         'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',         'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',         'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',         'scissors', 'teddy bear', 'hair drier', 'toothbrush'     ]     category_id_to_name = {         key: name         for key, name in zip(range(80), category_names)     }     image, seg_preds = images[idx], seg_preds[idx][:num_select]     cate_scores, cate_labels = cate_scores[idx][:num_select], cate_labels[idx][:num_select]     fig, ax = plt.subplots(1, 2, figsize=(20, 10))     img_with_mask = image     for seg_pred, cate_score, cate_label in zip(seg_preds, cate_scores, cate_labels):         seg_pred = cv2.resize(seg_pred, (image.shape[1], image.shape[0]))         mask = np.where(seg_pred &gt; 0.5, 255, 0)         mask = np.stack([mask, mask, mask], axis=-1)         mask = np.uint8(np.where(mask != [0, 0, 0], auto_color(), img_with_mask))         img_with_mask = cv2.addWeighted(img_with_mask, 0.5, mask, 0.5, 0)         label_name = category_id_to_name[cate_label]         center_y, center_x = center_of_mass(seg_pred)         if not np.isnan(center_y) and not np.isnan(center_x):             ax[1].text(int(center_x), int(center_y), label_name, color='white', fontsize=11)     ax[0].imshow(image)     ax[0].axis('off')     ax[0].set_title('Original')     ax[1].imshow(img_with_mask)     ax[1].axis('off')     ax[1].set_title('Prediction')     plt.show() In\u00a0[14]: Copied! <pre>visualize_test_results(0, images, seg_preds, cate_scores, cate_labels)\n</pre> visualize_test_results(0, images, seg_preds, cate_scores, cate_labels) In\u00a0[15]: Copied! <pre>visualize_test_results(5, images, seg_preds, cate_scores, cate_labels)\n</pre> visualize_test_results(5, images, seg_preds, cate_scores, cate_labels)"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#solov2", "title": "SOLOv2\u00b6", "text": "<p>Segmenting Object by Locations (SOLO) is a family of anchor-free instance segmentation work flows. Traditionally, instance segmentation heavily built on anchor-based prior works such as Faster-RCNN and Mask-RCNN.  SOLO, on the other hand, proposed a new angle to the instance segmentation problem.</p> <p>The core idea of SOLO is very similar to YOLO: dividing the image into several sub-regions so that each sub-region will handle the segmentation of object that falls into the region.  This idea has been pursued by researchers for many years, however, there are many challenges that come with it:</p> <ul> <li>How does it handle segmentation of objects that fall into same region?</li> <li>How to provide a sense of location to the network?</li> <li>Segmenting on each sub-region is extremely resource demanding, especially memory. For example, if we divide the image into 40x40 grids, and each grid creates a mask. Then the final prediction size is 1600 times of normal semantic segmentation.</li> </ul> <p>What makes SOLOv2 successful is how it managed to address these challenges with simple and intuitive solutions. In this example, we will walk you through the details of SOLOv2. Here is a technical summary of SOLOv2:</p> <ul> <li><p>New Use of Feature Pyramid: Feature Pyramid Network (FPN) is a network that can extract features from different stages of backbone. Normally people have been using FPN to assign objects of all sizes to all stages of network to improve performance. SOLO took different approach and leverages FPN to handle objects for different sizes. For example, smaller objects are assigned to early stages and larger objects will be assigned to later stages. This can effectively mitigate the <code>multiple objects same location</code> issue.</p> </li> <li><p>Location Encoding with Coordinate Padding: In the classification and mask prediction, the feature will be concatenated by relative X and Y coordinates on the channel dimension to provide sense of location to the network.</p> </li> <li><p>Dynamic Convolution for Mask: Normally, a convolution operation has a pre-defined sizes of input, kernel and output. In dynamic convolution, only a subset kernel of interest will be selected and used to generate final results. This can greatly reduce the resource requirement needed, and it is what makes producing a mask from each sub-grid feasible.</p> </li> <li><p>Matrix NMS: SOLO proposed a new matrix NMS methodology that is much more efficient and faster than other NMS approaches. This helps speed up the inferencing significantly.</p> </li> </ul> <p>Now let's get into the implementation. First let's import the necessary functions:</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#pipeline", "title": "Pipeline\u00b6", "text": "<p>In this example we will use MSCOCO data. The images and masks will be resized to ensure target longest side, then padded as square size.  Horizontal flipping is the only data augmentation.</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#visualize-the-ground-truth-matching", "title": "Visualize the Ground Truth Matching\u00b6", "text": "<p>One critical step before training the neural network is to visually inspect the data generated by the pipeline. In this case, we will visualize how different objects are assigned to different grids at different feature level.</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#solov2-model", "title": "SOLOv2 model\u00b6", "text": "<p>The SOLOv2 model include the following component: Backbone, Feature Pyramid Net (Neck), classification head (for classification and creating mask kernels) , mask head (creating feature used to compute mask).</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#training-and-evaluation-network-operations", "title": "Training and Evaluation Network Operations\u00b6", "text": "<p>The loss of SOLOv2 is binary cross entropy for classification and dice loss for segmentation. Loss is calculated independently for each feature pyramid level, and finally combined together for training.  During evaluation, mask is dynamically calculated based on confidence score, and only the kernels corresponding to high enough confidence score will be used to produce masks.</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#metrics-and-learning-rate-scheduling", "title": "Metrics and Learning Rate Scheduling\u00b6", "text": "<p>The evaluation metric of the task is mask mean average precision (mAP). The metric calculation takes a long time for the entire evaluation data, therefore it is only used during final testing phase after the training.  During the training, we use evaluation loss as metric.</p> <p>The learning rate schedule is a combination of LR warm up and one-cycle cosine decay. The LR warm up is only applied in epoch 1 for the first 1000 steps, the one-cycle cosine decay starts at epoch 2.</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#start-training", "title": "Start Training\u00b6", "text": "<p>12 epochs of training with 1024 image size will take around 11 hours on 4 A100 GPUs. If less GPU memory and computation budget are needed, you can reduce the <code>im_size</code> and <code>batch_per_gpu</code>.</p> <p>After 12 epochs of training, the mAP can reach ~0.32mAP, setting the <code>epochs</code> to 36 will match paper's result(~0.34 mAP).</p>"}, {"location": "apphub/instance_segmentation/solov2/solov2.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>After training the SOLOv2 model, let's visualize some sample predictions:</p>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html", "title": "Super-Convergence Learning Rate Schedule  (TensorFlow Backend)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import ResNet9\nfrom fastestimator.dataset.data.cifair10 import load_data\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import Suppressor\nimport matplotlib.pyplot as plt\n</pre> import tempfile  import fastestimator as fe from fastestimator.architecture.tensorflow import ResNet9 from fastestimator.dataset.data.cifair10 import load_data from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import Suppressor import matplotlib.pyplot as plt  In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs=24\nbatch_size=128\nlr_epochs=100\ntrain_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # Parameters epochs=24 batch_size=128 lr_epochs=100 train_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre># prepare dataset\ntrain_data, test_data = load_data()\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n        CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n        Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=10, label_smoothing=0.2)\n    ])\n\n# prepare network\nmodel = fe.build(model_fn=ResNet9, optimizer_fn=\"sgd\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> # prepare dataset train_data, test_data = load_data() pipeline = fe.Pipeline(     train_data=train_data,     eval_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),         Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=10, label_smoothing=0.2)     ])  # prepare network model = fe.build(model_fn=ResNet9, optimizer_fn=\"sgd\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) In\u00a0[4]: Copied! <pre>def linear_increase(step, min_lr=0.0, max_lr=6.0, num_steps=1000):\n    lr = step / num_steps * (max_lr - min_lr) + min_lr\n    return lr\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: linear_increase(step))\n]\n\n# prepare estimator\nLR_range_test = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=lr_epochs,\n                             traces=traces,\n                             train_steps_per_epoch=10, \n                             log_steps=10)\n\n# run the LR_range_test this \nprint(\"Running LR range testing... It will take a while\")\nwith Suppressor():\n    summary = LR_range_test.fit(\"LR_range_test\")\n</pre> def linear_increase(step, min_lr=0.0, max_lr=6.0, num_steps=1000):     lr = step / num_steps * (max_lr - min_lr) + min_lr     return lr  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: linear_increase(step)) ]  # prepare estimator LR_range_test = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=lr_epochs,                              traces=traces,                              train_steps_per_epoch=10,                               log_steps=10)  # run the LR_range_test this  print(\"Running LR range testing... It will take a while\") with Suppressor():     summary = LR_range_test.fit(\"LR_range_test\") <pre>Running LR range testing... It will take a while\n</pre> <p>Let's plot the accuracy vs LR graph and see the maximum LR.</p> In\u00a0[5]: Copied! <pre>acc_steps = [step for step in summary.history[\"eval\"][\"accuracy\"].keys()]    \nacc_values = [acc for acc in summary.history[\"eval\"][\"accuracy\"].values()]    \nbest_step, best_acc = max(summary.history[\"eval\"][\"accuracy\"].items(), key=lambda k: k[1])   \nlr_max = summary.history[\"train\"][\"model_lr\"][best_step]    \nlr_values = [summary.history[\"train\"][\"model_lr\"][x] for x in acc_steps]    \nassert len(lr_values) == len(acc_values)    \nplt.plot(lr_values, acc_values)    \nplt.plot(lr_max,\n         best_acc,             \n         'o',             \n         color='r',             \n         label=\"Best Acc={}, LR={}\".format(best_acc, lr_max))    \nplt.xlabel(\"Learning Rate\")    \nplt.ylabel(\"Evaluation Accuracy\")    \nplt.legend(loc='upper left', frameon=False)\n</pre> acc_steps = [step for step in summary.history[\"eval\"][\"accuracy\"].keys()]     acc_values = [acc for acc in summary.history[\"eval\"][\"accuracy\"].values()]     best_step, best_acc = max(summary.history[\"eval\"][\"accuracy\"].items(), key=lambda k: k[1])    lr_max = summary.history[\"train\"][\"model_lr\"][best_step]     lr_values = [summary.history[\"train\"][\"model_lr\"][x] for x in acc_steps]     assert len(lr_values) == len(acc_values)     plt.plot(lr_values, acc_values)     plt.plot(lr_max,          best_acc,                       'o',                       color='r',                       label=\"Best Acc={}, LR={}\".format(best_acc, lr_max))     plt.xlabel(\"Learning Rate\")     plt.ylabel(\"Evaluation Accuracy\")     plt.legend(loc='upper left', frameon=False)         Out[5]: <pre>&lt;matplotlib.legend.Legend at 0x7f08f4121da0&gt;</pre> In\u00a0[6]: Copied! <pre>lr_min = lr_max / 40\nmid = int(epochs * 0.45 * len(train_data) / batch_size)\nend = int(epochs * len(train_data) / batch_size)\n\ndef super_schedule(step):\n    if step &lt; mid:\n        lr = step / mid * (lr_max - lr_min) + lr_min  # linear increase from lr_min to lr_max\n\n    elif mid &lt;= step &lt; mid * 2:\n        lr = lr_max - (step - mid) / mid * (lr_max - lr_min)  # linear decrease from lr_max to lr_min\n\n    else:\n        lr = max(lr_min - (step - 2 * mid) / (end - 2 * mid) * lr_min, 0)  # linear decrease from lr_min to 0\n\n    return lr\n</pre> lr_min = lr_max / 40 mid = int(epochs * 0.45 * len(train_data) / batch_size) end = int(epochs * len(train_data) / batch_size)  def super_schedule(step):     if step &lt; mid:         lr = step / mid * (lr_max - lr_min) + lr_min  # linear increase from lr_min to lr_max      elif mid &lt;= step &lt; mid * 2:         lr = lr_max - (step - mid) / mid * (lr_max - lr_min)  # linear decrease from lr_max to lr_min      else:         lr = max(lr_min - (step - 2 * mid) / (end - 2 * mid) * lr_min, 0)  # linear decrease from lr_min to 0      return lr <p>Before we start the main training, the model needs to be reinitialized.  Therefore we re-instantiate the same network and plug the new LR scheduler in the estimator.</p> In\u00a0[7]: Copied! <pre># reinitialize the model\nmodel = fe.build(model_fn=ResNet9, optimizer_fn=\"sgd\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lambda step: super_schedule(step))\n]\n\n# prepare estimator \nmain_train = fe.Estimator(pipeline=pipeline,\n                          network=network,\n                          epochs=epochs,\n                          traces=traces,\n                          train_steps_per_epoch=train_steps_per_epoch)\n\nmain_train.fit()\n</pre> # reinitialize the model model = fe.build(model_fn=ResNet9, optimizer_fn=\"sgd\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lambda step: super_schedule(step)) ]  # prepare estimator  main_train = fe.Estimator(pipeline=pipeline,                           network=network,                           epochs=epochs,                           traces=traces,                           train_steps_per_epoch=train_steps_per_epoch)  main_train.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 1;\nFastEstimator-Train: step: 1; ce: 5.776663; model1_lr: 0.10142923;\nFastEstimator-Train: step: 100; ce: 4.7230773; model1_lr: 0.19342318; steps/sec: 33.37;\nFastEstimator-Train: step: 200; ce: 5.80881; model1_lr: 0.28634638; steps/sec: 33.49;\nFastEstimator-Train: step: 300; ce: 2.3463163; model1_lr: 0.37926957; steps/sec: 33.87;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 12.47 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 391; epoch: 1; accuracy: 0.4008; ce: 1.7539872; max_accuracy: 0.4008; since_best_accuracy: 0;\nFastEstimator-Train: step: 400; ce: 2.0435948; model1_lr: 0.47219273; steps/sec: 28.48;\nFastEstimator-Train: step: 500; ce: 1.8763064; model1_lr: 0.5651159; steps/sec: 33.02;\nFastEstimator-Train: step: 600; ce: 1.8691326; model1_lr: 0.6580391; steps/sec: 33.46;\nFastEstimator-Train: step: 700; ce: 1.7711973; model1_lr: 0.7509623; steps/sec: 33.59;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 11.77 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 782; epoch: 2; accuracy: 0.5117; ce: 1.3814019; max_accuracy: 0.5117; since_best_accuracy: 0;\nFastEstimator-Train: step: 800; ce: 1.7517064; model1_lr: 0.8438855; steps/sec: 32.87;\nFastEstimator-Train: step: 900; ce: 1.7174997; model1_lr: 0.93680865; steps/sec: 33.38;\nFastEstimator-Train: step: 1000; ce: 1.684868; model1_lr: 1.0297319; steps/sec: 33.42;\nFastEstimator-Train: step: 1100; ce: 1.7329108; model1_lr: 1.122655; steps/sec: 33.62;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 11.75 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 1173; epoch: 3; accuracy: 0.5604; ce: 1.2900568; max_accuracy: 0.5604; since_best_accuracy: 0;\nFastEstimator-Train: step: 1200; ce: 1.4787366; model1_lr: 1.2155782; steps/sec: 32.36;\nFastEstimator-Train: step: 1300; ce: 1.54497; model1_lr: 1.3085014; steps/sec: 33.81;\nFastEstimator-Train: step: 1400; ce: 1.482827; model1_lr: 1.4014246; steps/sec: 33.83;\nFastEstimator-Train: step: 1500; ce: 1.4375826; model1_lr: 1.4943478; steps/sec: 33.86;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 11.69 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 1564; epoch: 4; accuracy: 0.6218; ce: 1.1329432; max_accuracy: 0.6218; since_best_accuracy: 0;\nFastEstimator-Train: step: 1600; ce: 1.481755; model1_lr: 1.587271; steps/sec: 32.33;\nFastEstimator-Train: step: 1700; ce: 1.3139637; model1_lr: 1.6801941; steps/sec: 33.68;\nFastEstimator-Train: step: 1800; ce: 1.41434; model1_lr: 1.7731173; steps/sec: 33.81;\nFastEstimator-Train: step: 1900; ce: 1.3996168; model1_lr: 1.8660406; steps/sec: 33.85;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 11.68 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 1955; epoch: 5; accuracy: 0.6826; ce: 0.99024403; max_accuracy: 0.6826; since_best_accuracy: 0;\nFastEstimator-Train: step: 2000; ce: 1.3781737; model1_lr: 1.9589638; steps/sec: 32.92;\nFastEstimator-Train: step: 2100; ce: 1.3097632; model1_lr: 2.0518868; steps/sec: 33.76;\nFastEstimator-Train: step: 2200; ce: 1.2388372; model1_lr: 2.1448102; steps/sec: 33.82;\nFastEstimator-Train: step: 2300; ce: 1.4561124; model1_lr: 2.2377334; steps/sec: 33.74;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 11.66 sec;\nFastEstimator-Eval: step: 2346; epoch: 6; accuracy: 0.6533; ce: 1.0776143; max_accuracy: 0.6826; since_best_accuracy: 1;\nFastEstimator-Train: step: 2400; ce: 1.3061807; model1_lr: 2.3306565; steps/sec: 32.75;\nFastEstimator-Train: step: 2500; ce: 1.3196818; model1_lr: 2.4235797; steps/sec: 33.8;\nFastEstimator-Train: step: 2600; ce: 1.3396112; model1_lr: 2.5165029; steps/sec: 33.69;\nFastEstimator-Train: step: 2700; ce: 1.2553616; model1_lr: 2.609426; steps/sec: 33.71;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 11.68 sec;\nFastEstimator-Eval: step: 2737; epoch: 7; accuracy: 0.0981; ce: 5.6395764; max_accuracy: 0.6826; since_best_accuracy: 2;\nFastEstimator-Train: step: 2800; ce: 2.0294547; model1_lr: 2.7023492; steps/sec: 32.96;\nFastEstimator-Train: step: 2900; ce: 1.5744395; model1_lr: 2.7952724; steps/sec: 33.66;\nFastEstimator-Train: step: 3000; ce: 1.3871385; model1_lr: 2.8881955; steps/sec: 33.71;\nFastEstimator-Train: step: 3100; ce: 1.4019853; model1_lr: 2.9811187; steps/sec: 33.59;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 11.71 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 3128; epoch: 8; accuracy: 0.7834; ce: 0.68519104; max_accuracy: 0.7834; since_best_accuracy: 0;\nFastEstimator-Train: step: 3200; ce: 1.377032; model1_lr: 3.0740418; steps/sec: 32.6;\nFastEstimator-Train: step: 3300; ce: 1.3246766; model1_lr: 3.1669652; steps/sec: 33.61;\nFastEstimator-Train: step: 3400; ce: 1.3278141; model1_lr: 3.2598884; steps/sec: 33.82;\nFastEstimator-Train: step: 3500; ce: 1.2375286; model1_lr: 3.3528116; steps/sec: 33.11;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 11.72 sec;\nFastEstimator-Eval: step: 3519; epoch: 9; accuracy: 0.7133; ce: 0.98150647; max_accuracy: 0.7834; since_best_accuracy: 1;\nFastEstimator-Train: step: 3600; ce: 1.2265539; model1_lr: 3.4457347; steps/sec: 32.91;\nFastEstimator-Train: step: 3700; ce: 1.4098625; model1_lr: 3.538658; steps/sec: 33.76;\nFastEstimator-Train: step: 3800; ce: 1.2486908; model1_lr: 3.631581; steps/sec: 33.77;\nFastEstimator-Train: step: 3900; ce: 1.2460911; model1_lr: 3.7245042; steps/sec: 33.43;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 11.65 sec;\nFastEstimator-Eval: step: 3910; epoch: 10; accuracy: 0.7253; ce: 0.88525856; max_accuracy: 0.7834; since_best_accuracy: 2;\nFastEstimator-Train: step: 4000; ce: 1.1750683; model1_lr: 3.8174274; steps/sec: 33.16;\nFastEstimator-Train: step: 4100; ce: 1.2834225; model1_lr: 3.9103506; steps/sec: 33.57;\nFastEstimator-Train: step: 4200; ce: 1.3357248; model1_lr: 4.003274; steps/sec: 33.72;\nFastEstimator-Train: step: 4300; ce: 1.1901045; model1_lr: 3.943803; steps/sec: 33.13;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 11.71 sec;\nFastEstimator-Eval: step: 4301; epoch: 11; accuracy: 0.7001; ce: 0.9235585; max_accuracy: 0.7834; since_best_accuracy: 3;\nFastEstimator-Train: step: 4400; ce: 1.2157128; model1_lr: 3.8508797; steps/sec: 33.24;\nFastEstimator-Train: step: 4500; ce: 1.1261709; model1_lr: 3.7579565; steps/sec: 33.79;\nFastEstimator-Train: step: 4600; ce: 1.1690426; model1_lr: 3.6650333; steps/sec: 33.69;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 11.66 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 4692; epoch: 12; accuracy: 0.8259; ce: 0.6520777; max_accuracy: 0.8259; since_best_accuracy: 0;\nFastEstimator-Train: step: 4700; ce: 1.2337161; model1_lr: 3.5721102; steps/sec: 33.06;\nFastEstimator-Train: step: 4800; ce: 1.1818956; model1_lr: 3.479187; steps/sec: 33.77;\nFastEstimator-Train: step: 4900; ce: 1.2220985; model1_lr: 3.3862638; steps/sec: 33.78;\nFastEstimator-Train: step: 5000; ce: 1.1691927; model1_lr: 3.2933407; steps/sec: 33.64;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 11.68 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 5083; epoch: 13; accuracy: 0.8285; ce: 0.6635361; max_accuracy: 0.8285; since_best_accuracy: 0;\nFastEstimator-Train: step: 5100; ce: 1.0983471; model1_lr: 3.2004175; steps/sec: 32.71;\nFastEstimator-Train: step: 5200; ce: 1.1542699; model1_lr: 3.1074944; steps/sec: 33.7;\nFastEstimator-Train: step: 5300; ce: 1.1538193; model1_lr: 3.0145712; steps/sec: 33.79;\nFastEstimator-Train: step: 5400; ce: 1.1344184; model1_lr: 2.921648; steps/sec: 33.8;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 11.67 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 5474; epoch: 14; accuracy: 0.8652; ce: 0.5354095; max_accuracy: 0.8652; since_best_accuracy: 0;\nFastEstimator-Train: step: 5500; ce: 1.0998952; model1_lr: 2.8287246; steps/sec: 32.61;\nFastEstimator-Train: step: 5600; ce: 1.141614; model1_lr: 2.7358015; steps/sec: 33.68;\nFastEstimator-Train: step: 5700; ce: 1.1103913; model1_lr: 2.6428783; steps/sec: 33.8;\nFastEstimator-Train: step: 5800; ce: 1.1439428; model1_lr: 2.5499551; steps/sec: 33.66;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 11.72 sec;\nFastEstimator-Eval: step: 5865; epoch: 15; accuracy: 0.8397; ce: 0.64034617; max_accuracy: 0.8652; since_best_accuracy: 1;\nFastEstimator-Train: step: 5900; ce: 1.1166975; model1_lr: 2.457032; steps/sec: 32.52;\nFastEstimator-Train: step: 6000; ce: 1.0917354; model1_lr: 2.3641088; steps/sec: 33.79;\nFastEstimator-Train: step: 6100; ce: 1.079163; model1_lr: 2.2711856; steps/sec: 33.72;\nFastEstimator-Train: step: 6200; ce: 1.07494; model1_lr: 2.1782625; steps/sec: 33.61;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 11.68 sec;\nFastEstimator-Eval: step: 6256; epoch: 16; accuracy: 0.8358; ce: 0.6846055; max_accuracy: 0.8652; since_best_accuracy: 2;\nFastEstimator-Train: step: 6300; ce: 1.073894; model1_lr: 2.0853393; steps/sec: 32.93;\nFastEstimator-Train: step: 6400; ce: 1.0491798; model1_lr: 1.992416; steps/sec: 33.59;\nFastEstimator-Train: step: 6500; ce: 1.1131084; model1_lr: 1.8994929; steps/sec: 33.54;\nFastEstimator-Train: step: 6600; ce: 1.0556369; model1_lr: 1.8065697; steps/sec: 33.39;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 11.72 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 6647; epoch: 17; accuracy: 0.8826; ce: 0.5354589; max_accuracy: 0.8826; since_best_accuracy: 0;\nFastEstimator-Train: step: 6700; ce: 1.041785; model1_lr: 1.7136465; steps/sec: 32.93;\nFastEstimator-Train: step: 6800; ce: 1.0919912; model1_lr: 1.6207234; steps/sec: 33.66;\nFastEstimator-Train: step: 6900; ce: 1.0730321; model1_lr: 1.5278001; steps/sec: 33.58;\nFastEstimator-Train: step: 7000; ce: 1.0604472; model1_lr: 1.4348769; steps/sec: 33.76;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 11.7 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 7038; epoch: 18; accuracy: 0.8927; ce: 0.4799063; max_accuracy: 0.8927; since_best_accuracy: 0;\nFastEstimator-Train: step: 7100; ce: 1.0326926; model1_lr: 1.3419538; steps/sec: 32.7;\nFastEstimator-Train: step: 7200; ce: 1.0019332; model1_lr: 1.2490306; steps/sec: 33.74;\nFastEstimator-Train: step: 7300; ce: 1.0385857; model1_lr: 1.1561074; steps/sec: 33.59;\nFastEstimator-Train: step: 7400; ce: 1.0323408; model1_lr: 1.0631843; steps/sec: 33.19;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 11.77 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 7429; epoch: 19; accuracy: 0.898; ce: 0.45474234; max_accuracy: 0.898; since_best_accuracy: 0;\nFastEstimator-Train: step: 7500; ce: 1.0034448; model1_lr: 0.97026104; steps/sec: 31.99;\nFastEstimator-Train: step: 7600; ce: 1.0002831; model1_lr: 0.8773378; steps/sec: 33.4;\nFastEstimator-Train: step: 7700; ce: 0.9859823; model1_lr: 0.78441465; steps/sec: 32.05;\nFastEstimator-Train: step: 7800; ce: 1.0219489; model1_lr: 0.6914915; steps/sec: 33.25;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 11.94 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 7820; epoch: 20; accuracy: 0.9061; ce: 0.43634492; max_accuracy: 0.9061; since_best_accuracy: 0;\nFastEstimator-Train: step: 7900; ce: 0.9971979; model1_lr: 0.59856826; steps/sec: 32.97;\nFastEstimator-Train: step: 8000; ce: 0.987967; model1_lr: 0.5056451; steps/sec: 33.69;\nFastEstimator-Train: step: 8100; ce: 1.0111182; model1_lr: 0.4127219; steps/sec: 33.83;\nFastEstimator-Train: step: 8200; ce: 1.0593884; model1_lr: 0.3197987; steps/sec: 33.23;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 11.69 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 8211; epoch: 21; accuracy: 0.9065; ce: 0.4418834; max_accuracy: 0.9065; since_best_accuracy: 0;\nFastEstimator-Train: step: 8300; ce: 0.9793607; model1_lr: 0.22687553; steps/sec: 32.46;\nFastEstimator-Train: step: 8400; ce: 0.97971845; model1_lr: 0.13395235; steps/sec: 31.46;\nFastEstimator-Train: step: 8500; ce: 1.0591702; model1_lr: 0.09365016; steps/sec: 33.74;\nFastEstimator-Train: step: 8600; ce: 1.0542408; model1_lr: 0.082947284; steps/sec: 32.67;\nFastEstimator-Train: step: 8602; epoch: 22; epoch_time: 11.97 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 8602; epoch: 22; accuracy: 0.9074; ce: 0.43536025; max_accuracy: 0.9074; since_best_accuracy: 0;\nFastEstimator-Train: step: 8700; ce: 0.98823214; model1_lr: 0.072244406; steps/sec: 33.52;\nFastEstimator-Train: step: 8800; ce: 0.97655475; model1_lr: 0.061541535; steps/sec: 33.76;\nFastEstimator-Train: step: 8900; ce: 0.95179796; model1_lr: 0.050838657; steps/sec: 33.69;\nFastEstimator-Train: step: 8993; epoch: 23; epoch_time: 11.68 sec;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp9s08iyos/model1_best_accuracy.h5\nFastEstimator-Eval: step: 8993; epoch: 23; accuracy: 0.9106; ce: 0.43370104; max_accuracy: 0.9106; since_best_accuracy: 0;\nFastEstimator-Train: step: 9000; ce: 0.99330795; model1_lr: 0.040135782; steps/sec: 32.76;\nFastEstimator-Train: step: 9100; ce: 0.96883345; model1_lr: 0.029432908; steps/sec: 33.6;\nFastEstimator-Train: step: 9200; ce: 1.012826; model1_lr: 0.018730031; steps/sec: 33.8;\nFastEstimator-Train: step: 9300; ce: 1.0013785; model1_lr: 0.008027157; steps/sec: 33.65;\nFastEstimator-Train: step: 9384; epoch: 24; epoch_time: 11.7 sec;\nFastEstimator-Eval: step: 9384; epoch: 24; accuracy: 0.9093; ce: 0.4368044; max_accuracy: 0.9106; since_best_accuracy: 1;\nFastEstimator-Finish: step: 9384; model1_lr: 0.0; total_time: 321.15 sec;\n</pre>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html#super-convergence-learning-rate-schedule-tensorflow-backend", "title": "Super-Convergence Learning Rate Schedule  (TensorFlow Backend)\u00b6", "text": "<p>In this example we will implement super-convergence learning rate (LR) schedule (https://arxiv.org/pdf/1708.07120.pdf) and test it on a CIFAR10 image classification task. Super-covergence is a phenomenon where neural networks can be trained an order of magnitude faster than with standard training methods. The paper proposes a LR schedule which incorporates two parts: a LR range test to find the appropriate LR range and a cyclical LR schedule that uses the obtained information.</p>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html#network-architecture-and-data-pipeline", "title": "Network Architecture and Data Pipeline\u00b6", "text": "<p>We will use almost the same image classification configuration of the other Apphub example: CIFAR10 Fast including network architecture and data pipeline. The only difference is that we use SGD optimizer instead of Adam because author of the paper specially pointed out the incompatibility between Adam optimizer and super-convergence.</p>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html#lr-range-test", "title": "LR Range Test\u00b6", "text": "<p>The preparation of the super-convergence schedule is to search the suitable LR range. The process is training the target network with a linearly increasing LR and observing the validation accuracy. Generally, the accuracy will keep increase until at some certain point when the LR get too high and start making training diverge. The very LR of that moment is the \"maximum LR\".</p> <p>To run the test we need to implement the trace to record the maximum LR. After running the training with linear increaseing LR, we will get the maximum LR.</p> <p> [The typical learning rate and metircs plot from https://arxiv.org/pdf/1708.07120.pdf]</p>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html#super-convergence-lr-schedule", "title": "Super-Convergence LR Schedule\u00b6", "text": "<p>Once we get the maximum LR, the minimum LR can be computed by dividing it by 40. Although this number is set to 4 in the paragraph of the original paper, it falls in range of [4, 40] in its experiment section. We empirically found 40 is the best value for this task.</p> <p>The LR change has 3 phases:</p> <ol> <li>increase LR from minimum LR to maximum LR at 0~45% of training process</li> <li>decrase LR from maximum LR to minimum LR at 45%~90% of training process</li> <li>decrase LR from minimum LR to 0 at 90%~100% of training process</li> </ol> <p></p>"}, {"location": "apphub/lr_controller/super_convergence/super_convergence.html#result-discussion", "title": "Result Discussion\u00b6", "text": "<p>The result of it might not be super impressive when comparing with original example CIFAR10 Fast. But please be aware that the example has its own LR schedules which is specially tuned on that configuration (plus that scheduler is also cyclical LR schedule).</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html", "title": "Multi-Task Learning using Uncertainty Weighted Loss", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nimport numpy as np\nfrom torch.nn.init import kaiming_normal_ as he_normal\nfrom torchvision import models\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy, Dice\n</pre> import os import tempfile  import cv2 import torch import torch.nn as nn import torch.nn.functional as fn import numpy as np from torch.nn.init import kaiming_normal_ as he_normal from torchvision import models  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy, Dice In\u00a0[2]: parameters Copied! <pre>#parameters\nepochs = 25\nbatch_size = 8\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> #parameters epochs = 25 batch_size = 8 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cub200\n\ntrain_data = cub200.load_data(root_dir=data_dir)\neval_data = train_data.split(0.3)\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cub200  train_data = cub200.load_data(root_dir=data_dir) eval_data = train_data.split(0.3) test_data = eval_data.split(0.5)  In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n    batch_size=batch_size,\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\", parent_path=train_data.parent_path),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        ReadMat(inputs='annotation', outputs=\"seg\", parent_path=train_data.parent_path),\n        Delete(keys=\"annotation\"),\n        LongestMaxSize(max_size=512, image_in=\"image\", image_out=\"image\", mask_in=\"seg\", mask_out=\"seg\"),\n        PadIfNeeded(min_height=512,\n                    min_width=512,\n                    image_in=\"image\",\n                    image_out=\"image\",\n                    mask_in=\"seg\",\n                    mask_out=\"seg\",\n                    border_mode=cv2.BORDER_CONSTANT,\n                    value=0,\n                    mask_value=0),\n        ShiftScaleRotate(image_in=\"image\",\n                         mask_in=\"seg\",\n                         image_out=\"image\",\n                         mask_out=\"seg\",\n                         mode=\"train\",\n                         shift_limit=0.2,\n                         rotate_limit=15.0,\n                         scale_limit=0.2,\n                         border_mode=cv2.BORDER_CONSTANT,\n                         value=0,\n                         mask_value=0),\n        Sometimes(HorizontalFlip(image_in=\"image\", mask_in=\"seg\", image_out=\"image\", mask_out=\"seg\", mode=\"train\")),\n        ChannelTranspose(inputs=\"image\", outputs=\"image\"),\n        Reshape(shape=(1, 512, 512), inputs=\"seg\", outputs=\"seg\")\n    ])\n</pre> pipeline = fe.Pipeline(     batch_size=batch_size,     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\", parent_path=train_data.parent_path),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         ReadMat(inputs='annotation', outputs=\"seg\", parent_path=train_data.parent_path),         Delete(keys=\"annotation\"),         LongestMaxSize(max_size=512, image_in=\"image\", image_out=\"image\", mask_in=\"seg\", mask_out=\"seg\"),         PadIfNeeded(min_height=512,                     min_width=512,                     image_in=\"image\",                     image_out=\"image\",                     mask_in=\"seg\",                     mask_out=\"seg\",                     border_mode=cv2.BORDER_CONSTANT,                     value=0,                     mask_value=0),         ShiftScaleRotate(image_in=\"image\",                          mask_in=\"seg\",                          image_out=\"image\",                          mask_out=\"seg\",                          mode=\"train\",                          shift_limit=0.2,                          rotate_limit=15.0,                          scale_limit=0.2,                          border_mode=cv2.BORDER_CONSTANT,                          value=0,                          mask_value=0),         Sometimes(HorizontalFlip(image_in=\"image\", mask_in=\"seg\", image_out=\"image\", mask_out=\"seg\", mode=\"train\")),         ChannelTranspose(inputs=\"image\", outputs=\"image\"),         Reshape(shape=(1, 512, 512), inputs=\"seg\", outputs=\"seg\")     ])  In\u00a0[5]: Copied! <pre>from fastestimator.util import ImageDisplay, GridDisplay\n\nresult = pipeline.get_results()\n\nGridDisplay([ImageDisplay(image=result[\"image\"][1], \n                          title=\"Original Image\"),\n             ImageDisplay(image=result[\"image\"][1], \n                          masks=np.squeeze(result[\"seg\"][1].numpy()), \n                          title=\"Mask Overlay\"),\n            ]).show()\n</pre> from fastestimator.util import ImageDisplay, GridDisplay  result = pipeline.get_results()  GridDisplay([ImageDisplay(image=result[\"image\"][1],                            title=\"Original Image\"),              ImageDisplay(image=result[\"image\"][1],                            masks=np.squeeze(result[\"seg\"][1].numpy()),                            title=\"Mask Overlay\"),             ]).show() In\u00a0[6]: Copied! <pre>class Upsample2D(nn.Module):\n\"\"\"Upsampling Block\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.upsample = nn.Sequential(\n            nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.upsample:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass DecBlock(nn.Module):\n\"\"\"Decoder Block\"\"\"\n    def __init__(self, upsample_in_ch, conv_in_ch, out_ch):\n        super().__init__()\n        self.upsample = Upsample2D(upsample_in_ch, out_ch)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.conv_layers:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x_up, x_down):\n        x = self.upsample(x_up)\n        x = torch.cat([x, x_down], 1)\n        x = self.conv_layers(x)\n        return x\n\n\nclass ResUnet50(nn.Module):\n\"\"\"Network Architecture\"\"\"\n    def __init__(self, num_classes=200):\n        super().__init__()\n        base_model = models.resnet50(pretrained=True)\n\n        self.enc1 = nn.Sequential(*list(base_model.children())[:3])\n        self.input_pool = list(base_model.children())[3]\n        self.enc2 = nn.Sequential(*list(base_model.children())[4])\n        self.enc3 = nn.Sequential(*list(base_model.children())[5])\n        self.enc4 = nn.Sequential(*list(base_model.children())[6])\n        self.enc5 = nn.Sequential(*list(base_model.children())[7])\n        self.fc = nn.Linear(2048, num_classes)\n\n        self.dec6 = DecBlock(2048, 1536, 512)\n        self.dec7 = DecBlock(512, 768, 256)\n        self.dec8 = DecBlock(256, 384, 128)\n        self.dec9 = DecBlock(128, 128, 64)\n        self.dec10 = Upsample2D(64, 2)\n        self.mask = nn.Conv2d(2, 1, kernel_size=1)\n\n    def forward(self, x):\n        x_e1 = self.enc1(x)\n        x_e1_1 = self.input_pool(x_e1)\n        x_e2 = self.enc2(x_e1_1)\n        x_e3 = self.enc3(x_e2)\n        x_e4 = self.enc4(x_e3)\n        x_e5 = self.enc5(x_e4)\n\n        x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])\n        x_label = x_label.view(x_label.shape[0], -1)\n        x_label = self.fc(x_label)\n        x_label = torch.softmax(x_label, dim=-1)\n\n        x_d6 = self.dec6(x_e5, x_e4)\n        x_d7 = self.dec7(x_d6, x_e3)\n        x_d8 = self.dec8(x_d7, x_e2)\n        x_d9 = self.dec9(x_d8, x_e1)\n        x_d10 = self.dec10(x_d9)\n        x_mask = self.mask(x_d10)\n        x_mask = torch.sigmoid(x_mask)\n        return x_label, x_mask\n</pre> class Upsample2D(nn.Module):     \"\"\"Upsampling Block\"\"\"     def __init__(self, in_channels, out_channels):         super().__init__()         self.upsample = nn.Sequential(             nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.upsample:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x):         return self.upsample(x)   class DecBlock(nn.Module):     \"\"\"Decoder Block\"\"\"     def __init__(self, upsample_in_ch, conv_in_ch, out_ch):         super().__init__()         self.upsample = Upsample2D(upsample_in_ch, out_ch)         self.conv_layers = nn.Sequential(             nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True),             nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.conv_layers:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x_up, x_down):         x = self.upsample(x_up)         x = torch.cat([x, x_down], 1)         x = self.conv_layers(x)         return x   class ResUnet50(nn.Module):     \"\"\"Network Architecture\"\"\"     def __init__(self, num_classes=200):         super().__init__()         base_model = models.resnet50(pretrained=True)          self.enc1 = nn.Sequential(*list(base_model.children())[:3])         self.input_pool = list(base_model.children())[3]         self.enc2 = nn.Sequential(*list(base_model.children())[4])         self.enc3 = nn.Sequential(*list(base_model.children())[5])         self.enc4 = nn.Sequential(*list(base_model.children())[6])         self.enc5 = nn.Sequential(*list(base_model.children())[7])         self.fc = nn.Linear(2048, num_classes)          self.dec6 = DecBlock(2048, 1536, 512)         self.dec7 = DecBlock(512, 768, 256)         self.dec8 = DecBlock(256, 384, 128)         self.dec9 = DecBlock(128, 128, 64)         self.dec10 = Upsample2D(64, 2)         self.mask = nn.Conv2d(2, 1, kernel_size=1)      def forward(self, x):         x_e1 = self.enc1(x)         x_e1_1 = self.input_pool(x_e1)         x_e2 = self.enc2(x_e1_1)         x_e3 = self.enc3(x_e2)         x_e4 = self.enc4(x_e3)         x_e5 = self.enc5(x_e4)          x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])         x_label = x_label.view(x_label.shape[0], -1)         x_label = self.fc(x_label)         x_label = torch.softmax(x_label, dim=-1)          x_d6 = self.dec6(x_e5, x_e4)         x_d7 = self.dec7(x_d6, x_e3)         x_d8 = self.dec8(x_d7, x_e2)         x_d9 = self.dec9(x_d8, x_e1)         x_d10 = self.dec10(x_d9)         x_mask = self.mask(x_d10)         x_mask = torch.sigmoid(x_mask)         return x_label, x_mask <p>Other than the ResUnet50, we will have another network to contain the trainable weighted parameter in the weighted loss. We call it our uncertainty model. In the network <code>ops</code>, ResUnet50 produces both a predicted label and predicted mask. These two predictions are then fed to classification loss and segmentation loss operators respectively. Finally, both losses are passed to the uncertainty model to create a final loss.</p> In\u00a0[7]: Copied! <pre>class UncertaintyLossNet(nn.Module):\n\"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.w1 = nn.Parameter(torch.zeros(1))\n        self.w2 = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(\n            -self.w2) * x[1] + self.w2\n        return loss\n</pre> class UncertaintyLossNet(nn.Module):     \"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115     \"\"\"     def __init__(self):         super().__init__()         self.w1 = nn.Parameter(torch.zeros(1))         self.w2 = nn.Parameter(torch.zeros(1))      def forward(self, x):         loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(             -self.w2) * x[1] + self.w2         return loss <p>We also implement a <code>TensorOp</code> to average the output of <code>UncertaintyLossNet</code> for each batch:</p> In\u00a0[8]: Copied! <pre>class ReduceLoss(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> class ReduceLoss(TensorOp):     def forward(self, data, state):         return reduce_mean(data) In\u00a0[9]: Copied! <pre>resunet50 = fe.build(model_fn=ResUnet50,\n                     model_name=\"resunet50\",\n                     optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4))\nuncertainty = fe.build(model_fn=UncertaintyLossNet,\n                       model_name=\"uncertainty\",\n                       optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs='image',\n            model=resunet50,\n            outputs=[\"label_pred\", \"mask_pred\"]),\n    CrossEntropy(inputs=[\"label_pred\", \"label\"],\n                 outputs=\"cls_loss\",\n                 form=\"sparse\",\n                 average_loss=False),\n    CrossEntropy(inputs=[\"mask_pred\", \"seg\"],\n                 outputs=\"seg_loss\",\n                 form=\"binary\",\n                 average_loss=False),\n    ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],\n            model=uncertainty,\n            outputs=\"total_loss\"),\n    ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),\n    UpdateOp(model=resunet50, loss_name=\"total_loss\"),\n    UpdateOp(model=uncertainty, loss_name=\"total_loss\")\n])\n</pre> resunet50 = fe.build(model_fn=ResUnet50,                      model_name=\"resunet50\",                      optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4)) uncertainty = fe.build(model_fn=UncertaintyLossNet,                        model_name=\"uncertainty\",                        optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))  network = fe.Network(ops=[     ModelOp(inputs='image',             model=resunet50,             outputs=[\"label_pred\", \"mask_pred\"]),     CrossEntropy(inputs=[\"label_pred\", \"label\"],                  outputs=\"cls_loss\",                  form=\"sparse\",                  average_loss=False),     CrossEntropy(inputs=[\"mask_pred\", \"seg\"],                  outputs=\"seg_loss\",                  form=\"binary\",                  average_loss=False),     ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],             model=uncertainty,             outputs=\"total_loss\"),     ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),     UpdateOp(model=resunet50, loss_name=\"total_loss\"),     UpdateOp(model=uncertainty, loss_name=\"total_loss\") ]) In\u00a0[10]: Copied! <pre>traces = [\n    Accuracy(true_key=\"label\", pred_key=\"label_pred\"),\n    Dice(true_key=\"seg\", pred_key='mask_pred'),\n    BestModelSaver(model=resunet50,\n                   save_dir=save_dir,\n                   metric=\"total_loss\",\n                   save_best_mode=\"min\"),\n    LRScheduler(model=resunet50,\n                lr_fn=lambda step: cosine_decay(\n                    step, cycle_length=13200, init_lr=1e-4))\n]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=traces,\n                         epochs=epochs,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch,\n                         log_steps=500)\n</pre> traces = [     Accuracy(true_key=\"label\", pred_key=\"label_pred\"),     Dice(true_key=\"seg\", pred_key='mask_pred'),     BestModelSaver(model=resunet50,                    save_dir=save_dir,                    metric=\"total_loss\",                    save_best_mode=\"min\"),     LRScheduler(model=resunet50,                 lr_fn=lambda step: cosine_decay(                     step, cycle_length=13200, init_lr=1e-4)) ] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=traces,                          epochs=epochs,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch,                          log_steps=500) In\u00a0[11]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 1;\nFastEstimator-Train: step: 1; resunet50_lr: 1e-04; total_loss: 9.154388;\nFastEstimator-Train: step: 500; resunet50_lr: 9.964993e-05; steps/sec: 6.11; total_loss: 4.1662703;\nFastEstimator-Train: step: 528; epoch: 1; epoch_time: 91.81 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.85;\nEval Progress: 75/113; steps/sec: 14.48;\nEval Progress: 113/113; steps/sec: 14.36;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 528; epoch: 1; accuracy: 0.15248618784530388; Dice: 0.78948456; min_total_loss: 4.0875998; since_best_total_loss: 0; total_loss: 4.0875998;\nFastEstimator-Train: step: 1000; resunet50_lr: 9.860467e-05; steps/sec: 6.55; total_loss: 2.3772597;\nFastEstimator-Train: step: 1056; epoch: 2; epoch_time: 79.99 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 9.92;\nEval Progress: 75/113; steps/sec: 12.16;\nEval Progress: 113/113; steps/sec: 14.27;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1056; epoch: 2; accuracy: 0.4276243093922652; Dice: 0.8241268; min_total_loss: 2.297407; since_best_total_loss: 0; total_loss: 2.297407;\nFastEstimator-Train: step: 1500; resunet50_lr: 9.687901e-05; steps/sec: 5.85; total_loss: 1.4891936;\nFastEstimator-Train: step: 1584; epoch: 3; epoch_time: 89.91 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.1;\nEval Progress: 75/113; steps/sec: 13.71;\nEval Progress: 113/113; steps/sec: 13.34;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1584; epoch: 3; accuracy: 0.5580110497237569; Dice: 0.8163089; min_total_loss: 1.7144347; since_best_total_loss: 0; total_loss: 1.7144347;\nFastEstimator-Train: step: 2000; resunet50_lr: 9.449736e-05; steps/sec: 6.48; total_loss: 0.92048085;\nFastEstimator-Train: step: 2112; epoch: 4; epoch_time: 81.09 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 13.91;\nEval Progress: 75/113; steps/sec: 15.05;\nEval Progress: 113/113; steps/sec: 15.09;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 2112; epoch: 4; accuracy: 0.6397790055248619; Dice: 0.83269066; min_total_loss: 1.4245821; since_best_total_loss: 0; total_loss: 1.4245821;\nFastEstimator-Train: step: 2500; resunet50_lr: 9.149339e-05; steps/sec: 5.54; total_loss: 0.5726477;\nFastEstimator-Train: step: 2640; epoch: 5; epoch_time: 93.91 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.88;\nEval Progress: 75/113; steps/sec: 13.97;\nEval Progress: 113/113; steps/sec: 13.62;\nFastEstimator-Eval: step: 2640; epoch: 5; accuracy: 0.63646408839779; Dice: 0.8394248; min_total_loss: 1.4245821; since_best_total_loss: 1; total_loss: 1.4509172;\nFastEstimator-Train: step: 3000; resunet50_lr: 8.7909604e-05; steps/sec: 6.52; total_loss: 1.4370325;\nFastEstimator-Train: step: 3168; epoch: 6; epoch_time: 89.83 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 6.42;\nEval Progress: 75/113; steps/sec: 13.17;\nEval Progress: 113/113; steps/sec: 12.92;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 3168; epoch: 6; accuracy: 0.687292817679558; Dice: 0.8373534; min_total_loss: 1.2567004; since_best_total_loss: 0; total_loss: 1.2567004;\nFastEstimator-Train: step: 3500; resunet50_lr: 8.379669e-05; steps/sec: 5.61; total_loss: 0.2995667;\nFastEstimator-Train: step: 3696; epoch: 7; epoch_time: 84.76 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.1;\nEval Progress: 75/113; steps/sec: 14.14;\nEval Progress: 113/113; steps/sec: 13.69;\nFastEstimator-Eval: step: 3696; epoch: 7; accuracy: 0.6486187845303868; Dice: 0.83454293; min_total_loss: 1.2567004; since_best_total_loss: 1; total_loss: 1.5572991;\nFastEstimator-Train: step: 4000; resunet50_lr: 7.921282e-05; steps/sec: 5.84; total_loss: 0.44874218;\nFastEstimator-Train: step: 4224; epoch: 8; epoch_time: 94.05 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.68;\nEval Progress: 75/113; steps/sec: 14.45;\nEval Progress: 113/113; steps/sec: 14.19;\nFastEstimator-Eval: step: 4224; epoch: 8; accuracy: 0.7038674033149172; Dice: 0.8413349; min_total_loss: 1.2567004; since_best_total_loss: 2; total_loss: 1.2848965;\nFastEstimator-Train: step: 4500; resunet50_lr: 7.422282e-05; steps/sec: 6.16; total_loss: 0.30338854;\nFastEstimator-Train: step: 4752; epoch: 9; epoch_time: 80.02 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 13.72;\nEval Progress: 75/113; steps/sec: 11.25;\nEval Progress: 113/113; steps/sec: 14.8;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 4752; epoch: 9; accuracy: 0.7149171270718232; Dice: 0.8224843; min_total_loss: 1.132937; since_best_total_loss: 0; total_loss: 1.132937;\nFastEstimator-Train: step: 5000; resunet50_lr: 6.8897294e-05; steps/sec: 5.57; total_loss: 0.5157542;\nFastEstimator-Train: step: 5280; epoch: 10; epoch_time: 94.31 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.57;\nEval Progress: 75/113; steps/sec: 14.03;\nEval Progress: 113/113; steps/sec: 14.15;\nFastEstimator-Eval: step: 5280; epoch: 10; accuracy: 0.7116022099447514; Dice: 0.8515037; min_total_loss: 1.132937; since_best_total_loss: 1; total_loss: 1.2132615;\nFastEstimator-Train: step: 5500; resunet50_lr: 6.331154e-05; steps/sec: 6.52; total_loss: 0.4250332;\nFastEstimator-Train: step: 5808; epoch: 11; epoch_time: 80.24 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 8.37;\nEval Progress: 75/113; steps/sec: 10.42;\nEval Progress: 113/113; steps/sec: 5.6;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 5808; epoch: 11; accuracy: 0.7425414364640884; Dice: 0.85177195; min_total_loss: 1.0848694; since_best_total_loss: 0; total_loss: 1.0848694;\nFastEstimator-Train: step: 6000; resunet50_lr: 5.7544585e-05; steps/sec: 6.35; total_loss: -0.0076454906;\nFastEstimator-Train: step: 6336; epoch: 12; epoch_time: 84.4 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.07;\nEval Progress: 75/113; steps/sec: 13.34;\nEval Progress: 113/113; steps/sec: 13.27;\nFastEstimator-Eval: step: 6336; epoch: 12; accuracy: 0.7337016574585635; Dice: 0.8479459; min_total_loss: 1.0848694; since_best_total_loss: 1; total_loss: 1.115356;\nFastEstimator-Train: step: 6500; resunet50_lr: 5.1677987e-05; steps/sec: 6.16; total_loss: -0.019147485;\nFastEstimator-Train: step: 6864; epoch: 13; epoch_time: 89.97 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.67;\nEval Progress: 75/113; steps/sec: 13.32;\nEval Progress: 113/113; steps/sec: 12.7;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 6864; epoch: 13; accuracy: 0.7712707182320442; Dice: 0.8462679; min_total_loss: 0.89779496; since_best_total_loss: 0; total_loss: 0.89779496;\nFastEstimator-Train: step: 7000; resunet50_lr: 4.5794724e-05; steps/sec: 5.94; total_loss: -0.0025181528;\nFastEstimator-Train: step: 7392; epoch: 14; epoch_time: 82.91 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.44;\nEval Progress: 75/113; steps/sec: 12.84;\nEval Progress: 113/113; steps/sec: 12.13;\nFastEstimator-Eval: step: 7392; epoch: 14; accuracy: 0.7668508287292818; Dice: 0.850353; min_total_loss: 0.89779496; since_best_total_loss: 1; total_loss: 0.93487144;\nFastEstimator-Train: step: 7500; resunet50_lr: 3.997802e-05; steps/sec: 6.45; total_loss: -0.05418662;\nFastEstimator-Train: step: 7920; epoch: 15; epoch_time: 88.47 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 10.31;\nEval Progress: 75/113; steps/sec: 11.96;\nEval Progress: 113/113; steps/sec: 12.33;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 7920; epoch: 15; accuracy: 0.7911602209944751; Dice: 0.8533265; min_total_loss: 0.8720767; since_best_total_loss: 0; total_loss: 0.8720767;\nFastEstimator-Train: step: 8000; resunet50_lr: 3.4310135e-05; steps/sec: 5.69; total_loss: -0.059786916;\nFastEstimator-Train: step: 8448; epoch: 16; epoch_time: 85.13 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 14.1;\nEval Progress: 75/113; steps/sec: 15.31;\nEval Progress: 113/113; steps/sec: 15.02;\nFastEstimator-Eval: step: 8448; epoch: 16; accuracy: 0.7977900552486188; Dice: 0.85619587; min_total_loss: 0.8720767; since_best_total_loss: 1; total_loss: 0.89978665;\nFastEstimator-Train: step: 8500; resunet50_lr: 2.8871247e-05; steps/sec: 6.32; total_loss: -0.05804207;\nFastEstimator-Train: step: 8976; epoch: 17; epoch_time: 86.89 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.14;\nEval Progress: 75/113; steps/sec: 12.06;\nEval Progress: 113/113; steps/sec: 10.28;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 8976; epoch: 17; accuracy: 0.8033149171270718; Dice: 0.8537778; min_total_loss: 0.7831172; since_best_total_loss: 0; total_loss: 0.7831172;\nFastEstimator-Train: step: 9000; resunet50_lr: 2.373828e-05; steps/sec: 5.83; total_loss: -0.07401227;\nFastEstimator-Train: step: 9500; resunet50_lr: 1.8983836e-05; steps/sec: 6.64; total_loss: -0.058266915;\nFastEstimator-Train: step: 9504; epoch: 18; epoch_time: 88.88 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.93;\nEval Progress: 75/113; steps/sec: 12.3;\nEval Progress: 113/113; steps/sec: 11.76;\nFastEstimator-Eval: step: 9504; epoch: 18; accuracy: 0.7966850828729282; Dice: 0.856397; min_total_loss: 0.7831172; since_best_total_loss: 1; total_loss: 0.84377414;\nFastEstimator-Train: step: 10000; resunet50_lr: 1.4675165e-05; steps/sec: 5.84; total_loss: 0.5967636;\nFastEstimator-Train: step: 10032; epoch: 19; epoch_time: 90.66 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.62;\nEval Progress: 75/113; steps/sec: 13.34;\nEval Progress: 113/113; steps/sec: 13.43;\nFastEstimator-Eval: step: 10032; epoch: 19; accuracy: 0.8022099447513812; Dice: 0.8553481; min_total_loss: 0.7831172; since_best_total_loss: 2; total_loss: 0.8032281;\nFastEstimator-Train: step: 10500; resunet50_lr: 1.0873208e-05; steps/sec: 5.98; total_loss: -0.12161801;\nFastEstimator-Train: step: 10560; epoch: 20; epoch_time: 87.34 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.92;\nEval Progress: 75/113; steps/sec: 13.64;\nEval Progress: 113/113; steps/sec: 13.76;\nFastEstimator-Eval: step: 10560; epoch: 20; accuracy: 0.8; Dice: 0.8542475; min_total_loss: 0.7831172; since_best_total_loss: 3; total_loss: 0.8481178;\nFastEstimator-Train: step: 11000; resunet50_lr: 7.631743e-06; steps/sec: 6.28; total_loss: -0.13021652;\nFastEstimator-Train: step: 11088; epoch: 21; epoch_time: 82.8 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.62;\nEval Progress: 75/113; steps/sec: 13.0;\nEval Progress: 113/113; steps/sec: 12.55;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11088; epoch: 21; accuracy: 0.8110497237569061; Dice: 0.8564908; min_total_loss: 0.7806792; since_best_total_loss: 0; total_loss: 0.7806792;\nFastEstimator-Train: step: 11500; resunet50_lr: 4.996615e-06; steps/sec: 5.76; total_loss: -0.14522819;\nFastEstimator-Train: step: 11616; epoch: 22; epoch_time: 91.08 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.16;\nEval Progress: 75/113; steps/sec: 12.74;\nEval Progress: 113/113; steps/sec: 10.87;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11616; epoch: 22; accuracy: 0.8154696132596685; Dice: 0.85830724; min_total_loss: 0.7108743; since_best_total_loss: 0; total_loss: 0.7108743;\nFastEstimator-Train: step: 12000; resunet50_lr: 3.0050978e-06; steps/sec: 6.15; total_loss: -0.112137154;\nFastEstimator-Train: step: 12144; epoch: 23; epoch_time: 84.93 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 11.72;\nEval Progress: 75/113; steps/sec: 14.94;\nEval Progress: 113/113; steps/sec: 15.09;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12144; epoch: 23; accuracy: 0.8121546961325967; Dice: 0.8582533; min_total_loss: 0.70829; since_best_total_loss: 0; total_loss: 0.70829;\nFastEstimator-Train: step: 12500; resunet50_lr: 1.6853595e-06; steps/sec: 6.25; total_loss: -0.06531774;\nFastEstimator-Train: step: 12672; epoch: 24; epoch_time: 85.24 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 12.69;\nEval Progress: 75/113; steps/sec: 13.95;\nEval Progress: 113/113; steps/sec: 14.0;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmpxbzrtksw/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12672; epoch: 24; accuracy: 0.8198895027624309; Dice: 0.85663784; min_total_loss: 0.70189565; since_best_total_loss: 0; total_loss: 0.70189565;\nFastEstimator-Train: step: 13000; resunet50_lr: 1.0560667e-06; steps/sec: 6.06; total_loss: -0.17042789;\nFastEstimator-Train: step: 13200; epoch: 25; epoch_time: 86.13 sec;\nEval Progress: 1/113;\nEval Progress: 37/113; steps/sec: 13.74;\nEval Progress: 75/113; steps/sec: 15.23;\nEval Progress: 113/113; steps/sec: 7.14;\nFastEstimator-Eval: step: 13200; epoch: 25; accuracy: 0.8209944751381215; Dice: 0.86008275; min_total_loss: 0.70189565; since_best_total_loss: 1; total_loss: 0.70731735;\nFastEstimator-Finish: step: 13200; resunet50_lr: 1e-06; total_time: 2554.61 sec; uncertainty_lr: 1e-05;\n</pre> <p>Let's load the model with best loss and check our performance on the test set:</p> In\u00a0[12]: Copied! <pre>fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt'))\nestimator.test()\n</pre> fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt')) estimator.test() <pre>FastEstimator-Test: step: 13200; epoch: 25; accuracy: 0.825414364640884; Dice: 0.8634865; total_loss: 0.6717207;\n</pre> <p>We randomly select an image from the test dataset and use <code>pipeline.transform</code> to process the image. We generate the results using <code>network.transform</code> and visualize the prediction.</p> In\u00a0[13]: Copied! <pre>data = test_data[np.random.randint(low=0, high=len(test_data))]\nresult = pipeline.transform(data, mode=\"infer\")\n\nimg = np.squeeze(result[\"image\"])\nimg = np.transpose(img, (1, 2, 0))\nmask_gt = np.squeeze(result[\"seg\"])\n</pre> data = test_data[np.random.randint(low=0, high=len(test_data))] result = pipeline.transform(data, mode=\"infer\")  img = np.squeeze(result[\"image\"]) img = np.transpose(img, (1, 2, 0)) mask_gt = np.squeeze(result[\"seg\"]) In\u00a0[14]: Copied! <pre>GridDisplay([ImageDisplay(image=img, \n                          title=\"Original Image\"),\n             ImageDisplay(image=img, \n                          masks=mask_gt, \n                          title=\"Mask Overlay\"),\n            ]).show()\n</pre> GridDisplay([ImageDisplay(image=img,                            title=\"Original Image\"),              ImageDisplay(image=img,                            masks=mask_gt,                            title=\"Mask Overlay\"),             ]).show() In\u00a0[15]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"])\n])\n\npredictions = network.transform(result, mode=\"infer\")\npredicted_mask = predictions[\"mask_pred\"].numpy() \npred_mask = np.squeeze(predicted_mask)\npred_mask = np.round(pred_mask).astype(mask_gt.dtype)\n\nGridDisplay([ImageDisplay(image=img, \n                          title=\"Original Image\"),\n             ImageDisplay(image=img, \n                          masks=pred_mask, \n                          title=\"Mask Overlay\"),\n            ]).show()\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"]) ])  predictions = network.transform(result, mode=\"infer\") predicted_mask = predictions[\"mask_pred\"].numpy()  pred_mask = np.squeeze(predicted_mask) pred_mask = np.round(pred_mask).astype(mask_gt.dtype)  GridDisplay([ImageDisplay(image=img,                            title=\"Original Image\"),              ImageDisplay(image=img,                            masks=pred_mask,                            title=\"Mask Overlay\"),             ]).show()"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#multi-task-learning-using-uncertainty-weighted-loss", "title": "Multi-Task Learning using Uncertainty Weighted Loss\u00b6", "text": "<p>Multi-task learning is popular in many deep learning applications. For example, in object detection the network performs both classification and localization for each object. As a result, the final loss will be a combination of classification loss and regression loss. The most frequent way of combining two losses is by simply adding them together:</p> <p>$loss_{total} = loss_1 + loss_2$</p> <p>However, a problem emerges when the two losses are on different numerical scales. To resolve this issue, people usually manually design/experimentally determine the best weight, which is very time consuming and computationally expensive:</p> <p>$loss_{total} = w_1loss_1 + w_2loss_2$</p> <p>This paper presents an interesting idea: make the weights w1 and w2 trainable parameters based on the uncertainty of each task, such that the network can dynamically focus more on the task with higher uncertainty.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#dataset", "title": "Dataset\u00b6", "text": "<p>We will use the CUB200 2010 dataset by Caltech. It contains 6033 bird images from 200 categories, where each image also has a corresponding mask. Therefore, our task is to classify and segment the bird given the image.</p> <p>We use a FastEstimator API to load the CUB200 dataset and split the dataset to get train, evaluation and test sets.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We read the images with <code>ReadImage</code>, and the masks stored in a MAT file with <code>ReadMat</code>. There is other information stored in the MAT file, so we specify the key <code>seg</code> to retrieve the mask only.</p> <p>Here the main task is to resize the images and masks into 512 by 512 pixels. We use <code>LongestMaxSize</code> (to preserve the aspect ratio) and <code>PadIfNeeded</code> to resize the image. We will augment both image and mask in the same way and rescale the image pixel values between -1 and 1 since we are using pre-trained ImageNet weights.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#lets-visualize-our-pipeline-results", "title": "Let's visualize our <code>Pipeline</code> results\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>In this implementation, the network architecture is not the focus. Therefore, we are going to create something out of the blue :). How about a combination of resnet50 and Unet that can do both classification and segmentation? We can call it - ResUnet50</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": "<p>We will have four different traces to control/monitor the training: <code>Dice</code> and <code>Accuracy</code> will be used to measure segmentation and classification results, <code>BestModelSaver</code> will save the model with best loss, and <code>LRScheduler</code> will apply a cosine learning rate decay throughout the training loop.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#training-and-testing", "title": "Training and Testing\u00b6", "text": "<p>The whole training (25 epochs) will take about 1 hour 20 mins on single V100 GPU. We are going to reach ~0.87 dice and ~83% accuracy by the end of the training.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-ground-truth", "title": "Visualize Ground Truth\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-prediction", "title": "Visualize Prediction\u00b6", "text": ""}, {"location": "apphub/neural_architecture_search/naswot/naswot.html", "title": "Neural Architecture Search without Training (NASWOT) on CI10 (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\nimport wget\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, layers\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.search import GridSearch\nfrom fastestimator.util import to_number\nfrom fastestimator.util.wget_util import bar_custom, callback_progress\n</pre> import os import tempfile import wget import numpy as np import pandas as pd from scipy import stats  import tensorflow as tf from tensorflow.keras import Model, layers  import fastestimator as fe from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop.univariate import Normalize from fastestimator.search import GridSearch from fastestimator.util import to_number from fastestimator.util.wget_util import bar_custom, callback_progress <p>Let's define our training parameters. We have extracted the information required for our showcase from <code>NAS-Bench-201</code> into a csv file for ease of use, which we will be downloading here. We will be randomly selecting 50 architectures for demonstrative purposes.</p> In\u00a0[2]: parameters Copied! <pre># Parameters\nbatch_size=128 \nnum_archs=50\nsave_dir = tempfile.mkdtemp()\ndownload_link = \"https://github.com/fastestimator-util/fastestimator-misc/raw/master/resource/nasbench201_info.csv\"\n</pre> # Parameters batch_size=128  num_archs=50 save_dir = tempfile.mkdtemp() download_link = \"https://github.com/fastestimator-util/fastestimator-misc/raw/master/resource/nasbench201_info.csv\" In\u00a0[3]: Copied! <pre>wget.callback_progress = callback_progress\n\nwget.download(download_link, save_dir, bar=bar_custom)\nconfig_info = pd.read_csv(os.path.join(save_dir, \"nasbench201_info.csv\"))\n\n# Id's of architectures selected randomly\nuid_list = np.random.choice(15625, size=num_archs, replace=False)\n</pre> wget.callback_progress = callback_progress  wget.download(download_link, save_dir, bar=bar_custom) config_info = pd.read_csv(os.path.join(save_dir, \"nasbench201_info.csv\"))  # Id's of architectures selected randomly uid_list = np.random.choice(15625, size=num_archs, replace=False)  <pre>100% [......................................................]    1.99 / 1.99 MB\n</pre> In\u00a0[4]: Copied! <pre>train_data, _ = cifar10.load_data()\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n    ])\n\nbatch_data = pipeline.get_results()\n</pre> train_data, _ = cifar10.load_data() pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),     ])  batch_data = pipeline.get_results() <pre> FastEstimator-Warn: Consider using the ciFAIR10 dataset instead.\n</pre> In\u00a0[5]: Copied! <pre># Define the operation set\nOPS = {\n    'none':\n    lambda inputs,\n    n_filters,\n    stride: _zero(inputs, n_filters),\n    'avg_pool_3x3':\n    lambda inputs,\n    n_filters,\n    stride: _pooling(inputs, n_filters, stride),\n    'nor_conv_3x3':\n    lambda inputs,\n    n_filters,\n    stride: _relu_conv_bn_block(inputs, n_filters, (3, 3), stride, \"same\", 1),\n    'nor_conv_1x1':\n    lambda inputs,\n    n_filters,\n    stride: _relu_conv_bn_block(inputs, n_filters, (1, 1), stride, \"valid\", 1),\n    'skip_connect':\n    lambda inputs,\n    n_filters,\n    stride: _identity(inputs)\n    if stride == 1 and inputs.shape[-1] == n_filters else _factorize_reduce(inputs, n_filters, stride),\n}\n\n\ndef _resnet_basic_block(inputs, n_filters, stride):\n    assert stride == 1 or stride == 2, 'invalid stride {:}'.format(stride)\n    x = _relu_conv_bn_block(inputs, n_filters, kernel_size=3, stride=stride, padding=\"same\", dilation=1)\n    x = _relu_conv_bn_block(x, n_filters, kernel_size=3, stride=1, padding=\"same\", dilation=1)\n\n    if stride == 2:\n        residual = layers.AveragePooling2D(pool_size=2, strides=stride, padding=\"valid\")(inputs)\n        residual = layers.Conv2D(n_filters, 1, 1, padding=\"valid\", use_bias=False)(residual)\n    elif inputs.shape[-1] != n_filters:\n        residual = _relu_conv_bn_block(inputs, kernel_size=1, stride=1, padding=\"valid\", dilation=1)\n    else:\n        residual = inputs\n\n    return residual + x\n\n\ndef _relu_conv_bn_block(inputs, n_filters, kernel_size, stride, padding, dilation):\n    x = layers.ReLU()(inputs)\n    x = layers.Conv2D(n_filters, kernel_size, stride, padding=padding, dilation_rate=dilation, use_bias=False)(x)\n    x = layers.BatchNormalization(momentum=0.9)(x)\n    return x\n\n\ndef _pooling(inputs, n_filters, stride):\n    if inputs.shape[-1] != n_filters:\n        inputs = _relu_conv_bn_block(inputs, n_filters, kernel_size=1, stride=1, padding=\"valid\", dilation=1)\n\n    x = layers.AveragePooling2D(pool_size=3, strides=stride, padding=\"same\")(inputs)\n    return x\n\n\ndef _identity(inputs):\n    return inputs\n\n\ndef _zero(inputs, n_filters):\n    inp_shape = inputs.shape\n\n    if inp_shape[-1] == n_filters:\n        return 0. * inputs\n    else:\n        inp_shape[-1] = n_filters\n        return tf.zeros(inp_shape, inputs.dtype)\n\n\ndef _factorize_reduce(inputs, n_filters, stride):\n    if stride == 2:\n        filters_list = [n_filters // 2, n_filters - n_filters // 2]\n        x = layers.ReLU()(inputs)\n        y = tf.pad(inputs, [0, 0, 1, 1], mode=\"CONSTANT\")\n        x = layers.Conv2D(filters_list[0], kernel_size=1, stride=stride, padding=\"valid\", use_bias=False)(x)\n        y = layers.Conv2D(filters_list[1], kernel_size=1, stride=stride, padding=\"valid\",\n                          use_bias=False)(y[:, 1:, 1:, :])\n        out = tf.cat([x, y], dim=1)\n    elif stride == 1:\n        out = layers.Conv2D(n_filters, kernel_size=1, stride=stride, padding=\"valid\", use_bias=False)(inputs)\n    else:\n        raise ValueError('Invalid stride : {:}'.format(stride))\n\n    out = layers.BatchNormalization(momentum=0.9)(out)\n    return out\n\n\ndef str2structure(xstr):\n    assert isinstance(xstr, str), 'must take string (not {:}) as input'.format(type(xstr))\n    nodestrs = xstr.split('+')\n    genotypes = []\n    for node_str in nodestrs:\n        inputs = list(filter(lambda x: x != '', node_str.split('|')))\n        for xinput in inputs:\n            assert len(xinput.split('~')) == 2, 'invalid input length : {:}'.format(xinput)\n        inputs = (xi.split('~') for xi in inputs)\n        input_infos = tuple((op, int(IDX)) for (op, IDX) in inputs)\n        genotypes.append(input_infos)\n    return genotypes\n\n\ndef _infer_cell(inputs, genotype, n_filters, stride):\n    x_in = [inputs]\n\n    for i in range(len(genotype)):\n        node_info = genotype[i]\n        if len(node_info) == 1:\n            op_name, op_in = node_info[0]\n            x = OPS[op_name](x_in[op_in], n_filters, stride) if op_in == 0 else OPS[op_name](x_in[op_in], n_filters, 1)\n        else:\n            x = layers.Add()([\n                OPS[op_name](x_in[op_in], n_filters, stride) if op_in == 0 else OPS[op_name](x_in[op_in], n_filters, 1)\n                for (op_name, op_in) in node_info\n            ])\n        x_in.append(x)\n\n    return x\n\n\ndef nasbench_network(input_shape, genotype, C=16, N=5, num_classes=10):\n    layer_channels = [C] * N + [C * 2] + [C * 2] * N + [C * 4] + [C * 4] * N\n    layer_reductions = [False] * N + [True] + [False] * N + [True] + [False] * N\n\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv2D(C, kernel_size=3, padding=\"same\", use_bias=False)(inputs)\n    x = layers.BatchNormalization(momentum=0.9)(x)\n\n    for (C_curr, reduction) in zip(layer_channels, layer_reductions):\n        if reduction:\n            x = _resnet_basic_block(x, n_filters=C_curr, stride=2)\n        else:\n            x = _infer_cell(x, genotype=genotype, n_filters=C_curr, stride=1)\n\n    x = layers.BatchNormalization(momentum=0.9)(x)\n    x = layers.ReLU()(x)\n\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(num_classes, activation=\"softmax\")(x)\n    model = Model(inputs, x)\n    return model\n</pre> # Define the operation set OPS = {     'none':     lambda inputs,     n_filters,     stride: _zero(inputs, n_filters),     'avg_pool_3x3':     lambda inputs,     n_filters,     stride: _pooling(inputs, n_filters, stride),     'nor_conv_3x3':     lambda inputs,     n_filters,     stride: _relu_conv_bn_block(inputs, n_filters, (3, 3), stride, \"same\", 1),     'nor_conv_1x1':     lambda inputs,     n_filters,     stride: _relu_conv_bn_block(inputs, n_filters, (1, 1), stride, \"valid\", 1),     'skip_connect':     lambda inputs,     n_filters,     stride: _identity(inputs)     if stride == 1 and inputs.shape[-1] == n_filters else _factorize_reduce(inputs, n_filters, stride), }   def _resnet_basic_block(inputs, n_filters, stride):     assert stride == 1 or stride == 2, 'invalid stride {:}'.format(stride)     x = _relu_conv_bn_block(inputs, n_filters, kernel_size=3, stride=stride, padding=\"same\", dilation=1)     x = _relu_conv_bn_block(x, n_filters, kernel_size=3, stride=1, padding=\"same\", dilation=1)      if stride == 2:         residual = layers.AveragePooling2D(pool_size=2, strides=stride, padding=\"valid\")(inputs)         residual = layers.Conv2D(n_filters, 1, 1, padding=\"valid\", use_bias=False)(residual)     elif inputs.shape[-1] != n_filters:         residual = _relu_conv_bn_block(inputs, kernel_size=1, stride=1, padding=\"valid\", dilation=1)     else:         residual = inputs      return residual + x   def _relu_conv_bn_block(inputs, n_filters, kernel_size, stride, padding, dilation):     x = layers.ReLU()(inputs)     x = layers.Conv2D(n_filters, kernel_size, stride, padding=padding, dilation_rate=dilation, use_bias=False)(x)     x = layers.BatchNormalization(momentum=0.9)(x)     return x   def _pooling(inputs, n_filters, stride):     if inputs.shape[-1] != n_filters:         inputs = _relu_conv_bn_block(inputs, n_filters, kernel_size=1, stride=1, padding=\"valid\", dilation=1)      x = layers.AveragePooling2D(pool_size=3, strides=stride, padding=\"same\")(inputs)     return x   def _identity(inputs):     return inputs   def _zero(inputs, n_filters):     inp_shape = inputs.shape      if inp_shape[-1] == n_filters:         return 0. * inputs     else:         inp_shape[-1] = n_filters         return tf.zeros(inp_shape, inputs.dtype)   def _factorize_reduce(inputs, n_filters, stride):     if stride == 2:         filters_list = [n_filters // 2, n_filters - n_filters // 2]         x = layers.ReLU()(inputs)         y = tf.pad(inputs, [0, 0, 1, 1], mode=\"CONSTANT\")         x = layers.Conv2D(filters_list[0], kernel_size=1, stride=stride, padding=\"valid\", use_bias=False)(x)         y = layers.Conv2D(filters_list[1], kernel_size=1, stride=stride, padding=\"valid\",                           use_bias=False)(y[:, 1:, 1:, :])         out = tf.cat([x, y], dim=1)     elif stride == 1:         out = layers.Conv2D(n_filters, kernel_size=1, stride=stride, padding=\"valid\", use_bias=False)(inputs)     else:         raise ValueError('Invalid stride : {:}'.format(stride))      out = layers.BatchNormalization(momentum=0.9)(out)     return out   def str2structure(xstr):     assert isinstance(xstr, str), 'must take string (not {:}) as input'.format(type(xstr))     nodestrs = xstr.split('+')     genotypes = []     for node_str in nodestrs:         inputs = list(filter(lambda x: x != '', node_str.split('|')))         for xinput in inputs:             assert len(xinput.split('~')) == 2, 'invalid input length : {:}'.format(xinput)         inputs = (xi.split('~') for xi in inputs)         input_infos = tuple((op, int(IDX)) for (op, IDX) in inputs)         genotypes.append(input_infos)     return genotypes   def _infer_cell(inputs, genotype, n_filters, stride):     x_in = [inputs]      for i in range(len(genotype)):         node_info = genotype[i]         if len(node_info) == 1:             op_name, op_in = node_info[0]             x = OPS[op_name](x_in[op_in], n_filters, stride) if op_in == 0 else OPS[op_name](x_in[op_in], n_filters, 1)         else:             x = layers.Add()([                 OPS[op_name](x_in[op_in], n_filters, stride) if op_in == 0 else OPS[op_name](x_in[op_in], n_filters, 1)                 for (op_name, op_in) in node_info             ])         x_in.append(x)      return x   def nasbench_network(input_shape, genotype, C=16, N=5, num_classes=10):     layer_channels = [C] * N + [C * 2] + [C * 2] * N + [C * 4] + [C * 4] * N     layer_reductions = [False] * N + [True] + [False] * N + [True] + [False] * N      inputs = layers.Input(shape=input_shape)     x = layers.Conv2D(C, kernel_size=3, padding=\"same\", use_bias=False)(inputs)     x = layers.BatchNormalization(momentum=0.9)(x)      for (C_curr, reduction) in zip(layer_channels, layer_reductions):         if reduction:             x = _resnet_basic_block(x, n_filters=C_curr, stride=2)         else:             x = _infer_cell(x, genotype=genotype, n_filters=C_curr, stride=1)      x = layers.BatchNormalization(momentum=0.9)(x)     x = layers.ReLU()(x)      x = layers.GlobalAveragePooling2D()(x)     x = layers.Dense(num_classes, activation=\"softmax\")(x)     model = Model(inputs, x)     return model In\u00a0[6]: Copied! <pre>def score_fn(search_idx, uid, batch_data, config_info):\n    config = config_info.loc[uid, :]\n    nasbench201_model = nasbench_network((32, 32, 3),\n                                     str2structure(config[\"architecture\"]),\n                                     config[\"C\"],\n                                     config[\"N\"],\n                                     10)\n    feature_list = [layer.output for layer in nasbench201_model.layers if \"re_lu\" in layer.name]\n    model = fe.build(model_fn=lambda: Model(nasbench201_model.input, feature_list), optimizer_fn=None)\n    \n    # Only a single forward pass through the network is required\n    relu_result = fe.backend.feed_forward(model, batch_data[\"x\"], training=False)\n    matrix = np.zeros((relu_result[0].shape[0], relu_result[0].shape[0]))\n    for sample in relu_result:\n        sample = to_number(sample)\n        sample = sample.reshape((sample.shape[0], -1))\n        x = (sample &gt; 0.).astype(float)\n        x_t = np.transpose(x)\n        mat = x @ x_t\n        mat2 = (1. - x) @ (1. - x_t)\n        matrix = matrix + mat + mat2\n\n    _, score = np.linalg.slogdet(matrix)\n    return score\n</pre> def score_fn(search_idx, uid, batch_data, config_info):     config = config_info.loc[uid, :]     nasbench201_model = nasbench_network((32, 32, 3),                                      str2structure(config[\"architecture\"]),                                      config[\"C\"],                                      config[\"N\"],                                      10)     feature_list = [layer.output for layer in nasbench201_model.layers if \"re_lu\" in layer.name]     model = fe.build(model_fn=lambda: Model(nasbench201_model.input, feature_list), optimizer_fn=None)          # Only a single forward pass through the network is required     relu_result = fe.backend.feed_forward(model, batch_data[\"x\"], training=False)     matrix = np.zeros((relu_result[0].shape[0], relu_result[0].shape[0]))     for sample in relu_result:         sample = to_number(sample)         sample = sample.reshape((sample.shape[0], -1))         x = (sample &gt; 0.).astype(float)         x_t = np.transpose(x)         mat = x @ x_t         mat2 = (1. - x) @ (1. - x_t)         matrix = matrix + mat + mat2      _, score = np.linalg.slogdet(matrix)     return score In\u00a0[7]: Copied! <pre>search = GridSearch(\n    eval_fn=lambda search_idx,\n    uid: score_fn(search_idx, uid, batch_data=batch_data, config_info=config_info),\n    params={\"uid\": uid_list},\n    best_mode=\"max\")\n    \nsearch.fit()\n</pre> search = GridSearch(     eval_fn=lambda search_idx,     uid: score_fn(search_idx, uid, batch_data=batch_data, config_info=config_info),     params={\"uid\": uid_list},     best_mode=\"max\")      search.fit() <pre>FastEstimator-Search: Evaluated {'uid': 1100, 'search_idx': 1}, score: 1556.5129935894533\nFastEstimator-Search: Evaluated {'uid': 4003, 'search_idx': 2}, score: 1568.7514623128604\nFastEstimator-Search: Evaluated {'uid': 2950, 'search_idx': 3}, score: 1481.2117086672417\nFastEstimator-Search: Evaluated {'uid': 14647, 'search_idx': 4}, score: 1633.212016094945\nFastEstimator-Search: Evaluated {'uid': 11962, 'search_idx': 5}, score: 1380.482954956914\nFastEstimator-Search: Evaluated {'uid': 5807, 'search_idx': 6}, score: 1166.5637417966832\nFastEstimator-Search: Evaluated {'uid': 15514, 'search_idx': 7}, score: 1380.4204749164292\nFastEstimator-Search: Evaluated {'uid': 6745, 'search_idx': 8}, score: 1592.3035197306224\nFastEstimator-Search: Evaluated {'uid': 3991, 'search_idx': 9}, score: 1482.438068951114\nFastEstimator-Search: Evaluated {'uid': 13236, 'search_idx': 10}, score: 1512.2366164324421\nFastEstimator-Search: Evaluated {'uid': 916, 'search_idx': 11}, score: 1575.5872402983732\nFastEstimator-Search: Evaluated {'uid': 9003, 'search_idx': 12}, score: 1539.0675668105275\nFastEstimator-Search: Evaluated {'uid': 10955, 'search_idx': 13}, score: 1572.5983665130975\nFastEstimator-Search: Evaluated {'uid': 10347, 'search_idx': 14}, score: 1557.5255471737103\nFastEstimator-Search: Evaluated {'uid': 14251, 'search_idx': 15}, score: 1632.7727632138717\nFastEstimator-Search: Evaluated {'uid': 9101, 'search_idx': 16}, score: 1551.3728155718209\nFastEstimator-Search: Evaluated {'uid': 5499, 'search_idx': 17}, score: 1102.1256906140627\nFastEstimator-Search: Evaluated {'uid': 4491, 'search_idx': 18}, score: 1508.6663294809982\nFastEstimator-Search: Evaluated {'uid': 4657, 'search_idx': 19}, score: 1432.3689067921841\nFastEstimator-Search: Evaluated {'uid': 3710, 'search_idx': 20}, score: 1508.3969169514805\nFastEstimator-Search: Evaluated {'uid': 5323, 'search_idx': 21}, score: 1541.2049303877486\nFastEstimator-Search: Evaluated {'uid': 11927, 'search_idx': 22}, score: 1601.6277800041032\nFastEstimator-Search: Evaluated {'uid': 1137, 'search_idx': 23}, score: 1532.049670142728\nFastEstimator-Search: Evaluated {'uid': 1154, 'search_idx': 24}, score: 1608.460582154595\nFastEstimator-Search: Evaluated {'uid': 3194, 'search_idx': 25}, score: 1531.9639036551544\nFastEstimator-Search: Evaluated {'uid': 1939, 'search_idx': 26}, score: 1603.0139447279253\nFastEstimator-Search: Evaluated {'uid': 4641, 'search_idx': 27}, score: 1635.8802666171473\nFastEstimator-Search: Evaluated {'uid': 3072, 'search_idx': 28}, score: 1495.7764870183612\nFastEstimator-Search: Evaluated {'uid': 1253, 'search_idx': 29}, score: 1533.0190609769795\nFastEstimator-Search: Evaluated {'uid': 14298, 'search_idx': 30}, score: 1467.477857546112\nFastEstimator-Search: Evaluated {'uid': 15610, 'search_idx': 31}, score: 1498.0951162048214\nFastEstimator-Search: Evaluated {'uid': 8056, 'search_idx': 32}, score: 1377.7824574361225\nFastEstimator-Search: Evaluated {'uid': 1486, 'search_idx': 33}, score: 1562.8503370896894\nFastEstimator-Search: Evaluated {'uid': 13649, 'search_idx': 34}, score: 1498.2865157424108\nFastEstimator-Search: Evaluated {'uid': 9322, 'search_idx': 35}, score: 1485.621136527852\nFastEstimator-Search: Evaluated {'uid': 6271, 'search_idx': 36}, score: 1598.9505055790894\nFastEstimator-Search: Evaluated {'uid': 5461, 'search_idx': 37}, score: 1191.2176526062672\nFastEstimator-Search: Evaluated {'uid': 4927, 'search_idx': 38}, score: 1547.8684340004174\nFastEstimator-Search: Evaluated {'uid': 5549, 'search_idx': 39}, score: 1471.542506810238\nFastEstimator-Search: Evaluated {'uid': 644, 'search_idx': 40}, score: 1515.5682231700168\nFastEstimator-Search: Evaluated {'uid': 1144, 'search_idx': 41}, score: 1608.8126395213196\nFastEstimator-Search: Evaluated {'uid': 4904, 'search_idx': 42}, score: 1417.4096789676787\nFastEstimator-Search: Evaluated {'uid': 1275, 'search_idx': 43}, score: 1626.4676579404968\nFastEstimator-Search: Evaluated {'uid': 1294, 'search_idx': 44}, score: 1481.5673626644962\nFastEstimator-Search: Evaluated {'uid': 13631, 'search_idx': 45}, score: 1524.6186100305747\nFastEstimator-Search: Evaluated {'uid': 6320, 'search_idx': 46}, score: 1425.1757628949863\nFastEstimator-Search: Evaluated {'uid': 15399, 'search_idx': 47}, score: 1626.930802183595\nFastEstimator-Search: Evaluated {'uid': 9554, 'search_idx': 48}, score: 1451.1605301097172\nFastEstimator-Search: Evaluated {'uid': 5825, 'search_idx': 49}, score: 1501.0362045684346\nFastEstimator-Search: Evaluated {'uid': 1452, 'search_idx': 50}, score: 1408.2703409927124\nFastEstimator-Search: Grid Search Finished, best parameters: {'uid': 4641, 'search_idx': 27}, best score: 1635.8802666171473\n</pre> <p>We can the get the best results using the <code>get_best_results</code> and all the results through <code>get_search_results</code> methods. Let's assemble the scores and the test accuracies of the trained networks (available through <code>NAS-Bench-201</code>) for checking how well the scores and final performance correlate.</p> In\u00a0[8]: Copied! <pre>best_results = search.get_best_results()\nscore_list = [result['result']['value'] for result in search.get_search_summary()]\nacc_list = [config_info.loc[i, :][\"accuracy\"] for i in uid_list]\n\ntau, _ = stats.kendalltau(acc_list, score_list)\nprint(\"Kendall's Tau correlation coefficient: \", tau)\n</pre> best_results = search.get_best_results() score_list = [result['result']['value'] for result in search.get_search_summary()] acc_list = [config_info.loc[i, :][\"accuracy\"] for i in uid_list]  tau, _ = stats.kendalltau(acc_list, score_list) print(\"Kendall's Tau correlation coefficient: \", tau) <pre>Kendall's Tau correlation coefficient:  0.6533279434000336\n</pre> In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.scatter(acc_list, score_list)\nplt.title(\"Plot to show correlation between test accuracy and scores on CIFAR10\")\nplt.xlabel(\"Test Accuracy\")\nplt.ylabel(\"Scores\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  plt.scatter(acc_list, score_list) plt.title(\"Plot to show correlation between test accuracy and scores on CIFAR10\") plt.xlabel(\"Test Accuracy\") plt.ylabel(\"Scores\") plt.show() <p>This indicates that there is a noticeable correlation between score for an untrained network and the final accuracy when trained. Let's also check how the network with the best score performed on the actual task as compared to all the other candidates.</p> In\u00a0[10]: Copied! <pre>print(\"Maximum accuracy among all the networks tested: \", np.max(acc_list))\nprint(\"Params for best network: {}, best score: {} and corresponding accuracy: {}\".format(\n    best_results['param'],\n    best_results['result']['value'],\n    config_info.loc[best_results['param'][\"uid\"], :][\"accuracy\"]))\nprint(\n    \"The best network is the top - {} network among the selected networks, based on trained performance (accuracy)\".\n    format(len(acc_list) - list(np.sort(acc_list)).index(config_info.loc[best_results['param'][\"uid\"], :][\"accuracy\"])))\n</pre> print(\"Maximum accuracy among all the networks tested: \", np.max(acc_list)) print(\"Params for best network: {}, best score: {} and corresponding accuracy: {}\".format(     best_results['param'],     best_results['result']['value'],     config_info.loc[best_results['param'][\"uid\"], :][\"accuracy\"])) print(     \"The best network is the top - {} network among the selected networks, based on trained performance (accuracy)\".     format(len(acc_list) - list(np.sort(acc_list)).index(config_info.loc[best_results['param'][\"uid\"], :][\"accuracy\"])))  <pre>Maximum accuracy among all the networks tested:  90.29\nParams for best network: {'uid': 4641, 'search_idx': 27}, best score: 1635.8802666171473 and corresponding accuracy: 89.815\nThe highest scoring network is the top - 3 network among the selected networks, based on trained performance (accuracy)\n</pre>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#neural-architecture-search-without-training-naswot-on-ci10-tensorflow-backend", "title": "Neural Architecture Search without Training (NASWOT) on CI10 (Tensorflow Backend)\u00b6", "text": "<p>In this notebook we will demonstrate how to search for high performing networks by evaluating measures at initialization which are indicative of their trained performance as described in Neural Architecture Search without Training. Cost of hand designing neural networks is very high. To automate this <code>Neural Architecture Search (NAS)</code> methods were devised but they are very slow and expensive. <code>NASWOT</code> allows <code>NAS</code> to be performed without training.</p> <p>NASWOT aims at estimating how well a network can distinguish the input images at initialization. The idea is that the network able to distinguish input images better at initialization has more expressivity and hence ability to have better performance post training. For a neural network with rectified linear units (<code>ReLU</code>s), we can identify when the unit is active (value greater than zero) or inactive (negative value) and use it to create a binary indicator, thereby defining the network by a linear operator. We can use this to generate a binary code (active as <code>1</code> and inactive as <code>0</code>) at each <code>ReLU</code> layer. It\u2019s more difficult for the network to distinguish the inputs with similar binary codes as they lie in the same linear region of the network. Conversely, it\u2019s easier when the binary codes are significantly different. So, NASWOT uses <code>Hamming distance</code> between two binary codes to estimate how dissimilar two inputs are.</p> <p>In the algorithm demonstrated below, we'll score the networks at initialization and observe the correlation between these scores and their final trained performance. We'll use <code>NAS-Bench-201</code> which is search space consisting of architectures and information about their performance.</p>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>Let's download the <code>NAS-Bench-201</code> information. We will randomly select few architectures to evaluate.</p>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We require only one batch of training data for evaluating the networks. Also, we need to ensure same data input to all the networks. So, we'll use <code>get_results</code> method of pipeline to retrieve a batch of data.</p>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>We need to dynamically create a network from the architecture definition string available in <code>NAS-Bench-201</code>. Networks belonging to NAS-Bench-201 have the following structure:  Image Credit: NAS-Bench-201 Paper</p> <ul> <li>Cells are directed acyclic graphs consisting of nodes and edges.</li> <li>Each cell has 4 nodes.</li> <li>Each edge can have one of the 5 predefined edge operations.</li> </ul>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#step-3-define-scoring-function", "title": "Step 3: Define scoring function\u00b6", "text": "<p>Now that we have the network and input data defined, we'll be using <code>FE</code>'s <code>GridSearch</code> to search for the best network according to the score generated through <code>NASWOT</code> method. Let's define the score function to score each network. The score is generated by calculate the log of determinant of the kernel matrix created using hamming distance between the binary codes for a batch of data. For more deatils, please refer to the original paper. Note that in the score function, one of the arguments must be <code>search_idx</code>. This is to help user differentiate multiple search runs</p>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#step-4-apply-gridsearch", "title": "Step 4: Apply <code>GridSearch</code>\u00b6", "text": "<p>Now we can search for the best network using <code>GridSearch</code> API. We call <code>fit</code> method to apply grid search.</p>"}, {"location": "apphub/neural_architecture_search/naswot/naswot.html#results", "title": "Results\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html", "title": "One-Shot Learning using a Siamese Network in FastEstimator", "text": "<p>This notebook demonstrates how to perform one-shot learning using a Siamese Network in FastEstimator.</p> <p>In one-shot learning we classify based on only a single example of each class. This ability to learn from very little data could be useful in many machine learning problems. The details of the method are presented in Siamese neural networks for one-shot image recognition.</p> <p>We will use the Omniglot dataset for training and evaluation. The Omniglot dataset consists of 50 different alphabets split into background (30 alphabets) and evaluation (20 alphabets) sets. Each alphabet has a number of characters, with 20 images for each character.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport os\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\nimport fastestimator as fe\n\nfrom matplotlib import pyplot as plt\n</pre> import tempfile  import os import cv2 import numpy as np  import tensorflow as tf import fastestimator as fe  from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs = 200\nbatch_size = 128\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> # Parameters epochs = 200 batch_size = 128 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import omniglot\n\ntrain_data, eval_data = omniglot.load_data(root_dir=data_dir)\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import omniglot  train_data, eval_data = omniglot.load_data(root_dir=data_dir) test_data = eval_data.split(0.5) <p>For training, batches of data are created with half of the batch consisting of image pairs drawn from the same character, and the other half consisting of image pairs drawn from different characters. The target label is 1 for image pairs from the same character and 0 otherwise. The aim is to learn to quantify similarity between any given pair of images.</p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),\n        ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_a\",\n                             image_out=\"x_a\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_b\",\n                             image_out=\"x_b\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Minmax(inputs=\"x_a\", outputs=\"x_a\"),\n        Minmax(inputs=\"x_b\", outputs=\"x_b\")\n    ])\n</pre> from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import ShiftScaleRotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),         ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),         Sometimes(             ShiftScaleRotate(image_in=\"x_a\",                              image_out=\"x_a\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Sometimes(             ShiftScaleRotate(image_in=\"x_b\",                              image_out=\"x_b\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Minmax(inputs=\"x_a\", outputs=\"x_a\"),         Minmax(inputs=\"x_b\", outputs=\"x_b\")     ]) <p>We can visualize sample images from the <code>Pipeline</code> using the <code>get_results</code> method:</p> In\u00a0[5]: Copied! <pre>sample_batch = pipeline.get_results()\n\npair1_img_a = sample_batch[\"x_a\"][0]\npair1_img_b = sample_batch[\"x_b\"][0]\n\npair2_img_a = sample_batch[\"x_a\"][1]\npair2_img_b = sample_batch[\"x_b\"][1]\n\nif sample_batch[\"y\"][0] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair1_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair1_img_b))\n\nplt.show()\n    \nif sample_batch[\"y\"][1] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair2_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair2_img_b))\n\nplt.show()\n</pre> sample_batch = pipeline.get_results()  pair1_img_a = sample_batch[\"x_a\"][0] pair1_img_b = sample_batch[\"x_b\"][0]  pair2_img_a = sample_batch[\"x_a\"][1] pair2_img_b = sample_batch[\"x_b\"][1]  if sample_batch[\"y\"][0] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair1_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair1_img_b))  plt.show()      if sample_batch[\"y\"][1] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair2_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair2_img_b))  plt.show() <pre>Image pair from same character\n</pre> <pre>Image pair from different characters\n</pre> <p>Our siamese network has two convolutional arms which accept distinct inputs. However, the weights on both these convolutional arms are shared. Each convolutional arm works as a feature extractor which produces a feature vector. L1 component-wise distance between these vectors is computed which is used to classify whether the image pair belongs to the same or different classes (characters).</p> In\u00a0[6]: Copied! <pre>from tensorflow.keras import Model, Sequential, layers\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.regularizers import l2\n    \n\ndef siamese_network(input_shape=(105, 105, 1), classes=1):\n\"\"\"Network Architecture\"\"\"\n    left_input = layers.Input(shape=input_shape)\n    right_input = layers.Input(shape=input_shape)\n\n    #Creating the convnet which shares weights between the left and right legs of Siamese network\n    siamese_convnet = Sequential()\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=64,\n                      kernel_size=10,\n                      strides=1,\n                      input_shape=input_shape,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=7,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=256,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.Flatten())\n\n    siamese_convnet.add(\n        layers.Dense(4096,\n                     activation='sigmoid',\n                     kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                     kernel_regularizer=l2(1e-4),\n                     bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    encoded_left_input = siamese_convnet(left_input)\n    encoded_right_input = siamese_convnet(right_input)\n\n    l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])\n\n    output = layers.Dense(classes,\n                          activation='sigmoid',\n                          kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                          bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)\n\n    return Model(inputs=[left_input, right_input], outputs=output)\n</pre> from tensorflow.keras import Model, Sequential, layers from tensorflow.keras.initializers import RandomNormal from tensorflow.keras.regularizers import l2       def siamese_network(input_shape=(105, 105, 1), classes=1):     \"\"\"Network Architecture\"\"\"     left_input = layers.Input(shape=input_shape)     right_input = layers.Input(shape=input_shape)      #Creating the convnet which shares weights between the left and right legs of Siamese network     siamese_convnet = Sequential()      siamese_convnet.add(         layers.Conv2D(filters=64,                       kernel_size=10,                       strides=1,                       input_shape=input_shape,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=7,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=256,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.Flatten())      siamese_convnet.add(         layers.Dense(4096,                      activation='sigmoid',                      kernel_initializer=RandomNormal(mean=0, stddev=0.2),                      kernel_regularizer=l2(1e-4),                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      encoded_left_input = siamese_convnet(left_input)     encoded_right_input = siamese_convnet(right_input)      l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])      output = layers.Dense(classes,                           activation='sigmoid',                           kernel_initializer=RandomNormal(mean=0, stddev=0.2),                           bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)      return Model(inputs=[left_input, right_input], outputs=output) <p>We now prepare the <code>model</code> and define a <code>Network</code> object.</p> In\u00a0[7]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nmodel = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   model = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")  network = fe.Network(ops=[     ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>LRScheduler with a constant decay schedule as described in the paper.</li> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>EarlyStopping for stopping training if the monitored metric doesn't improve within a specified number of epochs.</li> <li>A custom trace to calculate one shot classification accuracy as described in the paper. This trace performs a 20-way within-alphabet classification task in which an alphabet is first chosen from among those reserved for the evaluation set. Then, nineteen other characters are taken uniformly at random from the alphabet. The first character's image is compared with another image of the same character and with images of the other nineteen characters. This is called a one-shot trial. The trial is considered a success if the network outputs the highest similarity (probability) score for the image pair belonging to same character.</li> </ol> In\u00a0[8]: Copied! <pre>from fastestimator.backend import feed_forward\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.adapt import EarlyStopping, LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import Data\n\n\ndef lr_schedule(epoch):\n\"\"\"Learning rate schedule\"\"\"\n    lr = 0.0001*np.power(0.99, epoch)\n    return lr\n\n\nclass OneShotAccuracy(Trace):\n\"\"\"Trace for calculating one shot accuracy\"\"\"\n    def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):\n\n        super().__init__(mode=mode, outputs=output_name)\n        self.dataset = dataset\n        self.model = model\n        self.total = 0\n        self.correct = 0\n        self.output_name = output_name\n        self.N = N\n        self.trials = trials\n\n    def on_epoch_begin(self, data: Data):\n        self.total = 0\n        self.correct = 0\n\n    def on_epoch_end(self, data: Data):\n        for _ in range(self.trials):\n            img_path = self.dataset.one_shot_trial(self.N)\n            input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                                  dtype=np.float32),\n                         np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                                  dtype=np.float32))\n            prediction_score = feed_forward(self.model, input_img, training=False).numpy()\n\n            if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:\n                self.correct += 1\n\n            self.total += 1\n\n        data.write_with_log(self.outputs[0], self.correct / self.total)\n\n        \ntraces = [\n    LRScheduler(model=model, lr_fn=lr_schedule),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),\n    EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\")\n]\n</pre> from fastestimator.backend import feed_forward from fastestimator.trace import Trace from fastestimator.trace.adapt import EarlyStopping, LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import Data   def lr_schedule(epoch):     \"\"\"Learning rate schedule\"\"\"     lr = 0.0001*np.power(0.99, epoch)     return lr   class OneShotAccuracy(Trace):     \"\"\"Trace for calculating one shot accuracy\"\"\"     def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):          super().__init__(mode=mode, outputs=output_name)         self.dataset = dataset         self.model = model         self.total = 0         self.correct = 0         self.output_name = output_name         self.N = N         self.trials = trials      def on_epoch_begin(self, data: Data):         self.total = 0         self.correct = 0      def on_epoch_end(self, data: Data):         for _ in range(self.trials):             img_path = self.dataset.one_shot_trial(self.N)             input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                                   dtype=np.float32),                          np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                                   dtype=np.float32))             prediction_score = feed_forward(self.model, input_img, training=False).numpy()              if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:                 self.correct += 1              self.total += 1          data.write_with_log(self.outputs[0], self.correct / self.total)           traces = [     LRScheduler(model=model, lr_fn=lr_schedule),     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),     BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),     EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\") ] In\u00a0[9]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) In\u00a0[10]: Copied! <pre># Training\nestimator.fit()\n</pre> # Training estimator.fit() <p>Now, we can load the best model to check its one-shot accuracy on the test set:</p> In\u00a0[11]: Copied! <pre># Testing\nfe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5'))\nestimator.test()\n</pre> # Testing fe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5')) estimator.test() <pre>Loaded model weights from /tmp/tmptw2czltd/siamese_net_best_one_shot_accuracy.h5\nFastEstimator-Test: epoch: 110; accuracy: 0.9256060606060607; one_shot_accuracy: 0.7875; \n</pre> <p>Let's perform inferencing on some elements in the test dataset. Here, we generate a 5-way one shot trial for demo purposes.</p> In\u00a0[12]: Copied! <pre>#Generating one-shot trial set for 5-way one shot trial\nimg_path = test_data.one_shot_trial(5)\ninput_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                      dtype=np.float32),\n             np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                      dtype=np.float32))\n\nprediction_score = feed_forward(model, input_img, training=False).numpy()\n</pre> #Generating one-shot trial set for 5-way one shot trial img_path = test_data.one_shot_trial(5) input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                       dtype=np.float32),              np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                       dtype=np.float32))  prediction_score = feed_forward(model, input_img, training=False).numpy() <p>The test image is predicted to be belonging to the class with the maximum similarity.</p> In\u00a0[13]: Copied! <pre>plt.figure(figsize=(4, 4))\nplt.imshow(np.squeeze(input_img[0][0]));\nplt.title('test image')\nplt.axis('off');\nplt.show()\n\nplt.figure(figsize=(18, 18))\nplt.subplot(151)\nplt.imshow(np.squeeze(input_img[1][0]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0]))\nplt.axis('off');\n\nplt.subplot(152)\nplt.imshow(np.squeeze(input_img[1][1]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0]))\nplt.axis('off');\n\nplt.subplot(153)\nplt.imshow(np.squeeze(input_img[1][2]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0]))\nplt.axis('off');\n\nplt.subplot(154)\nplt.imshow(np.squeeze(input_img[1][3]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0]))\nplt.axis('off');\n\nplt.subplot(155)\nplt.imshow(np.squeeze(input_img[1][4]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0]))\nplt.axis('off');\n\nplt.tight_layout()\n</pre> plt.figure(figsize=(4, 4)) plt.imshow(np.squeeze(input_img[0][0])); plt.title('test image') plt.axis('off'); plt.show()  plt.figure(figsize=(18, 18)) plt.subplot(151) plt.imshow(np.squeeze(input_img[1][0])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0])) plt.axis('off');  plt.subplot(152) plt.imshow(np.squeeze(input_img[1][1])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0])) plt.axis('off');  plt.subplot(153) plt.imshow(np.squeeze(input_img[1][2])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0])) plt.axis('off');  plt.subplot(154) plt.imshow(np.squeeze(input_img[1][3])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0])) plt.axis('off');  plt.subplot(155) plt.imshow(np.squeeze(input_img[1][4])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0])) plt.axis('off');  plt.tight_layout()"}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#one-shot-learning-using-a-siamese-network-in-fastestimator", "title": "One-Shot Learning using a Siamese Network in FastEstimator\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#training-and-testing", "title": "Training and Testing\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html", "title": "Lung Segmentation Using the Montgomery Dataset", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\nfrom typing import Any, Dict, List\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\n\nimport fastestimator as fe\nfrom fastestimator.architecture.pytorch import UNet\nfrom fastestimator.dataset.data import montgomery\nfrom fastestimator.op.numpyop import Delete, NumpyOp\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Dice\nfrom fastestimator.util import ImageDisplay, GridDisplay\n</pre> import os import tempfile from typing import Any, Dict, List  import cv2 import numpy as np import pandas as pd import torch  import fastestimator as fe from fastestimator.architecture.pytorch import UNet from fastestimator.dataset.data import montgomery from fastestimator.op.numpyop import Delete, NumpyOp from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Dice from fastestimator.util import ImageDisplay, GridDisplay In\u00a0[2]: Copied! <pre>pd.set_option('display.max_colwidth', 500)\n</pre> pd.set_option('display.max_colwidth', 500) In\u00a0[3]: parameters Copied! <pre>batch_size = 4\nepochs = 25\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> batch_size = 4 epochs = 25 train_steps_per_epoch = None eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We download the Montgomery data first:</p> In\u00a0[4]: Copied! <pre>csv = montgomery.load_data(root_dir=data_dir)\n</pre> csv = montgomery.load_data(root_dir=data_dir) <p>This creates a <code>CSVDataset</code>. Let's see what is inside:</p> In\u00a0[5]: Copied! <pre>df = pd.DataFrame.from_dict(csv.data, orient='index')\n</pre> df = pd.DataFrame.from_dict(csv.data, orient='index') In\u00a0[6]: Copied! <pre>df.head()\n</pre> df.head() Out[6]: image mask_left mask_right 0 MontgomerySet/CXR_png/MCUCXR_0383_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0383_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0383_1.png 1 MontgomerySet/CXR_png/MCUCXR_0255_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0255_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0255_1.png 2 MontgomerySet/CXR_png/MCUCXR_0016_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0016_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0016_0.png 3 MontgomerySet/CXR_png/MCUCXR_0182_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0182_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0182_1.png 4 MontgomerySet/CXR_png/MCUCXR_0338_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0338_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0338_1.png <p>Now let's set the stage for training:</p> In\u00a0[7]: Copied! <pre>class CombineLeftRightMask(NumpyOp):    \n    def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n        mask_left, mask_right = data\n        data = mask_left + mask_right\n        return data\n</pre> class CombineLeftRightMask(NumpyOp):         def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:         mask_left, mask_right = data         data = mask_left + mask_right         return data In\u00a0[8]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=csv,\n    eval_data=csv.split(0.2),\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),\n        ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),\n        ReadImage(inputs=\"mask_right\",\n                  parent_path=csv.parent_path,\n                  outputs=\"mask_right\",\n                  color_flag=\"gray\",\n                  mode='!infer'),\n        CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),\n        Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),\n        Resize(image_in=\"image\", width=512, height=512),\n        Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),\n        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),\n        Sometimes(numpy_op=Rotate(\n            image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),\n        Minmax(inputs=\"image\", outputs=\"image\"),\n        Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),\n        Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),\n        Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=csv,     eval_data=csv.split(0.2),     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),         ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),         ReadImage(inputs=\"mask_right\",                   parent_path=csv.parent_path,                   outputs=\"mask_right\",                   color_flag=\"gray\",                   mode='!infer'),         CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),         Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),         Resize(image_in=\"image\", width=512, height=512),         Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),         Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),         Sometimes(numpy_op=Rotate(             image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),         Minmax(inputs=\"image\", outputs=\"image\"),         Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),         Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),         Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')     ]) <p>Let's see if the <code>Pipeline</code> output is reasonable. We call <code>get_results</code> to get outputs from <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre>batch_data = pipeline.get_results()\n</pre> batch_data = pipeline.get_results() In\u00a0[10]: Copied! <pre>batch_index = 1\nGridDisplay([ImageDisplay(image=np.squeeze(batch_data['image'][batch_index])), \n             ImageDisplay(image=np.squeeze(batch_data['mask'][batch_index]))\n            ]).show()\n</pre> batch_index = 1 GridDisplay([ImageDisplay(image=np.squeeze(batch_data['image'][batch_index])),               ImageDisplay(image=np.squeeze(batch_data['mask'][batch_index]))             ]).show() In\u00a0[11]: Copied! <pre>model = fe.build(\n    model_fn=lambda: UNet(input_size=(1, 512, 512)),\n    optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n    model_name=\"lung_segmentation\"\n)\n</pre> model = fe.build(     model_fn=lambda: UNet(input_size=(1, 512, 512)),     optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),     model_name=\"lung_segmentation\" ) In\u00a0[12]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),\n    CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),     CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) In\u00a0[13]: Copied! <pre>traces = [\n    Dice(true_key=\"mask\", pred_key=\"pred_segment\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max')\n]\n</pre> traces = [     Dice(true_key=\"mask\", pred_key=\"pred_segment\"),     BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max') ] In\u00a0[14]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         log_steps=20,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          log_steps=20,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) In\u00a0[15]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 20; num_device: 1;\nFastEstimator-Train: step: 1; loss: 0.61223024;\nFastEstimator-Train: step: 20; loss: 0.3766136; steps/sec: 7.9;\nFastEstimator-Train: step: 28; epoch: 1; epoch_time: 11.86 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.82;\nEval Progress: 4/7; steps/sec: 13.23;\nEval Progress: 7/7; steps/sec: 16.75;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 28; epoch: 1; Dice: 0.8265303; loss: 0.2719451; max_Dice: 0.8265303; since_best_Dice: 0;\nFastEstimator-Train: step: 40; loss: 0.14792308; steps/sec: 1.8;\nFastEstimator-Train: step: 56; epoch: 2; epoch_time: 11.99 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 10.29;\nEval Progress: 4/7; steps/sec: 13.7;\nEval Progress: 7/7; steps/sec: 10.47;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 56; epoch: 2; Dice: 0.9142313; loss: 0.12388889; max_Dice: 0.9142313; since_best_Dice: 0;\nFastEstimator-Train: step: 60; loss: 0.22085804; steps/sec: 1.87;\nFastEstimator-Train: step: 80; loss: 0.13877149; steps/sec: 8.15;\nFastEstimator-Train: step: 84; epoch: 3; epoch_time: 11.61 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 4.56;\nEval Progress: 4/7; steps/sec: 11.72;\nEval Progress: 7/7; steps/sec: 18.49;\nFastEstimator-Eval: step: 84; epoch: 3; Dice: 0.8878712; loss: 0.13381886; max_Dice: 0.9142313; since_best_Dice: 1;\nFastEstimator-Train: step: 100; loss: 0.09233445; steps/sec: 1.8;\nFastEstimator-Train: step: 112; epoch: 4; epoch_time: 12.2 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 8.0;\nEval Progress: 4/7; steps/sec: 11.08;\nEval Progress: 7/7; steps/sec: 10.61;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 112; epoch: 4; Dice: 0.93393403; loss: 0.09484797; max_Dice: 0.93393403; since_best_Dice: 0;\nFastEstimator-Train: step: 120; loss: 0.08613084; steps/sec: 1.83;\nFastEstimator-Train: step: 140; loss: 0.06546959; steps/sec: 5.96;\nFastEstimator-Train: step: 140; epoch: 5; epoch_time: 11.91 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.08;\nEval Progress: 4/7; steps/sec: 16.35;\nEval Progress: 7/7; steps/sec: 9.35;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 140; epoch: 5; Dice: 0.9473874; loss: 0.069016606; max_Dice: 0.9473874; since_best_Dice: 0;\nFastEstimator-Train: step: 160; loss: 0.050840445; steps/sec: 1.97;\nFastEstimator-Train: step: 168; epoch: 6; epoch_time: 11.94 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 1.92;\nEval Progress: 4/7; steps/sec: 10.57;\nEval Progress: 7/7; steps/sec: 13.94;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 168; epoch: 6; Dice: 0.9499039; loss: 0.066906914; max_Dice: 0.9499039; since_best_Dice: 0;\nFastEstimator-Train: step: 180; loss: 0.05280122; steps/sec: 1.83;\nFastEstimator-Train: step: 196; epoch: 7; epoch_time: 11.97 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 10.8;\nEval Progress: 4/7; steps/sec: 14.02;\nEval Progress: 7/7; steps/sec: 15.59;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 196; epoch: 7; Dice: 0.9537325; loss: 0.064191505; max_Dice: 0.9537325; since_best_Dice: 0;\nFastEstimator-Train: step: 200; loss: 0.08146733; steps/sec: 1.78;\nFastEstimator-Train: step: 220; loss: 0.06502332; steps/sec: 8.16;\nFastEstimator-Train: step: 224; epoch: 8; epoch_time: 12.21 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 8.1;\nEval Progress: 4/7; steps/sec: 12.47;\nEval Progress: 7/7; steps/sec: 10.71;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 224; epoch: 8; Dice: 0.9608661; loss: 0.053247754; max_Dice: 0.9608661; since_best_Dice: 0;\nFastEstimator-Train: step: 240; loss: 0.04226496; steps/sec: 1.82;\nFastEstimator-Train: step: 252; epoch: 9; epoch_time: 12.04 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 11.38;\nEval Progress: 4/7; steps/sec: 13.69;\nEval Progress: 7/7; steps/sec: 14.5;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 252; epoch: 9; Dice: 0.9623051; loss: 0.049131673; max_Dice: 0.9623051; since_best_Dice: 0;\nFastEstimator-Train: step: 260; loss: 0.03119469; steps/sec: 1.91;\nFastEstimator-Train: step: 280; loss: 0.04104006; steps/sec: 5.95;\nFastEstimator-Train: step: 280; epoch: 10; epoch_time: 11.39 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.46;\nEval Progress: 4/7; steps/sec: 13.65;\nEval Progress: 7/7; steps/sec: 11.05;\nFastEstimator-Eval: step: 280; epoch: 10; Dice: 0.960547; loss: 0.05416977; max_Dice: 0.9623051; since_best_Dice: 1;\nFastEstimator-Train: step: 300; loss: 0.047458448; steps/sec: 1.91;\nFastEstimator-Train: step: 308; epoch: 11; epoch_time: 12.33 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 8.09;\nEval Progress: 4/7; steps/sec: 14.3;\nEval Progress: 7/7; steps/sec: 15.39;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 308; epoch: 11; Dice: 0.9653207; loss: 0.04500082; max_Dice: 0.9653207; since_best_Dice: 0;\nFastEstimator-Train: step: 320; loss: 0.03846242; steps/sec: 1.84;\nFastEstimator-Train: step: 336; epoch: 12; epoch_time: 11.88 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 6.19;\nEval Progress: 4/7; steps/sec: 10.46;\nEval Progress: 7/7; steps/sec: 16.47;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 336; epoch: 12; Dice: 0.9654893; loss: 0.044765513; max_Dice: 0.9654893; since_best_Dice: 0;\nFastEstimator-Train: step: 340; loss: 0.037619226; steps/sec: 1.84;\nFastEstimator-Train: step: 360; loss: 0.035972364; steps/sec: 8.24;\nFastEstimator-Train: step: 364; epoch: 13; epoch_time: 11.82 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 1.54;\nEval Progress: 4/7; steps/sec: 11.95;\nEval Progress: 7/7; steps/sec: 12.2;\nFastEstimator-Eval: step: 364; epoch: 13; Dice: 0.96202266; loss: 0.0538786; max_Dice: 0.9654893; since_best_Dice: 1;\nFastEstimator-Train: step: 380; loss: 0.03339825; steps/sec: 1.8;\nFastEstimator-Train: step: 392; epoch: 14; epoch_time: 11.93 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 1.77;\nEval Progress: 4/7; steps/sec: 10.01;\nEval Progress: 7/7; steps/sec: 13.33;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 392; epoch: 14; Dice: 0.96931; loss: 0.043865643; max_Dice: 0.96931; since_best_Dice: 0;\nFastEstimator-Train: step: 400; loss: 0.031866133; steps/sec: 1.79;\nFastEstimator-Train: step: 420; loss: 0.07556894; steps/sec: 5.8;\nFastEstimator-Train: step: 420; epoch: 15; epoch_time: 12.42 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 2.77;\nEval Progress: 4/7; steps/sec: 14.6;\nEval Progress: 7/7; steps/sec: 14.13;\nFastEstimator-Eval: step: 420; epoch: 15; Dice: 0.9669868; loss: 0.045212775; max_Dice: 0.96931; since_best_Dice: 1;\nFastEstimator-Train: step: 440; loss: 0.06996163; steps/sec: 1.96;\nFastEstimator-Train: step: 448; epoch: 16; epoch_time: 12.0 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 9.26;\nEval Progress: 4/7; steps/sec: 4.09;\nEval Progress: 7/7; steps/sec: 9.05;\nFastEstimator-Eval: step: 448; epoch: 16; Dice: 0.96843016; loss: 0.043515813; max_Dice: 0.96931; since_best_Dice: 2;\nFastEstimator-Train: step: 460; loss: 0.04542666; steps/sec: 1.77;\nFastEstimator-Train: step: 476; epoch: 17; epoch_time: 12.26 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.92;\nEval Progress: 4/7; steps/sec: 16.38;\nEval Progress: 7/7; steps/sec: 13.81;\nFastEstimator-Eval: step: 476; epoch: 17; Dice: 0.9618596; loss: 0.049392406; max_Dice: 0.96931; since_best_Dice: 3;\nFastEstimator-Train: step: 480; loss: 0.10188608; steps/sec: 1.88;\nFastEstimator-Train: step: 500; loss: 0.06218755; steps/sec: 8.36;\nFastEstimator-Train: step: 504; epoch: 18; epoch_time: 11.62 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.89;\nEval Progress: 4/7; steps/sec: 12.78;\nEval Progress: 7/7; steps/sec: 11.01;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 504; epoch: 18; Dice: 0.9720732; loss: 0.03817087; max_Dice: 0.9720732; since_best_Dice: 0;\nFastEstimator-Train: step: 520; loss: 0.08881314; steps/sec: 1.86;\nFastEstimator-Train: step: 532; epoch: 19; epoch_time: 11.7 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.32;\nEval Progress: 4/7; steps/sec: 11.15;\nEval Progress: 7/7; steps/sec: 8.55;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 532; epoch: 19; Dice: 0.97263706; loss: 0.03696952; max_Dice: 0.97263706; since_best_Dice: 0;\nFastEstimator-Train: step: 540; loss: 0.02760274; steps/sec: 1.78;\nFastEstimator-Train: step: 560; loss: 0.031888127; steps/sec: 5.93;\nFastEstimator-Train: step: 560; epoch: 20; epoch_time: 12.29 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.17;\nEval Progress: 4/7; steps/sec: 10.18;\nEval Progress: 7/7; steps/sec: 12.52;\nFastEstimator-Eval: step: 560; epoch: 20; Dice: 0.9725167; loss: 0.036553062; max_Dice: 0.97263706; since_best_Dice: 1;\nFastEstimator-Train: step: 580; loss: 0.02704905; steps/sec: 1.96;\nFastEstimator-Train: step: 588; epoch: 21; epoch_time: 12.08 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.61;\nEval Progress: 4/7; steps/sec: 12.06;\nEval Progress: 7/7; steps/sec: 16.31;\nFastEstimator-Eval: step: 588; epoch: 21; Dice: 0.96258086; loss: 0.05462244; max_Dice: 0.97263706; since_best_Dice: 2;\nFastEstimator-Train: step: 600; loss: 0.047244675; steps/sec: 1.86;\nFastEstimator-Train: step: 616; epoch: 22; epoch_time: 11.64 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 1.21;\nEval Progress: 4/7; steps/sec: 11.61;\nEval Progress: 7/7; steps/sec: 12.71;\nFastEstimator-BestModelSaver: Saved model to /tmp/tmp22fq9j1i/lung_segmentation_best_Dice.pt\nFastEstimator-Eval: step: 616; epoch: 22; Dice: 0.9769085; loss: 0.031764; max_Dice: 0.9769085; since_best_Dice: 0;\nFastEstimator-Train: step: 620; loss: 0.029085426; steps/sec: 1.81;\nFastEstimator-Train: step: 640; loss: 0.03575117; steps/sec: 7.88;\nFastEstimator-Train: step: 644; epoch: 23; epoch_time: 12.17 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 7.77;\nEval Progress: 4/7; steps/sec: 14.03;\nEval Progress: 7/7; steps/sec: 13.28;\nFastEstimator-Eval: step: 644; epoch: 23; Dice: 0.97067636; loss: 0.039316583; max_Dice: 0.9769085; since_best_Dice: 1;\nFastEstimator-Train: step: 660; loss: 0.021667594; steps/sec: 1.84;\nFastEstimator-Train: step: 672; epoch: 24; epoch_time: 11.72 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 4.98;\nEval Progress: 4/7; steps/sec: 7.56;\nEval Progress: 7/7; steps/sec: 12.22;\nFastEstimator-Eval: step: 672; epoch: 24; Dice: 0.9691401; loss: 0.04364162; max_Dice: 0.9769085; since_best_Dice: 2;\nFastEstimator-Train: step: 680; loss: 0.05466278; steps/sec: 1.84;\nFastEstimator-Train: step: 700; loss: 0.034504842; steps/sec: 6.04;\nFastEstimator-Train: step: 700; epoch: 25; epoch_time: 11.94 sec;\nEval Progress: 1/7;\nEval Progress: 2/7; steps/sec: 14.17;\nEval Progress: 4/7; steps/sec: 17.96;\nEval Progress: 7/7; steps/sec: 17.99;\nFastEstimator-Eval: step: 700; epoch: 25; Dice: 0.974748; loss: 0.033991832; max_Dice: 0.9769085; since_best_Dice: 3;\nFastEstimator-Finish: step: 700; lung_segmentation_lr: 0.0001; total_time: 528.8 sec;\n</pre> <p>Let's visualize the prediction from the neural network. We select a random image from the dataset:</p> In\u00a0[16]: Copied! <pre>image_path = df['image'].sample(random_state=3).values[0]\n</pre> image_path = df['image'].sample(random_state=3).values[0] <p>We create a data dict, and call <code>Pipeline.transform()</code>.</p> In\u00a0[17]: Copied! <pre>data = {'image': image_path}\ndata = pipeline.transform(data, mode=\"infer\")\n</pre> data = {'image': image_path} data = pipeline.transform(data, mode=\"infer\") <p>After the <code>Pipeline</code>, we rebuild our model by providing the trained weights path and pass it to a new <code>Network</code>:</p> In\u00a0[18]: Copied! <pre>weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path\n\nmodel = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),\n                 optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n                 model_name=\"lung_segmentation\",\n                 weights_path=weights_path)\n</pre> weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path  model = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),                  optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),                  model_name=\"lung_segmentation\",                  weights_path=weights_path) In\u00a0[19]: Copied! <pre>network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")])\n</pre> network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")]) <p>We call <code>Network.transform()</code> to get outputs from our <code>Network</code>:</p> In\u00a0[20]: Copied! <pre>pred = network.transform(data, mode=\"infer\")\n</pre> pred = network.transform(data, mode=\"infer\") In\u00a0[21]: Copied! <pre>img = np.squeeze(pred['image'].numpy())\nmask = np.squeeze(1.0 * (pred['pred_segment'] &gt; 0.5))\n\nGridDisplay([ImageDisplay(image=img, title=\"Original Lung\"), \n             ImageDisplay(image=img, masks=mask, title=\"Prediction Mask\")]).show()\n</pre> img = np.squeeze(pred['image'].numpy()) mask = np.squeeze(1.0 * (pred['pred_segment'] &gt; 0.5))  GridDisplay([ImageDisplay(image=img, title=\"Original Lung\"),               ImageDisplay(image=img, masks=mask, title=\"Prediction Mask\")]).show()"}, {"location": "apphub/semantic_segmentation/unet/unet.html#lung-segmentation-using-the-montgomery-dataset", "title": "Lung Segmentation Using the Montgomery Dataset\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#download-data", "title": "Download Data\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#pass-the-image-through-pipeline-and-network", "title": "Pass the image through <code>Pipeline</code> and <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#visualize-outputs", "title": "Visualize Outputs\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html", "title": "Fast Style Transfer with FastEstimator", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport cv2\nimport tensorflow as tf\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage\nfrom fastestimator.trace.io import ModelSaver\nfrom fastestimator.util import ImageDisplay, GridDisplay\n\nfrom matplotlib import pyplot as plt\n</pre> import tempfile import cv2 import tensorflow as tf import numpy as np  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop import LambdaOp from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import Normalize, ReadImage from fastestimator.trace.io import ModelSaver from fastestimator.util import ImageDisplay, GridDisplay  from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre>#Parameters\nbatch_size = 4\nepochs = 2\ntrain_steps_per_epoch = None\nlog_steps = 2000\nstyle_weight=5.0\ncontent_weight=1.0\ntv_weight=1e-4\nsave_dir = tempfile.mkdtemp()\nstyle_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg'\ntest_img_path = 'panda.jpeg'\ndata_dir = None\n</pre> #Parameters batch_size = 4 epochs = 2 train_steps_per_epoch = None log_steps = 2000 style_weight=5.0 content_weight=1.0 tv_weight=1e-4 save_dir = tempfile.mkdtemp() style_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg' test_img_path = 'panda.jpeg' data_dir = None <p>In this notebook we will use Vassily Kandinsky's Composition 7 as a style image. We will also resize the style image to $256 \\times 256$ to make the dimension consistent with that of COCO images.</p> In\u00a0[3]: Copied! <pre>style_img = cv2.imread(style_img_path)\nassert style_img is not None, \"cannot load the style image, please go to the folder with style image\"\nstyle_img = cv2.resize(style_img, (256, 256))\nstyle_img = (style_img.astype(np.float32) - 127.5) / 127.5\nstyle_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB)\n\nImageDisplay(image=style_img_disp, title='Vassily Kandinsky\\'s Composition 7').show()\n</pre> style_img = cv2.imread(style_img_path) assert style_img is not None, \"cannot load the style image, please go to the folder with style image\" style_img = cv2.resize(style_img, (256, 256)) style_img = (style_img.astype(np.float32) - 127.5) / 127.5 style_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB)  ImageDisplay(image=style_img_disp, title='Vassily Kandinsky\\'s Composition 7').show() In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\ntrain_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False)\n</pre> from fastestimator.dataset.data import mscoco train_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False) In\u00a0[5]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        Resize(height=256, width=256, image_in=\"image\", image_out=\"image\"),\n        LambdaOp(fn=lambda: style_img, outputs=\"style_image\"),\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         Resize(height=256, width=256, image_in=\"image\", image_out=\"image\"),         LambdaOp(fn=lambda: style_img, outputs=\"style_image\"),     ]) <p>We can visualize sample images from our <code>Pipeline</code> using the 'get_results' method:</p> In\u00a0[6]: Copied! <pre>sample_batch = pipeline.get_results()\nImageDisplay(image=sample_batch[\"image\"][0]).show()\n</pre> sample_batch = pipeline.get_results() ImageDisplay(image=sample_batch[\"image\"][0]).show() In\u00a0[7]: Copied! <pre>from typing import Dict, List, Tuple, Union\n\nimport tensorflow as tf\n\nfrom fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D\n\n\ndef _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x0)\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.Add()([x, x0_cropped])\n    return x\n\n\ndef _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    if apply_relu:\n        x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2DTranspose(filters=num_filter,\n                                        kernel_size=kernel_size,\n                                        strides=strides,\n                                        padding=padding,\n                                        kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):\n\"\"\"Creates the Style Transfer Network.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    x = ReflectionPadding2D(padding=(40, 40))(x0)\n    x = _conv_block(x, num_filter=32)\n    x = _downsample(x, num_filter=64)\n    x = _downsample(x, num_filter=128)\n\n    for _ in range(num_resblock):\n        x = _residual_block(x, num_filter=128)\n\n    x = _upsample(x, num_filter=64)\n    x = _upsample(x, num_filter=32)\n    x = _conv_block(x, num_filter=3, apply_relu=False)\n    x = tf.keras.layers.Activation(\"tanh\")(x)\n    return tf.keras.Model(inputs=x0, outputs=x)\n\n\ndef LossNet(input_shape=(256, 256, 3),\n            style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],\n            content_layers=[\"block3_conv3\"]):\n\"\"\"Creates the network to compute the style loss.\n    This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16\n    for each.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)\n    # Compute style loss\n    style_output = [mdl.get_layer(name).output for name in style_layers]\n    content_output = [mdl.get_layer(name).output for name in content_layers]\n    output = {\"style\": style_output, \"content\": content_output}\n    return tf.keras.Model(inputs=x0, outputs=output)\n</pre> from typing import Dict, List, Tuple, Union  import tensorflow as tf  from fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D   def _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):     initializer = tf.random_normal_initializer(0., 0.02)     x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x0)     x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x)      x = InstanceNormalization()(x)     x = tf.keras.layers.Add()([x, x0_cropped])     return x   def _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     if apply_relu:         x = tf.keras.layers.ReLU()(x)     return x   def _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2DTranspose(filters=num_filter,                                         kernel_size=kernel_size,                                         strides=strides,                                         padding=padding,                                         kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):     \"\"\"Creates the Style Transfer Network.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     x = ReflectionPadding2D(padding=(40, 40))(x0)     x = _conv_block(x, num_filter=32)     x = _downsample(x, num_filter=64)     x = _downsample(x, num_filter=128)      for _ in range(num_resblock):         x = _residual_block(x, num_filter=128)      x = _upsample(x, num_filter=64)     x = _upsample(x, num_filter=32)     x = _conv_block(x, num_filter=3, apply_relu=False)     x = tf.keras.layers.Activation(\"tanh\")(x)     return tf.keras.Model(inputs=x0, outputs=x)   def LossNet(input_shape=(256, 256, 3),             style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],             content_layers=[\"block3_conv3\"]):     \"\"\"Creates the network to compute the style loss.     This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16     for each.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)     # Compute style loss     style_output = [mdl.get_layer(name).output for name in style_layers]     content_output = [mdl.get_layer(name).output for name in content_layers]     output = {\"style\": style_output, \"content\": content_output}     return tf.keras.Model(inputs=x0, outputs=output) In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=StyleTransferNet, \n                 model_name=\"style_transfer_net\",\n                 optimizer_fn=lambda: tf.optimizers.Adam(1e-3))\n</pre> model = fe.build(model_fn=StyleTransferNet,                   model_name=\"style_transfer_net\",                  optimizer_fn=lambda: tf.optimizers.Adam(1e-3)) <pre>2022-05-19 16:39:16.502668: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-19 16:39:17.136446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n</pre> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass ExtractVGGFeatures(TensorOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.vgg = LossNet()\n\n    def forward(self, data, state):\n        return self.vgg(data)\n\n\nclass StyleContentLoss(TensorOp):\n    def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.style_weight = style_weight\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n        self.average_loss = average_loss\n\n    def calculate_style_recon_loss(self, y_true, y_pred):\n        y_true_gram = self.calculate_gram_matrix(y_true)\n        y_pred_gram = self.calculate_gram_matrix(y_pred)\n        y_diff_gram = y_pred_gram - y_true_gram\n        y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))\n        return y_norm\n\n    def calculate_feature_recon_loss(self, y_true, y_pred):\n        y_diff = y_pred - y_true\n        num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)\n        y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts\n        return y_diff_norm\n\n    def calculate_gram_matrix(self, x):\n        x = tf.cast(x, tf.float32)\n        num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)\n        gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)\n        gram_matrix /= num_elts\n        return gram_matrix\n\n    def calculate_total_variation(self, y_pred):\n        return tf.image.total_variation(y_pred)\n\n    def forward(self, data, state):\n        y_pred, y_style, y_content, image_out = data\n\n        style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]\n        style_loss = tf.add_n(style_loss)\n        style_loss *= self.style_weight\n\n        content_loss = [\n            self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])\n        ]\n        content_loss = tf.add_n(content_loss)\n        content_loss *= self.content_weight\n\n        total_variation_reg = self.calculate_total_variation(image_out)\n        total_variation_reg *= self.tv_weight\n        loss = style_loss + content_loss + total_variation_reg\n\n        if self.average_loss:\n            loss = reduce_mean(loss)\n\n        return loss\n</pre> from fastestimator.op.tensorop import TensorOp  class ExtractVGGFeatures(TensorOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs, outputs, mode)         self.vgg = LossNet()      def forward(self, data, state):         return self.vgg(data)   class StyleContentLoss(TensorOp):     def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.style_weight = style_weight         self.content_weight = content_weight         self.tv_weight = tv_weight         self.average_loss = average_loss      def calculate_style_recon_loss(self, y_true, y_pred):         y_true_gram = self.calculate_gram_matrix(y_true)         y_pred_gram = self.calculate_gram_matrix(y_pred)         y_diff_gram = y_pred_gram - y_true_gram         y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))         return y_norm      def calculate_feature_recon_loss(self, y_true, y_pred):         y_diff = y_pred - y_true         num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)         y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts         return y_diff_norm      def calculate_gram_matrix(self, x):         x = tf.cast(x, tf.float32)         num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)         gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)         gram_matrix /= num_elts         return gram_matrix      def calculate_total_variation(self, y_pred):         return tf.image.total_variation(y_pred)      def forward(self, data, state):         y_pred, y_style, y_content, image_out = data          style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]         style_loss = tf.add_n(style_loss)         style_loss *= self.style_weight          content_loss = [             self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])         ]         content_loss = tf.add_n(content_loss)         content_loss *= self.content_weight          total_variation_reg = self.calculate_total_variation(image_out)         total_variation_reg *= self.tv_weight         loss = style_loss + content_loss + total_variation_reg          if self.average_loss:             loss = reduce_mean(loss)          return loss <p>We now define the <code>Network</code> object:</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),\n    ExtractVGGFeatures(inputs=\"style_image\", outputs=\"y_style\"),\n    ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),\n    ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),\n    StyleContentLoss(style_weight=style_weight,\n                     content_weight=content_weight,\n                     tv_weight=tv_weight,\n                     inputs=('y_pred', 'y_style', 'y_content', 'image_out'),\n                     outputs='loss'),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),     ExtractVGGFeatures(inputs=\"style_image\", outputs=\"y_style\"),     ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),     ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),     StyleContentLoss(style_weight=style_weight,                      content_weight=content_weight,                      tv_weight=tv_weight,                      inputs=('y_pred', 'y_style', 'y_content', 'image_out'),                      outputs='loss'),     UpdateOp(model=model, loss_name=\"loss\") ]) <pre>2022-05-19 16:39:18.707513: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),\n                         epochs=epochs,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         log_steps=log_steps)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),                          epochs=epochs,                          train_steps_per_epoch=train_steps_per_epoch,                          log_steps=log_steps) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: the key 'image_id' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>2022-05-19 16:39:27.370274: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n</pre> <pre>FastEstimator-Start: step: 1; logging_interval: 2000; num_device: 1;\nFastEstimator-Train: step: 1; loss: 678.7794;\nFastEstimator-Train: step: 2000; loss: 180.73315; steps/sec: 18.63;\nFastEstimator-Train: step: 4000; loss: 172.81424; steps/sec: 18.74;\nFastEstimator-Train: step: 6000; loss: 158.67215; steps/sec: 18.12;\nFastEstimator-Train: step: 8000; loss: 139.94647; steps/sec: 18.73;\nFastEstimator-Train: step: 10000; loss: 158.4191; steps/sec: 18.24;\nFastEstimator-Train: step: 12000; loss: 162.1073; steps/sec: 18.7;\nFastEstimator-Train: step: 14000; loss: 162.72255; steps/sec: 18.74;\nFastEstimator-Train: step: 16000; loss: 151.46005; steps/sec: 18.48;\nFastEstimator-Train: step: 18000; loss: 158.79842; steps/sec: 18.28;\nFastEstimator-Train: step: 20000; loss: 152.6825; steps/sec: 18.73;\nFastEstimator-Train: step: 22000; loss: 158.96213; steps/sec: 18.18;\nFastEstimator-Train: step: 24000; loss: 156.94514; steps/sec: 18.53;\nFastEstimator-Train: step: 26000; loss: 156.16516; steps/sec: 18.65;\nFastEstimator-Train: step: 28000; loss: 148.0354; steps/sec: 18.73;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpk_g_jiyx/style_transfer_net_epoch_1.h5\nFastEstimator-Train: step: 29317; epoch: 1; epoch_time: 1597.38 sec;\nFastEstimator-Train: step: 30000; loss: 166.97371; steps/sec: 16.12;\nFastEstimator-Train: step: 32000; loss: 164.09729; steps/sec: 17.62;\nFastEstimator-Train: step: 34000; loss: 161.75993; steps/sec: 17.96;\nFastEstimator-Train: step: 36000; loss: 155.41905; steps/sec: 17.96;\nFastEstimator-Train: step: 38000; loss: 149.0742; steps/sec: 17.99;\nFastEstimator-Train: step: 40000; loss: 146.61253; steps/sec: 18.01;\nFastEstimator-Train: step: 42000; loss: 149.72264; steps/sec: 17.89;\nFastEstimator-Train: step: 44000; loss: 149.73695; steps/sec: 18.02;\nFastEstimator-Train: step: 46000; loss: 134.90253; steps/sec: 17.99;\nFastEstimator-Train: step: 48000; loss: 162.61612; steps/sec: 18.02;\nFastEstimator-Train: step: 50000; loss: 143.8054; steps/sec: 18.02;\nFastEstimator-Train: step: 52000; loss: 154.11063; steps/sec: 18.03;\nFastEstimator-Train: step: 54000; loss: 148.56241; steps/sec: 17.99;\nFastEstimator-Train: step: 56000; loss: 161.2628; steps/sec: 18.01;\nFastEstimator-Train: step: 58000; loss: 130.92851; steps/sec: 17.82;\nFastEstimator-ModelSaver: Saved model to /tmp/tmpk_g_jiyx/style_transfer_net_epoch_2.h5\nFastEstimator-Train: step: 58634; epoch: 2; epoch_time: 1645.13 sec;\nFastEstimator-Finish: step: 58634; style_transfer_net_lr: 0.001; total_time: 3242.62 sec;\n</pre> In\u00a0[13]: Copied! <pre>data = {\"image\":test_img_path}\nresult = pipeline.transform(data, mode=\"infer\")\ntest_img = np.squeeze(result[\"image\"])\n</pre> data = {\"image\":test_img_path} result = pipeline.transform(data, mode=\"infer\") test_img = np.squeeze(result[\"image\"]) In\u00a0[14]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=model, outputs=\"image_out\")\n])\n\npredictions = network.transform(result, mode=\"infer\")\noutput_img = np.squeeze(predictions[\"image_out\"])\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=model, outputs=\"image_out\") ])  predictions = network.transform(result, mode=\"infer\") output_img = np.squeeze(predictions[\"image_out\"]) In\u00a0[15]: Copied! <pre>output_img_disp = cv2.cvtColor((output_img + 1) * 0.5, cv2.COLOR_BGR2RGB)\ntest_img_disp = cv2.cvtColor((test_img + 1) * 0.5, cv2.COLOR_BGR2RGB)\n\nGridDisplay([ImageDisplay(image=test_img_disp, title='Original Image'), \n             ImageDisplay(image=style_img_disp, title='Style Image'),\n             ImageDisplay(image=output_img_disp, title='Transferred Image')\n            ]).show()\n</pre> output_img_disp = cv2.cvtColor((output_img + 1) * 0.5, cv2.COLOR_BGR2RGB) test_img_disp = cv2.cvtColor((test_img + 1) * 0.5, cv2.COLOR_BGR2RGB)  GridDisplay([ImageDisplay(image=test_img_disp, title='Original Image'),               ImageDisplay(image=style_img_disp, title='Style Image'),              ImageDisplay(image=output_img_disp, title='Transferred Image')             ]).show()"}, {"location": "apphub/style_transfer/fst_coco/fst.html#fast-style-transfer-with-fastestimator", "title": "Fast Style Transfer with FastEstimator\u00b6", "text": "<p>In this notebook we will demonstrate how to do a neural image style transfer with perceptual loss as described in Perceptual Losses for Real-Time Style Transfer and Super-Resolution. Typical neural style transfer involves two images: an image containing semantics that you want to preserve, and another image serving as a reference style. The first image is often referred as the content image and the other image as the style image. In this paper training images from the COCO2014 dataset are used to learn style transfer from any content image.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download training images from the COCO2014 dataset via our dataset API. Downloading the images will take a while.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model is a modified ResNet:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#defining-loss", "title": "Defining Loss\u00b6", "text": "<p>The perceptual loss described in the paper is computed based on intermediate layers of VGG16 pretrained on ImageNet; specifically, <code>relu1_2</code>, <code>relu2_2</code>, <code>relu3_3</code>, and <code>relu4_3</code> of VGG16 are used.</p> <p>The style loss term is computed as the squared l2 norm of the difference in Gram Matrix of these feature maps between an input image and the reference style image.</p> <p>The content loss is simply the l2 norm of the difference in <code>relu3_3</code> of the input image and the reference style image. In addition, the method also uses total variation loss to enforce spatial smoothness in the output image.</p> <p>The final loss is a weighted sum of the style loss term, the content loss term (feature reconstruction term in the paper), and the total variation term.</p> <p>We first define a custom <code>TensorOp</code> that outputs intermediate layers of VGG16. Given these intermediate layers returned by the loss network as a dictionary, we define a custom <code>StyleContentLoss</code> class that encapsulates all the logic of the loss calculation.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-3-estimator", "title": "Step 3: Estimator\u00b6", "text": "<p>We can now define the <code>Estimator</code>. We will use <code>Trace</code> to save intermediate models:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will apply the model to perform style transfer on arbitrary images. Here we use a photo of a panda.</p>"}, {"location": "apphub/tabular/dnn/dnn.html", "title": "Breast Cancer Detection", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import breast_cancer\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  import tensorflow as tf import pandas as pd from sklearn.preprocessing import StandardScaler  import fastestimator as fe from fastestimator.dataset.data import breast_cancer from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre>#training parameters\nbatch_size = 4\nepochs = 10\nsave_dir = tempfile.mkdtemp()\ntrain_steps_per_epoch = None\neval_steps_per_epoch = None\n</pre> #training parameters batch_size = 4 epochs = 10 save_dir = tempfile.mkdtemp() train_steps_per_epoch = None eval_steps_per_epoch = None <p>This downloads some tabular data with different features stored in numerical format in a table. We then split the data into train, evaluation, and testing data sets.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = breast_cancer.load_data()\ntest_data = eval_data.split(0.5)\n</pre> train_data, eval_data = breast_cancer.load_data() test_data = eval_data.split(0.5) <p>This is what the raw data looks like:</p> In\u00a0[4]: Copied! <pre>df = pd.DataFrame.from_dict(train_data.data, orient='index')\ndf.head()\n</pre> df = pd.DataFrame.from_dict(train_data.data, orient='index') df.head() Out[4]: x y 0 [9.029, 17.33, 58.79, 250.5, 0.1066, 0.1413, 0... 1 1 [21.09, 26.57, 142.7, 1311.0, 0.1141, 0.2832, ... 0 2 [9.173, 13.86, 59.2, 260.9, 0.07721, 0.08751, ... 1 3 [10.65, 25.22, 68.01, 347.0, 0.09657, 0.07234,... 1 4 [10.17, 14.88, 64.55, 311.9, 0.1134, 0.08061, ... 1 In\u00a0[5]: Copied! <pre>scaler = StandardScaler()\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\ntest_data[\"x\"] = scaler.transform(test_data[\"x\"])\n</pre> scaler = StandardScaler() train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) test_data[\"x\"] = scaler.transform(test_data[\"x\"]) <p>We create the <code>Pipeline</code> with the usual train, eval, and test data along with the batch size:</p> In\u00a0[6]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size)\n</pre> pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size) <p>We first define the neural network in a function that can then be passed on to the FastEstimator <code>Network</code>:</p> In\u00a0[7]: Copied! <pre>def create_dnn():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    return model\n</pre> def create_dnn():     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(16, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))     return model In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\")\n])\n</pre> model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\") ]) In\u00a0[9]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         log_steps=10,\n                         traces=traces,\n                         train_steps_per_epoch=train_steps_per_epoch,\n                         eval_steps_per_epoch=eval_steps_per_epoch)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          log_steps=10,                          traces=traces,                          train_steps_per_epoch=train_steps_per_epoch,                          eval_steps_per_epoch=eval_steps_per_epoch) In\u00a0[10]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \nFastEstimator-Train: step: 1; ce: 0.58930933; \nFastEstimator-Train: step: 10; ce: 1.2191963; steps/sec: 342.02; \nFastEstimator-Train: step: 20; ce: 0.6330318; steps/sec: 422.95; \nFastEstimator-Train: step: 30; ce: 0.68403095; steps/sec: 400.86; \nFastEstimator-Train: step: 40; ce: 0.70622563; steps/sec: 277.93; \nFastEstimator-Train: step: 50; ce: 0.7649698; steps/sec: 443.68; \nFastEstimator-Train: step: 60; ce: 0.70189; steps/sec: 455.18; \nFastEstimator-Train: step: 70; ce: 0.6120157; steps/sec: 486.19; \nFastEstimator-Train: step: 80; ce: 0.6461396; steps/sec: 495.01; \nFastEstimator-Train: step: 90; ce: 0.709924; steps/sec: 397.38; \nFastEstimator-Train: step: 100; ce: 0.69695604; steps/sec: 545.18; \nFastEstimator-Train: step: 110; ce: 0.5406225; steps/sec: 587.24; \nFastEstimator-Train: step: 114; epoch: 1; epoch_time: 3.3 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 114; epoch: 1; ce: 0.49621966; min_ce: 0.49621966; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 120; ce: 0.55340487; steps/sec: 7.94; \nFastEstimator-Train: step: 130; ce: 0.31839967; steps/sec: 308.44; \nFastEstimator-Train: step: 140; ce: 0.16889682; steps/sec: 482.01; \nFastEstimator-Train: step: 150; ce: 0.5158031; steps/sec: 407.94; \nFastEstimator-Train: step: 160; ce: 0.6378304; steps/sec: 386.75; \nFastEstimator-Train: step: 170; ce: 1.1647241; steps/sec: 447.16; \nFastEstimator-Train: step: 180; ce: 0.5274984; steps/sec: 500.64; \nFastEstimator-Train: step: 190; ce: 0.68258667; steps/sec: 481.22; \nFastEstimator-Train: step: 200; ce: 0.35559005; steps/sec: 476.14; \nFastEstimator-Train: step: 210; ce: 0.46034247; steps/sec: 508.85; \nFastEstimator-Train: step: 220; ce: 0.95580393; steps/sec: 548.84; \nFastEstimator-Train: step: 228; epoch: 2; epoch_time: 1.39 sec; \nFastEstimator-Eval: step: 228; epoch: 2; ce: 0.3193086; min_ce: 0.3193086; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 230; ce: 0.33260575; steps/sec: 8.49; \nFastEstimator-Train: step: 240; ce: 0.2510308; steps/sec: 232.08; \nFastEstimator-Train: step: 250; ce: 0.2878321; steps/sec: 666.82; \nFastEstimator-Train: step: 260; ce: 0.1154226; steps/sec: 368.11; \nFastEstimator-Train: step: 270; ce: 0.26300237; steps/sec: 414.99; \nFastEstimator-Train: step: 280; ce: 0.5653368; steps/sec: 421.39; \nFastEstimator-Train: step: 290; ce: 0.5872185; steps/sec: 402.68; \nFastEstimator-Train: step: 300; ce: 0.27621573; steps/sec: 440.24; \nFastEstimator-Train: step: 310; ce: 0.5477217; steps/sec: 481.69; \nFastEstimator-Train: step: 320; ce: 0.4602429; steps/sec: 398.85; \nFastEstimator-Train: step: 330; ce: 0.38244748; steps/sec: 546.57; \nFastEstimator-Train: step: 340; ce: 0.5337428; steps/sec: 571.02; \nFastEstimator-Train: step: 342; epoch: 3; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 342; epoch: 3; ce: 0.18308732; min_ce: 0.18308732; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 350; ce: 0.13466343; steps/sec: 8.53; \nFastEstimator-Train: step: 360; ce: 0.22628057; steps/sec: 368.34; \nFastEstimator-Train: step: 370; ce: 0.5836228; steps/sec: 485.06; \nFastEstimator-Train: step: 380; ce: 0.37300625; steps/sec: 409.87; \nFastEstimator-Train: step: 390; ce: 0.2717349; steps/sec: 413.59; \nFastEstimator-Train: step: 400; ce: 0.07554119; steps/sec: 433.84; \nFastEstimator-Train: step: 410; ce: 0.20552614; steps/sec: 439.36; \nFastEstimator-Train: step: 420; ce: 0.28509304; steps/sec: 448.96; \nFastEstimator-Train: step: 430; ce: 0.32158756; steps/sec: 492.58; \nFastEstimator-Train: step: 440; ce: 1.1102628; steps/sec: 525.49; \nFastEstimator-Train: step: 450; ce: 0.31964102; steps/sec: 548.06; \nFastEstimator-Train: step: 456; epoch: 4; epoch_time: 1.4 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 456; epoch: 4; ce: 0.105911165; min_ce: 0.105911165; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 460; ce: 0.4391592; steps/sec: 8.37; \nFastEstimator-Train: step: 470; ce: 0.29870045; steps/sec: 297.42; \nFastEstimator-Train: step: 480; ce: 0.03247342; steps/sec: 597.74; \nFastEstimator-Train: step: 490; ce: 0.13323224; steps/sec: 393.92; \nFastEstimator-Train: step: 500; ce: 0.58429027; steps/sec: 405.0; \nFastEstimator-Train: step: 510; ce: 0.2376658; steps/sec: 455.97; \nFastEstimator-Train: step: 520; ce: 0.4150503; steps/sec: 424.88; \nFastEstimator-Train: step: 530; ce: 0.22695109; steps/sec: 451.62; \nFastEstimator-Train: step: 540; ce: 0.42051294; steps/sec: 402.12; \nFastEstimator-Train: step: 550; ce: 0.17364319; steps/sec: 389.83; \nFastEstimator-Train: step: 560; ce: 0.06320181; steps/sec: 466.97; \nFastEstimator-Train: step: 570; ce: 0.13996354; steps/sec: 518.8; \nFastEstimator-Train: step: 570; epoch: 5; epoch_time: 1.46 sec; \nFastEstimator-Eval: step: 570; epoch: 5; ce: 0.066059396; min_ce: 0.066059396; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 580; ce: 0.12985338; steps/sec: 8.17; \nFastEstimator-Train: step: 590; ce: 0.6419388; steps/sec: 373.15; \nFastEstimator-Train: step: 600; ce: 0.2857446; steps/sec: 404.92; \nFastEstimator-Train: step: 610; ce: 0.21400735; steps/sec: 381.65; \nFastEstimator-Train: step: 620; ce: 0.27899668; steps/sec: 394.87; \nFastEstimator-Train: step: 630; ce: 0.31599885; steps/sec: 472.31; \nFastEstimator-Train: step: 640; ce: 0.036415085; steps/sec: 457.09; \nFastEstimator-Train: step: 650; ce: 0.10052729; steps/sec: 461.82; \nFastEstimator-Train: step: 660; ce: 0.40688303; steps/sec: 474.46; \nFastEstimator-Train: step: 670; ce: 0.40816957; steps/sec: 517.75; \nFastEstimator-Train: step: 680; ce: 0.40120217; steps/sec: 555.53; \nFastEstimator-Train: step: 684; epoch: 6; epoch_time: 1.44 sec; \nFastEstimator-Eval: step: 684; epoch: 6; ce: 0.04396173; min_ce: 0.04396173; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 690; ce: 0.20741543; steps/sec: 8.33; \nFastEstimator-Train: step: 700; ce: 0.12485474; steps/sec: 324.64; \nFastEstimator-Train: step: 710; ce: 2.8970864e-05; steps/sec: 534.71; \nFastEstimator-Train: step: 720; ce: 0.110491954; steps/sec: 402.98; \nFastEstimator-Train: step: 730; ce: 0.10486858; steps/sec: 432.34; \nFastEstimator-Train: step: 740; ce: 0.2951797; steps/sec: 421.45; \nFastEstimator-Train: step: 750; ce: 0.65293443; steps/sec: 433.71; \nFastEstimator-Train: step: 760; ce: 0.32570755; steps/sec: 461.43; \nFastEstimator-Train: step: 770; ce: 0.35400242; steps/sec: 433.46; \nFastEstimator-Train: step: 780; ce: 0.023054674; steps/sec: 483.0; \nFastEstimator-Train: step: 790; ce: 0.16433364; steps/sec: 540.17; \nFastEstimator-Train: step: 798; epoch: 7; epoch_time: 1.43 sec; \nFastEstimator-Eval: step: 798; epoch: 7; ce: 0.040205613; min_ce: 0.040205613; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 800; ce: 0.42427045; steps/sec: 8.52; \nFastEstimator-Train: step: 810; ce: 0.39827985; steps/sec: 266.34; \nFastEstimator-Train: step: 820; ce: 0.43165076; steps/sec: 775.07; \nFastEstimator-Train: step: 830; ce: 0.06976031; steps/sec: 412.9; \nFastEstimator-Train: step: 840; ce: 0.37039524; steps/sec: 441.05; \nFastEstimator-Train: step: 850; ce: 0.10960688; steps/sec: 418.01; \nFastEstimator-Train: step: 860; ce: 0.0070317476; steps/sec: 450.1; \nFastEstimator-Train: step: 870; ce: 0.020452987; steps/sec: 434.06; \nFastEstimator-Train: step: 880; ce: 0.12914097; steps/sec: 476.49; \nFastEstimator-Train: step: 890; ce: 0.25528443; steps/sec: 466.42; \nFastEstimator-Train: step: 900; ce: 0.18017673; steps/sec: 549.95; \nFastEstimator-Train: step: 910; ce: 0.31777602; steps/sec: 583.67; \nFastEstimator-Train: step: 912; epoch: 8; epoch_time: 1.41 sec; \nFastEstimator-Eval: step: 912; epoch: 8; ce: 0.028554583; min_ce: 0.028554583; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 920; ce: 0.24684253; steps/sec: 8.42; \nFastEstimator-Train: step: 930; ce: 0.19438684; steps/sec: 365.05; \nFastEstimator-Train: step: 940; ce: 0.1568121; steps/sec: 477.85; \nFastEstimator-Train: step: 950; ce: 0.3368427; steps/sec: 371.39; \nFastEstimator-Train: step: 960; ce: 0.20518681; steps/sec: 411.72; \nFastEstimator-Train: step: 970; ce: 0.13320616; steps/sec: 401.91; \nFastEstimator-Train: step: 980; ce: 0.1800138; steps/sec: 470.79; \nFastEstimator-Train: step: 990; ce: 0.10868286; steps/sec: 421.41; \nFastEstimator-Train: step: 1000; ce: 0.040300086; steps/sec: 467.66; \nFastEstimator-Train: step: 1010; ce: 0.42622733; steps/sec: 505.31; \nFastEstimator-Train: step: 1020; ce: 0.06701453; steps/sec: 530.27; \nFastEstimator-Train: step: 1026; epoch: 9; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 1026; epoch: 9; ce: 0.019402837; min_ce: 0.019402837; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 1030; ce: 0.27714887; steps/sec: 8.63; \nFastEstimator-Train: step: 1040; ce: 0.074241355; steps/sec: 302.11; \nFastEstimator-Train: step: 1050; ce: 0.025415465; steps/sec: 640.07; \nFastEstimator-Train: step: 1060; ce: 0.21693969; steps/sec: 447.73; \nFastEstimator-Train: step: 1070; ce: 0.120441705; steps/sec: 432.52; \nFastEstimator-Train: step: 1080; ce: 0.25360084; steps/sec: 459.08; \nFastEstimator-Train: step: 1090; ce: 0.22401881; steps/sec: 493.29; \nFastEstimator-Train: step: 1100; ce: 0.112028226; steps/sec: 485.33; \nFastEstimator-Train: step: 1110; ce: 2.4293017; steps/sec: 446.65; \nFastEstimator-Train: step: 1120; ce: 0.19810674; steps/sec: 514.16; \nFastEstimator-Train: step: 1130; ce: 0.12599353; steps/sec: 529.87; \nFastEstimator-Train: step: 1140; ce: 0.23468983; steps/sec: 580.97; \nFastEstimator-Train: step: 1140; epoch: 10; epoch_time: 1.38 sec; \nFastEstimator-Eval: step: 1140; epoch: 10; ce: 0.017586827; min_ce: 0.017586827; since_best: 0; accuracy: 1.0; \nFastEstimator-Finish: step: 1140; total_time: 28.21 sec; model_lr: 0.001; \n</pre> In\u00a0[11]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: epoch: 10; accuracy: 0.9649122807017544; \n</pre>"}, {"location": "apphub/tabular/dnn/dnn.html#breast-cancer-detection", "title": "Breast Cancer Detection\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#download-data", "title": "Download data\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing with the test dataset that was specified in our <code>Pipeline</code>. We can use this to evaluate our model's accuracy on previously unseen data:</p>"}, {"location": "fastestimator/estimator.html", "title": "estimator", "text": ""}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.EarlyStop", "title": "<code>EarlyStop</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>class EarlyStop(Exception):\n\"\"\"An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.\n    This class is intentionally not @traceable.\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator", "title": "<code>Estimator</code>", "text": "<p>One class to rule them all.</p> <p>Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train (estimator.fit) or test (estimator.test) models. It wraps <code>Pipeline</code>, <code>Network</code>, <code>Trace</code> objects together and defines the whole optimization process.</p> <p>If the data fed into pipeline is a TensorFlow Dataset, then the parameters <code>train_steps_per_epoch</code> and <code>eval_steps_per_epoch</code> can only reduce the number of steps per epoch. If these parameters are higher than the dimension of the stated Dataset then the whole Dataset will be used.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>An fe.Pipeline object that defines the data processing workflow.</p> required <code>network</code> <code>BaseNetwork</code> <p>An fe.Network object that contains models and other training graph definitions.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to run.</p> required <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Training will be cut short or extended to complete N steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>eval_steps_per_epoch</code> <code>Optional[int]</code> <p>Evaluation will be cut short or extended to complete N steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>traces</code> <code>Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]]</code> <p>What Traces to run during training. If None, only the system's default Traces will be included.</p> <code>None</code> <code>log_steps</code> <code>Optional[int]</code> <p>Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch information will still print). None to completely disable printing.</p> <code>100</code> <code>eval_log_steps</code> <code>Sequence[int]</code> <p>The list of steps on which evaluation progress logs need to be printed.</p> <code>()</code> <code>monitor_names</code> <code>Union[None, str, Iterable[str]]</code> <p>Additional keys from the data dictionary to be written into the logs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>@traceable()\nclass Estimator:\n\"\"\"One class to rule them all.\n    Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train\n    (estimator.fit) or test (estimator.test) models. It wraps `Pipeline`, `Network`, `Trace` objects together and\n    defines the whole optimization process.\n    If the data fed into pipeline is a TensorFlow Dataset, then the parameters `train_steps_per_epoch` and\n    `eval_steps_per_epoch` can only reduce the number of steps per epoch. If these parameters are higher than the\n    dimension of the stated Dataset then the whole Dataset will be used.\n    Args:\n        pipeline: An fe.Pipeline object that defines the data processing workflow.\n        network: An fe.Network object that contains models and other training graph definitions.\n        epochs: The number of epochs to run.\n        train_steps_per_epoch: Training will be cut short or extended to complete N steps even if loader is not yet\n            exhausted. If None, all data will be used.\n        eval_steps_per_epoch: Evaluation will be cut short or extended to complete N steps even if loader is not yet\n            exhausted. If None, all data will be used.\n        traces: What Traces to run during training. If None, only the system's default Traces will be included.\n        log_steps: Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch\n            information will still print). None to completely disable printing.\n        eval_log_steps: The list of steps on which evaluation progress logs need to be printed.\n        monitor_names: Additional keys from the data dictionary to be written into the logs.\n    \"\"\"\nmonitor_names: Set[str]\ntraces_in_use: List[Union[Trace, Scheduler[Trace]]]\nsystem: System\nfilepath: str\ndef __init__(self,\npipeline: Pipeline,\nnetwork: BaseNetwork,\nepochs: int,\ntrain_steps_per_epoch: Optional[int] = None,\neval_steps_per_epoch: Optional[int] = None,\ntraces: Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]] = None,\nlog_steps: Optional[int] = 100,\neval_log_steps: Sequence[int] = (),\nmonitor_names: Union[None, str, Iterable[str]] = None):\nself.traces_in_use = []\nself.filepath = os.path.realpath(inspect.stack()[2].filename)  # Record this for history tracking\nassert log_steps is None or log_steps &gt;= 0, \\\n            \"log_steps must be None or positive (or 0 to disable only train logging)\"\nself.monitor_names = to_set(monitor_names) | network.get_loss_keys()\nself.system = System(network=network,\npipeline=pipeline,\ntraces=to_list(traces),\nlog_steps=log_steps,\ntotal_epochs=epochs,\ntrain_steps_per_epoch=train_steps_per_epoch,\neval_steps_per_epoch=eval_steps_per_epoch,\neval_log_steps=eval_log_steps,\nsystem_config=self.fe_summary())\n@property\ndef pipeline(self) -&gt; Pipeline:\nreturn self.system.pipeline\n@property\ndef network(self) -&gt; BaseNetwork:\nreturn self.system.network\n@property\ndef traces(self) -&gt; List[Union[Trace, Scheduler[Trace]]]:\nreturn self.system.traces\ndef fit(self, summary: Optional[str] = None, warmup: bool = True, eager: bool = False) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training.\n            warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n                epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n                also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n                mismatch.\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        Returns:\n            A summary object containing the training history for this session iff a `summary` name was provided.\n        \"\"\"\n_verify_dependency_versions()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Prevent tf from constantly printing useless information\ndraw()\nself.system.reset(summary, self.fe_summary())\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup(eager=eager)\nself._start(run_modes={\"train\", \"eval\"}, eager=eager)\nreturn self.system.summary or None\ndef _prepare_traces(self, run_modes: Set[str]) -&gt; None:\n\"\"\"Prepare information about the traces for execution.\n        Add default traces into the traces_in_use list, also prints a warning if no model saver trace is detected.\n        Args:\n            run_modes: The current execution modes.\n        \"\"\"\nself.traces_in_use = [trace for trace in self.traces]\nif self.system.log_steps is not None:\nself.traces_in_use.append(Logger())\n# Look for any monitor names which should be automagically added.\ntrace_outputs = set()\nextra_monitor_keys = set()\nfor trace in sort_traces(get_current_items(self.traces_in_use, run_modes=run_modes), ds_ids=[]):\ntrace_outputs.update(trace.get_outputs(ds_ids=[]))\nextra_monitor_keys.update(trace.fe_monitor_names - trace_outputs)\n# Add the essential traces\nif \"train\" in run_modes:\nself.traces_in_use.insert(0, TrainEssential(monitor_names=self.monitor_names.union(extra_monitor_keys)))\nno_save_warning = True\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\nif isinstance(trace, (ModelSaver, BestModelSaver)):\nno_save_warning = False\nif no_save_warning:\nprint(\"FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\")\nif \"eval\" in run_modes and \"eval\" in self.pipeline.get_modes():\nself.traces_in_use.insert(1, EvalEssential(monitor_names=self.monitor_names.union(extra_monitor_keys)))\nif \"test\" in run_modes and \"test\" in self.pipeline.get_modes():\nself.traces_in_use.insert(0, TestEssential(monitor_names=self.monitor_names.union(extra_monitor_keys)))\n# insert system instance to trace\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\ntrace.system = self.system\ndef test(self, summary: Optional[str] = None, eager: bool = False) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training. If None, the default value will be whatever `summary` name was\n                most recently provided to this Estimator's .fit() or .test() methods.\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        Returns:\n            A summary object containing the training history for this session iff the `summary` name is not None (after\n            considering the default behavior above).\n        \"\"\"\n_verify_dependency_versions()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Prevent tf from constantly printing useless information\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"}, eager=eager)\nreturn self.system.summary or None\ndef _warmup(self, eager: bool = True) -&gt; None:\n\"\"\"Perform a test run of each pipeline and network signature epoch to make sure that training won't fail later.\n        Traces are not executed in the warmup since they are likely to contain state variables which could become\n        corrupted by running extra steps.\n        Args:\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nall_traces = get_current_items(self.traces_in_use, run_modes={\"train\", \"eval\"})\nsort_traces(all_traces, ds_ids=[])  # This ensures that the traces can sort properly for on_begin and on_end\nmonitor_names = self.monitor_names\nfor mode in self.pipeline.get_modes() - {\"test\"}:\nscheduled_items = self.pipeline.get_scheduled_items(mode) + self.network.get_scheduled_items(\nmode) + self.get_scheduled_items(mode)\nsignature_epochs = get_signature_epochs(scheduled_items, self.system.total_epochs, mode=mode)\nepochs_with_data = self.pipeline.get_epochs_with_data(total_epochs=self.system.total_epochs, mode=mode)\nfor epoch in signature_epochs:\nif epoch not in epochs_with_data:\ncontinue\nds_ids = self.pipeline.get_ds_ids(epoch, mode)\nfor ds_id in ds_ids:\nnetwork_output_keys = self.network.get_all_output_keys(mode, epoch, ds_id=ds_id)\nnetwork_input_keys = self.network.get_effective_input_keys(mode, epoch, ds_id=ds_id)\ntrace_input_keys = set()\ntrace_output_keys = {\"*\"}\ntraces = get_current_items(self.traces_in_use, run_modes=mode, epoch=epoch, ds_id=ds_id)\nfor idx, trace in enumerate(traces):\nif idx &gt; 0:  # ignore TrainEssential and EvalEssential's inputs for unmet requirement checking\ntrace_input_keys.update(trace.inputs)\ntrace_output_keys.update(trace.get_outputs(ds_ids=ds_ids))\n# key checking\nwith self.pipeline(mode=mode,\nepoch=epoch,\nds_id=ds_id,\nsteps_per_epoch=None,\noutput_keys=trace_input_keys - network_output_keys\n| network_input_keys) as loader:\nloader = self._configure_loader(loader)\nwith Suppressor():\nif isinstance(loader, tf.data.Dataset):\nbatch = list(loader.take(1))[0]\nelse:\nbatch = next(iter(loader))\nbatch = self._configure_tensor(loader, batch)\nassert isinstance(batch, dict), \"please make sure data output format is dictionary\"\npipeline_output_keys = to_set(batch.keys())\nmonitor_names = monitor_names - (pipeline_output_keys | network_output_keys)\nunmet_requirements = trace_input_keys - (pipeline_output_keys | network_output_keys\n| trace_output_keys)\nassert not unmet_requirements, \\\n                        \"found missing key(s) during epoch {} mode {} ds_id {}: {}\".format(epoch, mode, ds_id,\nunmet_requirements)\nsort_traces(traces, ds_ids=ds_ids, available_outputs=pipeline_output_keys | network_output_keys)\ntrace_input_keys.update(traces[0].inputs)\nself.network.load_epoch(mode, epoch, ds_id, output_keys=trace_input_keys, warmup=True, eager=eager)\nself.network.run_step(batch)\nself.network.unload_epoch()\nassert not monitor_names, \"found missing key(s): {}\".format(monitor_names)\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in estimator.\n        \"\"\"\nreturn self.traces_in_use\ndef _start(self, run_modes: Set[str], eager: bool) -&gt; None:\n\"\"\"The outer training loop.\n        This method invokes the trace on_begin method, runs the necessary 'train' and 'eval' epochs, and then invokes\n        the trace on_end method.\n        Args:\n            run_modes: The current execution modes.\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nall_traces = sort_traces(get_current_items(self.traces_in_use, run_modes=run_modes), ds_ids=[])\nwith NonContext() if fe.fe_history_path is False else HistoryRecorder(\nself.system, self.filepath, db_path=fe.fe_history_path):\ntry:\nself._run_traces_on_begin(traces=all_traces)\nif \"train\" in run_modes or \"eval\" in run_modes:\n# If the training is re-starting from a restore wizard, it should re-run the last eval epoch\nif self.system.epoch_idx &gt; 0 and \"eval\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"eval\"\nself._run_epoch(eager=eager)\nfor self.system.epoch_idx in range(self.system.epoch_idx + 1, self.system.total_epochs + 1):\nif \"train\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"train\"\nself._run_epoch(eager=eager)\nif \"eval\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"eval\"\nself._run_epoch(eager=eager)\nelse:\nself._run_epoch(eager=eager)\nexcept EarlyStop:\npass  # On early stopping we still want to run the final traces and return results\nself._run_traces_on_end(traces=all_traces)\ndef _run_epoch(self, eager: bool) -&gt; None:\n\"\"\"A method to perform an epoch of activity.\n        This method requires that the current mode and epoch already be specified within the self.system object.\n        Args:\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nds_ids = self.pipeline.get_ds_ids(self.system.epoch_idx, self.system.mode)\nepoch_traces = sort_traces(\nget_current_items(self.traces_in_use, run_modes=self.system.mode, epoch=self.system.epoch_idx),\nds_ids=ds_ids)\nself._run_traces_on_epoch_begin(traces=epoch_traces)\nself.system.batch_idx = None\nend_epoch_data = Data()  # We will aggregate data over on_ds_end and put it into on_epoch_end for printing\n# run for each dataset\nfor self.system.ds_id in ds_ids:\nds_traces = get_current_items(self.traces_in_use,\nrun_modes=self.system.mode,\nepoch=self.system.epoch_idx,\nds_id=self.system.ds_id)\ntrace_input_keys = set()\nfor ds_trace in ds_traces:\ntrace_input_keys.update(ds_trace.inputs)\nnetwork_input_keys = self.network.get_effective_input_keys(mode=self.system.mode,\nepoch=self.system.epoch_idx,\nds_id=self.system.ds_id)\nnetwork_output_keys = self.network.get_all_output_keys(mode=self.system.mode,\nepoch=self.system.epoch_idx,\nds_id=self.system.ds_id)\nself.network.load_epoch(mode=self.system.mode,\nepoch=self.system.epoch_idx,\nds_id=self.system.ds_id,\noutput_keys=trace_input_keys,\neager=eager)\nwith self.pipeline(mode=self.system.mode,\nepoch=self.system.epoch_idx,\nds_id=self.system.ds_id,\nsteps_per_epoch=self.system.steps_per_epoch,\noutput_keys=trace_input_keys - network_output_keys | network_input_keys) as loader:\nif self.system.mode == 'eval':\nlog_steps_per_epoch = len(loader) // loader.get_batch_size(\n) if not self.system.steps_per_epoch else self.system.steps_per_epoch\nself.system.eval_log_steps = [\n1, log_steps_per_epoch // 3, (2 * log_steps_per_epoch) // 3, log_steps_per_epoch\n] if not self.system.eval_log_steps else self.system.eval_log_steps\nloader = self._configure_loader(loader)\niterator = iter(loader)\nwith Suppressor():\nbatch = next(iterator)\nds_traces = sort_traces(ds_traces,\navailable_outputs=to_set(batch.keys()) | network_output_keys,\nds_ids=ds_ids)\nper_ds_traces = [trace for trace in ds_traces if isinstance(trace, PerDSTrace)]\nself._run_traces_on_ds_begin(traces=per_ds_traces)\nwhile True:\ntry:\nif self.system.mode == \"train\":\nself.system.update_global_step()\nself.system.update_batch_idx()\nbatch = self._configure_tensor(loader, batch)\nself._run_traces_on_batch_begin(batch, traces=ds_traces)\nbatch, prediction = self.network.run_step(batch)\nself._run_traces_on_batch_end(batch, prediction, traces=ds_traces)\nif isinstance(loader, DataLoader) and (\n(self.system.batch_idx == self.system.train_steps_per_epoch and self.system.mode == \"train\")\nor\n(self.system.batch_idx == self.system.eval_steps_per_epoch and self.system.mode == \"eval\")):\nraise StopIteration\nwith Suppressor():\nbatch = next(iterator)\nexcept StopIteration:\nbreak\nself._run_traces_on_ds_end(traces=per_ds_traces, data=end_epoch_data)\nself.network.unload_epoch()\nself._run_traces_on_epoch_end(traces=epoch_traces, data=end_epoch_data)\ndef _configure_loader(self, loader: Union[DataLoader, tf.data.Dataset]) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"A method to configure a given dataloader for use with this Estimator's Network.\n        This method will ensure that the `loader` returns the correct data type (tf.Tensor or torch.Tensor) depending on\n         the requirements of the Network. It also handles issues with multi-gpu data sharding.\n        Args:\n            loader: A data loader to be modified.\n        Returns:\n            The potentially modified dataloader to be used for training.\n        \"\"\"\nnew_loader = loader\nif isinstance(new_loader, DataLoader) and isinstance(self.network, TFNetwork):\nadd_batch = bool(new_loader.batch_size)\nif hasattr(loader, 'fe_postprocess_fn') and loader.fe_postprocess_fn is not None:\n# The user is manually batching data and running ops on data batches. No reliable way to shortcut this\n# since ops might require specific batch composition.\ndata_instance = next(iter(loader))\nadd_batch = False\nelse:\n# No batch-based ops so we can try and just use the OpDataset to more quickly get our data summary\ndata_instance = loader.dataset[0]\nif isinstance(data_instance, list):\n# This is a batched dataset\ndata_instance = data_instance[0]\nadd_batch = True\nif isinstance(data_instance, FilteredData):\n# We got unlucky and drew filtered data as the zeroth element. Fall back to a slower but more robust\n# analysis of the batch\ndata_instance = next(iter(loader))\nadd_batch = False\ndata_instance = to_tensor(data_instance, target_type=\"tf\")\ndata_type = to_type(data_instance)\ndata_shape = to_shape(data_instance, add_batch=add_batch, exact_shape=False)\nnew_loader = tf.data.Dataset.from_generator(lambda: loader, data_type, output_shapes=data_shape)\nnew_loader = new_loader.prefetch(1)\nif isinstance(new_loader, tf.data.Dataset):\nif self.system.train_steps_per_epoch and self.system.mode == \"train\":\nnew_loader = new_loader.take(self.system.train_steps_per_epoch)\nif self.system.eval_steps_per_epoch and self.system.mode == \"eval\":\nnew_loader = new_loader.take(self.system.eval_steps_per_epoch)\nif isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy) and isinstance(\nself.network, TFNetwork) and not isinstance(new_loader, DistributedDataset):\n# The default autoshard policy is file, changing it to data to avoid warning\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\nnew_loader = new_loader.with_options(options)\nnew_loader = tf.distribute.get_strategy().experimental_distribute_dataset(new_loader)\nreturn new_loader\ndef _configure_tensor(self, loader: Union[DataLoader, tf.data.Dataset], batch: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"A function to convert a batch of tf.Tensors to torch.Tensors if required.\n        Returns:\n            Either the original `batch`, or the `batch` converted to torch.Tensors if required.\n        \"\"\"\n# TODO - if user has torch loader but custom collate that doesn't return torch tensor, need to cast here\nif isinstance(loader, tf.data.Dataset) and isinstance(self.network, TorchNetwork):\nbatch = to_tensor(batch, target_type=\"torch\")\nreturn batch\ndef _run_traces_on_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nrestore = None\nfor trace in traces:\n# Delay RestoreWizard until the end so that it can overwrite everyone's on_begin methods\nif isinstance(trace, RestoreWizard):\nrestore = trace\ncontinue\n# Restore does need to run before the logger though\nif isinstance(trace, Logger) and restore:\nrestore.on_begin(data)\nrestore = None\ntrace.on_begin(data)\nif restore:\nrestore.on_begin(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_epoch_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_epoch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_ds_begin(self, traces: Iterable[PerDSTrace]) -&gt; None:\n\"\"\"Invoke the on_ds_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_ds_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_begin(self, batch: Dict[str, Any], traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_begin methods of given traces.\n        Args:\n            batch: The batch data which was provided by the pipeline.\n            traces: List of traces.\n        \"\"\"\ndata = Data(batch)\nfor trace in traces:\ntrace.on_batch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_end(self, batch: Dict[str, Any], prediction: Dict[str, Any],\ntraces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_end methods of given traces.\n        Args:\n            batch: The batch data which was provided by the pipeline.\n            prediction: The prediction data which was generated by the network.\n            traces: List of traces.\n        \"\"\"\ndata = Data(ChainMap(prediction, batch))\nfor trace in traces:\ntrace.on_batch_end(data)\nself._check_early_exit()\ndef _run_traces_on_ds_end(self, traces: Iterable[PerDSTrace], data: Data) -&gt; None:\n\"\"\"Invoke the on_ds_begin methods of given traces.\n        Args:\n            traces: List of traces.\n            data: Data into which to record results.\n        \"\"\"\nfor trace in traces:\ntrace.on_ds_end(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_end(self, traces: Iterable[Trace], data: Data) -&gt; None:\n\"\"\"Invoke the on_epoch_end methods of of given traces.\n        Args:\n            traces: List of traces.\n            data: Data into which to record results.\n        \"\"\"\nfor trace in traces:\ntrace.on_epoch_end(data)\nself._check_early_exit()\n@staticmethod\ndef _run_traces_on_end(traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_end methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\ntraceability = None\nfor trace in traces:\nif isinstance(trace, Traceability):\n# Delay traceability until the end so that it can capture all data including the total training time\ntraceability = trace\ncontinue\ntrace.on_end(data)\nif traceability:\ntraceability.on_end(data)\ndef _check_early_exit(self) -&gt; None:\n\"\"\"Determine whether training should be prematurely aborted.\n        Raises:\n            EarlyStop: If the system.stop_training flag has been set to True.\n        \"\"\"\nif self.system.stop_training:\nraise EarlyStop\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.fit", "title": "<code>fit</code>", "text": "<p>Train the network for the number of epochs specified by the estimator's constructor.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to perform warmup before training begins. The warmup procedure will test one step at every epoch where schedulers cause the execution graph to change. This can take some time up front, but can also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size mismatch.</p> <code>True</code> <code>eager</code> <code>bool</code> <p>Whether to run the training in eager mode. This is only related to TensorFlow training because PyTorch by nature is always in eager mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff a <code>summary</code> name was provided.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def fit(self, summary: Optional[str] = None, warmup: bool = True, eager: bool = False) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training.\n        warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n            epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n            also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n            mismatch.\n        eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n            PyTorch by nature is always in eager mode.\n    Returns:\n        A summary object containing the training history for this session iff a `summary` name was provided.\n    \"\"\"\n_verify_dependency_versions()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Prevent tf from constantly printing useless information\ndraw()\nself.system.reset(summary, self.fe_summary())\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup(eager=eager)\nself._start(run_modes={\"train\", \"eval\"}, eager=eager)\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in estimator.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in estimator.\n    \"\"\"\nreturn self.traces_in_use\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.test", "title": "<code>test</code>", "text": "<p>Run the pipeline / network in test mode for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training. If None, the default value will be whatever <code>summary</code> name was most recently provided to this Estimator's .fit() or .test() methods.</p> <code>None</code> <code>eager</code> <code>bool</code> <p>Whether to run the training in eager mode. This is only related to TensorFlow training because PyTorch by nature is always in eager mode.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff the <code>summary</code> name is not None (after</p> <code>Optional[Summary]</code> <p>considering the default behavior above).</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def test(self, summary: Optional[str] = None, eager: bool = False) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training. If None, the default value will be whatever `summary` name was\n            most recently provided to this Estimator's .fit() or .test() methods.\n        eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n            PyTorch by nature is always in eager mode.\n    Returns:\n        A summary object containing the training history for this session iff the `summary` name is not None (after\n        considering the default behavior above).\n    \"\"\"\n_verify_dependency_versions()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Prevent tf from constantly printing useless information\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"}, eager=eager)\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.enable_deterministic", "title": "<code>enable_deterministic</code>", "text": "<p>Invoke to set random seed for deterministic training.</p> <p>The determinism only works for tensorflow &gt;= 2.1 and pytorch &gt;= 1.14, and some model layers don't support.</p> <p>Known failing layers: * tf.keras.layers.UpSampling2D</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed to use for training.</p> required Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def enable_deterministic(seed: int) -&gt; None:\n\"\"\"Invoke to set random seed for deterministic training.\n    The determinism only works for tensorflow &gt;= 2.1 and pytorch &gt;= 1.14, and some model layers don't support.\n    Known failing layers:\n    * tf.keras.layers.UpSampling2D\n    Args:\n        seed: The random seed to use for training.\n    \"\"\"\nfe.fe_deterministic_seed = seed\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = str(1)\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntorch.manual_seed(seed)\ntf.keras.utils.set_random_seed(seed)\ntf.config.experimental.enable_op_determinism()\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.record_history", "title": "<code>record_history</code>", "text": "<p>Change the default location for history tracking.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[bool, str]</code> <p>The path to save experiment histories. Pass True to use the default location of ~/fastestimator_data/history.db. Pass False to disable history tracking.</p> required Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def record_history(path: Union[bool, str]) -&gt; None:\n\"\"\"Change the default location for history tracking.\n    Args:\n        path: The path to save experiment histories. Pass True to use the default location of\n            ~/fastestimator_data/history.db. Pass False to disable history tracking.\n    \"\"\"\nif path in (None, True):\nfe.fe_history_path = None\nelse:\nfe.fe_history_path = path\n</code></pre>"}, {"location": "fastestimator/network.html", "title": "network", "text": ""}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork", "title": "<code>BaseNetwork</code>", "text": "<p>A base class for Network objects.</p> <p>Networks are used to define the computation graph surrounding one or more models during training.</p> <p>Parameters:</p> Name Type Description Default <code>target_type</code> <code>str</code> <p>What tensor type is expected by this network ('torch' or 'tf').</p> required <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The operators to be executed throughout training / testing / inference. These are likely to contain one or more model ops, as well as loss ops and update ops.</p> required <code>postprocessing</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>A collection of NumpyOps to be run on the CPU after all of the normal <code>ops</code> have been executed. Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Mixed precision settings for all models are not the same.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass BaseNetwork:\n\"\"\"A base class for Network objects.\n    Networks are used to define the computation graph surrounding one or more models during training.\n    Args:\n        target_type: What tensor type is expected by this network ('torch' or 'tf').\n        ops: The operators to be executed throughout training / testing / inference. These are likely to contain one or\n            more model ops, as well as loss ops and update ops.\n        postprocessing: A collection of NumpyOps to be run on the CPU after all of the normal `ops` have been executed.\n            Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.\n    Raises:\n        ValueError: Mixed precision settings for all models are not the same.\n    \"\"\"\ndef __init__(\nself,\ntarget_type: str,\ndevice: Optional[torch.device],\nops: Iterable[Union[TensorOp, Scheduler[TensorOp]]],\npostprocessing: Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]] = None\n) -&gt; None:\nself.ops = to_list(ops)\nself.target_type = target_type\nself.device = device\nfor op in get_current_items(self.ops):\nop.build(framework=self.target_type, device=self.device)\nself.models = to_list(_collect_models(ops))\nself.postprocessing = to_list(postprocessing)\nfor pop in self.postprocessing:\nif isinstance(pop, RemoveIf):\nraise ValueError(\"Filtering is currently not supported in network post-processing\")\nif isinstance(pop, Batch):\nraise ValueError(\"Post-processing data is already batched, so Batch Op is not supported here.\")\nself._verify_inputs()\nself.effective_inputs = dict()\nself.effective_outputs = dict()\nself.epoch_ops = []\nself.epoch_postprocessing = []\nself.epoch_models = set()\nself.epoch_state = dict()\nself.mixed_precision = any([model.mixed_precision for model in self.models])\nif self.mixed_precision and not all([model.mixed_precision for model in self.models]):\nraise ValueError(\"Cannot mix full precision and mixed-precision models\")\ndef _verify_inputs(self) -&gt; None:\n\"\"\"Ensure that all ops are TensorOps.\n        Raises:\n            AssertionError: If any of the ops are not TensorOps.\n        \"\"\"\nfor op in get_current_items(self.ops):\nassert isinstance(op, TensorOp), \"unsupported op format, Network ops must be TensorOps\"\nfor op in get_current_items(self.postprocessing):\nassert isinstance(op, NumpyOp), \"unsupported op format, Network postprocessing must be NumpyOps\"\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Network.\n        \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models] + self.postprocessing\nelse:\nall_items = self.ops + self.postprocessing\nreturn all_items\ndef load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch.\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            ds_id: The current dataset id.\n            output_keys: What keys can be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch, ds_id)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch, ds_id)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(\noutput_keys) | self._get_effective_postprocessing_input_keys(mode, epoch, ds_id)\nself.epoch_ops = get_current_items(self.ops, mode, epoch, ds_id=ds_id)\nself.epoch_postprocessing = get_current_items(self.postprocessing, mode, epoch, ds_id=ds_id)\nself.epoch_models = set.union(*[op.get_fe_models() for op in self.epoch_ops])\ngradient_ops = [op for op in self.epoch_ops if op.fe_retain_graph() is not None]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.fe_retain_graph(idx != len(gradient_ops) - 1)\nself.epoch_state = {\n\"warmup\": warmup,\n\"mode\": mode,\n\"req_grad\": len(gradient_ops) &gt; 0,\n\"epoch\": epoch,\n\"deferred\": {},\n\"eager\": eager\n}\n# warmup: bool, mode: str, req_grad: bool, epoch: int, deferred: Dict[str, List[Callable]]]\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        \"\"\"\npass\ndef get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n        Returns:\n            All of the keys associated with model losses in this network.\n        \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nloss_keys |= op.get_fe_loss_keys()\nreturn loss_keys\ndef get_effective_input_keys(self, mode: str, epoch: int, ds_id: str = '') -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider for determining inputs.\n            ds_id: The current dataset id.\n        Returns:\n            The necessary inputs for the network to execute the given `epoch` and `mode`.\n        \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops + self.postprocessing, mode, epoch, ds_id=ds_id):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\ndef _get_effective_postprocessing_input_keys(self, mode: str, epoch: int, ds_id: str = '') -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the postprocessing during the given `epoch`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider for determining inputs.\n            ds_id: The current dataset id.\n        Returns:\n            The necessary inputs for the postprocessing to execute the given `epoch` and `mode`.\n        \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.postprocessing, mode, epoch, ds_id=ds_id):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\ndef get_all_output_keys(self, mode: str, epoch: int, ds_id: str = '') -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider when searching for outputs.\n            ds_id: The current dataset id.\n        Returns:\n            The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n        \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops + self.postprocessing, mode, epoch, ds_id=ds_id):\noutput_keys.update(op.outputs)\nreturn output_keys\n@staticmethod\ndef _forward_batch(batch: MutableMapping[str, Any], state: Dict[str, Any], ops: List[TensorOp]) -&gt; None:\n\"\"\"Run a forward pass through the network's Op chain given a `batch` of data.\n        Args:\n            batch: A batch of input data. Predictions from the network will be written back into this dictionary.\n            state: A dictionary holding information about the current execution context. The TF gradient tape, for\n                example will be stored here.\n            ops: Which ops to execute.\n        \"\"\"\nfor op in ops:\ndata = get_inputs_by_op(op, batch)\ndata = op.forward(data, state)\nif op.outputs:\nwrite_outputs_by_op(op, batch, data)\nfor fn_list in state['deferred'].values():\nfor fn in fn_list:\nfn()\nstate['deferred'].clear()\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data, including postprocessing.\n        This method expects that Network.load_epoch() has already been invoked. The return data will be on the CPU.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nbatch, prediction = self._run_step(batch)\nforward_numpyop(ops=self.epoch_postprocessing,\ndata=ChainMap(prediction, batch),\nstate=self.epoch_state,\nbatched=self.target_type)\nreturn batch, prediction\ndef _run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data, excluding postprocessing.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nraise NotImplementedError\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1, ds_id: str = '') -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n            ds_id: The current dataset id.\n        Returns:\n            prediction_data overlaid on the input `data`.\n        \"\"\"\nself.load_epoch(mode, epoch, ds_id, warmup=False, eager=True)\ndata = to_tensor(data, target_type=self.target_type)\ndata, prediction = self.run_step(data)\nself.unload_epoch()\nreturn {**data, **prediction}\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_all_output_keys", "title": "<code>get_all_output_keys</code>", "text": "<p>Get all of the keys that will be generated by the network during the given <code>epoch</code> and <code>mode</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider when searching for outputs.</p> required <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The keys that will be generated by the network's Ops during the <code>epoch</code> for the given <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_all_output_keys(self, mode: str, epoch: int, ds_id: str = '') -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider when searching for outputs.\n        ds_id: The current dataset id.\n    Returns:\n        The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n    \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops + self.postprocessing, mode, epoch, ds_id=ds_id):\noutput_keys.update(op.outputs)\nreturn output_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_effective_input_keys", "title": "<code>get_effective_input_keys</code>", "text": "<p>Determine which keys need to be provided as input to the network during the given <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider for determining inputs.</p> required <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The necessary inputs for the network to execute the given <code>epoch</code> and <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_effective_input_keys(self, mode: str, epoch: int, ds_id: str = '') -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider for determining inputs.\n        ds_id: The current dataset id.\n    Returns:\n        The necessary inputs for the network to execute the given `epoch` and `mode`.\n    \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops + self.postprocessing, mode, epoch, ds_id=ds_id):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_loss_keys", "title": "<code>get_loss_keys</code>", "text": "<p>Find all of the keys associated with model losses.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>All of the keys associated with model losses in this network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n    Returns:\n        All of the keys associated with model losses in this network.\n    \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nloss_keys |= op.get_fe_loss_keys()\nreturn loss_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Network.\n    \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models] + self.postprocessing\nelse:\nall_items = self.ops + self.postprocessing\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys can be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> <code>eager</code> <code>bool</code> <p>Whether to run the training in eager mode. This is only related to TensorFlow training because PyTorch by nature is always in eager mode.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch.\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        ds_id: The current dataset id.\n        output_keys: What keys can be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n            PyTorch by nature is always in eager mode.\n    \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch, ds_id)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch, ds_id)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(\noutput_keys) | self._get_effective_postprocessing_input_keys(mode, epoch, ds_id)\nself.epoch_ops = get_current_items(self.ops, mode, epoch, ds_id=ds_id)\nself.epoch_postprocessing = get_current_items(self.postprocessing, mode, epoch, ds_id=ds_id)\nself.epoch_models = set.union(*[op.get_fe_models() for op in self.epoch_ops])\ngradient_ops = [op for op in self.epoch_ops if op.fe_retain_graph() is not None]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.fe_retain_graph(idx != len(gradient_ops) - 1)\nself.epoch_state = {\n\"warmup\": warmup,\n\"mode\": mode,\n\"req_grad\": len(gradient_ops) &gt; 0,\n\"epoch\": epoch,\n\"deferred\": {},\n\"eager\": eager\n}\n# warmup: bool, mode: str, req_grad: bool, epoch: int, deferred: Dict[str, List[Callable]]]\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data, including postprocessing.</p> <p>This method expects that Network.load_epoch() has already been invoked. The return data will be on the CPU.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data, including postprocessing.\n    This method expects that Network.load_epoch() has already been invoked. The return data will be on the CPU.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nbatch, prediction = self._run_step(batch)\nforward_numpyop(ops=self.epoch_postprocessing,\ndata=ChainMap(prediction, batch),\nstate=self.epoch_state,\nbatched=self.target_type)\nreturn batch, prediction\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>prediction_data overlaid on the input <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1, ds_id: str = '') -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n        ds_id: The current dataset id.\n    Returns:\n        prediction_data overlaid on the input `data`.\n    \"\"\"\nself.load_epoch(mode, epoch, ds_id, warmup=False, eager=True)\ndata = to_tensor(data, target_type=self.target_type)\ndata, prediction = self.run_step(data)\nself.unload_epoch()\nreturn {**data, **prediction}\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork", "title": "<code>TFNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for TensorFlow models.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The ops defining the execution graph for this Network.</p> required <code>postprocessing</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>A collection of NumpyOps to be run on the CPU after all of the normal <code>ops</code> have been executed. Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass TFNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for TensorFlow models.\n    Args:\n        ops: The ops defining the execution graph for this Network.\n        postprocessing: A collection of NumpyOps to be run on the CPU after all of the normal `ops` have been executed.\n            Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.\n    \"\"\"\ndef __init__(\nself,\nops: Iterable[Union[TensorOp, Scheduler[TensorOp]]],\npostprocessing: Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]] = None\n) -&gt; None:\nsuper().__init__(target_type='tf', device=None, ops=ops, postprocessing=postprocessing)\ndef load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            ds_id: The current dataset id.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nsuper().load_epoch(mode=mode, epoch=epoch, ds_id=ds_id, output_keys=output_keys, warmup=warmup, eager=eager)\n# Don't cause a re-trace just because epoch changed\nself.epoch_state[\"epoch\"] = tf.convert_to_tensor(self.epoch_state[\"epoch\"])\n# Need to re-trace the TF graph if optimizer or layer trainable setting is changing:\ntrainable_str = \"\".join([str(layer.trainable) for model in self.epoch_models for layer in model.layers])\nopt_str = \"x\".join(\n[str(id(model.current_optimizer)) for model in self.epoch_models if hasattr(model, 'current_optimizer')])\nself.epoch_state[\"_force_tf_retrace\"] = hash(trainable_str + opt_str)  # Hash to keep at fixed memory overhead\ndef unload_epoch(self) -&gt; None:\n# This prevents a tf graph memory leak that would slow down long trainings. Since we\n# re-build graphs every epoch there is no reason to keep old ones around.\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nreturn  # TODO - Find a way to clear graph for multi-gpu\nelse:\ntf.keras.backend.clear_session()\ndef _run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nif self.epoch_state[\"eager\"]:\nprediction = strategy.run(\nself._forward_step_eager,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nelse:\nprediction = strategy.run(\nself._forward_step_static,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nbatch = self._per_replica_to_global(batch)\nprediction = self._per_replica_to_global(prediction)\nelse:\nif self.epoch_state[\"eager\"]:\nprediction = self._forward_step_eager(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nelse:\nprediction = self._forward_step_static(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nreturn batch, prediction\ndef _per_replica_to_global(self, data: T) -&gt; T:\n\"\"\"Combine data from \"per-replica\" values recursively.\n        For multi-GPU training, data are distributed using `tf.distribute.Strategy.experimental_distribute_dataset`.\n        This method collects data from all replicas and combines them into one.\n        Args:\n            data: Distributed data.\n        Returns:\n            Combined data from all replicas.\n        \"\"\"\nif isinstance(data, DistributedValues):\nif data.values[0].shape.rank == 0:\nreturn tf.reduce_mean(tuple(d for d in data.values if not tf.math.is_nan(d)))\nelse:\nreturn tf.concat(data.values, axis=0)\nelif isinstance(data, dict):\nresult = {}\nfor key, val in data.items():\nresult[key] = self._per_replica_to_global(val)\nreturn result\nelif isinstance(data, list):\nreturn [self._per_replica_to_global(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._per_replica_to_global(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._per_replica_to_global(val) for val in data])\nelse:\nreturn data\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Filter input data so that only the data required by the Network is moved onto the GPU.\n        Args:\n            batch: An unfiltered batch of input data.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The filtered input data ready for use on GPU(s).\n        \"\"\"\nnew_batch = {}\nfor key in self.effective_inputs[mode]:\nif key in batch:\nnew_batch[key] = batch[key]\nreturn new_batch\ndef _forward_step_eager(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in eager (non-static graph) mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = ChainMap({}, batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\n@tf.function(reduce_retracing=True)\ndef _forward_step_static(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in static graph mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = dict(batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1, ds_id: str = '') -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n            ds_id: The current dataset id.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\n# Distribute multi-gpu data for processing\nsub_sample = False\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nbatch_size, num_devices = get_batch_size(data), strategy.num_replicas_in_sync\nif batch_size &lt; num_devices:\ndata = self._fill_batch(data, num_devices - batch_size)\nsub_sample = True\ndata = next(iter(strategy.experimental_distribute_dataset(tf.data.Dataset.from_tensors(data))))\nresults = super().transform(data, mode, epoch, ds_id=ds_id)\nif sub_sample:\nresults = self._subsample_data(results, batch_size)\nreturn results\ndef _fill_batch(self, data: T, n: int) -&gt; T:\n\"\"\"Fill data on batch dimension repeating the first n indices at the end.\n        Args:\n            data: The data to be filled.\n            n: The number of times to be repeated.\n        Returns:\n            Filled data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._fill_batch(val, n) for (key, val) in data.items()}\nelif isinstance(data, list):\nreturn [self._fill_batch(val, n) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._fill_batch(val, n) for val in data])\nelif isinstance(data, set):\nreturn set([self._fill_batch(val, n) for val in data])\nelif hasattr(data, \"shape\"):\npaddings = [[0, n]] + [[0, 0] for _ in range(len(data.shape) - 1)]\nreturn np.pad(data, pad_width=paddings, mode=\"symmetric\")\nelse:\nreturn data\ndef _subsample_data(self, data: T, n: int) -&gt; T:\n\"\"\"Subsample data by selecting the first n indices recursively.\n        Args:\n            data: The data to be subsampled.\n            n: The number of indices to be subsampled.\n        Returns:\n            Subsampled data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._subsample_data(val, n) for (key, val) in data.items()}\nelif isinstance(data, list):\nreturn [self._subsample_data(val, n) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._subsample_data(val, n) for val in data])\nelif isinstance(data, set):\nreturn set([self._subsample_data(val, n) for val in data])\nelif hasattr(data, \"shape\") and list(data.shape) and data.shape[0] &gt; n:\nreturn data[0:n]\nelse:\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> <code>eager</code> <code>bool</code> <p>Whether to run the training in eager mode. This is only related to TensorFlow training because PyTorch by nature is always in eager mode.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch. This also converts the epoch index a tensor to avoid tensorflow graph rebuilding.\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        ds_id: The current dataset id.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n            PyTorch by nature is always in eager mode.\n    \"\"\"\nsuper().load_epoch(mode=mode, epoch=epoch, ds_id=ds_id, output_keys=output_keys, warmup=warmup, eager=eager)\n# Don't cause a re-trace just because epoch changed\nself.epoch_state[\"epoch\"] = tf.convert_to_tensor(self.epoch_state[\"epoch\"])\n# Need to re-trace the TF graph if optimizer or layer trainable setting is changing:\ntrainable_str = \"\".join([str(layer.trainable) for model in self.epoch_models for layer in model.layers])\nopt_str = \"x\".join(\n[str(id(model.current_optimizer)) for model in self.epoch_models if hasattr(model, 'current_optimizer')])\nself.epoch_state[\"_force_tf_retrace\"] = hash(trainable_str + opt_str)  # Hash to keep at fixed memory overhead\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1, ds_id: str = '') -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n        ds_id: The current dataset id.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\n# Distribute multi-gpu data for processing\nsub_sample = False\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nbatch_size, num_devices = get_batch_size(data), strategy.num_replicas_in_sync\nif batch_size &lt; num_devices:\ndata = self._fill_batch(data, num_devices - batch_size)\nsub_sample = True\ndata = next(iter(strategy.experimental_distribute_dataset(tf.data.Dataset.from_tensors(data))))\nresults = super().transform(data, mode, epoch, ds_id=ds_id)\nif sub_sample:\nresults = self._subsample_data(results, batch_size)\nreturn results\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork", "title": "<code>TorchNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for PyTorch models.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The ops defining the execution graph for this Network.</p> required <code>postprocessing</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>A collection of NumpyOps to be run on the CPU after all of the normal <code>ops</code> have been executed. Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>@traceable()\nclass TorchNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for PyTorch models.\n    Args:\n        ops: The ops defining the execution graph for this Network.\n        postprocessing: A collection of NumpyOps to be run on the CPU after all of the normal `ops` have been executed.\n            Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.\n    \"\"\"\ndef __init__(\nself,\nops: Iterable[Union[TensorOp, Scheduler[TensorOp]]],\npostprocessing: Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]] = None\n) -&gt; None:\nsuper().__init__(target_type='torch',\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\nops=ops,\npostprocessing=postprocessing)\ndef load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            ds_id: The current dataset id.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n            eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n                PyTorch by nature is always in eager mode.\n        \"\"\"\nsuper().load_epoch(mode=mode, epoch=epoch, ds_id=ds_id, output_keys=output_keys, warmup=warmup, eager=eager)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\nif model.current_optimizer and mode == \"train\":\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\n# Set all of the contiguous final updates to defer their updates by default to enable things like CycleGan\n# This is not necessary for TF because overriding tf weights does not confuse the gradient tape computation\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop._old_defer = op.defer\nop.defer = True\nelse:\nbreak\ndef _move_optimizer_between_device(self, data: Dict[str, Any], device: Union[str, torch.device]) -&gt; None:\n\"\"\"Move optimizer state between gpu and cpu recursively.\n        Args:\n            data: Optimizer state.\n            device: The target device.\n        \"\"\"\nfor key in data:\nif isinstance(data[key], dict):\nself._move_optimizer_between_device(data[key], device)\nelse:\ntry:\ndata[key] = data[key].to(device)\nexcept (RuntimeError, AssertionError, AttributeError):\npass\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        In this case we move all of the models from the GPU(s) back to the CPU.\n        \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\nif model.current_optimizer and self.epoch_state[\"mode\"] == \"train\":\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\n# Set the final update ops back to their original defer status\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop.defer = op.__dict__.get('_old_defer', op.defer)\nelse:\nbreak\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Copy input data from the the CPU onto the GPU(s).\n        This method will filter inputs from the batch so that only data required by the network during execution will be\n        copied to the GPU.\n        Args:\n            batch: The input data to be moved.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The input data ready for use on GPU(s).\n        \"\"\"\nif self.device.type == \"cuda\":\nnew_batch = {\nkey: self._move_tensor_between_device(batch[key], self.device)\nfor key in self.effective_inputs[mode] if key in batch\n}\nelse:\nnew_batch = {key: batch[key] for key in self.effective_inputs[mode] if key in batch}\nreturn new_batch\ndef _run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nself.epoch_state[\"tape\"] = NonContext()\n# gpu operation\nwith torch.no_grad() if not self.epoch_state[\"req_grad\"] else NonContext():\nwith torch.cuda.amp.autocast() if self.mixed_precision else NonContext():\nself._forward_batch(batch_in, self.epoch_state, self.epoch_ops)\n# copy data to cpu\nif self.device.type == \"cuda\":\nprediction = {\nkey: self._move_tensor_between_device(self._detach_tensor(batch_in[key]), \"cpu\")\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nelse:\nprediction = {\nkey: self._detach_tensor(batch_in[key])\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nreturn batch, prediction\ndef _move_tensor_between_device(self, data: T, device: Union[str, torch.device]) -&gt; T:\n\"\"\"Move tensor between gpu and cpu recursively.\n        Args:\n            data: The input data to be moved.\n            device: The target device.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._move_tensor_between_device(value, device) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._move_tensor_between_device(val, device) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, set):\nreturn set([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.to(device)\nelse:\nreturn data\ndef _detach_tensor(self, data: T) -&gt; T:\n\"\"\"Detach tensor from current graph recursively.\n        Args:\n            data: The data to be detached.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._detach_tensor(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._detach_tensor(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._detach_tensor(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._detach_tensor(val) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.detach()\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> <code>eager</code> <code>bool</code> <p>Whether to run the training in eager mode. This is only related to TensorFlow training because PyTorch by nature is always in eager mode.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self,\nmode: str,\nepoch: int,\nds_id: str,\noutput_keys: Optional[Set[str]] = None,\nwarmup: bool = False,\neager: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        ds_id: The current dataset id.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        eager: Whether to run the training in eager mode. This is only related to TensorFlow training because\n            PyTorch by nature is always in eager mode.\n    \"\"\"\nsuper().load_epoch(mode=mode, epoch=epoch, ds_id=ds_id, output_keys=output_keys, warmup=warmup, eager=eager)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\nif model.current_optimizer and mode == \"train\":\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\n# Set all of the contiguous final updates to defer their updates by default to enable things like CycleGan\n# This is not necessary for TF because overriding tf weights does not confuse the gradient tape computation\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop._old_defer = op.defer\nop.defer = True\nelse:\nbreak\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> <p>In this case we move all of the models from the GPU(s) back to the CPU.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    In this case we move all of the models from the GPU(s) back to the CPU.\n    \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\nif model.current_optimizer and self.epoch_state[\"mode\"] == \"train\":\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\n# Set the final update ops back to their original defer status\nfor op in reversed(self.epoch_ops):\nif isinstance(op, UpdateOp):\nop.defer = op.__dict__.get('_old_defer', op.defer)\nelse:\nbreak\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.Network", "title": "<code>Network</code>", "text": "<p>A function to automatically instantiate the correct Network derived class based on the given <code>ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch models within the same network.</p> required <code>pops</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>Postprocessing Ops. A collection of NumpyOps to be run on the CPU after all of the normal <code>ops</code> have been executed. Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than single points.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseNetwork</code> <p>A network instance containing the given <code>ops</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If TensorFlow and PyTorch models are mixed, or if no models are provided.</p> <code>ValueError</code> <p>If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def Network(\nops: Iterable[Union[TensorOp, Scheduler[TensorOp]]],\npops: Union[None, NumpyOp, Scheduler[NumpyOp], Iterable[Union[NumpyOp,\nScheduler[NumpyOp]]]] = None) -&gt; BaseNetwork:\n\"\"\"A function to automatically instantiate the correct Network derived class based on the given `ops`.\n    Args:\n        ops: A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all\n            models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch\n            models within the same network.\n        pops: Postprocessing Ops. A collection of NumpyOps to be run on the CPU after all of the normal `ops` have been\n            executed. Unlike the NumpyOps found in the pipeline, these ops will run on batches of data rather than\n            single points.\n    Returns:\n        A network instance containing the given `ops`.\n    Raises:\n        AssertionError: If TensorFlow and PyTorch models are mixed, or if no models are provided.\n        ValueError: If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.\n    \"\"\"\nops = to_list(ops)\nmodels = _collect_models(ops)\nframework = set()\nmodel_names = set()\nfor model in models:\n# 'Model' and 'model' should not be considered unique in case you are saving on a non-case-sensitive filesystem\nmodel_names.add(model.model_name.lower())\nif isinstance(model, tf.keras.Model):\nframework.add(\"tf\")\nelif isinstance(model, torch.nn.Module):\nframework.add(\"torch\")\nelse:\nframework.add(\"unknown\")\nif len(framework) == 0:\nframework.add('tf')  # We will use tf as default framework if no models are found\nassert len(framework) == 1, \"please make sure either tensorflow or torch model is used in network\"\nassert len(model_names) == len(models), \"all models must have unique model names\"\nframework = framework.pop()\nif framework == \"tf\":\nnetwork = TFNetwork(ops, pops)\nelif framework == \"torch\":\nnetwork = TorchNetwork(ops, pops)\nelse:\nraise ValueError(\"Unknown model type\")\nreturn network\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.build", "title": "<code>build</code>", "text": "<p>Build model instances and associate them with optimizers.</p> <p>This method can be used with TensorFlow models / optimizers: <pre><code>model_def = fe.architecture.tensorflow.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models / optimizers: <pre><code>model_def = fe.architecture.pytorch.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>Callable[[], Union[Model, List[Model]]]</code> <p>A function that define model(s).</p> required <code>optimizer_fn</code> <code>Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None]</code> <p>Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers provided here should match the number of models generated by the <code>model_fn</code>.</p> required <code>model_name</code> <code>Union[str, List[str], None]</code> <p>Name(s) of the model(s) that will be used for logging purpose. If None, a name will be automatically generated and assigned.</p> <code>None</code> <code>weights_path</code> <code>Union[str, None, List[Union[str, None]]]</code> <p>Path(s) from which to load model weights. If not None, then the number of weight paths provided should match the number of models generated by the <code>model_fn</code>.</p> <code>None</code> <code>mixed_precision</code> <code>bool</code> <p>Whether to enable mixed-precision network operations.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>models</code> <code>Union[Model, List[Model]]</code> <p>The model(s) built by FastEstimator.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def build(model_fn: Callable[[], Union[Model, List[Model]]],\noptimizer_fn: Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None],\nweights_path: Union[str, None, List[Union[str, None]]] = None,\nmodel_name: Union[str, List[str], None] = None,\nmixed_precision: bool = False) -&gt; Union[Model, List[Model]]:\n\"\"\"Build model instances and associate them with optimizers.\n    This method can be used with TensorFlow models / optimizers:\n    ```python\n    model_def = fe.architecture.tensorflow.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n    ```\n    This method can be used with PyTorch models / optimizers:\n    ```python\n    model_def = fe.architecture.pytorch.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n    ```\n    Args:\n        model_fn: A function that define model(s).\n        optimizer_fn: Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers\n            provided here should match the number of models generated by the `model_fn`.\n        model_name: Name(s) of the model(s) that will be used for logging purpose. If None, a name will be\n            automatically generated and assigned.\n        weights_path: Path(s) from which to load model weights. If not None, then the number of weight paths provided\n            should match the number of models generated by the `model_fn`.\n        mixed_precision: Whether to enable mixed-precision network operations.\n    Returns:\n        models: The model(s) built by FastEstimator.\n    \"\"\"\ndef _generate_model_names(num_names):\nnames = [\"model\" if i + fe.fe_build_count == 0 else \"model{}\".format(i + fe.fe_build_count) for i in\nrange(num_names)]\nfe.fe_build_count += num_names\nreturn names\n# The following garbage collection is needed for if a TF model was running, but then died due to an exception being\n# thrown, but the exception was then caught, whereupon the user wanted to switch to a pytorch model instead. Absent\n# this collection, you would see: \"Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\". This\n# would be followed by the death of the pytorch multi-processor which would report something like the following:\n# RuntimeError: DataLoader worker (pid 4225) is killed by signal: Aborted.\n# RuntimeError: DataLoader worker (pid(s) 4225, 4226, 4227) exited unexpectedly\ngc.collect()\n# tensorflow models requires setting global policies prior to model creation. Since there is no way to know the\n# framework of model, setting the policy for both tf and pytorch here.\nif mixed_precision:\nif sys.platform == 'darwin':\nprint(\"\\033[93m{}\\033[00m\".format(\"FastEstimator-Warn: Mixed Precision is not currently supported on Mac / \"\n\"Metal. This flag will be ignored.\"))\nmixed_precision = False\nelse:\nmixed_precision_tf.set_global_policy(mixed_precision_tf.Policy('mixed_float16'))\nelse:\nmixed_precision_tf.set_global_policy(mixed_precision_tf.Policy('float32'))\nmodels = None\nif torch.cuda.device_count() &gt; 1:\n# We need to figure out whether model_fn returns tf models or torch models\nif not isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\n# If we've already done this and gotten TF model, the above flag will be set and this will be skipped. If we\n# are dealing with pytorch models, the model_fn() invocation will be kept so as to not waste clock cycles.\nmodels = to_list(model_fn())\nif isinstance(models[0], tf.keras.Model):\nmodels = None  # We will re-instantiate the models again now that we know we need MirroredStrategy\ntf.keras.backend.clear_session()  # This will reset the automatic layer naming in case user is\n# extracting intermediate layer outputs by name\ntf.distribute.experimental_set_strategy(tf.distribute.MirroredStrategy())\nmodels, optimizer_fn = to_list(model_fn()) if models is None else models, to_list(optimizer_fn)\n# fill optimizers if optimizer_fn is None\nif not optimizer_fn:\noptimizer_fn = [None] * len(models)\n# generate names\nif not model_name:\nmodel_name = _generate_model_names(len(models))\nmodel_name = to_list(model_name)\n# load weights\nif weights_path:\nweights_path = to_list(weights_path)\nelse:\nweights_path = [None] * len(models)\nassert len(models) == len(optimizer_fn) == len(weights_path) == len(model_name), \\\n        \"Found inconsistency in number of models, optimizers, model_name or weights\"\n# create optimizer\nfor idx, (model, optimizer_def, weight, name) in enumerate(zip(models, optimizer_fn, weights_path, model_name)):\nmodels[idx] = trace_model(_fe_compile(model, optimizer_def, weight, name, mixed_precision),\nmodel_idx=idx if len(models) &gt; 1 else -1,\nmodel_fn=model_fn,\noptimizer_fn=optimizer_def,\nweights_path=weight)\nif len(models) == 1:\nmodels = models[0]\nreturn models\n</code></pre>"}, {"location": "fastestimator/pipeline.html", "title": "pipeline", "text": ""}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline", "title": "<code>Pipeline</code>", "text": "<p>A data pipeline class that takes care of data pre-processing.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Union[None, DataSource, Scheduler[DataSource], Dict[str, Union[DataSource, Scheduler[DataSource]]]]</code> <p>The training data, or None if no training data is available.</p> <code>None</code> <code>eval_data</code> <code>Union[None, DataSource, Scheduler[DataSource], Dict[str, DataSource]]</code> <p>The evaluation data, or None if no evaluation data is available.</p> <code>None</code> <code>test_data</code> <code>Union[None, DataSource, Scheduler[DataSource], Dict[str, DataSource]]</code> <p>The testing data, or None if no evaluation data is available.</p> <code>None</code> <code>batch_size</code> <code>Union[None, int, Scheduler[int]]</code> <p>The batch size to be used by the pipeline. If the batch_size is also set by a Batch Op, that value will take precedence over this one (for example, if you want to set the batch_size based on mode or ds_is). NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>ops</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>num_process</code> <code>Optional[int]</code> <p>Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset. None will default to min(n_cpus, max(32, 32*n_gpus)). Multiprocessing can be disabled by passing 0 here, which can be useful for debugging.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>@traceable(blacklist=('ctx_loader', 'ctx_lock'))\nclass Pipeline:\n\"\"\"A data pipeline class that takes care of data pre-processing.\n    Args:\n        train_data: The training data, or None if no training data is available.\n        eval_data: The evaluation data, or None if no evaluation data is available.\n        test_data: The testing data, or None if no evaluation data is available.\n        batch_size: The batch size to be used by the pipeline. If the batch_size is also set by a Batch Op, that value\n            will take precedence over this one (for example, if you want to set the batch_size based on mode or ds_is).\n            NOTE: This argument is only applicable when using a FastEstimator Dataset.\n        ops: NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator\n            Dataset.\n        num_process: Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when\n            using a FastEstimator Dataset. None will default to min(n_cpus, max(32, 32*n_gpus)). Multiprocessing can be\n            disabled by passing 0 here, which can be useful for debugging.\n    \"\"\"\nops: List[Union[NumpyOp, Scheduler[NumpyOp]]]\ndata: Dict[str, Dict[Optional[str], Union[DataSource, Scheduler[DataSource]]]]  # {\"mode\": {\"ds_id\": ds}}\ndef __init__(self,\ntrain_data: Union[None,\nDataSource,\nScheduler[DataSource],\nDict[str, Union[DataSource, Scheduler[DataSource]]]] = None,\neval_data: Union[None, DataSource, Scheduler[DataSource], Dict[str, DataSource]] = None,\ntest_data: Union[None, DataSource, Scheduler[DataSource], Dict[str, DataSource]] = None,\nbatch_size: Union[None, int, Scheduler[int]] = None,\nops: Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]] = None,\nnum_process: Optional[int] = None):\ndata = {x: y for (x, y) in zip([\"train\", \"eval\", \"test\"], [train_data, eval_data, test_data]) if y}\nself.data = self._register_ds_ids(data)\nself.batch_size = batch_size\nself.ops = to_list(ops)\nif mp.get_start_method(allow_none=True) is None and os.name != 'nt':\nmp.set_start_method('fork')\nif mp.get_start_method(allow_none=True) != 'fork':\nprint(\"FastEstimator-Warn: Pipeline multiprocessing is disabled. OS must support the 'fork' start method.\")\nnum_process = 0\nself.num_process = num_process if num_process is not None else min(cpu_count(), 32 * get_num_devices())\nself._verify_inputs(**{k: v for k, v in locals().items() if k != 'self'})\n# Loader Variables\nself.ctx_lock = Lock()\nself.ctx_mode = 'train'\nself.ctx_epoch = 1\nself.ctx_shuffle = True\nself.ctx_output_keys = None\nself.ctx_loader = None\nself.ctx_ds_id = None\nself.ctx_batch_size = None\nself.ctx_ops = []\nself.ctx_batch_info = Batch()\nself.ctx_batch_ops = []\nself.ctx_batch_input_keys = set()\n@staticmethod\ndef _register_ds_ids(\ndata: Dict[\nstr, Union[DataSource, Scheduler[DataSource], Dict[str, Union[DataSource, Scheduler[DataSource]]]]]\n) -&gt; Dict[str, Dict[Optional[str], Union[DataSource, Scheduler[DataSource]]]]:\n\"\"\"Associate dataset of each mode with a `ds_id`.\n        Args:\n            data: A dictionary with mode as key, dataset as value.\n        \"\"\"\nforbidden_ds_id_chars = {\":\", \"!\", \";\", \"|\"}\nfor mode, dataset in data.items():\nif isinstance(dataset, dict):\nfor ds_name in dataset:\nassert isinstance(ds_name, str) and len(ds_name) &gt; 0, \\\n                        \"dataset id must be a string, found {}\".format(ds_name)\nassert not any(char in ds_name for char in forbidden_ds_id_chars), \\\n                        \"dataset id should not contain forbidden characters like ':', ';', '!', '|', \" + \\\n                        \"found {} in pipeline\".format(ds_name)\nelse:\n# Empty string is special, matches against ops which require '!ds1' but not 'ds1'\ndata[mode] = {\"\": dataset}\nreturn data\ndef _verify_inputs(self, **kwargs) -&gt; None:\n\"\"\"A helper method to ensure that the Pipeline inputs are valid.\n        Args:\n            **kwargs: A collection of variable / value pairs to validate.\n        Raises:\n            AssertionError: If `batch_size`, `ops`, or `num_process` were specified in the absence of a FastEstimator\n                Dataset.\n        \"\"\"\nfe_dataset = False\nfor dataset in get_current_items(set(d for ds in self.data.values() for d in ds.values())):\nfe_dataset = self._verify_dataset(dataset, **kwargs) or fe_dataset\nif not fe_dataset:\nassert kwargs['batch_size'] is None, \"Pipeline only supports batch_size with built-in (FE) datasets\"\nassert kwargs['ops'] is None, \"Pipeline only supports ops with built-in (FE) datasets\"\nassert kwargs['num_process'] is None, \"Pipeline only support num_process with built-in (FE) datasets\"\n# Make sure that the user provides at most 1 Batch Op for a given epoch/mode/ds_id\nbatch_ops = []\nschedule_epochs = {1}\nschedule_cycles = set()\nfor op in self.ops:\nif isinstance(op, Batch):\nbatch_ops.append(op)\nif isinstance(op, Scheduler):\n# Only keep the scheduler if it contains at least one Batch op\nvals = op.get_all_values()\nfor val in vals:\nif isinstance(val, Batch):\nbatch_ops.append(op)\nif isinstance(op, EpochScheduler):\nschedule_epochs |= op.epoch_dict.keys()\nelif isinstance(op, RepeatScheduler):\nschedule_cycles.add(op.cycle_length)\nelse:\n# Some unknown scheduler, no known shortcuts so just try first 100 epochs to be safe\nschedule_epochs |= {*range(1, 100)}\nbreak\n# After m*n steps all possible m and n combinations will be visited\nschedule_cycles = functools.reduce(mul, schedule_cycles, 1)\n# Consider x + m*n epochs for each epoch scheduler x value\nschedule_epochs = sorted({epoch for base_epoch in schedule_epochs for epoch in\nlist(range(base_epoch, base_epoch + schedule_cycles))})\nfor mode, id_ds in self.data.items():\nfor ds_id in id_ds.keys():\nfor epoch in schedule_epochs:\nops = get_current_items(batch_ops, run_modes=mode, epoch=epoch, ds_id=ds_id)\n# We have to do an instance check again since the user could technically use a scheduler that has a\n# Batch Op at one point, but some other Op (or None) at a different point\nops = [op for op in ops if isinstance(op, Batch)]\nassert len(ops) &lt; 2, \"You may provide at most 1 batch op for a given epoch/mode/ds_id combination\"\ndef _verify_dataset(self, dataset: DataSource, **kwargs) -&gt; bool:\n\"\"\"A helper function to ensure that all of a dataset's arguments are correct.\n        Args:\n            dataset: The dataset to validate against.\n            **kwargs: A selection of variables and their values which must be validated.\n        Returns:\n            True iff the `dataset` is a PyTorch Dataset (as opposed to a DataLoader or tf.data.Dataset).\n        Raises:\n            AssertionError: If the `kwargs` are found to be invalid based on the given `dataset`.\n            ValueError: If the `dataset` is of an unknown type.\n        \"\"\"\nif isinstance(dataset, Dataset):\n# batch_size check\nfor batch_size in get_current_items(to_list(self.batch_size)):\nassert isinstance(batch_size, int), \"unsupported batch_size format: {}\".format(type(batch_size))\n# ops check\nfor op in get_current_items(self.ops):\nassert isinstance(op, NumpyOp), \"unsupported op format, must provide NumpyOp in Pipeline\"\n# num_process check\nassert isinstance(self.num_process, int), \"number of processes must be an integer\"\nreturn True\nelif isinstance(dataset, (DataLoader, tf.data.Dataset)):\nif kwargs['batch_size'] is not None:\nprint(\"FastEstimator-Warn: batch_size will only be used for built-in dataset\")\nif kwargs['ops'] is not None:\nprint(\"FastEstimator-Warn: ops will only be used for built-in dataset\")\nif kwargs['num_process'] is not None:\nprint(\"FastEstimator-Warn: num_process will only be used for built-in dataset\")\nreturn False\nelse:\nraise ValueError(\"Unsupported dataset type: {}\".format(type(dataset)))\ndef _get_op_split(self, mode: str, epoch: int, ds_id: str) -&gt; Tuple[List[NumpyOp], Batch, List[NumpyOp]]:\n\"\"\"Figure out which ops are pre-batch vs post-batch.\n        Args:\n            mode: The current mode.\n            epoch: The current epoch.\n            ds_id: The current dataset.\n        Returns:\n            (instance ops, batch info, batch ops).\n        \"\"\"\nbatch_info = Batch()\ninstance_ops = []\nbatch_ops = []\nops = get_current_items(self.ops, run_modes=mode, epoch=epoch, ds_id=ds_id)\ntarget = instance_ops\nfor op in ops:\nif isinstance(op, Batch):\nbatch_info = op\ntarget = batch_ops\ncontinue\ntarget.append(op)\nreturn instance_ops, batch_info, batch_ops\ndef get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n        Args:\n            epoch: The current epoch index\n        Returns:\n            The modes for which the Pipeline has data.\n        \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, datasets in self.data.items():\nfor dataset in datasets.values():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\ndef get_ds_ids(self, epoch: int, mode: str) -&gt; List[Union[str, None]]:\n\"\"\"Get the ds_ids for a given epoch and mode.\n        Args:\n            epoch: The current epoch index.\n            mode: The current execution mode.\n        Returns:\n            The ds_ids of the current epoch and mode.\n        \"\"\"\nds_ids = []\nif mode in self.data:\ndatasets = self.data[mode]\nfor ds_id, dataset in datasets.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nds_ids.append(ds_id)\nreturn ds_ids\ndef benchmark(self,\nmode: str = \"train\",\nepoch: int = 1,\nds_id: Optional[str] = None,\nnum_steps: int = 1000,\nlog_interval: int = 100,\ndetailed: bool = True) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n        Args:\n            mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n            ds_id: The ds_id to benchmark. If None, all ds_ids will be benchmarked.\n            num_steps: The number of steps over which to perform the benchmark.\n            log_interval: The logging interval.\n            detailed: Whether to display the detailed time used by each operator.\n        \"\"\"\nif ds_id is None:\nds_ids = self.get_ds_ids(epoch=epoch, mode=mode)\nelse:\nds_ids = [ds_id]\nfor ds_id in ds_ids:\nwith self(mode=mode, epoch=epoch, ds_id=ds_id, steps_per_epoch=num_steps) as loader:\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nds_str = f\"Dataset: {ds_id}, \" if ds_id else \"\"\nprint(\"FastEstimator-Benchmark ({}): {}Step: {}, Epoch: {}, Steps/sec: {}\".format(\nmode.capitalize(), ds_str, idx, epoch, iters_per_sec))\nstart = time.perf_counter()\n# Pipeline Operations Benchmarking when using FEDataset\nif isinstance(loader, FEDataLoader) and isinstance(loader.dataset, OpDataset) and detailed:\n# (n_visited, duration)\nduration_list = np.zeros(shape=(len(self.ctx_ops) + 1 + len(self.ctx_batch_ops), 2))\ndata_len = len(loader.dataset)\nds_str = f\", Dataset: {ds_id}\" if ds_id else \"\"\nprint(\"\\nBreakdown of time taken by Pipeline Operations (Mode: {}, Epoch: {}{})\\n\".format(\nmode.capitalize(), epoch, ds_str))\nextra_memory_management_time = 0\nfor _ in range(log_interval):\nfiltered = False\nbatch = []\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif isinstance(items, list):\nwhile not batch:\nfiltered = False\n# BatchDataset may randomly sample the same elements multiple times, avoid reprocessing\nunique_samples = set()\nfor item in items:\nif id(item) not in unique_samples:\nfor i, op in enumerate(self.ctx_ops):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], item, {'mode': loader.dataset.mode})\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nfiltered = True\nbreak\nunique_samples.add(id(item))\nif not filtered:\nbatch = items\nelse:\nwhile len(batch) &lt; (self.ctx_batch_size or 1):\nfiltered = False\nfor i, op in enumerate(self.ctx_ops):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], items, {'mode': mode})\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nfiltered = True\nbreak\nif not filtered:\nbatch.append(items)\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif not filtered:\n# Perform the batching\nstart = time.perf_counter()\nbatch = self.ctx_batch_info.collate_fn(batch)\nduration = time.perf_counter() - start\nduration_list[len(self.ctx_ops)][0] += 1\nduration_list[len(self.ctx_ops)][1] += duration\n# Perform batch ops\nstart = time.perf_counter()\n# Transform to numpy to not bias against the first op in the batch_op chain\nbatch = to_tensor(batch, target_type='np')\nextra_memory_management_time += time.perf_counter() - start\nfor i, op in enumerate(self.ctx_batch_ops, start=len(self.ctx_ops) + 1):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], data=batch, state={'mode': mode}, batched='np')\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nbreak\n# Count extra time needed to cast data back to torch\nstart = time.perf_counter()\nto_tensor(batch, target_type='torch', shared_memory=True)\nextra_memory_management_time += time.perf_counter() - start\nif self.ctx_batch_ops:\n# Extra memory management penalty is only incurred when using batch ops\nduration_list[len(self.ctx_ops)][1] += extra_memory_management_time\ntotal_time = np.sum(duration_list[:, 1])\nnormalized_times_ms = 1000 * duration_list[:, 1] / np.maximum(duration_list[:, 0], 1)\nop_names = [\"Op\"]\nfor op in self.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops:\nif isinstance(op, Sometimes) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, Repeat) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, OneOf) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelif isinstance(op, Fuse) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelif isinstance(op, Batch):\nop_names.append(\"&lt;Collating Batch&gt;\")\nelse:\nop_names.append(op.__class__.__name__)\nmax_op_len = max(len(op_name) for op_name in op_names)\nmax_in_len = max([len(\", \".join(op.inputs)) for op in\nself.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops] + [len(\"Inputs\")])\nmax_out_len = max([len(\", \".join(op.outputs)) for op in\nself.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops] + [len(\"Outputs\")])\nms_visit_len = max(len(\"{:.3f}\".format(max(normalized_times_ms))), len(\"ms / Visit\"))\nvisit_len = max(len(f\"{int(np.max(duration_list[:, 0]))}\"), len(\"Visits\"))\nprint(\"{}: {}: {}: {}: {}: {}\".format(\"Op\".ljust(max_op_len + 1),\n\"Inputs\".ljust(max_in_len + 1),\n\"Outputs\".ljust(max_out_len + 1),\n\"ms / Visit\".ljust(ms_visit_len + 1),\n\"Visits\".ljust(visit_len + 1),\n\"Time (Total)\".rjust(12)))\nprint(\"-\" * (max_op_len + max_in_len + max_out_len + visit_len + 37))\nfor i, op in enumerate(self.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops):\nprint(\"{}: {}: {}: {}: {}: {:11.2f}%\".format(\nop_names[i + 1].ljust(max_op_len + 1),\n\", \".join(op.inputs).ljust(max_in_len + 1),\n\", \".join(op.outputs).ljust(max_out_len + 1),\n\"{:.3f}\".format(normalized_times_ms[i]).ljust(ms_visit_len + 1),\nstr(int(duration_list[i][0])).ljust(visit_len + 1),\n100 * duration_list[i][1] / total_time))\nif self.ctx_batch_ops:\npenalty = round(100*(duration_list[len(self.ctx_ops)][1] - extra_memory_management_time) /\nduration_list[len(self.ctx_ops)][1], 1)\nprint(f\"\\nNote that collation time would be cut by ~{penalty}% if there were no batched ops.\")\nprint(\"\\n\")  # to make printing more obvious\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Pipeline.\n        \"\"\"\nall_items = self.ops + [self.batch_size] + list(self.data[mode].values())\nreturn all_items\ndef get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n        Args:\n            total_epochs: Total number of epochs.\n            mode: Current execution mode.\n        Returns:\n            Set of epoch indices.\n        \"\"\"\nepochs_with_data = set()\ndatasets = self.data[mode]\nfor dataset in datasets.values():\nif isinstance(dataset, Scheduler):\nepochs_with_data_ds = set(epoch for epoch in range(1, total_epochs + 1)\nif dataset.get_current_value(epoch))\nepochs_with_data = epochs_with_data | epochs_with_data_ds\nelif dataset:\nepochs_with_data_ds = set(range(1, total_epochs + 1))\nepochs_with_data = epochs_with_data | epochs_with_data_ds\nbreak\nreturn epochs_with_data\ndef transform(self,\ndata: Dict[str, Any],\nmode: str,\nepoch: int = 1,\nds_id: str = '',\ntarget_type: str = 'np') -&gt; Union[Dict[str, Any], FilteredData]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n        Args:\n            data: Input data in dictionary format.\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n            ds_id: The current dataset id.\n            target_type: What kind of tensor(s) to create. One of \"tf\", \"torch\", or \"np\".\n        Returns:\n            The transformed data.\n        \"\"\"\ndata = deepcopy(data)\ninstance_ops, batch_spec, batch_ops = self._get_op_split(mode=mode, epoch=epoch, ds_id=ds_id)\nstate = {'mode': mode}\nop_data = forward_numpyop(instance_ops, data, state)\nif isinstance(op_data, FilteredData):\nreturn op_data\ndata = batch_spec.collate_fn([data])\nop_data = forward_numpyop(batch_ops, data, state, batched='torch')\nif isinstance(op_data, FilteredData):\nreturn op_data\nreturn to_tensor(data, target_type=target_type)\ndef get_results(self,\nmode: str = \"train\",\nepoch: int = 1,\nds_id: str = '',\nnum_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n        Args:\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n            num_steps: Number of steps (batches) to get.\n            shuffle: Whether to use shuffling.\n            ds_id: The current dataset id.\n        Returns:\n            A list of batches of Pipeline outputs.\n        \"\"\"\nresults = []\nwith self(mode=mode, epoch=epoch, ds_id=ds_id, shuffle=shuffle) as loader:\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nif loader:\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef __call__(self,\nmode: str,\nepoch: int = 1,\nds_id: str = '',\nshuffle: Optional[bool] = None,\nsteps_per_epoch: Optional[int] = None,\noutput_keys: Optional[Set[str]] = None) -&gt; 'Pipeline':\n\"\"\"Prepare this Pipeline for a given `mode` and `epoch`.\n        A given pipeline can only provide one loader at a time. This helps to prevent issues with multi-threading.\n        ```python\n        pipe = Pipeline(...)\n        with pipe(mode='eval', epoch=2) as loader:\n            for batch in loader:\n                print(batch)\n        ```\n        Args:\n            mode: The execution mode for the loader. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index for the loader. Note that epoch indices are 1-indexed.\n            ds_id: The dataset id to consider for the loader.\n            shuffle: Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument\n                is only used with FastEstimator Datasets.\n            steps_per_epoch: Training or Evaluation will be cut short or extended to complete N steps even if loader is\n                not yet exhausted. If None, all data will be used.\n            output_keys: What keys can be produced from pipeline. If None or empty, all keys will be considered.\n        Returns:\n            The pipeline, but with `mode` and `epoch` set for use in a loader.\n        Raises:\n            ValueError: If called while the pipeline already has an active loader.\n        \"\"\"\n# Make sure that a loader isn't currently instantiated with other settings\nacquired = self.ctx_lock.acquire(blocking=False)\nif not acquired:\nraise ValueError(\"You cannot invoke a Pipeline's __call__ method while it already has an active loader.\")\nself.ctx_mode = mode\nself.ctx_epoch = epoch\nself.ctx_ds_id = ds_id\nself.ctx_shuffle = mode == 'train' if shuffle is None else shuffle\nself.ctx_steps_per_epoch = steps_per_epoch\nself.ctx_output_keys = output_keys or set()\nself.ctx_ops, self.ctx_batch_info, self.ctx_batch_ops = self._get_op_split(mode=mode, epoch=epoch, ds_id=ds_id)\n# Figure out which input keys are required by the batch ops (so they don't get pruned too early)\nself.ctx_batch_input_keys = set()\nbatch_produced_keys = set()\nfor op in get_current_items(self.ctx_batch_ops, mode, epoch, ds_id=ds_id):\nself.ctx_batch_input_keys.update(set(key for key in op.inputs if key not in batch_produced_keys))\nbatch_produced_keys.update(op.outputs)\n# Decide on the batch size (this might still be ignored later if the user is using a BatchDataset)\nself.ctx_batch_size = self.ctx_batch_info.batch_size\nif self.ctx_batch_size is None:\n# batch size\nbatch_size = self.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(self.ctx_epoch)\nself.ctx_batch_size = batch_size\nself.ctx_lock.release()\nreturn self\ndef __enter__(self) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"Get a data loader from the Pipeline for the current epoch and mode.\n        A given pipeline can only provide one loader at a time. This helps to prevent issues with multi-threading.\n        ```python\n        pipe = Pipeline(...)\n        with pipe(mode='eval', epoch=2) as loader:\n            for batch in loader:\n                print(batch)\n        ```\n        Returns:\n            A data loader for the current `mode` and `epoch`.\n        Raises:\n            ValueError: If called while the pipeline already has an active loader.\n        \"\"\"\nacquired = self.ctx_lock.acquire(blocking=False)\nif not acquired:\nraise ValueError(\"You cannot generate a new loader from this Pipeline before closing its other loader.\")\n# Release the lock if arguments are invalid so that people in Jupyter / debug consoles don't get stuck\nif self.ctx_mode not in self.data:\nself.ctx_lock.release()\nraise KeyError(f\"Pipeline has no data for mode '{self.ctx_mode}'\")\nif self.ctx_ds_id not in self.data[self.ctx_mode]:\nself.ctx_lock.release()\nraise KeyError(f\"The dataset id '{self.ctx_ds_id}' is not present in {self.ctx_mode} mode\")\ndata = self.data[self.ctx_mode][self.ctx_ds_id]\nif isinstance(data, Scheduler):\ndata = data.get_current_value(self.ctx_epoch)\nif isinstance(data, Dataset):\n# Results will be immediately converted to tensors, so don't need deep_remainder\nop_dataset = OpDataset(data,\nself.ctx_ops,\nself.ctx_mode,\nself.ctx_output_keys | self.ctx_batch_input_keys if self.ctx_output_keys else None,\ndeep_remainder=False)\n# check whether to batch the data\nbatch_size = None if op_dataset.fe_batch else self.ctx_batch_size\n# Figure out whether a postprocessing function is needed (for batched ops)\npostprocess_fn = None\nif self.ctx_batch_ops:\npostprocess_fn = functools.partial(_batch_postprocess,\nops=self.ctx_batch_ops,\noutput_keys=self.ctx_output_keys,\nmode=self.ctx_mode)\ntry:\ndata = FEDataLoader(op_dataset,\npostprocess_fn=postprocess_fn,\nbatch_size=batch_size,\nshuffle=self.ctx_shuffle,\nsteps_per_epoch=self.ctx_steps_per_epoch,\nnum_workers=self.num_process,\ndrop_last=self.ctx_batch_info.drop_last,\ncollate_fn=self.ctx_batch_info.collate_fn)\nexcept ValueError as err:\nself.ctx_lock.release()\nraise err\nself.ctx_loader = data\nreturn data\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nif self.ctx_loader is not None:\nself.ctx_loader.shutdown()\nself.ctx_loader = None\n# Manually triggering gc here seems to be necessary in order to avoid problems with repeated invocations of FE\n# killing one another through multi-processing.\ngc.collect()\nself.ctx_lock.release()\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.benchmark", "title": "<code>benchmark</code>", "text": "<p>Benchmark the pipeline processing speed.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to benchmark. This can be 'train', 'eval' or 'test'.</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to benchmark. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>ds_id</code> <code>Optional[str]</code> <p>The ds_id to benchmark. If None, all ds_ids will be benchmarked.</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>The number of steps over which to perform the benchmark.</p> <code>1000</code> <code>log_interval</code> <code>int</code> <p>The logging interval.</p> <code>100</code> <code>detailed</code> <code>bool</code> <p>Whether to display the detailed time used by each operator.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def benchmark(self,\nmode: str = \"train\",\nepoch: int = 1,\nds_id: Optional[str] = None,\nnum_steps: int = 1000,\nlog_interval: int = 100,\ndetailed: bool = True) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n    Args:\n        mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n        epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n        ds_id: The ds_id to benchmark. If None, all ds_ids will be benchmarked.\n        num_steps: The number of steps over which to perform the benchmark.\n        log_interval: The logging interval.\n        detailed: Whether to display the detailed time used by each operator.\n    \"\"\"\nif ds_id is None:\nds_ids = self.get_ds_ids(epoch=epoch, mode=mode)\nelse:\nds_ids = [ds_id]\nfor ds_id in ds_ids:\nwith self(mode=mode, epoch=epoch, ds_id=ds_id, steps_per_epoch=num_steps) as loader:\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nds_str = f\"Dataset: {ds_id}, \" if ds_id else \"\"\nprint(\"FastEstimator-Benchmark ({}): {}Step: {}, Epoch: {}, Steps/sec: {}\".format(\nmode.capitalize(), ds_str, idx, epoch, iters_per_sec))\nstart = time.perf_counter()\n# Pipeline Operations Benchmarking when using FEDataset\nif isinstance(loader, FEDataLoader) and isinstance(loader.dataset, OpDataset) and detailed:\n# (n_visited, duration)\nduration_list = np.zeros(shape=(len(self.ctx_ops) + 1 + len(self.ctx_batch_ops), 2))\ndata_len = len(loader.dataset)\nds_str = f\", Dataset: {ds_id}\" if ds_id else \"\"\nprint(\"\\nBreakdown of time taken by Pipeline Operations (Mode: {}, Epoch: {}{})\\n\".format(\nmode.capitalize(), epoch, ds_str))\nextra_memory_management_time = 0\nfor _ in range(log_interval):\nfiltered = False\nbatch = []\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif isinstance(items, list):\nwhile not batch:\nfiltered = False\n# BatchDataset may randomly sample the same elements multiple times, avoid reprocessing\nunique_samples = set()\nfor item in items:\nif id(item) not in unique_samples:\nfor i, op in enumerate(self.ctx_ops):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], item, {'mode': loader.dataset.mode})\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nfiltered = True\nbreak\nunique_samples.add(id(item))\nif not filtered:\nbatch = items\nelse:\nwhile len(batch) &lt; (self.ctx_batch_size or 1):\nfiltered = False\nfor i, op in enumerate(self.ctx_ops):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], items, {'mode': mode})\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nfiltered = True\nbreak\nif not filtered:\nbatch.append(items)\nindex = np.random.randint(data_len)\nitems = deepcopy(loader.dataset.dataset[index])\nif not filtered:\n# Perform the batching\nstart = time.perf_counter()\nbatch = self.ctx_batch_info.collate_fn(batch)\nduration = time.perf_counter() - start\nduration_list[len(self.ctx_ops)][0] += 1\nduration_list[len(self.ctx_ops)][1] += duration\n# Perform batch ops\nstart = time.perf_counter()\n# Transform to numpy to not bias against the first op in the batch_op chain\nbatch = to_tensor(batch, target_type='np')\nextra_memory_management_time += time.perf_counter() - start\nfor i, op in enumerate(self.ctx_batch_ops, start=len(self.ctx_ops) + 1):\nstart = time.perf_counter()\nop_data = forward_numpyop([op], data=batch, state={'mode': mode}, batched='np')\nduration = time.perf_counter() - start\nduration_list[i][0] += 1\nduration_list[i][1] += duration\nif isinstance(op_data, FilteredData):\nbreak\n# Count extra time needed to cast data back to torch\nstart = time.perf_counter()\nto_tensor(batch, target_type='torch', shared_memory=True)\nextra_memory_management_time += time.perf_counter() - start\nif self.ctx_batch_ops:\n# Extra memory management penalty is only incurred when using batch ops\nduration_list[len(self.ctx_ops)][1] += extra_memory_management_time\ntotal_time = np.sum(duration_list[:, 1])\nnormalized_times_ms = 1000 * duration_list[:, 1] / np.maximum(duration_list[:, 0], 1)\nop_names = [\"Op\"]\nfor op in self.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops:\nif isinstance(op, Sometimes) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, Repeat) and op.op:\nop_names.append(op.__class__.__name__ + \" (\" + op.op.__class__.__name__ + \")\")\nelif isinstance(op, OneOf) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelif isinstance(op, Fuse) and op.ops:\nop_names.append(op.__class__.__name__ + \" (\" +\n\", \".join([sub_op.__class__.__name__ for sub_op in op.ops]) + \")\")\nelif isinstance(op, Batch):\nop_names.append(\"&lt;Collating Batch&gt;\")\nelse:\nop_names.append(op.__class__.__name__)\nmax_op_len = max(len(op_name) for op_name in op_names)\nmax_in_len = max([len(\", \".join(op.inputs)) for op in\nself.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops] + [len(\"Inputs\")])\nmax_out_len = max([len(\", \".join(op.outputs)) for op in\nself.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops] + [len(\"Outputs\")])\nms_visit_len = max(len(\"{:.3f}\".format(max(normalized_times_ms))), len(\"ms / Visit\"))\nvisit_len = max(len(f\"{int(np.max(duration_list[:, 0]))}\"), len(\"Visits\"))\nprint(\"{}: {}: {}: {}: {}: {}\".format(\"Op\".ljust(max_op_len + 1),\n\"Inputs\".ljust(max_in_len + 1),\n\"Outputs\".ljust(max_out_len + 1),\n\"ms / Visit\".ljust(ms_visit_len + 1),\n\"Visits\".ljust(visit_len + 1),\n\"Time (Total)\".rjust(12)))\nprint(\"-\" * (max_op_len + max_in_len + max_out_len + visit_len + 37))\nfor i, op in enumerate(self.ctx_ops + [self.ctx_batch_info] + self.ctx_batch_ops):\nprint(\"{}: {}: {}: {}: {}: {:11.2f}%\".format(\nop_names[i + 1].ljust(max_op_len + 1),\n\", \".join(op.inputs).ljust(max_in_len + 1),\n\", \".join(op.outputs).ljust(max_out_len + 1),\n\"{:.3f}\".format(normalized_times_ms[i]).ljust(ms_visit_len + 1),\nstr(int(duration_list[i][0])).ljust(visit_len + 1),\n100 * duration_list[i][1] / total_time))\nif self.ctx_batch_ops:\npenalty = round(100*(duration_list[len(self.ctx_ops)][1] - extra_memory_management_time) /\nduration_list[len(self.ctx_ops)][1], 1)\nprint(f\"\\nNote that collation time would be cut by ~{penalty}% if there were no batched ops.\")\nprint(\"\\n\")  # to make printing more obvious\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_ds_ids", "title": "<code>get_ds_ids</code>", "text": "<p>Get the ds_ids for a given epoch and mode.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch index.</p> required <code>mode</code> <code>str</code> <p>The current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Union[str, None]]</code> <p>The ds_ids of the current epoch and mode.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_ds_ids(self, epoch: int, mode: str) -&gt; List[Union[str, None]]:\n\"\"\"Get the ds_ids for a given epoch and mode.\n    Args:\n        epoch: The current epoch index.\n        mode: The current execution mode.\n    Returns:\n        The ds_ids of the current epoch and mode.\n    \"\"\"\nds_ids = []\nif mode in self.data:\ndatasets = self.data[mode]\nfor ds_id, dataset in datasets.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nds_ids.append(ds_id)\nreturn ds_ids\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_epochs_with_data", "title": "<code>get_epochs_with_data</code>", "text": "<p>Get a set of epoch indices that contains data given mode.</p> <p>Parameters:</p> Name Type Description Default <code>total_epochs</code> <code>int</code> <p>Total number of epochs.</p> required <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>Set[int]</code> <p>Set of epoch indices.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n    Args:\n        total_epochs: Total number of epochs.\n        mode: Current execution mode.\n    Returns:\n        Set of epoch indices.\n    \"\"\"\nepochs_with_data = set()\ndatasets = self.data[mode]\nfor dataset in datasets.values():\nif isinstance(dataset, Scheduler):\nepochs_with_data_ds = set(epoch for epoch in range(1, total_epochs + 1)\nif dataset.get_current_value(epoch))\nepochs_with_data = epochs_with_data | epochs_with_data_ds\nelif dataset:\nepochs_with_data_ds = set(range(1, total_epochs + 1))\nepochs_with_data = epochs_with_data | epochs_with_data_ds\nbreak\nreturn epochs_with_data\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_modes", "title": "<code>get_modes</code>", "text": "<p>Get the modes for which the Pipeline has data.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>Optional[int]</code> <p>The current epoch index</p> <code>None</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes for which the Pipeline has data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n    Args:\n        epoch: The current epoch index\n    Returns:\n        The modes for which the Pipeline has data.\n    \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, datasets in self.data.items():\nfor dataset in datasets.values():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_results", "title": "<code>get_results</code>", "text": "<p>Get sample Pipeline outputs.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>num_steps</code> <code>int</code> <p>Number of steps (batches) to get.</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>Whether to use shuffling.</p> <code>False</code> <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, Any]], Dict[str, Any]]</code> <p>A list of batches of Pipeline outputs.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_results(self,\nmode: str = \"train\",\nepoch: int = 1,\nds_id: str = '',\nnum_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n    Args:\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        num_steps: Number of steps (batches) to get.\n        shuffle: Whether to use shuffling.\n        ds_id: The current dataset id.\n    Returns:\n        A list of batches of Pipeline outputs.\n    \"\"\"\nresults = []\nwith self(mode=mode, epoch=epoch, ds_id=ds_id, shuffle=shuffle) as loader:\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nif loader:\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Pipeline.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Pipeline.\n    \"\"\"\nall_items = self.ops + [self.batch_size] + list(self.data[mode].values())\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.transform", "title": "<code>transform</code>", "text": "<p>Apply all pipeline operations on a given data instance for the specified <code>mode</code> and <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Input data in dictionary format.</p> required <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".</p> required <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>ds_id</code> <code>str</code> <p>The current dataset id.</p> <code>''</code> <code>target_type</code> <code>str</code> <p>What kind of tensor(s) to create. One of \"tf\", \"torch\", or \"np\".</p> <code>'np'</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], FilteredData]</code> <p>The transformed data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def transform(self,\ndata: Dict[str, Any],\nmode: str,\nepoch: int = 1,\nds_id: str = '',\ntarget_type: str = 'np') -&gt; Union[Dict[str, Any], FilteredData]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n    Args:\n        data: Input data in dictionary format.\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        ds_id: The current dataset id.\n        target_type: What kind of tensor(s) to create. One of \"tf\", \"torch\", or \"np\".\n    Returns:\n        The transformed data.\n    \"\"\"\ndata = deepcopy(data)\ninstance_ops, batch_spec, batch_ops = self._get_op_split(mode=mode, epoch=epoch, ds_id=ds_id)\nstate = {'mode': mode}\nop_data = forward_numpyop(instance_ops, data, state)\nif isinstance(op_data, FilteredData):\nreturn op_data\ndata = batch_spec.collate_fn([data])\nop_data = forward_numpyop(batch_ops, data, state, batched='torch')\nif isinstance(op_data, FilteredData):\nreturn op_data\nreturn to_tensor(data, target_type=target_type)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/attention_unet.html", "title": "attention_unet", "text": ""}, {"location": "fastestimator/architecture/pytorch/attention_unet.html#fastestimator.fastestimator.architecture.pytorch.attention_unet.AttentionBlock", "title": "<code>AttentionBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>An Attention block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the attention block.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the attention block.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\attention_unet.py</code> <pre><code>class AttentionBlock(nn.Module):\n\"\"\"An Attention block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the attention block.\n        out_channels: How many channels leave the attention block.\n    \"\"\"\ndef __init__(self, in_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.dec_layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))\nself.enc_layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1), nn.BatchNorm2d(out_channels))\nself.attn_layers = nn.Sequential(nn.Conv2d(out_channels, 1, 1), nn.BatchNorm2d(1))\ndef forward(self, decoder_input: torch.Tensor, encoder_input: torch.Tensor) -&gt; torch.Tensor:\ndec_out = self.dec_layers(decoder_input)\nenc_out = self.enc_layers(encoder_input)\nattn = F.relu(dec_out + enc_out)\nattn = self.attn_layers(attn)\nattn = torch.sigmoid(attn)\nreturn encoder_input * attn\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/attention_unet.html#fastestimator.fastestimator.architecture.pytorch.attention_unet.AttentionUNet", "title": "<code>AttentionUNet</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>Attention based UNet implementation in PyTorch.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (channels, height, width).</p> <code>(1, 128, 128)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[1] or <code>input_size</code>[2] is not a multiple of 16.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\attention_unet.py</code> <pre><code>class AttentionUNet(nn.Module):\n\"\"\"Attention based UNet implementation in PyTorch.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        input_size: The size of the input tensor (channels, height, width).\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[1] or `input_size`[2] is not a multiple of 16.\n    \"\"\"\ndef __init__(self, input_size: Tuple[int, int, int] = (1, 128, 128)) -&gt; None:\nAttentionUNet._check_input_size(input_size)\nsuper().__init__()\nself.input_size = input_size\nself.enc1 = UNetEncoderBlock(in_channels=input_size[0], out_channels=64)\nself.enc2 = UNetEncoderBlock(in_channels=64, out_channels=128)\nself.enc3 = UNetEncoderBlock(in_channels=128, out_channels=256)\nself.enc4 = UNetEncoderBlock(in_channels=256, out_channels=512)\nself.bottle_neck = UNetDecoderBlock(in_channels=512, mid_channels=1024, out_channels=512)\nself.attn4 = AttentionBlock(in_channels=512, out_channels=512)\nself.attn3 = AttentionBlock(in_channels=256, out_channels=256)\nself.attn2 = AttentionBlock(in_channels=128, out_channels=128)\nself.attn1 = AttentionBlock(in_channels=64, out_channels=64)\nself.dec4 = UNetDecoderBlock(in_channels=1024, mid_channels=512, out_channels=256)\nself.dec3 = UNetDecoderBlock(in_channels=512, mid_channels=256, out_channels=128)\nself.dec2 = UNetDecoderBlock(in_channels=256, mid_channels=128, out_channels=64)\nself.dec1 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 1, 1),\nnn.Sigmoid())\nfor layer in self.dec1:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx1, x_e1 = self.enc1(x)\nx2, x_e2 = self.enc2(x_e1)\nx3, x_e3 = self.enc3(x_e2)\nx4, x_e4 = self.enc4(x_e3)\nx_bottle_neck = self.bottle_neck(x_e4)\nx4_attn = self.attn4(x_bottle_neck, x4)\nx_d4 = self.dec4(torch.cat((x_bottle_neck, x4_attn), 1))\nx3_attn = self.attn3(x_d4, x3)\nx_d3 = self.dec3(torch.cat((x_d4, x3_attn), 1))\nx2_attn = self.attn2(x_d3, x2)\nx_d2 = self.dec2(torch.cat((x_d3, x2_attn), 1))\nx1_attn = self.attn1(x_d2, x1)\nx_out = self.dec1(torch.cat((x_d2, x1_attn), 1))\nreturn x_out\n@staticmethod\ndef _check_input_size(input_size):\nif len(input_size) != 3:\nraise ValueError(\"Length of `input_size` is not 3 (channel, height, width)\")\n_, height, width = input_size\nif height &lt; 16 or not (height / 16.0).is_integer() or width &lt; 16 or not (width / 16.0).is_integer():\nraise ValueError(\"Both height and width of input_size need to be multiples of 16 (16, 32, 48...)\")\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/attention_unet.html#fastestimator.fastestimator.architecture.pytorch.attention_unet.UNetDecoderBlock", "title": "<code>UNetDecoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet decoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the decoder.</p> required <code>mid_channels</code> <code>int</code> <p>How many channels are used for the decoder's intermediate layer.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the decoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\attention_unet.py</code> <pre><code>class UNetDecoderBlock(nn.Module):\n\"\"\"A UNet decoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the decoder.\n        mid_channels: How many channels are used for the decoder's intermediate layer.\n        out_channels: How many channels leave the decoder.\n    \"\"\"\ndef __init__(self, in_channels: int, mid_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(mid_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\nnn.Conv2d(mid_channels, out_channels, 3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.layers(x)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/attention_unet.html#fastestimator.fastestimator.architecture.pytorch.attention_unet.UNetEncoderBlock", "title": "<code>UNetEncoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet encoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the encoder.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the encoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\attention_unet.py</code> <pre><code>class UNetEncoderBlock(nn.Module):\n\"\"\"A UNet encoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the encoder.\n        out_channels: How many channels leave the encoder.\n    \"\"\"\ndef __init__(self, in_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\nout = self.layers(x)\nreturn out, F.max_pool2d(out, 2)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/pytorch/lenet.html#fastestimator.fastestimator.architecture.pytorch.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>         Bases: <code>torch.nn.Module</code></p> <p>A standard LeNet implementation in pytorch.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>The shape of the model input (channels, height, width).</p> <code>(1, 28, 28)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_shape</code> is not 3.</p> <code>ValueError</code> <p><code>input_shape</code>[1] or <code>input_shape</code>[2] is smaller than 18.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\lenet.py</code> <pre><code>class LeNet(torch.nn.Module):\n\"\"\"A standard LeNet implementation in pytorch.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: The shape of the model input (channels, height, width).\n        classes: The number of outputs the model should generate.\n    Raises:\n        ValueError: Length of `input_shape` is not 3.\n        ValueError: `input_shape`[1] or `input_shape`[2] is smaller than 18.\n    \"\"\"\ndef __init__(self, input_shape: Tuple[int, int, int] = (1, 28, 28), classes: int = 10) -&gt; None:\nLeNet._check_input_shape(input_shape)\nsuper().__init__()\nconv_kernel = 3\nself.pool_kernel = 2\nself.conv1 = nn.Conv2d(input_shape[0], 32, conv_kernel)\nself.conv2 = nn.Conv2d(32, 64, conv_kernel)\nself.conv3 = nn.Conv2d(64, 64, conv_kernel)\nflat_x = ((((input_shape[1] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nflat_y = ((((input_shape[2] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nself.fc1 = nn.Linear(flat_x * flat_y * 64, 64)\nself.fc2 = nn.Linear(64, classes)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = fn.relu(self.conv1(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv2(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv3(x))\nx = x.view(x.size(0), -1)\nx = fn.relu(self.fc1(x))\nx = fn.softmax(self.fc2(x), dim=-1)\nreturn x\n@staticmethod\ndef _check_input_shape(input_shape):\nif len(input_shape) != 3:\nraise ValueError(\"Length of `input_shape` is not 3 (channel, height, width)\")\n_, height, width = input_shape\nif height &lt; 18 or width &lt; 18:\nraise ValueError(\"Both height and width of input_shape need to not smaller than 18\")\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/resnet9.html", "title": "resnet9", "text": ""}, {"location": "fastestimator/architecture/pytorch/resnet9.html#fastestimator.fastestimator.architecture.pytorch.resnet9.ResNet9", "title": "<code>ResNet9</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A 9-layer ResNet PyTorch model for cifar10 image classification. The model architecture is from https://github.com/davidcpage/cifar10-fast</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (channels, height, width). Both width and height of input_size should not be smaller than 16.</p> <code>(3, 32, 32)</code> <code>classes</code> <code>int</code> <p>The number of outputs.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[1] or <code>input_size</code>[2] is not a multiple of 16.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\resnet9.py</code> <pre><code>class ResNet9(nn.Module):\n\"\"\"A 9-layer ResNet PyTorch model for cifar10 image classification.\n    The model architecture is from https://github.com/davidcpage/cifar10-fast\n    Args:\n        input_size: The size of the input tensor (channels, height, width). Both width and height of input_size should\n            not be smaller than 16.\n        classes: The number of outputs.\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[1] or `input_size`[2] is not a multiple of 16.\n    \"\"\"\ndef __init__(self, input_size: Tuple[int, int, int] = (3, 32, 32), classes: int = 10):\nResNet9._check_input_size(input_size)\nsuper().__init__()\nself.conv0 = nn.Conv2d(input_size[0], 64, 3, padding=(1, 1))\nself.conv0_bn = nn.BatchNorm2d(64, momentum=0.2)\nself.conv1 = nn.Conv2d(64, 128, 3, padding=(1, 1))\nself.conv1_bn = nn.BatchNorm2d(128, momentum=0.2)\nself.residual1 = Residual(128)\nself.conv2 = nn.Conv2d(128, 256, 3, padding=(1, 1))\nself.conv2_bn = nn.BatchNorm2d(256, momentum=0.2)\nself.residual2 = Residual(256)\nself.conv3 = nn.Conv2d(256, 512, 3, padding=(1, 1))\nself.conv3_bn = nn.BatchNorm2d(512, momentum=0.2)\nself.residual3 = Residual(512)\nself.fc1 = nn.Linear(512, classes)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n# prep layer\nx = self.conv0(x)\nx = self.conv0_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\n# layer 1\nx = self.conv1(x)\nx = fn.max_pool2d(x, 2)\nx = self.conv1_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\nx = x + self.residual1(x)\n# layer 2\nx = self.conv2(x)\nx = fn.max_pool2d(x, 2)\nx = self.conv2_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\nx = x + self.residual2(x)\n# layer 3\nx = self.conv3(x)\nx = fn.max_pool2d(x, 2)\nx = self.conv3_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\nx = x + self.residual3(x)\n# layer 4\nx = nn.AdaptiveMaxPool2d((1, 1))(x)\nx = torch.flatten(x, 1)\nx = self.fc1(x)\nx = fn.softmax(x, dim=-1)\nreturn x\n@staticmethod\ndef _check_input_size(input_size):\nif len(input_size) != 3:\nraise ValueError(\"Length of `input_size` is not 3 (channel, height, width)\")\n_, height, width = input_size\nif height &lt; 16 or width &lt; 16:\nraise ValueError(\"Both height and width of input_size need to not smaller than 16\")\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/resnet9.html#fastestimator.fastestimator.architecture.pytorch.resnet9.Residual", "title": "<code>Residual</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A two-layer unit for ResNet9. The output size is the same as input.</p> <p>Parameters:</p> Name Type Description Default <code>channel</code> <p>Number of input channels.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\resnet9.py</code> <pre><code>class Residual(nn.Module):\n\"\"\"A two-layer unit for ResNet9. The output size is the same as input.\n    Args:\n        channel: Number of input channels.\n    \"\"\"\ndef __init__(self, channels: int):\nsuper().__init__()\nself.conv1 = nn.Conv2d(channels, channels, 3, padding=(1, 1))\nself.conv1_bn = nn.BatchNorm2d(channels)\nself.conv2 = nn.Conv2d(channels, channels, 3, padding=(1, 1))\nself.conv2_bn = nn.BatchNorm2d(channels)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = self.conv1(x)\nx = self.conv1_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\nx = self.conv2(x)\nx = self.conv2_bn(x)\nx = fn.leaky_relu(x, negative_slope=0.1)\nreturn x\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNet", "title": "<code>UNet</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A standard UNet implementation in PyTorch.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (channels, height, width).</p> <code>(1, 128, 128)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[1] or <code>input_size</code>[2] is not a multiple of 16.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNet(nn.Module):\n\"\"\"A standard UNet implementation in PyTorch.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        input_size: The size of the input tensor (channels, height, width).\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[1] or `input_size`[2] is not a multiple of 16.\n    \"\"\"\ndef __init__(self, input_size: Tuple[int, int, int] = (1, 128, 128)) -&gt; None:\nUNet._check_input_size(input_size)\nsuper().__init__()\nself.input_size = input_size\nself.enc1 = UNetEncoderBlock(in_channels=input_size[0], out_channels=64)\nself.enc2 = UNetEncoderBlock(in_channels=64, out_channels=128)\nself.enc3 = UNetEncoderBlock(in_channels=128, out_channels=256)\nself.enc4 = UNetEncoderBlock(in_channels=256, out_channels=512)\nself.bottle_neck = UNetDecoderBlock(in_channels=512, mid_channels=1024, out_channels=512)\nself.dec4 = UNetDecoderBlock(in_channels=1024, mid_channels=512, out_channels=256)\nself.dec3 = UNetDecoderBlock(in_channels=512, mid_channels=256, out_channels=128)\nself.dec2 = UNetDecoderBlock(in_channels=256, mid_channels=128, out_channels=64)\nself.dec1 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 1, 1),\nnn.Sigmoid())\nfor layer in self.dec1:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx1, x_e1 = self.enc1(x)\nx2, x_e2 = self.enc2(x_e1)\nx3, x_e3 = self.enc3(x_e2)\nx4, x_e4 = self.enc4(x_e3)\nx_bottle_neck = self.bottle_neck(x_e4)\nx_d4 = self.dec4(torch.cat((x_bottle_neck, x4), 1))\nx_d3 = self.dec3(torch.cat((x_d4, x3), 1))\nx_d2 = self.dec2(torch.cat((x_d3, x2), 1))\nx_out = self.dec1(torch.cat((x_d2, x1), 1))\nreturn x_out\n@staticmethod\ndef _check_input_size(input_size):\nif len(input_size) != 3:\nraise ValueError(\"Length of `input_size` is not 3 (channel, height, width)\")\n_, height, width = input_size\nif height &lt; 16 or not (height / 16.0).is_integer() or width &lt; 16 or not (width / 16.0).is_integer():\nraise ValueError(\"Both height and width of input_size need to be multiples of 16 (16, 32, 48...)\")\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetDecoderBlock", "title": "<code>UNetDecoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet decoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the decoder.</p> required <code>mid_channels</code> <code>int</code> <p>How many channels are used for the decoder's intermediate layer.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the decoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetDecoderBlock(nn.Module):\n\"\"\"A UNet decoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the decoder.\n        mid_channels: How many channels are used for the decoder's intermediate layer.\n        out_channels: How many channels leave the decoder.\n    \"\"\"\ndef __init__(self, in_channels: int, mid_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(mid_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\nnn.Conv2d(mid_channels, out_channels, 3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.layers(x)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetEncoderBlock", "title": "<code>UNetEncoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet encoder block.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the encoder.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the encoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetEncoderBlock(nn.Module):\n\"\"\"A UNet encoder block.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    Args:\n        in_channels: How many channels enter the encoder.\n        out_channels: How many channels leave the encoder.\n    \"\"\"\ndef __init__(self, in_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\nout = self.layers(x)\nreturn out, F.max_pool2d(out, 2)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/wideresnet.html", "title": "wideresnet", "text": ""}, {"location": "fastestimator/architecture/pytorch/wideresnet.html#fastestimator.fastestimator.architecture.pytorch.wideresnet.BasicBlock", "title": "<code>BasicBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A Wide Residual Network Basic block.</p> <p>This class creates a basic block.</p> <p>Parameters:</p> Name Type Description Default <code>in_planes</code> <code>int</code> <p>How many channels enter the block.</p> required <code>out_planes</code> <code>int</code> <p>How many channels leave the block.</p> required <code>stride</code> <code>int</code> <p>stride for convolution layer.</p> required <code>dropout</code> <code>float</code> <p>Adds dropout if value is greater than 0.0.</p> <code>0.0</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\wideresnet.py</code> <pre><code>class BasicBlock(nn.Module):\n\"\"\"A Wide Residual Network Basic block.\n    This class creates a basic block.\n    Args:\n        in_planes: How many channels enter the block.\n        out_planes: How many channels leave the block.\n        stride: stride for convolution layer.\n        dropout: Adds dropout if value is greater than 0.0.\n    \"\"\"\ndef __init__(self, in_planes: int, out_planes: int, stride: int, dropout: float = 0.0) -&gt; None:\nsuper(BasicBlock, self).__init__()\nself.bn1 = nn.BatchNorm2d(in_planes)\nself.relu1 = nn.ReLU(inplace=True)\nself.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\nself.bn2 = nn.BatchNorm2d(out_planes)\nself.relu2 = nn.ReLU(inplace=True)\nself.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\nself.dropout = dropout\nself.equalInOut = (in_planes == out_planes)\nself.convShortcut = (not self.equalInOut) and nn.Conv2d(\nin_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False) or None\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nif not self.equalInOut:\nx = self.relu1(self.bn1(x))\nelse:\nout = self.relu1(self.bn1(x))\nout = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\nif self.dropout &gt; 0:\nout = F.dropout(out, p=self.droprate, training=self.training)\nout = self.conv2(out)\nreturn torch.add(x if self.equalInOut else self.convShortcut(x), out)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/wideresnet.html#fastestimator.fastestimator.architecture.pytorch.wideresnet.NetworkBlock", "title": "<code>NetworkBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A Wide Residual Network block.</p> <p>This class utilizes the basic block to create a network block.</p> <p>Parameters:</p> Name Type Description Default <code>nb_layers</code> <code>int</code> <p>How many layers to create.</p> required <code>in_planes</code> <code>int</code> <p>How many channels enter the block.</p> required <code>out_planes</code> <code>int</code> <p>How many channels leave the block.</p> required <code>block</code> <code>BasicBlock</code> <p>Class for creating a network block.</p> required <code>stride</code> <code>int</code> <p>stride for convolution layer.</p> required <code>dropout</code> <code>float</code> <p>Adds dropout if value is greater than 0.0.</p> <code>0.0</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\wideresnet.py</code> <pre><code>class NetworkBlock(nn.Module):\n\"\"\"A Wide Residual Network block.\n    This class utilizes the basic block to create a network block.\n    Args:\n        nb_layers: How many layers to create.\n        in_planes: How many channels enter the block.\n        out_planes: How many channels leave the block.\n        block: Class for creating a network block.\n        stride: stride for convolution layer.\n        dropout: Adds dropout if value is greater than 0.0.\n    \"\"\"\ndef __init__(self,\nnb_layers: int,\nin_planes: int,\nout_planes: int,\nblock: BasicBlock,\nstride: int,\ndropout: float = 0.0) -&gt; None:\nsuper(NetworkBlock, self).__init__()\nself.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropout)\ndef _make_layer(self,\nblock: BasicBlock,\nin_planes: int,\nout_planes: int,\nnb_layers: int,\nstride: int,\ndropout: float) -&gt; torch.nn.Sequential:\nlayers = []\nfor i in range(int(nb_layers)):\nlayers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropout))\nreturn nn.Sequential(*layers)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.layer(x)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/wideresnet.html#fastestimator.fastestimator.architecture.pytorch.wideresnet.WideResidualNetwork", "title": "<code>WideResidualNetwork</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>Wide Residual Network.</p> <p>This class creates the Wide Residual Network with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>Depth of the network. Compute N = (n - 4) / 6.    For a depth of 16, n = 16, N = (16 - 4) / 6 = 2    For a depth of 28, n = 28, N = (28 - 4) / 6 = 4    For a depth of 40, n = 40, N = (40 - 4) / 6 = 6</p> <code>28</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <code>widen_factor</code> <code>int</code> <p>Width of the network.</p> <code>10</code> <code>dropout</code> <code>float</code> <p>Adds dropout if value is greater than 0.0.</p> <code>0.0</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If (depth - 4) is not divisible by 6.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\wideresnet.py</code> <pre><code>class WideResidualNetwork(nn.Module):\n\"\"\"Wide Residual Network.\n    This class creates the Wide Residual Network with specified parameters.\n    Args:\n        depth: Depth of the network. Compute N = (n - 4) / 6.\n               For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n               For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n               For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n        classes: The number of outputs the model should generate.\n        widen_factor: Width of the network.\n        dropout: Adds dropout if value is greater than 0.0.\n    Raises:\n        AssertionError: If (depth - 4) is not divisible by 6.\n    \"\"\"\ndef __init__(self, depth: int = 28, classes: int = 10, widen_factor: int = 10, dropout: float = 0.0) -&gt; None:\nsuper(WideResidualNetwork, self).__init__()\nnChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\nassert (depth - 4) % 6 == 0\nn = (depth - 4) / 6\nblock = BasicBlock\n# 1st conv before any network block\nself.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)\n# 1st block\nself.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropout)\n# 2nd block\nself.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropout)\n# 3rd block\nself.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropout)\n# global average pooling and classifier\nself.bn1 = nn.BatchNorm2d(nChannels[3])\nself.relu = nn.ReLU(inplace=True)\nself.avgpool = nn.AdaptiveAvgPool2d((1, 1))\nself.fc = nn.Linear(nChannels[3], classes)\nself.nChannels = nChannels[3]\nfor m in self.modules():\nif isinstance(m, nn.Conv2d):\nnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\nelif isinstance(m, nn.BatchNorm2d):\nm.weight.data.fill_(1)\nm.bias.data.zero_()\nelif isinstance(m, nn.Linear):\nm.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nout = self.conv1(x)\nout = self.block1(out)\nout = self.block2(out)\nout = self.block3(out)\nout = self.relu(self.bn1(out))\nout = self.avgpool(out)\nout = out.view(-1, self.nChannels)\nreturn self.fc(out)\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/attention_unet.html", "title": "attention_unet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/attention_unet.html#fastestimator.fastestimator.architecture.tensorflow.attention_unet.AttentionUNet", "title": "<code>AttentionUNet</code>", "text": "<p>Attention based UNet implementation in TensorFlow.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> <code>(128, 128, 1)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[0] or <code>input_size</code>[1] is not a multiple of 16.</p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow Attention UNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\attention_unet.py</code> <pre><code>def AttentionUNet(input_size: Tuple[int, int, int] = (128, 128, 1)) -&gt; tf.keras.Model:\n\"\"\"Attention based UNet implementation in TensorFlow.\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[0] or `input_size`[1] is not a multiple of 16.\n    Returns:\n        A TensorFlow Attention UNet model.\n    \"\"\"\n_check_input_size(input_size)\nconv_config = {'activation': 'relu', 'padding': 'same', 'kernel_initializer': 'he_normal'}\nup_config = {'size': (2, 2), 'interpolation': 'bilinear'}\ninputs = Input(input_size)\nconv1 = Conv2D(64, 3, **conv_config)(inputs)\nconv1 = Conv2D(64, 3, **conv_config)(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = Conv2D(128, 3, **conv_config)(pool1)\nconv2 = Conv2D(128, 3, **conv_config)(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = Conv2D(256, 3, **conv_config)(pool2)\nconv3 = Conv2D(256, 3, **conv_config)(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = Conv2D(512, 3, **conv_config)(pool3)\nconv4 = Conv2D(512, 3, **conv_config)(conv4)\ndrop4 = Dropout(0.5)(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\nconv5 = Conv2D(1024, 3, **conv_config)(pool4)\nconv5 = Conv2D(1024, 3, **conv_config)(conv5)\ndrop5 = Dropout(0.5)(conv5)\nup6 = Conv2D(512, 3, **conv_config)(UpSampling2D(**up_config)(drop5))\ndrop4 = attention_block(512, decoder_input=up6, encoder_input=drop4)\nmerge6 = concatenate([drop4, up6], axis=-1)\nconv6 = Conv2D(512, 3, **conv_config)(merge6)\nconv6 = Conv2D(512, 3, **conv_config)(conv6)\nup7 = Conv2D(256, 3, **conv_config)(UpSampling2D(**up_config)(conv6))\nconv3 = attention_block(256, decoder_input=up7, encoder_input=conv3)\nmerge7 = concatenate([conv3, up7], axis=-1)\nconv7 = Conv2D(256, 3, **conv_config)(merge7)\nconv7 = Conv2D(256, 3, **conv_config)(conv7)\nup8 = Conv2D(128, 3, **conv_config)(UpSampling2D(**up_config)(conv7))\nconv2 = attention_block(128, decoder_input=up8, encoder_input=conv2)\nmerge8 = concatenate([conv2, up8], axis=-1)\nconv8 = Conv2D(128, 3, **conv_config)(merge8)\nconv8 = Conv2D(128, 3, **conv_config)(conv8)\nup9 = Conv2D(64, 3, **conv_config)(UpSampling2D(**up_config)(conv8))\nconv1 = attention_block(64, decoder_input=up9, encoder_input=conv1)\nmerge9 = concatenate([conv1, up9], axis=-1)\nconv9 = Conv2D(64, 3, **conv_config)(merge9)\nconv9 = Conv2D(64, 3, **conv_config)(conv9)\nconv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\nmodel = Model(inputs=inputs, outputs=conv10)\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/attention_unet.html#fastestimator.fastestimator.architecture.tensorflow.attention_unet.attention_block", "title": "<code>attention_block</code>", "text": "<p>An attention unit for Attention Unet.</p> <p>Parameters:</p> Name Type Description Default <code>n_filters</code> <code>int</code> <p>How many filters for the convolution layer.</p> required <code>decoder_input</code> <code>tf.Tensor</code> <p>Input tensor in the decoder section.</p> required <code>encoder_input</code> <code>tf.Tensor</code> <p>Input tensor in the encoder section.</p> required Return <p>Output Keras tensor.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\attention_unet.py</code> <pre><code>def attention_block(n_filters: int, decoder_input: tf.Tensor, encoder_input: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"An attention unit for Attention Unet.\n    Args:\n        n_filters: How many filters for the convolution layer.\n        decoder_input: Input tensor in the decoder section.\n        encoder_input: Input tensor in the encoder section.\n    Return:\n        Output Keras tensor.\n    \"\"\"\nc1 = Conv2D(n_filters, kernel_size=1)(decoder_input)\nc1 = BatchNormalization()(c1)\nx1 = Conv2D(n_filters, kernel_size=1)(encoder_input)\nx1 = BatchNormalization()(x1)\natt = ReLU()(x1 + c1)\natt = Conv2D(1, kernel_size=1)(att)\natt = BatchNormalization()(att)\natt = tf.sigmoid(att)\nreturn encoder_input * att\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/lenet.html#fastestimator.fastestimator.architecture.tensorflow.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>A standard LeNet implementation in TensorFlow.</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>shape of the input data (height, width, channels).</p> <code>(28, 28, 1)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_shape</code> is not 3.</p> <code>ValueError</code> <p><code>input_shape</code>[0] or <code>input_shape</code>[1] is smaller than 18.</p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow LeNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\lenet.py</code> <pre><code>def LeNet(input_shape: Tuple[int, int, int] = (28, 28, 1), classes: int = 10) -&gt; tf.keras.Model:\n\"\"\"A standard LeNet implementation in TensorFlow.\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: shape of the input data (height, width, channels).\n        classes: The number of outputs the model should generate.\n    Raises:\n        ValueError: Length of `input_shape` is not 3.\n        ValueError: `input_shape`[0] or `input_shape`[1] is smaller than 18.\n    Returns:\n        A TensorFlow LeNet model.\n    \"\"\"\n_check_input_shape(input_shape)\nmodel = Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(classes, activation='softmax'))\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/resnet9.html", "title": "resnet9", "text": ""}, {"location": "fastestimator/architecture/tensorflow/resnet9.html#fastestimator.fastestimator.architecture.tensorflow.resnet9.ResNet9", "title": "<code>ResNet9</code>", "text": "<p>A small 9-layer ResNet Tensorflow model for cifar10 image classification. The model architecture is from https://github.com/davidcpage/cifar10-fast</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> <code>(32, 32, 3)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[0] or <code>input_size</code>[1] is not a multiple of 16.</p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow ResNet9 model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\resnet9.py</code> <pre><code>def ResNet9(input_size: Tuple[int, int, int] = (32, 32, 3), classes: int = 10) -&gt; tf.keras.Model:\n\"\"\"A small 9-layer ResNet Tensorflow model for cifar10 image classification.\n    The model architecture is from https://github.com/davidcpage/cifar10-fast\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n        classes: The number of outputs the model should generate.\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[0] or `input_size`[1] is not a multiple of 16.\n    Returns:\n        A TensorFlow ResNet9 model.\n    \"\"\"\n_check_input_size(input_size)\n# prep layers\ninp = layers.Input(shape=input_size)\nx = layers.Conv2D(64, 3, padding='same')(inp)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\n# layer1\nx = layers.Conv2D(128, 3, padding='same')(x)\nx = layers.MaxPool2D()(x)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\nx = layers.Add()([x, residual(x, 128)])\n# layer2\nx = layers.Conv2D(256, 3, padding='same')(x)\nx = layers.MaxPool2D()(x)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\n# layer3\nx = layers.Conv2D(512, 3, padding='same')(x)\nx = layers.MaxPool2D()(x)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\nx = layers.Add()([x, residual(x, 512)])\n# layers4\nx = layers.GlobalMaxPool2D()(x)\nx = layers.Flatten()(x)\nx = layers.Dense(classes)(x)\nx = layers.Activation('softmax', dtype='float32')(x)\nmodel = tf.keras.Model(inputs=inp, outputs=x)\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/resnet9.html#fastestimator.fastestimator.architecture.tensorflow.resnet9.residual", "title": "<code>residual</code>", "text": "<p>A ResNet unit for ResNet9.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tf.Tensor</code> <p>Input Keras tensor.</p> required <code>num_channel</code> <code>int</code> <p>The number of layer channel.</p> required Return <p>Output Keras tensor.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\resnet9.py</code> <pre><code>def residual(x: tf.Tensor, num_channel: int) -&gt; tf.Tensor:\n\"\"\"A ResNet unit for ResNet9.\n    Args:\n        x: Input Keras tensor.\n        num_channel: The number of layer channel.\n    Return:\n        Output Keras tensor.\n    \"\"\"\nx = layers.Conv2D(num_channel, 3, padding='same')(x)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\nx = layers.Conv2D(num_channel, 3, padding='same')(x)\nx = layers.BatchNormalization(momentum=0.8)(x)\nx = layers.LeakyReLU(alpha=0.1)(x)\nreturn x\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/unet.html#fastestimator.fastestimator.architecture.tensorflow.unet.UNet", "title": "<code>UNet</code>", "text": "<p>A standard UNet implementation in TensorFlow</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> <code>(128, 128, 1)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Length of <code>input_size</code> is not 3.</p> <code>ValueError</code> <p><code>input_size</code>[0] or <code>input_size</code>[1] is not a multiple of 16.</p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow UNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\unet.py</code> <pre><code>def UNet(input_size: Tuple[int, int, int] = (128, 128, 1)) -&gt; tf.keras.Model:\n\"\"\"A standard UNet implementation in TensorFlow\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n    Raises:\n        ValueError: Length of `input_size` is not 3.\n        ValueError: `input_size`[0] or `input_size`[1] is not a multiple of 16.\n    Returns:\n        A TensorFlow UNet model.\n    \"\"\"\n_check_input_size(input_size)\nconv_config = {'activation': 'relu', 'padding': 'same', 'kernel_initializer': 'he_normal'}\nup_config = {'size': (2, 2), 'interpolation': 'bilinear'}\ninputs = Input(input_size)\nconv1 = Conv2D(64, 3, **conv_config)(inputs)\nconv1 = Conv2D(64, 3, **conv_config)(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = Conv2D(128, 3, **conv_config)(pool1)\nconv2 = Conv2D(128, 3, **conv_config)(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = Conv2D(256, 3, **conv_config)(pool2)\nconv3 = Conv2D(256, 3, **conv_config)(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = Conv2D(512, 3, **conv_config)(pool3)\nconv4 = Conv2D(512, 3, **conv_config)(conv4)\ndrop4 = Dropout(0.5)(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\nconv5 = Conv2D(1024, 3, **conv_config)(pool4)\nconv5 = Conv2D(1024, 3, **conv_config)(conv5)\ndrop5 = Dropout(0.5)(conv5)\nup6 = Conv2D(512, 3, **conv_config)(UpSampling2D(**up_config)(drop5))\nmerge6 = concatenate([drop4, up6], axis=-1)\nconv6 = Conv2D(512, 3, **conv_config)(merge6)\nconv6 = Conv2D(512, 3, **conv_config)(conv6)\nup7 = Conv2D(256, 3, **conv_config)(UpSampling2D(**up_config)(conv6))\nmerge7 = concatenate([conv3, up7], axis=-1)\nconv7 = Conv2D(256, 3, **conv_config)(merge7)\nconv7 = Conv2D(256, 3, **conv_config)(conv7)\nup8 = Conv2D(128, 3, **conv_config)(UpSampling2D(**up_config)(conv7))\nmerge8 = concatenate([conv2, up8], axis=-1)\nconv8 = Conv2D(128, 3, **conv_config)(merge8)\nconv8 = Conv2D(128, 3, **conv_config)(conv8)\nup9 = Conv2D(64, 3, **conv_config)(UpSampling2D(**up_config)(conv8))\nmerge9 = concatenate([conv1, up9], axis=-1)\nconv9 = Conv2D(64, 3, **conv_config)(merge9)\nconv9 = Conv2D(64, 3, **conv_config)(conv9)\nconv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\nmodel = Model(inputs=inputs, outputs=conv10)\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/wideresnet.html", "title": "wideresnet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/wideresnet.html#fastestimator.fastestimator.architecture.tensorflow.wideresnet.WideResidualNetwork", "title": "<code>WideResidualNetwork</code>", "text": "<p>Creates a Wide Residual Network with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> required <code>depth</code> <code>int</code> <p>Depth of the network. Compute N = (n - 4) / 6.    For a depth of 16, n = 16, N = (16 - 4) / 6 = 2    For a depth of 28, n = 28, N = (28 - 4) / 6 = 4    For a depth of 40, n = 40, N = (40 - 4) / 6 = 6</p> <code>28</code> <code>widen_factor</code> <code>int</code> <p>Width of the network.</p> <code>10</code> <code>dropout</code> <code>float</code> <p>Adds dropout if value is greater than 0.0.</p> <code>0.0</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <code>activation</code> <code>Optional[str]</code> <p>activation function for last dense layer.</p> <code>'softmax'</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A Keras Model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If (depth - 4) is not divisible by 6.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\wideresnet.py</code> <pre><code>def WideResidualNetwork(input_shape: Tuple[int, int, int],\ndepth: int = 28,\nwiden_factor: int = 10,\ndropout: float = 0.0,\nclasses: int = 10,\nactivation: Optional[str] = 'softmax') -&gt; tf.keras.Model:\n\"\"\"Creates a Wide Residual Network with specified parameters.\n    Args:\n        input_shape: The size of the input tensor (height, width, channels).\n        depth: Depth of the network. Compute N = (n - 4) / 6.\n               For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n               For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n               For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n        widen_factor: Width of the network.\n        dropout: Adds dropout if value is greater than 0.0.\n        classes: The number of outputs the model should generate.\n        activation: activation function for last dense layer.\n    Returns:\n        A Keras Model.\n    Raises:\n        ValueError: If (depth - 4) is not divisible by 6.\n    \"\"\"\nif (depth - 4) % 6 != 0:\nraise ValueError('Depth of the network must be such that (depth - 4)' 'should be divisible by 6.')\nimg_input = layers.Input(shape=input_shape)\ninputs = img_input\nx = __create_wide_residual_network(classes, img_input, depth, widen_factor, dropout, activation)\n# Create model.\nmodel = Model(inputs, x)\nreturn model\n</code></pre>"}, {"location": "fastestimator/backend/_abs.html", "title": "_abs", "text": ""}, {"location": "fastestimator/backend/_abs.html#fastestimator.fastestimator.backend._abs.abs", "title": "<code>abs</code>", "text": "<p>Compute the absolute value of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.abs(n)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.abs(t)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.abs(p)  # [2, 7, 19]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute value of <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_abs.py</code> <pre><code>def abs(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the absolute value of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.abs(n)  # [2, 7, 19]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.abs(t)  # [2, 7, 19]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.abs(p)  # [2, 7, 19]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The absolute value of `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.abs(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.abs(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.abs(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_argmax.html", "title": "_argmax", "text": ""}, {"location": "fastestimator/backend/_argmax.html#fastestimator.fastestimator.backend._argmax.argmax", "title": "<code>argmax</code>", "text": "<p>Compute the index of the maximum value along a given axis of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>int</code> <p>Which axis to compute the index along.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices corresponding to the maximum values within <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_argmax.py</code> <pre><code>def argmax(tensor: Tensor, axis: int = 0) -&gt; Tensor:\n\"\"\"Compute the index of the maximum value along a given axis of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to compute the index along.\n    Returns:\n        The indices corresponding to the maximum values within `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.argmax(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.max(dim=axis, keepdim=False)[1]\nelif isinstance(tensor, np.ndarray):\nreturn np.argmax(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_binary_crossentropy.html", "title": "_binary_crossentropy", "text": ""}, {"location": "fastestimator/backend/_binary_crossentropy.html#fastestimator.fastestimator.backend._binary_crossentropy.binary_crossentropy", "title": "<code>binary_crossentropy</code>", "text": "<p>Compute binary crossentropy.</p> <p>This method is applicable when there are only two label classes (zero and one). There should be a single floating point prediction per example.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [1], [0]])\npred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\nweights = tf.lookup.StaticHashTable(\ntf.lookup.KeyValueTensorInitializer(tf.constant([1]), tf.constant([2.0])), default_value=1.0)\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.210, 0.356, 0.446, 0.105]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [1], [0]])\npred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\nweights = {1: 2.0}\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.210, 0.356, 0.446, 0.105]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (batch, ...). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with the same shape as <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <code>class_weights</code> <code>Optional[Weight_Dict]</code> <p>Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay more attention to samples from an under-represented class.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The binary crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a tensor with</p> <code>Tensor</code> <p>the same shape as <code>y_true</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_binary_crossentropy.py</code> <pre><code>def binary_crossentropy(y_pred: Tensor,\ny_true: Tensor,\nfrom_logits: bool = False,\naverage_loss: bool = True,\nclass_weights: Optional[Weight_Dict] = None) -&gt; Tensor:\n\"\"\"Compute binary crossentropy.\n    This method is applicable when there are only two label classes (zero and one). There should be a single floating\n    point prediction per example.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [1], [0]])\n    pred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\n    weights = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(tf.constant([1]), tf.constant([2.0])), default_value=1.0)\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.210, 0.356, 0.446, 0.105]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [1], [0]])\n    pred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\n    weights = {1: 2.0}\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.210, 0.356, 0.446, 0.105]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (batch, ...). dtype: float32 or float16.\n        y_true: Ground truth class labels with the same shape as `y_pred`. dtype: int or float32 or float16.\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n        class_weights: Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay\n            more attention to samples from an under-represented class.\n    Returns:\n        The binary crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a tensor with\n        the same shape as `y_true`.\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert isinstance(y_pred, torch.Tensor) or tf.is_tensor(y_pred), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, torch.Tensor) or tf.is_tensor(y_true), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.binary_crossentropy(y_pred=y_pred,\ny_true=tf.reshape(y_true, tf.shape(y_pred)),\nfrom_logits=from_logits)\nif class_weights is not None:\nsample_weights = class_weights.lookup(\ntf.cast(tf.reshape(y_true, tf.shape(ce)), dtype=class_weights.key_dtype))\nce = ce * sample_weights\nce = tf.reshape(ce, [tf.shape(ce)[0], -1])\nce = tf.reduce_mean(ce, 1)\nelse:\ny_true = y_true.to(torch.float)\nif from_logits:\nce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nelse:\nce = torch.nn.BCELoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nif class_weights is not None:\nsample_weights = torch.ones_like(y_true, dtype=torch.float)\nfor key in class_weights.keys():\nsample_weights[y_true == key] = class_weights[key]\nce = ce * sample_weights.reshape(ce.shape)\nce = ce.view(ce.shape[0], -1)\nce = torch.mean(ce, dim=1)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/_cast.html", "title": "_cast", "text": ""}, {"location": "fastestimator/backend/_cast.html#fastestimator.fastestimator.backend._cast.cast", "title": "<code>cast</code>", "text": "<p>Cast the data to a specific data type recursively.</p> This method can be used with Numpy data <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nfe.backend.to_type(data)\n# {'x': dtype('float64'), 'y': [dtype('float64'), dtype('float64')], 'z': {'key': dtype('float64')}}\ndata = fe.backend.cast(data, \"float16\")\nfe.backend.to_type(data)\n# {'x': dtype('float16'), 'y': [dtype('float16'), dtype('float16')], 'z': {'key': dtype('float16')}}\n</code></pre> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\nfe.backend.to_type(data) # {'x': tf.float32, 'y': [tf.float32, tf.float32], 'z': {'key': tf.float32}}\ndata = fe.backend.cast(data, \"uint8\")\nfe.backend.to_type(data) # {'x': tf.uint8, 'y': [tf.uint8, tf.uint8], 'z': {'key': tf.uint8}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nfe.backend.to_type(data) # {'x': torch.float32, 'y': [torch.float32, torch.float32], 'z': {'key': torch.float32}}\ndata = fe.backend.cast(data, \"float64\")\nfe.backend.to_type(data) # {'x': torch.float64, 'y': [torch.float64, torch.float64], 'z': {'key': torch.float64}}\n</code></pre></p> <p>Args:     data: A tensor or possibly nested collection of tensors.     dtype: Target reference data type, can be one of following: uint8, int8, int16, int32, int64, float16, float32, float64. Tensor.</p> <p>Returns:     A collection with the same structure as <code>data</code> with reference data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_cast.py</code> <pre><code>def cast(data: Union[Collection, Tensor], dtype: Union[str, Tensor]) -&gt; Union[Collection, Tensor]:\n\"\"\"Cast the data to a specific data type recursively.\n   This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    fe.backend.to_type(data)\n    # {'x': dtype('float64'), 'y': [dtype('float64'), dtype('float64')], 'z': {'key': dtype('float64')}}\n    data = fe.backend.cast(data, \"float16\")\n    fe.backend.to_type(data)\n    # {'x': dtype('float16'), 'y': [dtype('float16'), dtype('float16')], 'z': {'key': dtype('float16')}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    fe.backend.to_type(data) # {'x': tf.float32, 'y': [tf.float32, tf.float32], 'z': {'key': tf.float32}}\n    data = fe.backend.cast(data, \"uint8\")\n    fe.backend.to_type(data) # {'x': tf.uint8, 'y': [tf.uint8, tf.uint8], 'z': {'key': tf.uint8}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    fe.backend.to_type(data) # {'x': torch.float32, 'y': [torch.float32, torch.float32], 'z': {'key': torch.float32}}\n    data = fe.backend.cast(data, \"float64\")\n    fe.backend.to_type(data) # {'x': torch.float64, 'y': [torch.float64, torch.float64], 'z': {'key': torch.float64}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        dtype: Target reference data type, can be one of following: uint8, int8, int16, int32, int64, float16, float32, float64. Tensor.\n    Returns:\n        A collection with the same structure as `data` with reference data type.\n    \"\"\"\nif isinstance(dtype, str):\nif isinstance(data, dict):\nreturn {key: cast(value, dtype) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [cast(val, dtype) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([cast(val, dtype) for val in data])\nelif isinstance(data, set):\nreturn set([cast(val, dtype) for val in data])\nelif tf.is_tensor(data):\nreturn tf.cast(data, STRING_TO_TF_DTYPE[dtype])\nelif isinstance(data, torch.Tensor):\nreturn data.type(STRING_TO_TORCH_DTYPE[dtype])\nelse:\nreturn np.array(data, dtype=dtype)\nelif tf.is_tensor(dtype) or isinstance(dtype, torch.Tensor) or isinstance(dtype, np.ndarray):\nif tf.is_tensor(dtype):\nreturn tf.cast(data, dtype.dtype)\nelif isinstance(dtype, torch.Tensor):\nreturn torch.tensor(data).type(dtype.dtype)\nelse:\nreturn np.array(data, dtype=dtype.dtype)\nelse:\nValueError(\"Unexpected reference data type.\")\n</code></pre>"}, {"location": "fastestimator/backend/_categorical_crossentropy.html", "title": "_categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/_categorical_crossentropy.html#fastestimator.fastestimator.backend._categorical_crossentropy.categorical_crossentropy", "title": "<code>categorical_crossentropy</code>", "text": "<p>Compute categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nweights = tf.lookup.StaticHashTable(\ntf.lookup.KeyValueTensorInitializer(tf.constant([1, 2]), tf.constant([2.0, 3.0])), default_value=1.0)\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.446, 0.105, 1.068]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nweights = {1: 2.0, 2: 3.0}\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.446, 0.105, 1.068]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a softmax will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <code>class_weights</code> <code>Optional[Weight_Dict]</code> <p>Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay more attention to samples from an under-represented class.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_categorical_crossentropy.py</code> <pre><code>def categorical_crossentropy(y_pred: Tensor,\ny_true: Tensor,\nfrom_logits: bool = False,\naverage_loss: bool = True,\nclass_weights: Optional[Weight_Dict] = None) -&gt; Tensor:\n\"\"\"Compute categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    weights = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(tf.constant([1, 2]), tf.constant([2.0, 3.0])), default_value=1.0)\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.446, 0.105, 1.068]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    weights = {1: 2.0, 2: 3.0}\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.446, 0.105, 1.068]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32 or float16.\n        y_true: Ground truth class labels with a shape like `y_pred`. dtype: int or float32 or float16.\n        from_logits: Whether y_pred is from logits. If True, a softmax will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n        class_weights: Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay\n            more attention to samples from an under-represented class.\n    Returns:\n        The categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nif class_weights is not None:\nsample_weights = class_weights.lookup(tf.math.argmax(y_true, axis=-1, output_type=class_weights.key_dtype))\nce = ce * sample_weights\nelse:\ny_true = y_true.to(torch.float)\nce = _categorical_crossentropy_torch(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nif class_weights is not None:\ny_class = torch.argmax(y_true, dim=-1)\nsample_weights = torch.ones_like(y_class, dtype=torch.float)\nfor key in class_weights.keys():\nsample_weights[y_class == key] = class_weights[key]\nce = ce * sample_weights.reshape(ce.shape)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/_check_nan.html", "title": "_check_nan", "text": ""}, {"location": "fastestimator/backend/_check_nan.html#fastestimator.fastestimator.backend._check_nan.check_nan", "title": "<code>check_nan</code>", "text": "<p>Checks if the input contains NaN values.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, np.NaN]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[np.NaN, 6.0], [7.0, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [np.NaN, 8.0]]])\nb = fe.backend.check_nan(n)  # True\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>Union[int, float, np.ndarray, tf.Tensor, torch.Tensor]</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True iff <code>val</code> contains NaN</p> Source code in <code>fastestimator\\fastestimator\\backend\\_check_nan.py</code> <pre><code>def check_nan(val: Union[int, float, np.ndarray, tf.Tensor, torch.Tensor]) -&gt; bool:\n\"\"\"Checks if the input contains NaN values.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, np.NaN]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[np.NaN, 6.0], [7.0, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [np.NaN, 8.0]]])\n    b = fe.backend.check_nan(n)  # True\n    ```\n    Args:\n        val: The input value.\n    Returns:\n        True iff `val` contains NaN\n    \"\"\"\nif tf.is_tensor(val):\nreturn tf.reduce_any(tf.math.is_nan(val)) or tf.reduce_any(tf.math.is_inf(val))\nelif isinstance(val, torch.Tensor):\nreturn torch.isnan(val).any() or torch.isinf(val).any()\nelse:\nreturn np.isnan(val).any() or np.isinf(val).any()\n</code></pre>"}, {"location": "fastestimator/backend/_clip_by_value.html", "title": "_clip_by_value", "text": ""}, {"location": "fastestimator/backend/_clip_by_value.html#fastestimator.fastestimator.backend._clip_by_value.clip_by_value", "title": "<code>clip_by_value</code>", "text": "<p>Clip a tensor such that <code>min_value</code> &lt;= tensor &lt;= <code>max_value</code>.</p> <p>Given an interval, values outside the interval are clipped. If <code>min_value</code> or <code>max_value</code> is not provided then clipping is not performed on lower or upper interval edge respectively.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(n, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(t, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\nb = fe.backend.clip_by_value(p, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>min_value</code> <code>Union[int, float, Tensor, None]</code> <p>The minimum value to clip to.</p> <code>None</code> <code>max_value</code> <code>Union[int, float, Tensor, None]</code> <p>The maximum value to clip to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code>, with it's values clipped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_clip_by_value.py</code> <pre><code>def clip_by_value(tensor: Tensor,\nmin_value: Union[int, float, Tensor, None] = None,\nmax_value: Union[int, float, Tensor, None] = None) -&gt; Tensor:\n\"\"\"Clip a tensor such that `min_value` &lt;= tensor &lt;= `max_value`.\n    Given an interval, values outside the interval are clipped. If `min_value` or `max_value` is not provided then\n    clipping is not performed on lower or upper interval edge respectively.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(n, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(t, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    b = fe.backend.clip_by_value(p, min_value=-2) # [-2, 4, 2, 0, 9, -2]\n    ```\n    Args:\n        tensor: The input value.\n        min_value: The minimum value to clip to.\n        max_value: The maximum value to clip to.\n    Returns:\n        The `tensor`, with it's values clipped.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nassert min_value is not None or max_value is not None, \"Both min_value and max_value must not be NoneType\"\nif tf.is_tensor(tensor):\nif min_value is None:\nreturn tf.math.minimum(tensor, max_value)\nelif max_value is None:\nreturn tf.math.maximum(tensor, min_value)\nelse:\nreturn tf.clip_by_value(tensor, clip_value_min=min_value, clip_value_max=max_value)\nelif isinstance(tensor, torch.Tensor):\nif isinstance(min_value, torch.Tensor):\nmin_value = min_value.item()\nif isinstance(max_value, torch.Tensor):\nmax_value = max_value.item()\nreturn tensor.clamp(min=min_value, max=max_value)\nelif isinstance(tensor, np.ndarray):\nreturn np.clip(tensor, a_min=min_value, a_max=max_value)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_concat.html", "title": "_concat", "text": ""}, {"location": "fastestimator/backend/_concat.html#fastestimator.fastestimator.backend._concat.concat", "title": "<code>concat</code>", "text": "<p>Concatenate a list of <code>tensors</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\nb = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\nb = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\nb = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>List[Tensor]</code> <p>A list of tensors to be concatenated.</p> required <code>axis</code> <code>int</code> <p>The axis along which to concatenate the input.</p> <code>0</code> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_concat.py</code> <pre><code>def concat(tensors: List[Tensor], axis: int = 0) -&gt; Optional[Tensor]:\n\"\"\"Concatenate a list of `tensors` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\n    b = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\n    b = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\n    b = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    Args:\n        tensors: A list of tensors to be concatenated.\n        axis: The axis along which to concatenate the input.\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensors` is an unacceptable data type.\n    \"\"\"\nif len(tensors) == 0:\nreturn None\nif tf.is_tensor(tensors[0]):\nreturn tf.concat(tensors, axis=axis)\nelif isinstance(tensors[0], torch.Tensor):\nreturn torch.cat(tensors, dim=axis)\nelif isinstance(tensors[0], np.ndarray):\nreturn np.concatenate(tensors, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensors[0])))\n</code></pre>"}, {"location": "fastestimator/backend/_convert_tensor_precision.html", "title": "_convert_tensor_precision", "text": ""}, {"location": "fastestimator/backend/_convert_tensor_precision.html#fastestimator.fastestimator.backend._convert_tensor_precision.convert_tensor_precision", "title": "<code>convert_tensor_precision</code>", "text": "<p>Adjust the input data precision based of environment precision.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The precision adjusted data(16 bit for mixed precision, 32 bit otherwise).</p> Source code in <code>fastestimator\\fastestimator\\backend\\_convert_tensor_precision.py</code> <pre><code>def convert_tensor_precision(tensor: Tensor) -&gt; Tensor:\n\"\"\"\n        Adjust the input data precision based of environment precision.\n        Args:\n            tensor: The input value.\n        Returns:\n            The precision adjusted data(16 bit for mixed precision, 32 bit otherwise).\n    \"\"\"\nprecision = 'float32'\nif mixed_precision.global_policy().compute_dtype == 'float16':\nprecision = 'float16'\nreturn cast(tensor, precision)\n</code></pre>"}, {"location": "fastestimator/backend/_dice_score.html", "title": "_dice_score", "text": ""}, {"location": "fastestimator/backend/_dice_score.html#fastestimator.fastestimator.backend._dice_score.cast", "title": "<code>cast</code>", "text": "<p>Cast y_true, epsilon to desired data type.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth class labels with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: int or float32 or float16.</p> required <code>epsilon</code> <p>Floating point value to avoid divide by zero error.</p> required <code>dtype</code> <p>Datatype to which the y_true and epsilon should be converted to.</p> required <p>Returns:</p> Type Description <p>Converted y_true and epsilon values.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> are unacceptable data types. if data type is other than np.array, tensor.Tensor, tf.Tensor.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_dice_score.py</code> <pre><code>def cast(y_true, epsilon, dtype):\n\"\"\"\n        Cast y_true, epsilon to desired data type.\n        Args:\n            y_true: Ground truth class labels with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: int or float32 or float16.\n            epsilon: Floating point value to avoid divide by zero error.\n            dtype: Datatype to which the y_true and epsilon should be converted to.\n        Returns:\n            Converted y_true and epsilon values.\n        Raises:\n            AssertionError: If `y_true` are unacceptable data types. if data type is other than np.array, tensor.Tensor, tf.Tensor.\n    \"\"\"\nif dtype not in allowed_data_types:\nraise ValueError(\"Provided datatype {} is not supported, only {} data types are supported\".format(\ndtype, allowed_data_types))\nif tf.is_tensor(y_true):\nreturn tf.cast(y_true, dtype), tf.cast(epsilon, dtype)\nelif isinstance(y_true, torch.Tensor):\nreturn y_true.type(dtype), torch.tensor(epsilon).type(dtype)\nelif isinstance(y_true, np.ndarray):\nreturn np.array(y_true, dtype=dtype), np.array(epsilon, dtype=dtype)\nelse:\nraise ValueError(\"Unsupported tensor type.\")\n</code></pre>"}, {"location": "fastestimator/backend/_dice_score.html#fastestimator.fastestimator.backend._dice_score.dice_score", "title": "<code>dice_score</code>", "text": "<p>Compute Dice score.</p> <p>This method can be used with Numpy data: <pre><code>true = np.array([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\npred = np.array([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\nb = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\nb = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\nThis method can be used with TensorFlow tensors:\n```python\ntrue = tf.constant([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\npred = tf.constant([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\nb = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\nb = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\npred = torch.tensor([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\nb = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\nb = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\nb = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\n</code></pre></p> <p>```</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>soft_dice</code> <code>bool</code> <p>Whether to square elements. If True, square of elements is added.</p> <code>False</code> <code>sample_average</code> <code>bool</code> <p>Whether to average the element-wise dice score.</p> <code>False</code> <code>channel_average</code> <code>bool</code> <p>Whether to average the channel wise dice score.</p> <code>False</code> <code>epsilon</code> <code>float</code> <p>floating point value to avoid divide by zero error.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The dice score between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_sample</code> is True, else a tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types. if data type is other than np.array, tensor.Tensor, tf.Tensor.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_dice_score.py</code> <pre><code>def dice_score(y_pred: Tensor,\ny_true: Tensor,\nsoft_dice: bool = False,\nsample_average: bool = False,\nchannel_average: bool = False,\nepsilon: float = 1e-6) -&gt; Tensor:\n\"\"\"\n    Compute Dice score.\n    This method can be used with Numpy data:\n    ```python\n    true = np.array([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\n    pred = np.array([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\n    b = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\n    pred = tf.constant([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\n    b = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[[[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [1, 0, 1]]]])\n    pred = torch.tensor([[[[0, 1, 0], [1, 0, 0], [1, 0, 1]], [[0, 1, 1], [1, 0, 1], [0, 0, 0]], [[0, 0, 1], [1, 0, 1], [1, 0, 1]]]])\n    b = fe.backend.dice_score(y_pred=pred, y_true=true)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, soft_dice=True)  # 0.161\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, channel_average=True)  # 0.1636\n    b = fe.backend.dice_score(y_pred=pred, y_true=true, sample_average=True)  # 0.161\n    ```\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: float32 or float16.\n        y_true: Ground truth class labels with a shape like `y_pred`. dtype: int or float32 or float16.\n        soft_dice: Whether to square elements. If True, square of elements is added.\n        sample_average: Whether to average the element-wise dice score.\n        channel_average: Whether to average the channel wise dice score.\n        epsilon: floating point value to avoid divide by zero error.\n    Returns:\n        The dice score between `y_pred` and `y_true`. A scalar if `average_sample` is True, else a tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types. if data type is other than np.array, tensor.Tensor, tf.Tensor.\n    \"\"\"\ny_true, epsilon = cast(y_true, epsilon, y_pred.dtype)\naxis = get_axis(y_true, channel_average)\nkeep_dims = False\nif axis == None:\nkeep_dims = True\nnumerator = reduce_sum(y_true * y_pred, axis=axis, keepdims=keep_dims)\ndenominator = get_denominator(y_true, y_pred, soft_dice)\ndenominator = reduce_sum(denominator, axis=axis, keepdims=keep_dims)\ndice_score = (2 * numerator) / (denominator + epsilon)\nif channel_average:\ndice_score = reduce_mean(dice_score, axis=-1)\nif sample_average:\ndice_score = reduce_mean(dice_score)\nreturn dice_score\n</code></pre>"}, {"location": "fastestimator/backend/_dice_score.html#fastestimator.fastestimator.backend._dice_score.get_axis", "title": "<code>get_axis</code>", "text": "<p>Get the axis to apply reduced_sum on.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: int or float32 or float16.</p> required <code>channel_average</code> <code>bool</code> <p>Whether to average the channel wise dice score.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The axis on which reduce_sum needs to be applied.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_dice_score.py</code> <pre><code>def get_axis(y_true: Tensor, channel_average: bool) -&gt; Tensor:\n\"\"\"\n        Get the axis to apply reduced_sum on.\n        Args:\n            y_true: Ground truth class labels with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: int or float32 or float16.\n            channel_average: Whether to average the channel wise dice score.\n        Returns:\n            The axis on which reduce_sum needs to be applied.\n    \"\"\"\ndims = len(y_true.shape)\nif dims &lt;= 2:\nreturn None\nelse:\ninput_axis = list(range(dims))\naxis = input_axis[1:]\nif channel_average:\nif tf.is_tensor(y_true) or isinstance(y_true, np.ndarray):\naxis = input_axis[1:-1]\nelif isinstance(y_true, torch.Tensor):\naxis = input_axis[2:]\nelse:\nraise ValueError(\"Unsupported tensor type.\")\nreturn axis\n</code></pre>"}, {"location": "fastestimator/backend/_dice_score.html#fastestimator.fastestimator.backend._dice_score.get_denominator", "title": "<code>get_denominator</code>", "text": "<p>Calculate sum/squared sum of y_true and y_pred</p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like <code>y_pred</code>. dtype: int or float32 or float16.</p> required <code>soft_dice</code> <code>bool</code> <p>Whether to add direct sum or square sum of inputs</p> required Return <p>The sum or squared sum of y_pred and y_true</p> Source code in <code>fastestimator\\fastestimator\\backend\\_dice_score.py</code> <pre><code>def get_denominator(y_true: Tensor, y_pred: Tensor, soft_dice: bool) -&gt; Tensor:\n\"\"\"\n        Calculate sum/squared sum of y_true and y_pred\n        Args:\n            y_pred: Prediction with a shape like (Batch, C, H, W) for torch and (Batch, H, W, C) for tensorflow or numpy. dtype: float32 or float16.\n            y_true: Ground truth class labels with a shape like `y_pred`. dtype: int or float32 or float16.\n            soft_dice: Whether to add direct sum or square sum of inputs\n        Return:\n            The sum or squared sum of y_pred and y_true\n    \"\"\"\nif soft_dice:\nreturn y_true**2 + y_pred**2\nelse:\nreturn y_true + y_pred\n</code></pre>"}, {"location": "fastestimator/backend/_exp.html", "title": "_exp", "text": ""}, {"location": "fastestimator/backend/_exp.html#fastestimator.fastestimator.backend._exp.exp", "title": "<code>exp</code>", "text": "<p>Compute e^Tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 2, 1])\nb = fe.backend.exp(n)  # [0.1353, 7.3891, 2.7183]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2.0, 2, 1])\nb = fe.backend.exp(t)  # [0.1353, 7.3891, 2.7183]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2.0, 2, 1])\nb = fe.backend.exp(p)  # [0.1353, 7.3891, 2.7183]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The exponentiated <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_exp.py</code> <pre><code>def exp(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute e^Tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 2, 1])\n    b = fe.backend.exp(n)  # [0.1353, 7.3891, 2.7183]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2.0, 2, 1])\n    b = fe.backend.exp(t)  # [0.1353, 7.3891, 2.7183]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2.0, 2, 1])\n    b = fe.backend.exp(p)  # [0.1353, 7.3891, 2.7183]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The exponentiated `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.exp(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.exp(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.exp(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_expand_dims.html", "title": "_expand_dims", "text": ""}, {"location": "fastestimator/backend/_expand_dims.html#fastestimator.fastestimator.backend._expand_dims.expand_dims", "title": "<code>expand_dims</code>", "text": "<p>Create a new dimension in <code>tensor</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([2,7,5])\nb = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([2,7,5])\nb = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([2,7,5])\nb = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input to be modified, having n dimensions.</p> required <code>axis</code> <code>int</code> <p>Which axis should the new axis be inserted along. Must be in the range [-n-1, n].</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_expand_dims.py</code> <pre><code>def expand_dims(tensor: Tensor, axis: int = 1) -&gt; Tensor:\n\"\"\"Create a new dimension in `tensor` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([2,7,5])\n    b = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([2,7,5])\n    b = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([2,7,5])\n    b = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n    ```\n    Args:\n        tensor: The input to be modified, having n dimensions.\n        axis: Which axis should the new axis be inserted along. Must be in the range [-n-1, n].\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.expand_dims(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.unsqueeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.expand_dims(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_feed_forward.html", "title": "_feed_forward", "text": ""}, {"location": "fastestimator/backend/_feed_forward.html#fastestimator.fastestimator.backend._feed_forward.feed_forward", "title": "<code>feed_forward</code>", "text": "<p>Run a forward step on a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.architecture.tensorflow.LeNet(classes=2)\nx = tf.ones((3,28,28,1))  # (batch, height, width, channels)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.architecture.pytorch.LeNet(classes=2)\nx = torch.ones((3,1,28,28))  # (batch, channels, height, width)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network to run the forward step through.</p> required <code>x</code> <code>Union[Tensor, np.ndarray]</code> <p>One or more input tensor for the <code>model</code>. This value will be auto-cast to either a tf.Tensor or torch.Tensor as applicable for the <code>model</code>.</p> <code>()</code> <code>training</code> <code>bool</code> <p>Whether this forward step is part of training or not. This may impact the behavior of <code>model</code> layers such as dropout.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The result of <code>model(x)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_feed_forward.py</code> <pre><code>def feed_forward(model: Union[tf.keras.Model, torch.nn.Module], *x: Union[Tensor, np.ndarray],\ntraining: bool = True) -&gt; Tensor:\n\"\"\"Run a forward step on a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.architecture.tensorflow.LeNet(classes=2)\n    x = tf.ones((3,28,28,1))  # (batch, height, width, channels)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.architecture.pytorch.LeNet(classes=2)\n    x = torch.ones((3,1,28,28))  # (batch, channels, height, width)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    Args:\n        model: A neural network to run the forward step through.\n        x: One or more input tensor for the `model`. This value will be auto-cast to either a tf.Tensor or torch.Tensor\n            as applicable for the `model`.\n        training: Whether this forward step is part of training or not. This may impact the behavior of `model` layers\n            such as dropout.\n    Returns:\n        The result of `model(x)`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nif isinstance(model, tf.keras.Model):\nx = to_tensor(x, \"tf\")\nx = model(*x, training=training)\nelif isinstance(model, torch.nn.Module):\nmodel.train(mode=training)\nx = to_tensor(x, \"torch\")\nx = model(*x)\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn x\n</code></pre>"}, {"location": "fastestimator/backend/_flip.html", "title": "_flip", "text": ""}, {"location": "fastestimator/backend/_flip.html#fastestimator.fastestimator.backend._flip.flip", "title": "<code>flip</code>", "text": "<p>Reverse the order of a given <code>tensor</code> elements along a given axis.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.flip(n,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\nb = fe.backend.flip(n,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.flip(t,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\nb = fe.backend.flip(t,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.flip(p,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\nb = fe.backend.flip(p,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to flip.</p> required <code>axis</code> <code>List[int]</code> <p>The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> with axes flipped according to the <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_flip.py</code> <pre><code>def flip(tensor: Tensor, axis: List[int]) -&gt; Tensor:\n\"\"\"Reverse the order of a given `tensor` elements along a given axis.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.flip(n,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\n    b = fe.backend.flip(n,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.flip(t,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\n    b = fe.backend.flip(t,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.flip(p,axis = [0])  # [[[8,  9], [10, 11]], [[4,  5], [6,  7]], [[0,  1], [2,  3]]]\n    b = fe.backend.flip(p,axis = [0,1])  # [[[10, 11],[8,  9]], [[6,  7], [4,  5]], [[2,  3], [0,  1]]]\n    ```\n    Args:\n        tensor: The tensor to flip.\n        axis: The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).\n    Returns:\n        The `tensor` with axes flipped according to the `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reverse(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.flip(tensor, dims=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.flip(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_focal_loss.html", "title": "_focal_loss", "text": ""}, {"location": "fastestimator/backend/_focal_loss.html#fastestimator.fastestimator.backend._focal_loss.focal_loss", "title": "<code>focal_loss</code>", "text": "<p>Calculate the focal loss between two tensors.</p> <p>Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py . Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [1], [1], [0], [0], [0]])\npred = tf.constant([[0.97], [0.91], [0.73], [0.27], [0.09], [0.03]])\nb = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=None, alpha=None) #0.1464\nb = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=2.0, alpha=0.25) #0.00395\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [1], [1], [0], [0], [0]])\npred = torch.tensor([[0.97], [0.91], [0.73], [0.27], [0.09], [0.03]])\nb = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=None, alpha=None) #0.1464\nb = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=2.0, alpha=0.25) #0.004\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with shape([batch_size, d0, .. dN]), which should take values of 1 or 0.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <code>alpha</code> <code>float</code> <p>Weighting factor in range (0,1) to balance     positive vs negative examples or (-1/None) to ignore. Default = 0.25</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>Exponent of the modulating factor (1 - p_t) to    balance easy vs hard examples.</p> <code>2.0</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize focal loss along samples based on number of positive classes per samples.</p> <code>True</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without sigmoid).</p> <code>False</code> <code>reduction</code> <code>str</code> <p>'none' | 'mean' | 'sum'      'none': No reduction will be applied to the output.      'mean': The output will be averaged.      'sum': The output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Focal loss between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> or 'y_true' is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_focal_loss.py</code> <pre><code>def focal_loss(y_true: Tensor,\ny_pred: Tensor,\ngamma: float = 2.0,\nalpha: float = 0.25,\nfrom_logits: bool = False,\nnormalize: bool = True,\nreduction: str = \"mean\") -&gt; Tensor:\n\"\"\"Calculate the focal loss between two tensors.\n    Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [1], [1], [0], [0], [0]])\n    pred = tf.constant([[0.97], [0.91], [0.73], [0.27], [0.09], [0.03]])\n    b = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=None, alpha=None) #0.1464\n    b = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=2.0, alpha=0.25) #0.00395\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [1], [1], [0], [0], [0]])\n    pred = torch.tensor([[0.97], [0.91], [0.73], [0.27], [0.09], [0.03]])\n    b = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=None, alpha=None) #0.1464\n    b = fe.backend.focal_loss(y_pred=pred, y_true=true, gamma=2.0, alpha=0.25) #0.004\n    ```\n    Args:\n        y_true: Ground truth class labels with shape([batch_size, d0, .. dN]), which should take values of 1 or 0.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n        alpha: Weighting factor in range (0,1) to balance\n                positive vs negative examples or (-1/None) to ignore. Default = 0.25\n        gamma: Exponent of the modulating factor (1 - p_t) to\n               balance easy vs hard examples.\n        normalize: Whether to normalize focal loss along samples based on number of positive classes per samples.\n        from_logits: Whether y_pred is logits (without sigmoid).\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n    Returns:\n        The Focal loss between `y_true` and `y_pred`\n    Raises:\n        ValueError: If `y_pred` or 'y_true' is an unacceptable data type.\n    \"\"\"\nif gamma is None or gamma &lt; 0:\nraise ValueError(\"Value of gamma should be greater than or equal to zero.\")\nif alpha is None or (alpha &lt; 0 or alpha &gt; 1):\nraise ValueError(\"Value of alpha can either be -1 or None or within range (0, 1)\")\nif tf.is_tensor(y_true):\ny_true = tf.cast(y_true, dtype=y_pred.dtype)\nfocal_loss = SigmoidFocalCrossEntropy(from_logits=from_logits,\nalpha=alpha,\ngamma=gamma,\nreduction=tf.keras.losses.Reduction.NONE)(y_pred=y_pred, y_true=y_true)\nelif isinstance(y_true, torch.Tensor):\ny_true = y_true.to(y_pred.dtype)\nfocal_loss = pytorch_focal_loss(y_pred=y_pred, y_true=y_true, alpha=alpha, gamma=gamma, from_logits=from_logits)\nelse:\nraise ValueError(\"Unsupported tensor type.\")\n# normalize along the batch size based on number of positive classes\nif normalize:\nfocal_reduce_axis = [*range(len(focal_loss.shape))][1:]\nfocal_loss = reduce_sum(focal_loss, axis=focal_reduce_axis)\ngt_reduce_axis = [*range(len(y_true.shape))][1:]\ngt_count = clip_by_value(reduce_sum(y_true, axis=gt_reduce_axis), min_value=1)\nfocal_loss = focal_loss / gt_count\nif reduction == \"mean\":\nfocal_loss = reduce_mean(focal_loss)\nelif reduction == \"sum\":\nfocal_loss = reduce_sum(focal_loss)\nreturn focal_loss\n</code></pre>"}, {"location": "fastestimator/backend/_focal_loss.html#fastestimator.fastestimator.backend._focal_loss.pytorch_focal_loss", "title": "<code>pytorch_focal_loss</code>", "text": "<p>Calculate the focal loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>torch.Tensor</code> <p>Ground truth class labels with shape([batch_size, d0, .. dN]), which should take values of 1 or 0.</p> required <code>y_pred</code> <code>torch.Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <code>alpha</code> <code>float</code> <p>Weighting factor in range (0,1) to balance     positive vs negative examples or (-1/None) to ignore. Default = 0.25</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>Exponent of the modulating factor (1 - p_t) to    balance easy vs hard examples.</p> <code>2</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without sigmoid).</p> <code>False</code> <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>Loss tensor.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_focal_loss.py</code> <pre><code>def pytorch_focal_loss(y_pred: torch.Tensor,\ny_true: torch.Tensor,\nalpha: float = 0.25,\ngamma: float = 2,\nfrom_logits: bool = False) -&gt; torch.Tensor:\n\"\"\"\n    Calculate the focal loss between two tensors.\n    Args:\n        y_true: Ground truth class labels with shape([batch_size, d0, .. dN]), which should take values of 1 or 0.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n        alpha: Weighting factor in range (0,1) to balance\n                positive vs negative examples or (-1/None) to ignore. Default = 0.25\n        gamma: Exponent of the modulating factor (1 - p_t) to\n               balance easy vs hard examples.\n        from_logits: Whether y_pred is logits (without sigmoid).\n    Returns:\n        Loss tensor.\n    \"\"\"\nif from_logits:\np = torch.sigmoid(y_pred)\nce_loss = F.binary_cross_entropy_with_logits(y_pred, y_true, reduction=\"none\")\nelse:\np = y_pred\nce_loss = F.binary_cross_entropy(y_pred, y_true, reduction=\"none\")\np_t = p * y_true + (1 - p) * (1 - y_true)\nloss = ce_loss * ((1 - p_t)**gamma)\nif alpha &gt;= 0:\nalpha_t = alpha * y_true + (1 - alpha) * (1 - y_true)\nloss = alpha_t * loss\nreturn loss\n</code></pre>"}, {"location": "fastestimator/backend/_gather.html", "title": "_gather", "text": ""}, {"location": "fastestimator/backend/_gather.html#fastestimator.fastestimator.backend._gather.gather", "title": "<code>gather</code>", "text": "<p>Gather specific indices from a tensor.</p> <p>The <code>indices</code> will automatically be cast to the correct type (tf, torch, np) based on the type of the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>ind = np.array([1, 0, 1])\nn = np.array([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(n, ind)  # [[2, 3], [0, 1], [2, 3]]\nn = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(n, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>ind = tf.constant([1, 0, 1])\nt = tf.constant([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(t, ind)  # [[2, 3], [0, 1], [2, 3]]\nt = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(t, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>ind = torch.tensor([1, 0, 1])\np = torch.tensor([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather(p, ind)  # [[2, 3], [0, 1], [2, 3]]\np = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather(p, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>A tensor to gather values from.</p> required <code>indices</code> <code>Tensor</code> <p>A tensor indicating which indices should be selected. These represent locations along the 0 axis.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the elements from <code>tensor</code> at the given <code>indices</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_gather.py</code> <pre><code>def gather(tensor: Tensor, indices: Tensor) -&gt; Tensor:\n\"\"\"Gather specific indices from a tensor.\n    The `indices` will automatically be cast to the correct type (tf, torch, np) based on the type of the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    ind = np.array([1, 0, 1])\n    n = np.array([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(n, ind)  # [[2, 3], [0, 1], [2, 3]]\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(n, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    ind = tf.constant([1, 0, 1])\n    t = tf.constant([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(t, ind)  # [[2, 3], [0, 1], [2, 3]]\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(t, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    ind = torch.tensor([1, 0, 1])\n    p = torch.tensor([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather(p, ind)  # [[2, 3], [0, 1], [2, 3]]\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather(p, ind)  # [[[4, 5], [6, 7]], [[0, 1], [2, 3]], [[4, 5], [6, 7]]]\n    ```\n    Args:\n        tensor: A tensor to gather values from.\n        indices: A tensor indicating which indices should be selected. These represent locations along the 0 axis.\n    Returns:\n        A tensor containing the elements from `tensor` at the given `indices`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nindices = to_tensor(indices, 'tf')\nindices = tf.cast(indices, tf.int64)\nreturn tf.gather(tensor, indices=squeeze(indices), axis=0)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor[squeeze(indices).type(torch.int64)]\nelif isinstance(tensor, np.ndarray):\nreturn np.take(tensor, squeeze(indices).astype('int64'), axis=0)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_gather_from_batch.html", "title": "_gather_from_batch", "text": ""}, {"location": "fastestimator/backend/_gather_from_batch.html#fastestimator.fastestimator.backend._gather_from_batch.gather_from_batch", "title": "<code>gather_from_batch</code>", "text": "<p>Gather specific indices from a batch of data.</p> <p>This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values. The <code>indices</code> will automatically be cast to the correct type (tf, torch, np) based on the type of the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>ind = np.array([1, 0, 1])\nn = np.array([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\nn = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>ind = tf.constant([1, 0, 1])\nt = tf.constant([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\nt = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>ind = torch.tensor([1, 0, 1])\np = torch.tensor([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\np = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>A tensor of shape (batch, d1, ..., dn).</p> required <code>indices</code> <code>Tensor</code> <p>A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (batch, d2, ..., dn) containing the elements from <code>tensor</code> at the given <code>indices</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_gather_from_batch.py</code> <pre><code>def gather_from_batch(tensor: Tensor, indices: Tensor) -&gt; Tensor:\n\"\"\"Gather specific indices from a batch of data.\n    This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values.\n    The `indices` will automatically be cast to the correct type (tf, torch, np) based on the type of the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    ind = np.array([1, 0, 1])\n    n = np.array([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    ind = tf.constant([1, 0, 1])\n    t = tf.constant([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    ind = torch.tensor([1, 0, 1])\n    p = torch.tensor([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    Args:\n        tensor: A tensor of shape (batch, d1, ..., dn).\n        indices: A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.\n    Returns:\n        A tensor of shape (batch, d2, ..., dn) containing the elements from `tensor` at the given `indices`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nindices = to_tensor(indices, 'tf')\nindices = tf.cast(indices, tf.int64)\nif len(indices.shape) == 1:  # Indices not batched\nindices = expand_dims(indices, 1)\nreturn tf.gather_nd(tensor, indices=indices, batch_dims=1)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor[torch.arange(tensor.shape[0]), squeeze(indices)]\nelif isinstance(tensor, np.ndarray):\nreturn tensor[np.arange(tensor.shape[0]), squeeze(indices)]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_get_gradient.html", "title": "_get_gradient", "text": ""}, {"location": "fastestimator/backend/_get_gradient.html#fastestimator.fastestimator.backend._get_gradient.get_gradient", "title": "<code>get_gradient</code>", "text": "<p>Calculate gradients of a target w.r.t sources.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.Variable([1.0, 2.0, 3.0])\nwith tf.GradientTape(persistent=True) as tape:\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\nb = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target (final) tensor.</p> required <code>sources</code> <code>Union[Iterable[Tensor], Tensor]</code> <p>A sequence of source (initial) tensors.</p> required <code>higher_order</code> <code>bool</code> <p>Whether the gradient will be used for higher order gradients.</p> <code>False</code> <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>TensorFlow gradient tape. Only needed when using the TensorFlow backend.</p> <code>None</code> <code>retain_graph</code> <code>bool</code> <p>Whether to retain PyTorch graph. Only valid when using the PyTorch backend.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Iterable[Tensor], Tensor]</code> <p>Gradient(s) of the <code>target</code> with respect to the <code>sources</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_get_gradient.py</code> <pre><code>def get_gradient(target: Tensor,\nsources: Union[Iterable[Tensor], Tensor],\nhigher_order: bool = False,\ntape: Optional[tf.GradientTape] = None,\nretain_graph: bool = True) -&gt; Union[Iterable[Tensor], Tensor]:\n\"\"\"Calculate gradients of a target w.r.t sources.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.Variable([1.0, 2.0, 3.0])\n    with tf.GradientTape(persistent=True) as tape:\n        y = x * x\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n    y = x * x\n    b = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\n    b = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n    ```\n    Args:\n        target: The target (final) tensor.\n        sources: A sequence of source (initial) tensors.\n        higher_order: Whether the gradient will be used for higher order gradients.\n        tape: TensorFlow gradient tape. Only needed when using the TensorFlow backend.\n        retain_graph: Whether to retain PyTorch graph. Only valid when using the PyTorch backend.\n    Returns:\n        Gradient(s) of the `target` with respect to the `sources`.\n    Raises:\n        ValueError: If `target` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(target):\nwith NonContext() if higher_order else tape.stop_recording():\ngradients = tape.gradient(target, sources)\nelif isinstance(target, torch.Tensor):\ngradients = torch.autograd.grad(target,\nsources,\ngrad_outputs=torch.ones_like(target),\nretain_graph=retain_graph,\ncreate_graph=higher_order,\nallow_unused=True,\nonly_inputs=True)\nif isinstance(sources, torch.Tensor):\n#  The behavior table of tf and torch backend\n#  ---------------------------------------------------------------\n#        | case 1                     | case 2                    |\n#  ---------------------------------------------------------------|\n#  tf    | target: tf.Tensor          | target: tf.Tensor         |\n#        | sources: tf.Tensor         | sources: [tf.Tensor]      |\n#        | gradients: tf.Tensor       | gradients: [tf.Tensor]    |\n# ----------------------------------------------------------------|\n#  torch | target: torch.Tensor       | target: tf.Tensor         |\n#        | sources: torch.Tensor      | sources: [tf.Tensor]      |\n#        | gradients: (torch.Tensor,) | gradients: (torch.Tensor,)|\n# ----------------------------------------------------------------\n# In order to make the torch behavior become the same as tf in case 1, need to unwrap the gradients when\n# source is not Iterable.\ngradients = gradients[0]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(target)))\nreturn gradients\n</code></pre>"}, {"location": "fastestimator/backend/_get_image_dims.html", "title": "_get_image_dims", "text": ""}, {"location": "fastestimator/backend/_get_image_dims.html#fastestimator.fastestimator.backend._get_image_dims.get_image_dims", "title": "<code>get_image_dims</code>", "text": "<p>Get the <code>tensor</code> channels, height, and width.</p> <p>This method can be used with Numpy data: <pre><code>n = np.random.random((2, 12, 12, 3))\nb = fe.backend.get_image_dims(n)  # (3, 12, 12)\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.random.uniform((2, 12, 12, 3))\nb = fe.backend.get_image_dims(t)  # (3, 12, 12)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.rand((2, 3, 12, 12))\nb = fe.backend.get_image_dims(p)  # (3, 12, 12)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Channels, height and width of the <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_get_image_dims.py</code> <pre><code>def get_image_dims(tensor: Tensor) -&gt; Tuple[int, int, int]:\n\"\"\"Get the `tensor` channels, height, and width.\n    This method can be used with Numpy data:\n    ```python\n    n = np.random.random((2, 12, 12, 3))\n    b = fe.backend.get_image_dims(n)  # (3, 12, 12)\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.random.uniform((2, 12, 12, 3))\n    b = fe.backend.get_image_dims(t)  # (3, 12, 12)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.rand((2, 3, 12, 12))\n    b = fe.backend.get_image_dims(p)  # (3, 12, 12)\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        Channels, height and width of the `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nassert len(tensor.shape) == 3 or len(tensor.shape) == 4, \\\n        f\"Number of dimensions of input must be either 3 or 4, but found {len(tensor.shape)} (shape: {tensor.shape})\"\nif tf.is_tensor(tensor):\nshape = tf.shape(tensor)\nchannels, height, width = shape[-1], shape[-3], shape[-2]\nif hasattr(channels, 'numpy'):\n# Running in eager mode, so can convert to integer\nchannels, height, width = channels.numpy().item(), height.numpy().item(), width.numpy().item()\nreturn channels, height, width\nelif isinstance(tensor, np.ndarray):\nreturn tensor.shape[-1], tensor.shape[-3], tensor.shape[-2]\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.shape[-3], tensor.shape[-2], tensor.shape[-1]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_get_lr.html", "title": "_get_lr", "text": ""}, {"location": "fastestimator/backend/_get_lr.html#fastestimator.fastestimator.backend._get_lr.get_lr", "title": "<code>get_lr</code>", "text": "<p>Get the learning rate of a given <code>model</code> generated by <code>fe.build</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to inspect.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The learning rate of <code>model</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_get_lr.py</code> <pre><code>def get_lr(model: Union[tf.keras.Model, torch.nn.Module]) -&gt; float:\n\"\"\"Get the learning rate of a given `model` generated by `fe.build`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    Args:\n        model: A neural network instance to inspect.\n    Returns:\n        The learning rate of `model`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"get_lr only accept models from fe.build\"\nif isinstance(model, tf.keras.Model):\nlr = tf.keras.backend.get_value(model.current_optimizer.lr)\nelif isinstance(model, torch.nn.Module):\nlr = model.current_optimizer.param_groups[0]['lr']\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn lr\n</code></pre>"}, {"location": "fastestimator/backend/_get_shape.html", "title": "_get_shape", "text": ""}, {"location": "fastestimator/backend/_get_shape.html#fastestimator.fastestimator.backend._get_shape.get_shape", "title": "<code>get_shape</code>", "text": "<p>Find shape of a given <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.get_shape(n)  # [3,2,2]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.get_shape(t)  # [3,2,2]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.get_shape(p)  # [3,2,2]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to find shape of.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Shape of the given 'tensor'.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_get_shape.py</code> <pre><code>def get_shape(tensor: Tensor) -&gt; Tensor:\n\"\"\"Find shape of a given `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.get_shape(n)  # [3,2,2]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.get_shape(t)  # [3,2,2]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.get_shape(p)  # [3,2,2]\n    ```\n    Args:\n        tensor: The tensor to find shape of.\n    Returns:\n        Shape of the given 'tensor'.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.shape(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.shape\nelif isinstance(tensor, np.ndarray):\nreturn tensor.shape\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_hinge.html", "title": "_hinge", "text": ""}, {"location": "fastestimator/backend/_hinge.html#fastestimator.fastestimator.backend._hinge.hinge", "title": "<code>hinge</code>", "text": "<p>Calculate the hinge loss between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\nb = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\nb = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels which should take values of 1 or -1.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The hinge loss between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_hinge.py</code> <pre><code>def hinge(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate the hinge loss between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\n    b = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[-1,1,1,-1], [1,1,1,1], [-1,-1,1,-1], [1,-1,-1,-1]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,-0.2,0.0,-0.7], [0.0,0.15,0.8,0.05], [1.0,-1.0,-1.0,-1.0]])\n    b = fe.backend.hinge(y_pred=pred, y_true=true)  # [0.8  1.2  0.85 0.  ]\n    ```\n    Args:\n        y_true: Ground truth class labels which should take values of 1 or -1.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n    Returns:\n        The hinge loss between `y_true` and `y_pred`\n    Raises:\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\ny_true = cast(y_true, 'float32')\nreturn reduce_mean(clip_by_value(1.0 - y_true * y_pred, min_value=0), axis=-1)\n</code></pre>"}, {"location": "fastestimator/backend/_huber.html", "title": "_huber", "text": ""}, {"location": "fastestimator/backend/_huber.html#fastestimator.fastestimator.backend._huber.huber", "title": "<code>huber</code>", "text": "<p>Calculate Huber Loss between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nHuber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.0031, 0.0175, 0.0081, 0.0000]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nHuber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.4387, 1.7387, 0.0000, 0.4387]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nHuber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.0031, 0.0175, 0.0081, 0.0000]\ntrue = torch.tensor([[1], [3], [2], [0]])\npred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\nHuber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.4387, 1.7387, 0.0000, 0.4387]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <code>beta</code> <code>float</code> <p>Threshold factor. Needs to be a positive number. dtype: float16 or float32.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The Huber loss between <code>y_true</code> and <code>y_pred</code> wrt beta.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> <code>ValueError</code> <p>If beta is less than 1 for Smooth L1 loss and Huber Loss</p> Source code in <code>fastestimator\\fastestimator\\backend\\_huber.py</code> <pre><code>def huber(y_true: Tensor, y_pred: Tensor, beta: float = 1.0) -&gt; Tensor:\n\"\"\"Calculate Huber Loss between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    Huber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.0031, 0.0175, 0.0081, 0.0000]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    Huber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.4387, 1.7387, 0.0000, 0.4387]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    Huber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.0031, 0.0175, 0.0081, 0.0000]\n    true = torch.tensor([[1], [3], [2], [0]])\n    pred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\n    Huber_Loss = fe.backend.huber(y_pred=pred, y_true=true, loss_type='huber', beta=0.65)   #[0.4387, 1.7387, 0.0000, 0.4387]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n        beta: Threshold factor. Needs to be a positive number. dtype: float16 or float32.\n    Returns:\n        The Huber loss between `y_true` and `y_pred` wrt beta.\n    Raises:\n        ValueError: If `y_pred` is an unacceptable data type.\n        ValueError: If beta is less than 1 for Smooth L1 loss and Huber Loss\n    \"\"\"\nif beta &lt;= 0:\nraise ValueError(\"Beta cannot be less than or equal to 0\")\nif tf.is_tensor(y_pred):\nif y_pred.ndim == 1:\ny_true = tf.expand_dims(y_true, axis=-1)\ny_pred = tf.expand_dims(y_pred, axis=-1)\nregression_loss = tf.keras.losses.huber(y_true, y_pred, delta=beta)\nhuber_loss = reduce_mean(regression_loss, axis=[*range(len(regression_loss.shape))][1:])\nelif isinstance(y_pred, torch.Tensor):\nhuber_loss = reduce_mean(\ntorch.nn.HuberLoss(reduction=\"none\", delta=beta)(y_pred, y_true), axis=[*range(len(y_pred.shape))][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn huber_loss\n</code></pre>"}, {"location": "fastestimator/backend/_iwd.html", "title": "_iwd", "text": ""}, {"location": "fastestimator/backend/_iwd.html#fastestimator.fastestimator.backend._iwd.iwd", "title": "<code>iwd</code>", "text": "<p>Compute the Inverse Weighted Distance from the given input.</p> <p>This can be used as an activation function for the final layer of a neural network instead of softmax. For example, instead of: model.add(layers.Dense(classes, activation='softmax')), you could use: model.add(layers.Dense(classes, activation=lambda x: iwd(tf.nn.sigmoid(x))))</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0.5]*5, [0]+[1]*4])\nb = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value. Should be of shape (Batch, C) where every element in C corresponds to a (non-negative) distance to a target class.</p> required <code>power</code> <code>float</code> <p>The power to raise the inverse distances to. 1.0 results in a fairly intuitive probability output. Larger powers can widen regions of certainty, whereas values between 0 and 1 can widen regions of uncertainty.</p> <code>1.0</code> <code>max_prob</code> <code>float</code> <p>The maximum probability to assign to a class estimate when it is distance zero away from the target. For numerical stability this must be less than 1.0. We have found that using smaller values like 0.95 can lead to natural adversarial robustness.</p> <code>0.95</code> <code>pairwise_distance</code> <code>float</code> <p>The distance to any other class when the distance to a target class is zero. For example, if you have a perfect match for class 'a', what distance should be reported to class 'b'. If you have a metric where this isn't constant, just use an approximate expected distance. In that case <code>max_prob</code> will only give you approximate control over the true maximum probability.</p> <code>1.0</code> <code>eps</code> <code>Optional[Tensor]</code> <p>The numeric stability constant to be used when d approaches zero. If None then it will be computed using <code>max_prob</code> and <code>pairwise_distance</code>. If not None, then <code>max_prob</code> and <code>pairwise_distance</code> will be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A probability distribution of shape (Batch, C) where smaller distances from <code>tensor</code> correspond to larger</p> <code>Tensor</code> <p>probabilities.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_iwd.py</code> <pre><code>def iwd(tensor: Tensor,\npower: float = 1.0,\nmax_prob: float = 0.95,\npairwise_distance: float = 1.0,\neps: Optional[Tensor] = None) -&gt; Tensor:\n\"\"\"Compute the Inverse Weighted Distance from the given input.\n    This can be used as an activation function for the final layer of a neural network instead of softmax. For example,\n    instead of: model.add(layers.Dense(classes, activation='softmax')), you could use:\n    model.add(layers.Dense(classes, activation=lambda x: iwd(tf.nn.sigmoid(x))))\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0.5]*5, [0]+[1]*4])\n    b = fe.backend.iwd(n)  # [[0.2, 0.2, 0.2, 0.2, 0.2], [0.95, 0.0125, 0.0125, 0.0125, 0.0125]]\n    ```\n    Args:\n        tensor: The input value. Should be of shape (Batch, C) where every element in C corresponds to a (non-negative)\n            distance to a target class.\n        power: The power to raise the inverse distances to. 1.0 results in a fairly intuitive probability output. Larger\n            powers can widen regions of certainty, whereas values between 0 and 1 can widen regions of uncertainty.\n        max_prob: The maximum probability to assign to a class estimate when it is distance zero away from the target.\n            For numerical stability this must be less than 1.0. We have found that using smaller values like 0.95 can\n            lead to natural adversarial robustness.\n        pairwise_distance: The distance to any other class when the distance to a target class is zero. For example, if\n            you have a perfect match for class 'a', what distance should be reported to class 'b'. If you have a metric\n            where this isn't constant, just use an approximate expected distance. In that case `max_prob` will only give\n            you approximate control over the true maximum probability.\n        eps: The numeric stability constant to be used when d approaches zero. If None then it will be computed using\n            `max_prob` and `pairwise_distance`. If not None, then `max_prob` and `pairwise_distance` will be ignored.\n    Returns:\n        A probability distribution of shape (Batch, C) where smaller distances from `tensor` correspond to larger\n        probabilities.\n    \"\"\"\nif eps is None:\neps = np.array(pairwise_distance * math.pow((1.0 - max_prob) / (max_prob * (tensor.shape[-1] - 1)), 1 / power),\ndtype=TENSOR_TO_NP_DTYPE[tensor.dtype])\neps = to_tensor(\neps, target_type='torch' if isinstance(tensor, torch.Tensor) else 'tf' if tf.is_tensor(tensor) else 'np')\nif isinstance(eps, torch.Tensor):\neps = eps.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntensor = maximum(tensor, eps)\ntensor = tensor_pow(1.0 / tensor, power)\ntensor = tensor / reshape(reduce_sum(tensor, axis=-1), shape=[-1, 1])\nreturn tensor\n</code></pre>"}, {"location": "fastestimator/backend/_l1_loss.html", "title": "_l1_loss", "text": ""}, {"location": "fastestimator/backend/_l1_loss.html#fastestimator.fastestimator.backend._l1_loss.l1_loss", "title": "<code>l1_loss</code>", "text": "<p>Calculate Mean Absolute Error between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nL1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[0.0750, 0.1500, 0.1000, 0.0000]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nL1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[1., 3., 0., 1.]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nL1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[0.0750, 0.1500, 0.1000, 0.0000]\ntrue = torch.tensor([[1], [3], [2], [0]])\npred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\nL1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[1., 3., 0., 1.]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The L1 loss between <code>y_true</code> and <code>y_pred</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_l1_loss.py</code> <pre><code>def l1_loss(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate Mean Absolute Error between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    L1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[0.0750, 0.1500, 0.1000, 0.0000]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    L1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[1., 3., 0., 1.]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    L1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[0.0750, 0.1500, 0.1000, 0.0000]\n    true = torch.tensor([[1], [3], [2], [0]])\n    pred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\n    L1 = fe.backend.l1_loss(y_pred=pred, y_true=true)                                         #[1., 3., 0., 1.]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n    Returns:\n        The L1 loss between `y_true` and `y_pred`.\n    Raises:\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(y_pred):\nif y_pred.ndim == 1:\ny_true = tf.expand_dims(y_true, axis=-1)\ny_pred = tf.expand_dims(y_pred, axis=-1)\nregression_loss = tf.keras.losses.MAE(y_true, y_pred)\nmae = reduce_mean(regression_loss, axis=[ax for ax in range(len(regression_loss.shape))][1:])\nelif isinstance(y_pred, torch.Tensor):\nmae = reduce_mean(\ntorch.nn.L1Loss(reduction=\"none\")(y_pred, y_true), axis=[ax for ax in range(len(y_pred.shape))][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn mae\n</code></pre>"}, {"location": "fastestimator/backend/_l2_regularization.html", "title": "_l2_regularization", "text": ""}, {"location": "fastestimator/backend/_l2_regularization.html#fastestimator.fastestimator.backend._l2_regularization.l2_regularization", "title": "<code>l2_regularization</code>", "text": "<p>Calculate L2 Norm of model weights.</p> <p>l2_reg = sum(parameter**2)/2</p> <p>This method can be used with TensorFlow and Pytorch tensors</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A tensorflow or pytorch model</p> required <code>beta</code> <code>float</code> <p>The multiplicative factor, to weight the l2 regularization loss with the input loss</p> <code>0.01</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The L2 norm of model parameters</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> belongs to an unacceptable framework.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_l2_regularization.py</code> <pre><code>def l2_regularization(model: Union[tf.keras.Model, torch.nn.Module], beta: float = 0.01) -&gt; Tensor:\n\"\"\"Calculate L2 Norm of model weights.\n    l2_reg = sum(parameter**2)/2\n    This method can be used with TensorFlow and Pytorch tensors\n    Args:\n        model: A tensorflow or pytorch model\n        beta: The multiplicative factor, to weight the l2 regularization loss with the input loss\n    Returns:\n        The L2 norm of model parameters\n    Raises:\n        ValueError: If `model` belongs to an unacceptable framework.\n    \"\"\"\nif isinstance(model, torch.nn.Module):\nl2_loss = torch.sum(torch.stack([torch.sum(p**2) / 2 for p in model.parameters() if p.requires_grad]))\nelif isinstance(model, tf.keras.Model):\nl2_loss = tf.reduce_sum([tf.nn.l2_loss(p) for p in model.trainable_variables])\nelse:\nraise ValueError(\"Unrecognized model framework: Please make sure to pass either torch or tensorflow models\")\nreturn beta * l2_loss\n</code></pre>"}, {"location": "fastestimator/backend/_lambertw.html", "title": "_lambertw", "text": ""}, {"location": "fastestimator/backend/_lambertw.html#fastestimator.fastestimator.backend._lambertw.lambertw", "title": "<code>lambertw</code>", "text": "<p>Compute the k=0 branch of the Lambert W function.</p> <p>See https://en.wikipedia.org/wiki/Lambert_W_function for details. Only valid for inputs &gt;= -1/e (approx -0.368). We do not check this for the sake of speed, but if an input is out of domain the return value may be random / inconsistent or even NaN.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\nb = fe.backend.lambertw(n)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\nb = fe.backend.lambertw(t)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\nb = fe.backend.lambertw(p)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The lambertw function evaluated at <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_lambertw.py</code> <pre><code>def lambertw(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the k=0 branch of the Lambert W function.\n    See https://en.wikipedia.org/wiki/Lambert_W_function for details. Only valid for inputs &gt;= -1/e (approx -0.368). We\n    do not check this for the sake of speed, but if an input is out of domain the return value may be random /\n    inconsistent or even NaN.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\n    b = fe.backend.lambertw(n)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\n    b = fe.backend.lambertw(t)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-1.0/math.e, -0.34, -0.32, -0.2, 0, 0.12, 0.15, math.e, 5, math.exp(1 + math.e), 100])\n    b = fe.backend.lambertw(p)  # [-1, -0.654, -0.560, -0.259, 0, 0.108, 0.132, 1, 1.327, 2.718, 3.386]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The lambertw function evaluated at `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tfp.math.lambertw(tensor)\nif isinstance(tensor, torch.Tensor):\nreturn _torch_lambertw(tensor)\nelif isinstance(tensor, np.ndarray):\n# scipy implementation is numerically unstable at exactly -1/e, but the result should be -1.0\nreturn np.nan_to_num(lamw(tensor, k=0, tol=1e-6).real.astype(tensor.dtype), nan=-1.0)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_load_model.html", "title": "_load_model", "text": ""}, {"location": "fastestimator/backend/_load_model.html#fastestimator.fastestimator.backend._load_model.load_model", "title": "<code>load_model</code>", "text": "<p>Load saved weights for a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to load.</p> required <code>weights_path</code> <code>str</code> <p>Path to the <code>model</code> weights.</p> required <code>load_optimizer</code> <code>bool</code> <p>Whether to load optimizer. If True, then it will load  file in the path. <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_load_model.py</code> <pre><code>def load_model(model: Union[tf.keras.Model, torch.nn.Module], weights_path: str, load_optimizer: bool = False):\n\"\"\"Load saved weights for a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n    ```\n    Args:\n        model: A neural network instance to load.\n        weights_path: Path to the `model` weights.\n        load_optimizer: Whether to load optimizer. If True, then it will load &lt;weights_opt&gt; file in the path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif os.path.exists(weights_path):\nValueError(\"Weights path doesn't exist: \", weights_path)\nif isinstance(model, tf.keras.Model):\nmodel.load_weights(weights_path)\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pkl\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nwith open(optimizer_path, 'rb') as f:\nstate_dict = pickle.load(f)\nmodel.current_optimizer.set_weights(state_dict['weights'])\nweight_decay = None\nif isinstance(model.current_optimizer, tfa.optimizers.DecoupledWeightDecayExtension) or hasattr(\nmodel.current_optimizer, \"inner_optimizer\") and isinstance(\nmodel.current_optimizer.inner_optimizer, tfa.optimizers.DecoupledWeightDecayExtension):\nweight_decay = state_dict['weight_decay']\nset_lr(model, state_dict['lr'], weight_decay=weight_decay)\nelif isinstance(model, torch.nn.Module):\nif isinstance(model, torch.nn.DataParallel):\nmodel.module.load_state_dict(preprocess_torch_weights(weights_path))\nelse:\nmodel.load_state_dict(preprocess_torch_weights(weights_path))\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pt\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nmodel.current_optimizer.load_state_dict(torch.load(optimizer_path))\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/_load_model.html#fastestimator.fastestimator.backend._load_model.preprocess_torch_weights", "title": "<code>preprocess_torch_weights</code>", "text": "<p>Preprocess the torch weights dictionary.</p> <p>This method is used to remove the any DataParallel artifacts in torch weigths.</p> <p>Parameters:</p> Name Type Description Default <code>weights_path</code> <code>str</code> <p>Path to the model weights.</p> required Source code in <code>fastestimator\\fastestimator\\backend\\_load_model.py</code> <pre><code>def preprocess_torch_weights(weights_path: str) -&gt; OrderedDict:\n\"\"\"Preprocess the torch weights dictionary.\n    This method is used to remove the any DataParallel artifacts in torch weigths.\n    Args:\n        weights_path: Path to the model weights.\n    \"\"\"\nnew_state_dict = OrderedDict()\nfor key, value in torch.load(weights_path, map_location='cpu' if torch.cuda.device_count() == 0 else None).items():\n# remove `module.`\nnew_key = key\nif key.startswith('module.'):\nnew_key = key[7:]\nnew_state_dict[new_key] = value\nreturn new_state_dict\n</code></pre>"}, {"location": "fastestimator/backend/_matmul.html", "title": "_matmul", "text": ""}, {"location": "fastestimator/backend/_matmul.html#fastestimator.fastestimator.backend._matmul.matmul", "title": "<code>matmul</code>", "text": "<p>Perform matrix multiplication on <code>a</code> and <code>b</code>.</p> <p>This method can be used with Numpy data: <pre><code>a = np.array([[0,1,2],[3,4,5]])\nb = np.array([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>a = tf.constant([[0,1,2],[3,4,5]])\nb = tf.constant([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>a = torch.tensor([[0,1,2],[3,4,5]])\nb = torch.tensor([[1],[2],[3]])\nc = fe.backend.matmul(a, b)  # [[8], [26]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Tensor</code> <p>The first matrix.</p> required <code>b</code> <code>Tensor</code> <p>The second matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The matrix multiplication result of a * b.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If either <code>a</code> or <code>b</code> are unacceptable or non-matching data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_matmul.py</code> <pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n\"\"\"Perform matrix multiplication on `a` and `b`.\n    This method can be used with Numpy data:\n    ```python\n    a = np.array([[0,1,2],[3,4,5]])\n    b = np.array([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    a = tf.constant([[0,1,2],[3,4,5]])\n    b = tf.constant([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    a = torch.tensor([[0,1,2],[3,4,5]])\n    b = torch.tensor([[1],[2],[3]])\n    c = fe.backend.matmul(a, b)  # [[8], [26]]\n    ```\n    Args:\n        a: The first matrix.\n        b: The second matrix.\n    Returns:\n        The matrix multiplication result of a * b.\n    Raises:\n        ValueError: If either `a` or `b` are unacceptable or non-matching data types.\n    \"\"\"\nif tf.is_tensor(a) and tf.is_tensor(b):\nreturn tf.matmul(a, b)\nelif isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):\nreturn a.matmul(b)\nelif isinstance(a, np.ndarray) and isinstance(b, np.ndarray):\nreturn np.matmul(a, b)\nelif type(a) != type(b):\nraise ValueError(f\"Tensor types do not match ({type(a)} and {type(b)})\")\nelse:\nraise ValueError(f\"Unrecognized tensor type ({type(a)} or {type(b)})\")\n</code></pre>"}, {"location": "fastestimator/backend/_maximum.html", "title": "_maximum", "text": ""}, {"location": "fastestimator/backend/_maximum.html#fastestimator.fastestimator.backend._maximum.maximum", "title": "<code>maximum</code>", "text": "<p>Get the maximum of the given <code>tensors</code>.</p> <p>This method can be used with Numpy data: <pre><code>n1 = np.array([[2, 7, 6]])\nn2 = np.array([[2, 7, 5]])\nres = fe.backend.maximum(n1, n2) # [[2, 7, 6]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t1 = tf.constant([[2, 7, 6]])\nt2 = tf.constant([[2, 7, 5]])\nres = fe.backend.maximum(t1, t2) # [[2, 7, 6]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p1 = torch.tensor([[2, 7, 6]])\np2 = torch.tensor([[2, 7, 5]])\nres = fe.backend.maximum(p1, p2) # [[2, 7, 6]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor1</code> <code>Tensor</code> <p>First tensor.</p> required <code>tensor2</code> <code>Tensor</code> <p>Second tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum of two <code>tensors</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_maximum.py</code> <pre><code>def maximum(tensor1: Tensor, tensor2: Tensor) -&gt; Tensor:\n\"\"\"Get the maximum of the given `tensors`.\n    This method can be used with Numpy data:\n    ```python\n    n1 = np.array([[2, 7, 6]])\n    n2 = np.array([[2, 7, 5]])\n    res = fe.backend.maximum(n1, n2) # [[2, 7, 6]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t1 = tf.constant([[2, 7, 6]])\n    t2 = tf.constant([[2, 7, 5]])\n    res = fe.backend.maximum(t1, t2) # [[2, 7, 6]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p1 = torch.tensor([[2, 7, 6]])\n    p2 = torch.tensor([[2, 7, 5]])\n    res = fe.backend.maximum(p1, p2) # [[2, 7, 6]]\n    ```\n    Args:\n        tensor1: First tensor.\n        tensor2: Second tensor.\n    Returns:\n        The maximum of two `tensors`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor1) and tf.is_tensor(tensor2):\nreturn tf.maximum(tensor1, tensor2)\nelif isinstance(tensor1, torch.Tensor) and isinstance(tensor2, torch.Tensor):\nreturn torch.max(tensor1, tensor2)\nelif isinstance(tensor1, np.ndarray) and isinstance(tensor2, np.ndarray):\nreturn np.maximum(tensor1, tensor2)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor1)))\n</code></pre>"}, {"location": "fastestimator/backend/_mean_squared_error.html", "title": "_mean_squared_error", "text": ""}, {"location": "fastestimator/backend/_mean_squared_error.html#fastestimator.fastestimator.backend._mean_squared_error.mean_squared_error", "title": "<code>mean_squared_error</code>", "text": "<p>Calculate mean squared error between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = torch.tensor([[1], [3], [2], [0]])\npred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MSE between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_mean_squared_error.py</code> <pre><code>def mean_squared_error(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate mean squared error between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = torch.tensor([[1], [3], [2], [0]])\n    pred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n    Returns:\n        The MSE between `y_true` and `y_pred`\n    Raises:\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(y_pred):\nmse = tf.losses.MSE(y_true, y_pred)\nelif isinstance(y_pred, torch.Tensor):\nmse = reduce_mean(\ntorch.nn.MSELoss(reduction=\"none\")(y_pred, y_true), axis=[ax for ax in range(y_pred.ndim)][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn mse\n</code></pre>"}, {"location": "fastestimator/backend/_ones_like.html", "title": "_ones_like", "text": ""}, {"location": "fastestimator/backend/_ones_like.html#fastestimator.fastestimator.backend._ones_like.ones_like", "title": "<code>ones_like</code>", "text": "<p>Generate ones shaped like <code>tensor</code> with a specified <code>dtype</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.ones_like(n)  # [[1, 1], [1, 1]]\nb = fe.backend.ones_like(n, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.ones_like(t)  # [[1, 1], [1, 1]]\nb = fe.backend.ones_like(t, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.ones_like(p)  # [[1, 1], [1, 1]]\nb = fe.backend.ones_like(p, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. If None then the <code>tensor</code> dtype is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of ones with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_ones_like.py</code> <pre><code>def ones_like(tensor: Tensor, dtype: Union[None, str] = None) -&gt; Tensor:\n\"\"\"Generate ones shaped like `tensor` with a specified `dtype`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.ones_like(n)  # [[1, 1], [1, 1]]\n    b = fe.backend.ones_like(n, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.ones_like(t)  # [[1, 1], [1, 1]]\n    b = fe.backend.ones_like(t, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.ones_like(p)  # [[1, 1], [1, 1]]\n    b = fe.backend.ones_like(p, dtype=\"float32\")  # [[1.0, 1.0], [1.0, 1.0]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        dtype: The data type to be used when generating the resulting tensor. If None then the `tensor` dtype is used.\n    Returns:\n        A tensor of ones with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.ones_like(tensor, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.ones_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype])\nelif isinstance(tensor, np.ndarray):\nreturn np.ones_like(tensor, dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_percentile.html", "title": "_percentile", "text": ""}, {"location": "fastestimator/backend/_percentile.html#fastestimator.fastestimator.backend._percentile.percentile", "title": "<code>percentile</code>", "text": "<p>Compute the <code>percentiles</code> of a <code>tensor</code>.</p> <p>The n-th percentile of <code>tensor</code> is the value n/100 of the way from the minimum to the maximum in a sorted copy of <code>tensor</code>. If the percentile falls in between two values, the lower of the two values will be used.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor from which to extract percentiles.</p> required <code>percentiles</code> <code>Union[int, List[int]]</code> <p>One or more percentile values to be computed.</p> required <code>axis</code> <code>Union[None, int, List[int]]</code> <p>Along which axes to compute the percentile (None to compute over all axes).</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to maintain the number of dimensions from <code>tensor</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>percentiles</code> of the given <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_percentile.py</code> <pre><code>def percentile(tensor: Tensor,\npercentiles: Union[int, List[int]],\naxis: Union[None, int, List[int]] = None,\nkeepdims: bool = True) -&gt; Tensor:\n\"\"\"Compute the `percentiles` of a `tensor`.\n    The n-th percentile of `tensor` is the value n/100 of the way from the minimum to the maximum in a sorted copy of\n    `tensor`. If the percentile falls in between two values, the lower of the two values will be used.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    Args:\n        tensor: The tensor from which to extract percentiles.\n        percentiles: One or more percentile values to be computed.\n        axis: Along which axes to compute the percentile (None to compute over all axes).\n        keepdims: Whether to maintain the number of dimensions from `tensor`.\n    Returns:\n        The `percentiles` of the given `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nif isinstance(percentiles, List):\npercentiles = tf.convert_to_tensor(percentiles)\nreturn tf_percentile(tensor, percentiles, axis=axis, keepdims=keepdims, interpolation='lower')\nelif isinstance(tensor, torch.Tensor):\nn_dims = len(tensor.shape)\nif axis is None:\n# Default behavior in tf without axis is to compress all dimensions\naxis = list(range(n_dims))\n# Convert negative axis values to their positive counterparts\nif isinstance(axis, int):\naxis = [axis]\nfor idx, elem in enumerate(axis):\naxis[idx] = elem % n_dims\n# Extract dims which are not being considered\nother_dims = sorted(set(range(n_dims)).difference(axis))\n# Flatten all of the permutation axis down for kth-value computation\npermutation = other_dims + list(axis)\npermuted = tensor.permute(*permutation)\nother_shape = [tensor.shape[i] for i in other_dims]\nother_shape.append(np.prod([tensor.shape[i] for i in axis]))\npermuted = torch.reshape(permuted, other_shape)\nresults = []\nfor tile in to_list(percentiles):\ntarget = 1 + math.floor(tile / 100.0 * (permuted.shape[-1] - 1))\nkth_val = torch.kthvalue(permuted, k=target, dim=-1, keepdim=True)[0]\nfor dim in range(n_dims - len(kth_val.shape)):\nkth_val = torch.unsqueeze(kth_val, dim=-1)\n# Undo the permutation from earlier\nkth_val = kth_val.permute(*np.argsort(permutation))\nif not keepdims:\nfor dim in reversed(axis):\nkth_val = torch.squeeze(kth_val, dim=dim)\nresults.append(kth_val)\nif isinstance(percentiles, int):\nreturn results[0]\nelse:\nreturn torch.stack(results, dim=0)\nelif isinstance(tensor, np.ndarray):\nreturn np.percentile(tensor, percentiles, axis=axis, keepdims=keepdims, interpolation='lower')\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_permute.html", "title": "_permute", "text": ""}, {"location": "fastestimator/backend/_permute.html#fastestimator.fastestimator.backend._permute.permute", "title": "<code>permute</code>", "text": "<p>Perform the specified <code>permutation</code> on the axes of a given <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to permute.</p> required <code>permutation</code> <code>List[int]</code> <p>The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> with axes swapped according to the <code>permutation</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_permute.py</code> <pre><code>def permute(tensor: Tensor, permutation: List[int]) -&gt; Tensor:\n\"\"\"Perform the specified `permutation` on the axes of a given `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    Args:\n        tensor: The tensor to permute.\n        permutation: The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).\n    Returns:\n        The `tensor` with axes swapped according to the `permutation`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.transpose(tensor, perm=permutation)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.permute(*permutation)\nelif isinstance(tensor, np.ndarray):\nreturn np.transpose(tensor, axes=permutation)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_pow.html", "title": "_pow", "text": ""}, {"location": "fastestimator/backend/_pow.html#fastestimator.fastestimator.backend._pow.pow", "title": "<code>pow</code>", "text": "<p>Raise a <code>tensor</code> to a given <code>power</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.pow(n, 2)  # [4, 49, 361]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.pow(t, 2)  # [4, 49, 361]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.pow(p, 2)  # [4, 49, 361]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>power</code> <code>Union[int, float, Tensor]</code> <p>The exponent to raise <code>tensor</code> by.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The exponentiated <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_pow.py</code> <pre><code>def pow(tensor: Tensor, power: Union[int, float, Tensor] ) -&gt; Tensor:\n\"\"\"Raise a `tensor` to a given `power`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.pow(n, 2)  # [4, 49, 361]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.pow(t, 2)  # [4, 49, 361]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.pow(p, 2)  # [4, 49, 361]\n    ```\n    Args:\n        tensor: The input value.\n        power: The exponent to raise `tensor` by.\n    Returns:\n        The exponentiated `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.pow(tensor, power)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.pow(power)\nelif isinstance(tensor, np.ndarray):\nreturn np.power(tensor, power)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_random_normal_like.html", "title": "_random_normal_like", "text": ""}, {"location": "fastestimator/backend/_random_normal_like.html#fastestimator.fastestimator.backend._random_normal_like.random_normal_like", "title": "<code>random_normal_like</code>", "text": "<p>Generate noise shaped like <code>tensor</code> from a random normal distribution with a given <code>mean</code> and <code>std</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution to be sampled.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>The standard deviation of the normal distribution to be sampled.</p> <code>1.0</code> <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. This should be one of the floating point types.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of random normal noise with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_random_normal_like.py</code> <pre><code>def random_normal_like(tensor: Tensor, mean: float = 0.0, std: float = 1.0,\ndtype: Union[None, str] = 'float32') -&gt; Tensor:\n\"\"\"Generate noise shaped like `tensor` from a random normal distribution with a given `mean` and `std`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        mean: The mean of the normal distribution to be sampled.\n        std: The standard deviation of the normal distribution to be sampled.\n        dtype: The data type to be used when generating the resulting tensor. This should be one of the floating point\n            types.\n    Returns:\n        A tensor of random normal noise with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.random.normal(shape=tensor.shape, mean=mean, stddev=std, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.randn_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype]) * std + mean\nelif isinstance(tensor, np.ndarray):\nreturn np.random.normal(loc=mean, scale=std, size=tensor.shape).astype(dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_random_uniform_like.html", "title": "_random_uniform_like", "text": ""}, {"location": "fastestimator/backend/_random_uniform_like.html#fastestimator.fastestimator.backend._random_uniform_like.random_uniform_like", "title": "<code>random_uniform_like</code>", "text": "<p>Generate noise shaped like <code>tensor</code> from a random normal distribution with a given <code>mean</code> and <code>std</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.random_uniform_like(n)  # [[0.62, 0.49], [0.88, 0.37]]\nb = fe.backend.random_uniform_like(n, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.random_uniform_like(t)  # [[0.62, 0.49], [0.88, 0.37]]\nb = fe.backend.random_uniform_like(t, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.random_uniform_like(p)  # [[0.62, 0.49], [0.88, 0.37]]\nb = fe.backend.random_uniform_like(P, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>minval</code> <code>float</code> <p>The minimum bound of the uniform distribution.</p> <code>0.0</code> <code>maxval</code> <code>float</code> <p>The maximum bound of the uniform distribution.</p> <code>1.0</code> <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. This should be one of the floating point types.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of random uniform noise with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_random_uniform_like.py</code> <pre><code>def random_uniform_like(tensor: Tensor, minval: float = 0.0, maxval: float = 1.0,\ndtype: Union[None, str] = 'float32') -&gt; Tensor:\n\"\"\"Generate noise shaped like `tensor` from a random normal distribution with a given `mean` and `std`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.random_uniform_like(n)  # [[0.62, 0.49], [0.88, 0.37]]\n    b = fe.backend.random_uniform_like(n, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.random_uniform_like(t)  # [[0.62, 0.49], [0.88, 0.37]]\n    b = fe.backend.random_uniform_like(t, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.random_uniform_like(p)  # [[0.62, 0.49], [0.88, 0.37]]\n    b = fe.backend.random_uniform_like(P, minval=-5.0, maxval=-3)  # [[-3.8, -4.4], [-4.8, -4.9]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        minval: The minimum bound of the uniform distribution.\n        maxval: The maximum bound of the uniform distribution.\n        dtype: The data type to be used when generating the resulting tensor. This should be one of the floating point\n            types.\n    Returns:\n        A tensor of random uniform noise with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.random.uniform(shape=tensor.shape, minval=minval, maxval=maxval, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.rand_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype]) * (maxval - minval) + minval\nelif isinstance(tensor, np.ndarray):\nreturn np.random.uniform(low=minval, high=maxval, size=tensor.shape).astype(dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reduce_max.html", "title": "_reduce_max", "text": ""}, {"location": "fastestimator/backend/_reduce_max.html#fastestimator.fastestimator.backend._reduce_max.reduce_max", "title": "<code>reduce_max</code>", "text": "<p>Compute the maximum value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(n)  # 8\nb = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(t)  # 8\nb = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(p)  # 8\nb = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the maximum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reduce_max.py</code> <pre><code>def reduce_max(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the maximum value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(n)  # 8\n    b = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(t)  # 8\n    b = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(p)  # 8\n    b = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the maximum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The maximum values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_max(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.max(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.max(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reduce_mean.html", "title": "_reduce_mean", "text": ""}, {"location": "fastestimator/backend/_reduce_mean.html#fastestimator.fastestimator.backend._reduce_mean.reduce_mean", "title": "<code>reduce_mean</code>", "text": "<p>Compute the mean value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(n)  # 4.5\nb = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(t)  # 4.5\nb = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\nb = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(p)  # 4.5\nb = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the mean along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reduce_mean.py</code> <pre><code>def reduce_mean(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the mean value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(n)  # 4.5\n    b = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(t)  # 4.5\n    b = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\n    b = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(p)  # 4.5\n    b = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the mean along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The mean values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_mean(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nif not keepdims:\nreturn tensor.mean()\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.mean(dim=ax, keepdim=keepdims)\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.mean(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reduce_min.html", "title": "_reduce_min", "text": ""}, {"location": "fastestimator/backend/_reduce_min.html#fastestimator.fastestimator.backend._reduce_min.reduce_min", "title": "<code>reduce_min</code>", "text": "<p>Compute the min value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(n)  # 1\nb = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(t)  # 1\nb = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(p)  # 1\nb = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the min along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The min values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reduce_min.py</code> <pre><code>def reduce_min(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the min value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(n)  # 1\n    b = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(t)  # 1\n    b = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(p)  # 1\n    b = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the min along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The min values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_min(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.min(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.min(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reduce_std.html", "title": "_reduce_std", "text": ""}, {"location": "fastestimator/backend/_reduce_std.html#fastestimator.fastestimator.backend._reduce_std.reduce_std", "title": "<code>reduce_std</code>", "text": "<p>Compute the std value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_std(n)  # 2.2913\nb = fe.backend.reduce_std(n, axis=0)  # [[2., 2.], [2., 2.]]\nb = fe.backend.reduce_std(n, axis=1)  # [[1., 1.], [1., 1.]]\nb = fe.backend.reduce_std(n, axis=[0,2])  # [2.23606798 2.23606798]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_std(t)  # 2.2913\nb = fe.backend.reduce_std(t, axis=0)  # [[2., 2.], [2., 2.]]\nb = fe.backend.reduce_std(t, axis=1)  # [[2, 3], [3, 7]]\nb = fe.backend.reduce_std(t, axis=[0,2])  # [2.23606798 2.23606798]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_std(p)  # 2.2913\nb = fe.backend.reduce_std(p, axis=0)  # [[2., 2.], [2., 2.]]\nb = fe.backend.reduce_std(p, axis=1)  # [[1., 1.], [1., 1.]]\nb = fe.backend.reduce_std(p, axis=[0,2])  # [2.23606798 2.23606798]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the std along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The std values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reduce_std.py</code> <pre><code>def reduce_std(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the std value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_std(n)  # 2.2913\n    b = fe.backend.reduce_std(n, axis=0)  # [[2., 2.], [2., 2.]]\n    b = fe.backend.reduce_std(n, axis=1)  # [[1., 1.], [1., 1.]]\n    b = fe.backend.reduce_std(n, axis=[0,2])  # [2.23606798 2.23606798]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_std(t)  # 2.2913\n    b = fe.backend.reduce_std(t, axis=0)  # [[2., 2.], [2., 2.]]\n    b = fe.backend.reduce_std(t, axis=1)  # [[2, 3], [3, 7]]\n    b = fe.backend.reduce_std(t, axis=[0,2])  # [2.23606798 2.23606798]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_std(p)  # 2.2913\n    b = fe.backend.reduce_std(p, axis=0)  # [[2., 2.], [2., 2.]]\n    b = fe.backend.reduce_std(p, axis=1)  # [[1., 1.], [1., 1.]]\n    b = fe.backend.reduce_std(p, axis=[0,2])  # [2.23606798 2.23606798]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the std along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The std values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.math.reduce_std(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nif not keepdims:\nreturn tensor.std(unbiased=False)\naxis = list(range(len(tensor.shape)))\ntensor = tensor.std(dim=axis, unbiased=False, keepdim=keepdims)\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.std(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reduce_sum.html", "title": "_reduce_sum", "text": ""}, {"location": "fastestimator/backend/_reduce_sum.html#fastestimator.fastestimator.backend._reduce_sum.reduce_sum", "title": "<code>reduce_sum</code>", "text": "<p>Compute the sum along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(n)  # 36\nb = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(t)  # 36\nb = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(p)  # 36\nb = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the sum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reduce_sum.py</code> <pre><code>def reduce_sum(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the sum along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(n)  # 36\n    b = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(t)  # 36\n    b = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(p)  # 36\n    b = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the sum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The sum of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reduce_sum(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\nreturn tensor.sum(dim=axis, keepdim=keepdims)\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.sum(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_reshape.html", "title": "_reshape", "text": ""}, {"location": "fastestimator/backend/_reshape.html#fastestimator.fastestimator.backend._reshape.reshape", "title": "<code>reshape</code>", "text": "<p>Reshape a <code>tensor</code> to conform to a given shape.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>shape</code> <code>List[int]</code> <p>The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left should be packed into that axis.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_reshape.py</code> <pre><code>def reshape(tensor: Tensor, shape: List[int]) -&gt; Tensor:\n\"\"\"Reshape a `tensor` to conform to a given shape.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    Args:\n        tensor: The input value.\n        shape: The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left\n            should be packed into that axis.\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.reshape(tensor, shape=shape)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.reshape(tensor, shape=shape)\nelif isinstance(tensor, np.ndarray):\nreturn np.reshape(tensor, shape)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_resize3d.html", "title": "_resize3d", "text": ""}, {"location": "fastestimator/backend/_resize3d.html#fastestimator.fastestimator.backend._resize3d.resize_3d", "title": "<code>resize_3d</code>", "text": "<p>Reshape a <code>tensor</code> to conform to a given shape.</p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[[[0.], [1.]], [[2.], [3.]]], [[[4.], [5.]], [[6.], [7.]]]]])\nb = fe.backend.resize_3d(t, output_shape=[3, 3, 3])  # [[[[[0.], [0.], [1.], [1.]], [[0.], [0.], [1.], [1.]], [[2.], [2.], [3.], [3.]], [[2.], [2.], [3.], [3.]]],\n[[[0.], [0.], [1.], [1.]], [[0.], [0.], [1.], [1.]], [[2.], [2.], [3.], [3.]], [[2.], [2.], [3.], [3.]]],\n[[[4.], [4.], [5.], [5.]], [[4.], [4.], [5.], [5.]], [[6.], [6.], [7.], [7.]], [[6.], [6.], [7.], [7.]]],\n[[[4.], [4.], [5.], [5.]], [[4.], [4.], [5.], [5.]], [[6.], [6.], [7.], [7.]], [[6.], [6.], [7.], [7.]]]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[[[0., 1.], [2., 3.]], [[4., 5.], [6., 7.]]]]])\nb = fe.backend.resize_3d(p, output_shape=[3, 3, 3])  # [[[[[0., 0., 1., 1.], [0., 0., 1., 1.], [2., 2., 3., 3.], [2., 2., 3., 3.]],\n[[0., 0., 1., 1.], [0., 0., 1., 1.], [2., 2., 3., 3.], [2., 2., 3., 3.]],\n[[4., 4., 5., 5.], [4., 4., 5., 5.], [6., 6., 7., 7.], [6., 6., 7., 7.]],\n[[4., 4., 5., 5.], [4., 4., 5., 5.], [6., 6., 7., 7.], [6., 6., 7., 7.]]]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>output_shape</code> <code>List[int]</code> <p>The new size of the tensor.</p> required <code>resize_mode</code> <code>str</code> <p>mode to apply for resizing</p> <code>'nearest'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The resized <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_resize3d.py</code> <pre><code>def resize_3d(tensor: Tensor, output_shape: List[int], resize_mode: str = 'nearest') -&gt; Tensor:\n\"\"\"Reshape a `tensor` to conform to a given shape.\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[[[0.], [1.]], [[2.], [3.]]], [[[4.], [5.]], [[6.], [7.]]]]])\n    b = fe.backend.resize_3d(t, output_shape=[3, 3, 3])  # [[[[[0.], [0.], [1.], [1.]], [[0.], [0.], [1.], [1.]], [[2.], [2.], [3.], [3.]], [[2.], [2.], [3.], [3.]]],\n                                                            [[[0.], [0.], [1.], [1.]], [[0.], [0.], [1.], [1.]], [[2.], [2.], [3.], [3.]], [[2.], [2.], [3.], [3.]]],\n                                                            [[[4.], [4.], [5.], [5.]], [[4.], [4.], [5.], [5.]], [[6.], [6.], [7.], [7.]], [[6.], [6.], [7.], [7.]]],\n                                                            [[[4.], [4.], [5.], [5.]], [[4.], [4.], [5.], [5.]], [[6.], [6.], [7.], [7.]], [[6.], [6.], [7.], [7.]]]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[[[0., 1.], [2., 3.]], [[4., 5.], [6., 7.]]]]])\n    b = fe.backend.resize_3d(p, output_shape=[3, 3, 3])  # [[[[[0., 0., 1., 1.], [0., 0., 1., 1.], [2., 2., 3., 3.], [2., 2., 3., 3.]],\n                                                              [[0., 0., 1., 1.], [0., 0., 1., 1.], [2., 2., 3., 3.], [2., 2., 3., 3.]],\n                                                              [[4., 4., 5., 5.], [4., 4., 5., 5.], [6., 6., 7., 7.], [6., 6., 7., 7.]],\n                                                              [[4., 4., 5., 5.], [4., 4., 5., 5.], [6., 6., 7., 7.], [6., 6., 7., 7.]]]]]\n    ```\n    Args:\n        tensor: The input value.\n        output_shape: The new size of the tensor.\n        resize_mode: mode to apply for resizing\n    Returns:\n        The resized `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nassert resize_mode in ['nearest', 'area'], \"Only following resize modes are supported: 'nearest', 'area' \"\nif tf.is_tensor(tensor):\nreturn resize_tensorflow_tensor(tensor, output_shape, resize_mode)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.nn.functional.interpolate(tensor, output_shape, mode=resize_mode)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_resize3d.html#fastestimator.fastestimator.backend._resize3d.resize_tensorflow_tensor", "title": "<code>resize_tensorflow_tensor</code>", "text": "<p>Resize tensorflow tensor</p> Input <p>data: Input tensorflow tensor output_shape: (X, Y, Z) Expected output shape of tensor</p> Source code in <code>fastestimator\\fastestimator\\backend\\_resize3d.py</code> <pre><code>def resize_tensorflow_tensor(data: tf.Tensor, output_shape: List[int], resize_mode: str) -&gt; tf.Tensor:\n\"\"\"\n        Resize tensorflow tensor\n        Input:\n            data: Input tensorflow tensor\n            output_shape: (X, Y, Z) Expected output shape of tensor\n    \"\"\"\nd1_new, d2_new, d3_new = output_shape\ndata_shape = tf.shape(data)\nbatch_size, d1, d2, d3, c = data_shape[0], data_shape[1], data_shape[2], data_shape[3], data_shape[4]\n# resize d2-d3\nsqueeze_b_x = tf.reshape(data, [-1, d2, d3, c])\nresize_b_x = tf.image.resize(squeeze_b_x, [d2_new, d3_new], resize_mode)\nresume_b_x = tf.reshape(resize_b_x, [batch_size, d1, d2_new, d3_new, c])\n# resize d1\nreoriented = tf.transpose(resume_b_x, [0, 3, 2, 1, 4])\nsqueeze_b_z = tf.reshape(reoriented, [-1, d2_new, d1, c])\nresize_b_z = tf.image.resize(squeeze_b_z, [d2_new, d1_new], resize_mode)\nresume_b_z = tf.reshape(resize_b_z, [batch_size, d3_new, d2_new, d1_new, c])\noutput_tensor = tf.transpose(resume_b_z, [0, 3, 2, 1, 4])\nreturn output_tensor\n</code></pre>"}, {"location": "fastestimator/backend/_roll.html", "title": "_roll", "text": ""}, {"location": "fastestimator/backend/_roll.html#fastestimator.fastestimator.backend._roll.roll", "title": "<code>roll</code>", "text": "<p>Roll a <code>tensor</code> elements along a given axis.</p> <p>The elements are shifted forward or reverse direction by the offset of <code>shift</code>. Overflown elements beyond the last position will be re-introduced at the first position.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(n, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(n, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(n, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(n, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(t, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(t, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(t, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(t, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\nb = fe.backend.roll(p, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\nb = fe.backend.roll(p, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\nb = fe.backend.roll(p, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\nb = fe.backend.roll(p, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>shift</code> <code>Union[int, List[int]]</code> <p>The number of places by which the elements need to be shifted. If shift is a list, axis must be a list of same size.</p> required <code>axis</code> <code>Union[int, List[int]]</code> <p>axis along which elements will be rolled.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The rolled <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_roll.py</code> <pre><code>def roll(tensor: Tensor, shift: Union[int, List[int]], axis: Union[int, List[int]]) -&gt; Tensor:\n\"\"\"Roll a `tensor` elements along a given axis.\n    The elements are shifted forward or reverse direction by the offset of `shift`. Overflown elements beyond the last\n    position will be re-introduced at the first position.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(n, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(n, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(n, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(n, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(t, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(t, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(t, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(t, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1.0, 2.0, 3.0], [5.0, 6.0, 7.0]])\n    b = fe.backend.roll(p, shift=1, axis=0)  # [[5, 6, 7], [1, 2, 3]]\n    b = fe.backend.roll(p, shift=2, axis=1)  # [[2, 3, 1], [6, 7, 5]]\n    b = fe.backend.roll(p, shift=-2, axis=1)  # [[3, 1, 2], [7, 5, 6]]\n    b = fe.backend.roll(p, shift=[-1, -1], axis=[0, 1])  # [[6, 7, 5], [2, 3, 1]]\n    ```\n    Args:\n        tensor: The input value.\n        shift: The number of places by which the elements need to be shifted. If shift is a list, axis must be a list of\n            same size.\n        axis: axis along which elements will be rolled.\n    Returns:\n        The rolled `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.roll(tensor, shift=shift, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.roll(tensor, shifts=shift, dims=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.roll(tensor, shift=shift, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_save_model.html", "title": "_save_model", "text": ""}, {"location": "fastestimator/backend/_save_model.html#fastestimator.fastestimator.backend._save_model.save_model", "title": "<code>save_model</code>", "text": "<p>Save <code>model</code> weights to a specific directory.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to save.</p> required <code>save_dir</code> <code>str</code> <p>Directory into which to write the <code>model</code> weights.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model (used for naming the weights file). If None, model.model_name will be used.</p> <code>None</code> <code>save_optimizer</code> <code>bool</code> <p>Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.</p> <code>False</code> <code>save_architecture</code> <code>bool</code> <p>Whether to also save the entire model architecture so that the model can be reloaded without needing access to the code which generated it. This option is only available for TensorFlow models.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The saved model path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type, of if a user tries to save architecture of a PyTorch model.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_save_model.py</code> <pre><code>def save_model(model: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmodel_name: Optional[str] = None,\nsave_optimizer: bool = False,\nsave_architecture: bool = False) -&gt; str:\n\"\"\"Save `model` weights to a specific directory.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n    ```\n    Args:\n        model: A neural network instance to save.\n        save_dir: Directory into which to write the `model` weights.\n        model_name: The name of the model (used for naming the weights file). If None, model.model_name will be used.\n        save_optimizer: Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.\n        save_architecture: Whether to also save the entire model architecture so that the model can be reloaded without\n            needing access to the code which generated it. This option is only available for TensorFlow models.\n    Returns:\n        The saved model path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type, of if a user tries to save architecture of a PyTorch model.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif model_name is None:\nmodel_name = model.model_name\nsave_dir = os.path.normpath(save_dir)\nos.makedirs(save_dir, exist_ok=True)\nif isinstance(model, tf.keras.Model):\nmodel_path = os.path.join(save_dir, \"{}.h5\".format(model_name))\nmodel.save_weights(model_path)\nif save_architecture:\nmodel.save(filepath=os.path.join(save_dir, model_name), include_optimizer=save_optimizer)\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pkl\".format(model_name))\nwith open(optimizer_path, 'wb') as f:\nsaved_data = {'weights': model.current_optimizer.get_weights(), 'lr': get_lr(model)}\nif isinstance(model.current_optimizer, tfa.optimizers.DecoupledWeightDecayExtension) or hasattr(\nmodel.current_optimizer, \"inner_optimizer\") and isinstance(\nmodel.current_optimizer.inner_optimizer, tfa.optimizers.DecoupledWeightDecayExtension):\nsaved_data['weight_decay'] = tf.keras.backend.get_value(model.current_optimizer.weight_decay)\npickle.dump(saved_data, f)\nreturn model_path\nelif isinstance(model, torch.nn.Module):\nmodel_path = os.path.join(save_dir, \"{}.pt\".format(model_name))\nif isinstance(model, torch.nn.DataParallel):\ntorch.save(model.module.state_dict(), model_path)\nelse:\ntorch.save(model.state_dict(), model_path)\nif save_architecture:\nraise ValueError(\"Sorry, architecture saving is not currently enabled for PyTorch\")\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pt\".format(model_name))\ntorch.save(model.current_optimizer.state_dict(), optimizer_path)\nreturn model_path\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/_set_lr.html", "title": "_set_lr", "text": ""}, {"location": "fastestimator/backend/_set_lr.html#fastestimator.fastestimator.backend._set_lr.set_lr", "title": "<code>set_lr</code>", "text": "<p>Set the learning rate of a given <code>model</code> generated by <code>fe.build</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to modify.</p> required <code>lr</code> <code>float</code> <p>The learning rate to assign to the <code>model</code>.</p> required <code>weight_decay</code> <code>Optional[float]</code> <p>The weight decay parameter, this is only relevant when using <code>tfa.DecoupledWeightDecayExtension</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_set_lr.py</code> <pre><code>def set_lr(model: Union[tf.keras.Model, torch.nn.Module], lr: float, weight_decay: Optional[float] = None):\n\"\"\"Set the learning rate of a given `model` generated by `fe.build`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n    ```\n    Args:\n        model: A neural network instance to modify.\n        lr: The learning rate to assign to the `model`.\n        weight_decay: The weight decay parameter, this is only relevant when using `tfa.DecoupledWeightDecayExtension`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"set_lr only accept models from fe.build\"\nif isinstance(model, tf.keras.Model):\n# when using decoupled weight decay like SGDW or AdamW, weight decay factor needs to change together with lr\n# see https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/DecoupledWeightDecayExtension for detail\nif isinstance(model.current_optimizer, tfa.optimizers.DecoupledWeightDecayExtension) or hasattr(\nmodel.current_optimizer, \"inner_optimizer\") and isinstance(\nmodel.current_optimizer.inner_optimizer, tfa.optimizers.DecoupledWeightDecayExtension):\nif weight_decay is None:\nweight_decay = tf.keras.backend.get_value(model.current_optimizer.weight_decay) * lr / get_lr(model)\ntf.keras.backend.set_value(model.current_optimizer.weight_decay, weight_decay)\ntf.keras.backend.set_value(model.current_optimizer.lr, lr)\nelif isinstance(model, torch.nn.Module):\nfor param_group in model.current_optimizer.param_groups:\nparam_group['lr'] = lr\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/_sign.html", "title": "_sign", "text": ""}, {"location": "fastestimator/backend/_sign.html#fastestimator.fastestimator.backend._sign.sign", "title": "<code>sign</code>", "text": "<p>Compute the sign of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.sign(n)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.sign(t)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.sign(p)  # [-1, 1, -1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sign of each value of the <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_sign.py</code> <pre><code>def sign(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the sign of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.sign(n)  # [-1, 1, -1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.sign(t)  # [-1, 1, -1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.sign(p)  # [-1, 1, -1]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The sign of each value of the `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.sign(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.sign()\nelif isinstance(tensor, np.ndarray):\nreturn np.sign(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_smooth_l1_loss.html", "title": "_smooth_l1_loss", "text": ""}, {"location": "fastestimator/backend/_smooth_l1_loss.html#fastestimator.fastestimator.backend._smooth_l1_loss.smooth_l1_loss", "title": "<code>smooth_l1_loss</code>", "text": "<p>Calculate Smooth L1 Loss between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nSmooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.0048, 0.0269, 0.0125, 0.0000]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nSmooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.6750, 2.6750, 0.0000, 0.6750]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nSmooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.0048, 0.0269, 0.0125, 0.0000]\ntrue = torch.tensor([[1], [3], [2], [0]])\npred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\nSmooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.6750, 2.6750, 0.0000, 0.6750]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32 or float16.</p> required <code>beta</code> <code>float</code> <p>Threshold factor. Needs to be a positive number. dtype: float16 or float32.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Smooth L1 between <code>y_true</code> and <code>y_pred</code> wrt beta.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> <code>ValueError</code> <p>If beta is less than 1 for Smooth L1 loss.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_smooth_l1_loss.py</code> <pre><code>def smooth_l1_loss(y_true: Tensor, y_pred: Tensor, beta: float = 1.0) -&gt; Tensor:\n\"\"\"Calculate Smooth L1 Loss between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    Smooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.0048, 0.0269, 0.0125, 0.0000]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    Smooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.6750, 2.6750, 0.0000, 0.6750]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    Smooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.0048, 0.0269, 0.0125, 0.0000]\n    true = torch.tensor([[1], [3], [2], [0]])\n    pred = torch.tensor([[2.0], [0.0], [2.0], [1.0]])\n    Smooth_L1 = fe.backend.smooth_l1_loss(y_pred=pred, y_true=true, loss_type='smooth', beta=0.65)   #[0.6750, 2.6750, 0.0000, 0.6750]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int, float16, float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32 or float16.\n        beta: Threshold factor. Needs to be a positive number. dtype: float16 or float32.\n    Returns:\n        Smooth L1 between `y_true` and `y_pred` wrt beta.\n    Raises:\n        ValueError: If `y_pred` is an unacceptable data type.\n        ValueError: If beta is less than 1 for Smooth L1 loss.\n    \"\"\"\nif beta &lt;= 0:\nraise ValueError(\"Beta cannot be less than or equal to 0\")\nif tf.is_tensor(y_pred):\ny_true = tf.cast(y_true, y_pred.dtype)\nregression_diff = tf.math.abs(y_true - y_pred)  # |y - f(x)|\nregression_loss = tf.where(tf.math.less(regression_diff, beta),\n0.5 * tf.math.pow(regression_diff, 2) / beta,\nregression_diff - 0.5 * beta)\nsmooth_mae = reduce_mean(regression_loss, axis=[*range(len(regression_loss.shape))][1:])\nelif isinstance(y_pred, torch.Tensor):\nsmooth_mae = reduce_mean(\ntorch.nn.SmoothL1Loss(reduction=\"none\", beta=beta)(y_pred, y_true), axis=[*range(len(y_pred.shape))][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn smooth_mae\n</code></pre>"}, {"location": "fastestimator/backend/_sparse_categorical_crossentropy.html", "title": "_sparse_categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/_sparse_categorical_crossentropy.html#fastestimator.fastestimator.backend._sparse_categorical_crossentropy.sparse_categorical_crossentropy", "title": "<code>sparse_categorical_crossentropy</code>", "text": "<p>Compute sparse categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [2]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nweights = tf.lookup.StaticHashTable(\ntf.lookup.KeyValueTensorInitializer(tf.constant([1, 2]), tf.constant([2.0, 3.0])), default_value=1.0)\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.44, 0.11, 1.08]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [2]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nweights = {1: 2.0, 2: 3.0}\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n# [0.44, 0.11, 1.08]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32 or float16.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a softmax will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <code>class_weights</code> <code>Optional[Weight_Dict]</code> <p>Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay more attention to samples from an under-represented class.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sparse categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_sparse_categorical_crossentropy.py</code> <pre><code>def sparse_categorical_crossentropy(y_pred: Tensor,\ny_true: Tensor,\nfrom_logits: bool = False,\naverage_loss: bool = True,\nclass_weights: Optional[Weight_Dict] = None) -&gt; Tensor:\n\"\"\"Compute sparse categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [2]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    weights = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(tf.constant([1, 2]), tf.constant([2.0, 3.0])), default_value=1.0)\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.44, 0.11, 1.08]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [2]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    weights = {1: 2.0, 2: 3.0}\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False, class_weights=weights)\n    # [0.44, 0.11, 1.08]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32 or float16.\n        y_true: Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.\n        from_logits: Whether y_pred is from logits. If True, a softmax will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n        class_weights: Mapping of class indices to a weight for weighting the loss function. Useful when you need to pay\n            more attention to samples from an under-represented class.\n    Returns:\n        The sparse categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif tf.is_tensor(y_pred):\nce = tf.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nif class_weights is not None:\nsample_weights = class_weights.lookup(\ntf.cast(tf.reshape(y_true, tf.shape(ce)), dtype=class_weights.key_dtype))\nce = ce * sample_weights\nelse:\ny_true = y_true.view(-1)\nif from_logits:\nce = torch.nn.CrossEntropyLoss(reduction=\"none\")(input=y_pred, target=y_true.long())\nelse:\nce = torch.nn.NLLLoss(reduction=\"none\")(input=torch.log(y_pred), target=y_true.long())\nif class_weights is not None:\nsample_weights = torch.ones_like(y_true, dtype=torch.float)\nfor key in class_weights.keys():\nsample_weights[y_true == key] = class_weights[key]\nce = ce * sample_weights.reshape(ce.shape)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/_squeeze.html", "title": "_squeeze", "text": ""}, {"location": "fastestimator/backend/_squeeze.html#fastestimator.fastestimator.backend._squeeze.squeeze", "title": "<code>squeeze</code>", "text": "<p>Remove an <code>axis</code> from a <code>tensor</code> if that axis has length 1.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Optional[int]</code> <p>Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_squeeze.py</code> <pre><code>def squeeze(tensor: Tensor, axis: Optional[int] = None) -&gt; Tensor:\n\"\"\"Remove an `axis` from a `tensor` if that axis has length 1.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.squeeze(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nreturn torch.squeeze(tensor)\nelse:\nreturn torch.squeeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.squeeze(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_normalize.html", "title": "_tensor_normalize", "text": ""}, {"location": "fastestimator/backend/_tensor_normalize.html#fastestimator.fastestimator.backend._tensor_normalize.get_framework", "title": "<code>get_framework</code>", "text": "<p>Get the framework of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>framework</code> <code>str</code> <p>Framework which is used to load input data.</p> <code>device</code> <code>Optional[str]</code> <p>The device on which the method is executed (Eg. cuda, cpu). Only applicable to torch.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_normalize.py</code> <pre><code>def get_framework(tensor: Tensor) -&gt; Tuple[str, Optional[str]]:\n\"\"\"\n        Get the framework of the input data.\n        Args:\n            tensor: The input tensor.\n        Returns:\n            framework: Framework which is used to load input data.\n            device: The device on which the method is executed (Eg. cuda, cpu). Only applicable to torch.\n    \"\"\"\ndevice = None\nif tf.is_tensor(tensor):\nframework = 'tf'\nelif isinstance(tensor, torch.Tensor):\nframework = 'torch'\ndevice = tensor.device\nelif isinstance(tensor, np.ndarray):\nframework = 'np'\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\nreturn (framework, device)\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_normalize.html#fastestimator.fastestimator.backend._tensor_normalize.get_scaled_data", "title": "<code>get_scaled_data</code>", "text": "<p>Get the scaled value of a input data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[float, Sequence[float]]</code> <p>The data which needs to be scaled. (eg: 0.4, (0.485, 0.456, 0.406)).</p> <code>(0.485, 0.456, 0.406)</code> <code>scale_factor</code> <code>float</code> <p>Scale factor which needs to be multipled with input data.</p> <code>255.0</code> <code>framework</code> <code>str</code> <p>Framework currently method is running in.(Eg: 'np','tf', 'torch').</p> <code>'np'</code> <code>device</code> <code>Optional[torch.device]</code> <p>Current device. (eg: 'cpu','cuda').</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The scaled value of input data.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_normalize.py</code> <pre><code>def get_scaled_data(data: Union[float, Sequence[float]] = (0.485, 0.456, 0.406),\nscale_factor: float = 255.0,\nframework: str = 'np',\ndevice: Optional[torch.device] = None) -&gt; Tensor:\n\"\"\"\n        Get the scaled value of a input data.\n        Args:\n            data: The data which needs to be scaled. (eg: 0.4, (0.485, 0.456, 0.406)).\n            scale_factor: Scale factor which needs to be multipled with input data.\n            framework: Framework currently method is running in.(Eg: 'np','tf', 'torch').\n            device: Current device. (eg: 'cpu','cuda').\n        Returns:\n            The scaled value of input data.\n    \"\"\"\nif framework == 'torch':\ndata = torch.tensor(data, device=device)\nelif framework == 'tf':\ndata = tf.constant(data)\nelse:\ndata = np.array(data)\nreturn data * scale_factor\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_normalize.html#fastestimator.fastestimator.backend._tensor_normalize.normalize", "title": "<code>normalize</code>", "text": "<p>Compute the normalized value of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: python n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]) b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]]) b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]</p> <p>This method can be used with TensorFlow tensors: python t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]) b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]]) b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]</p> <p>This method can be used with PyTorch tensors: python p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]) b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]]) b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input 'tensor' value.</p> required <code>mean</code> <code>Union[float, Sequence[float]]</code> <p>The mean which needs to applied(eg: 3.8, (0.485, 0.456, 0.406)).</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Union[float, Sequence[float]]</code> <p>The standard deviation which needs to applied(eg: 3.8, (0.229, 0.224, 0.225)).</p> <code>(0.229, 0.224, 0.225)</code> <code>max_pixel_value</code> <code>float</code> <p>The max value of the input data(eg: 255, 65025) to be multipled with mean and std to get actual mean and std.             To directly use the mean and std provide set max_pixel_value as 1.</p> <code>255.0</code> <code>epsilon</code> <code>float</code> <p>Default value to be added to std to avoid divide by zero error.</p> <code>1e-07</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The normalized values of <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_normalize.py</code> <pre><code>def normalize(tensor: Tensor,\nmean: Union[float, Sequence[float]] = (0.485, 0.456, 0.406),\nstd: Union[float, Sequence[float]] = (0.229, 0.224, 0.225),\nmax_pixel_value: float = 255.0,\nepsilon: float = 1e-7) -&gt; Tensor:\n\"\"\"\n        Compute the normalized value of a `tensor`.\n        This method can be used with Numpy data:\n        python\n        n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n        b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]])\n        b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]\n        This method can be used with TensorFlow tensors:\n        python\n        t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n        b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]])\n        b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]\n        This method can be used with PyTorch tensors:\n        python\n        p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n        b = fe.backend.tensor_normalize(n, 0.5625, 0.2864, 8.0)  # ([[[-1.52752516, -1.0910894 ], [-0.65465364, -0.21821788]], [[ 0.21821788,  0.65465364], [ 1.0910894 ,  1.52752516]]])\n        b = fe.backend.tensor_normalize(n, (0.5, 0.625), (0.2795, 0.2795), 8.0) # [[[-1.34164073, -1.34164073], [-0.44721358, -0.44721358]], [[ 0.44721358,  0.44721358], [ 1.34164073,  1.34164073]]]\n        Args:\n            tensor: The input 'tensor' value.\n            mean: The mean which needs to applied(eg: 3.8, (0.485, 0.456, 0.406)).\n            std: The standard deviation which needs to applied(eg: 3.8, (0.229, 0.224, 0.225)).\n            max_pixel_value: The max value of the input data(eg: 255, 65025) to be multipled with mean and std to get actual mean and std.\n                            To directly use the mean and std provide set max_pixel_value as 1.\n            epsilon: Default value to be added to std to avoid divide by zero error.\n        Returns:\n            The normalized values of `tensor`.\n        Raises:\n            ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nframework, device = get_framework(tensor)\nmean = get_scaled_data(mean, max_pixel_value, framework, device)\nstd = get_scaled_data(std, max_pixel_value, framework, device)\ntensor = (convert_tensor_precision(tensor) - convert_tensor_precision(mean)) / (convert_tensor_precision(std) +\nepsilon)\nreturn tensor\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_pow.html", "title": "_tensor_pow", "text": ""}, {"location": "fastestimator/backend/_tensor_pow.html#fastestimator.fastestimator.backend._tensor_pow.tensor_pow", "title": "<code>tensor_pow</code>", "text": "<p>Computes x^power element-wise along <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(n, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(n, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(t, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(t, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1, 4, 6], [2.3, 0.5, 0]])\nb = fe.backend.tensor_pow(p, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\nb = fe.backend.tensor_pow(p, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>power</code> <code>Union[int, float]</code> <p>The power to which to raise the elements in the <code>tensor</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> raised element-wise to the given <code>power</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_pow.py</code> <pre><code>def tensor_pow(tensor: Tensor, power: Union[int, float]) -&gt; Tensor:\n\"\"\"Computes x^power element-wise along `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(n, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(n, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(t, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(t, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1, 4, 6], [2.3, 0.5, 0]])\n    b = fe.backend.tensor_pow(p, 3.2)  # [[1.0, 84.449, 309.089], [14.372, 0.109, 0]]\n    b = fe.backend.tensor_pow(p, 0.21)  # [[1.0, 1.338, 1.457], [1.191, 0.865, 0]]\n    ```\n    Args:\n        tensor: The input tensor.\n        power: The power to which to raise the elements in the `tensor`.\n    Returns:\n        The `tensor` raised element-wise to the given `power`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.pow(tensor, power)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.pow(tensor, power)\nelif isinstance(tensor, np.ndarray):\nreturn np.power(tensor, power)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_round.html", "title": "_tensor_round", "text": ""}, {"location": "fastestimator/backend/_tensor_round.html#fastestimator.fastestimator.backend._tensor_round.tensor_round", "title": "<code>tensor_round</code>", "text": "<p>Element-wise rounds the values of the <code>tensor</code> to nearest integer.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1.25, 4.5, 6], [4, 9.11, 16]])\nb = fe.backend.tensor_round(n)  # [[1, 4, 6], [4, 9, 16]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1.25, 4.5, 6], [4, 9.11, 16.9]])\nb = fe.backend.tensor_round(t)  # [[1, 4, 6], [4, 9, 17]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1.25, 4.5, 6], [4, 9.11, 16]])\nb = fe.backend.tensor_round(p)  # [[1, 4, 6], [4, 9, 16]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The rounded <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_round.py</code> <pre><code>def tensor_round(tensor: Tensor) -&gt; Tensor:\n\"\"\"Element-wise rounds the values of the `tensor` to nearest integer.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1.25, 4.5, 6], [4, 9.11, 16]])\n    b = fe.backend.tensor_round(n)  # [[1, 4, 6], [4, 9, 16]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1.25, 4.5, 6], [4, 9.11, 16.9]])\n    b = fe.backend.tensor_round(t)  # [[1, 4, 6], [4, 9, 17]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1.25, 4.5, 6], [4, 9.11, 16]])\n    b = fe.backend.tensor_round(p)  # [[1, 4, 6], [4, 9, 16]]\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        The rounded `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.round(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.round(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.round(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_tensor_sqrt.html", "title": "_tensor_sqrt", "text": ""}, {"location": "fastestimator/backend/_tensor_sqrt.html#fastestimator.fastestimator.backend._tensor_sqrt.tensor_sqrt", "title": "<code>tensor_sqrt</code>", "text": "<p>Computes element-wise square root of tensor elements.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 4, 6], [4, 9, 16]])\nb = fe.backend.tensor_sqrt(n)  # [[1.0, 2.0, 2.44948974], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 4, 6], [4, 9, 16]], dtype=tf.float32)\nb = fe.backend.tensor_sqrt(t)  # [[1.0, 2.0, 2.4494898], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[1, 4, 6], [4, 9, 16]], dtype=torch.float32)\nb = fe.backend.tensor_sqrt(p)  # [[1.0, 2.0, 2.4495], [2.0, 3.0, 4.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> that contains square root of input values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_tensor_sqrt.py</code> <pre><code>def tensor_sqrt(tensor: Tensor) -&gt; Tensor:\n\"\"\"Computes element-wise square root of tensor elements.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 4, 6], [4, 9, 16]])\n    b = fe.backend.tensor_sqrt(n)  # [[1.0, 2.0, 2.44948974], [2.0, 3.0, 4.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 4, 6], [4, 9, 16]], dtype=tf.float32)\n    b = fe.backend.tensor_sqrt(t)  # [[1.0, 2.0, 2.4494898], [2.0, 3.0, 4.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[1, 4, 6], [4, 9, 16]], dtype=torch.float32)\n    b = fe.backend.tensor_sqrt(p)  # [[1.0, 2.0, 2.4495], [2.0, 3.0, 4.0]]\n    ```\n    Args:\n        tensor: The input tensor.\n    Returns:\n        The `tensor` that contains square root of input values.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.sqrt(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.sqrt(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.sqrt(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_to_shape.html", "title": "_to_shape", "text": ""}, {"location": "fastestimator/backend/_to_shape.html#fastestimator.fastestimator.backend._to_shape.to_shape", "title": "<code>to_shape</code>", "text": "<p>Compute the shape of tensors within a collection of <code>data</code>recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>add_batch</code> <p>Whether to prepend a batch dimension to the shapes.</p> <code>False</code> <code>exact_shape</code> <p>Whether to return the exact shapes, or if False to fill the shapes with None values.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Collection, Tensor]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their shapes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_to_shape.py</code> <pre><code>def to_shape(data: Union[Collection, Tensor], add_batch=False, exact_shape=True) -&gt; Union[Collection, Tensor]:\n\"\"\"Compute the shape of tensors within a collection of `data`recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        add_batch: Whether to prepend a batch dimension to the shapes.\n        exact_shape: Whether to return the exact shapes, or if False to fill the shapes with None values.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their shapes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_shape(value, add_batch, exact_shape) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_shape(val, add_batch, exact_shape) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_shape(val, add_batch, exact_shape) for val in data])\nelif isinstance(data, set):\nreturn set([to_shape(val, add_batch, exact_shape) for val in data])\nelif hasattr(data, \"shape\"):\nshape = data.shape\nif not exact_shape:\nshape = [None] * len(shape)\nif add_batch:\nshape = [None] + list(shape)\nreturn shape\nelse:\nreturn to_shape(np.array(data), add_batch, exact_shape)\n</code></pre>"}, {"location": "fastestimator/backend/_to_tensor.html", "title": "_to_tensor", "text": ""}, {"location": "fastestimator/backend/_to_tensor.html#fastestimator.fastestimator.backend._to_tensor.to_tensor", "title": "<code>to_tensor</code>", "text": "<p>Convert tensors within a collection of <code>data</code> to a given <code>target_type</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor, float, int, None]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>target_type</code> <code>str</code> <p>What kind of tensor(s) to create, one of \"tf\", \"torch\", or \"np\".</p> required <code>shared_memory</code> <code>bool</code> <p>Whether to put the tensor(s) in shared memory (only applicable when <code>target_type</code> is 'torch').</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Collection, Tensor, None]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors converted to the <code>target_type</code>.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_to_tensor.py</code> <pre><code>def to_tensor(data: Union[Collection, Tensor, float, int, None],\ntarget_type: str,\nshared_memory: bool = False) -&gt; Union[Collection, Tensor, None]:\n\"\"\"Convert tensors within a collection of `data` to a given `target_type` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        target_type: What kind of tensor(s) to create, one of \"tf\", \"torch\", or \"np\".\n        shared_memory: Whether to put the tensor(s) in shared memory (only applicable when `target_type` is 'torch').\n    Returns:\n        A collection with the same structure as `data`, but with any tensors converted to the `target_type`.\n    \"\"\"\ntarget_instance = {\n\"tf\": (tf.Tensor, tf.Variable, tf.distribute.DistributedValues), \"torch\": torch.Tensor, \"np\": np.ndarray\n}\nconversion_function = {\"tf\": tf.convert_to_tensor, \"torch\": torch.from_numpy, \"np\": np.array}\nif isinstance(data, target_instance[target_type]):\nif shared_memory and target_type == \"torch\":\ndata.share_memory_()\nreturn data\nelif data is None:\nreturn None\nelif isinstance(data, dict):\nreturn {key: to_tensor(value, target_type) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_tensor(val, target_type) for val in data]\nelif isinstance(data, tuple) and hasattr(data, '_fields'):  # Named tuple\nreturn type(data)([to_tensor(val, target_type) for val in data])\nelif isinstance(data, tuple):\nreturn tuple([to_tensor(val, target_type) for val in data])\nelif isinstance(data, set):\nreturn set([to_tensor(val, target_type) for val in data])\nelse:\ndata = conversion_function[target_type](np.array(data))\nif shared_memory and target_type == \"torch\":\ndata.share_memory_()\nreturn data\n</code></pre>"}, {"location": "fastestimator/backend/_to_type.html", "title": "_to_type", "text": ""}, {"location": "fastestimator/backend/_to_type.html#fastestimator.fastestimator.backend._to_type.to_type", "title": "<code>to_type</code>", "text": "<p>Compute the data types of tensors within a collection of <code>data</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\ndtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\ntypes = fe.backend.to_type(data)\n# {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <p>Returns:</p> Type Description <code>Union[Collection, str]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their dtypes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_to_type.py</code> <pre><code>def to_type(data: Union[Collection, Tensor]) -&gt; Union[Collection, str]:\n\"\"\"Compute the data types of tensors within a collection of `data` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\n        dtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\n    types = fe.backend.to_type(data)\n    # {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their dtypes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_type(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_type(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_type(val) for val in data])\nelif isinstance(data, set):\nreturn set([to_type(val) for val in data])\nelif hasattr(data, \"dtype\"):\nreturn data.dtype\nelse:\nreturn np.array(data).dtype\n</code></pre>"}, {"location": "fastestimator/backend/_transpose.html", "title": "_transpose", "text": ""}, {"location": "fastestimator/backend/_transpose.html#fastestimator.fastestimator.backend._transpose.transpose", "title": "<code>transpose</code>", "text": "<p>Transpose the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(n)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(t)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\nb = fe.backend.transpose(p)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The transposed <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_transpose.py</code> <pre><code>def transpose(tensor: Tensor) -&gt; Tensor:\n\"\"\"Transpose the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(n)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(t)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1,2],[3,4,5],[6,7,8]])\n    b = fe.backend.transpose(p)  # [[0, 3, 6], [1, 4, 7], [2, 5, 8]]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The transposed `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.transpose(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.T\nelif isinstance(tensor, np.ndarray):\nreturn np.transpose(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_update_model.html", "title": "_update_model", "text": ""}, {"location": "fastestimator/backend/_update_model.html#fastestimator.fastestimator.backend._update_model.update_model", "title": "<code>update_model</code>", "text": "<p>Update <code>model</code> weights based on a given <code>gradients</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nx = tf.ones((3, 28, 28, 1))  # (batch, height, width, channels)\ny = tf.constant((1, 0, 1))\nwith tf.GradientTape(persistent=True) as tape:\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\ngradients = fe.backend.get_gradient(target=loss, sources=m.trainable_variables, tape=tape)\nfe.backend.update_model(m, gradients=gradients)\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nx = torch.ones((3, 1, 28, 28))  # (batch, channels, height, width)\ny = torch.tensor((1, 0, 1))\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\ngradients = fe.backend.get_gradient(target=loss,\nsources=[x for x in m.parameters() if x.requires_grad])\nfe.backend.update_model(m, gradients=gradients)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to update.</p> required <code>gradients</code> <code>List[Union[tf.Tensor, torch.Tensor]]</code> <p>A list of tensors to update the models.</p> required <code>defer</code> <code>bool</code> <p>If True, then the model update function will be stored into the <code>deferred</code> dictionary rather than applied immediately.</p> <code>False</code> <code>deferred</code> <code>Optional[Dict[str, List[Callable[[], None]]]]</code> <p>A dictionary in which model update functions are stored.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> <code>AssertionError</code> <p>If <code>model</code> doesn't have <code>current_optimizer</code> attribute</p> <code>AssertionError</code> <p>If Pytorch <code>model.current_optimizer</code> doesn't have <code>scaler</code> attribute</p> Source code in <code>fastestimator\\fastestimator\\backend\\_update_model.py</code> <pre><code>def update_model(model: Union[tf.keras.Model, torch.nn.Module],\ngradients: List[Union[tf.Tensor, torch.Tensor]],\ndefer: bool = False,\ndeferred: Optional[Dict[str, List[Callable[[], None]]]] = None) -&gt; None:\n\"\"\"Update `model` weights based on a given `gradients`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    x = tf.ones((3, 28, 28, 1))  # (batch, height, width, channels)\n    y = tf.constant((1, 0, 1))\n    with tf.GradientTape(persistent=True) as tape:\n        pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n        loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n        gradients = fe.backend.get_gradient(target=loss, sources=m.trainable_variables, tape=tape)\n        fe.backend.update_model(m, gradients=gradients)\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    x = torch.ones((3, 1, 28, 28))  # (batch, channels, height, width)\n    y = torch.tensor((1, 0, 1))\n    pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n    gradients = fe.backend.get_gradient(target=loss,\n                                        sources=[x for x in m.parameters() if x.requires_grad])\n    fe.backend.update_model(m, gradients=gradients)\n    ```\n    Args:\n        model: A neural network instance to update.\n        gradients: A list of tensors to update the models.\n        defer: If True, then the model update function will be stored into the `deferred` dictionary rather than\n            applied immediately.\n        deferred: A dictionary in which model update functions are stored.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n        AssertionError: If `model` doesn't have `current_optimizer` attribute\n        AssertionError: If Pytorch `model.current_optimizer` doesn't have `scaler` attribute\n    \"\"\"\nassert hasattr(model, \"current_optimizer\"), (\"The model needs to have 'current_optimizer' attribute. Please \"\n\"instantiate the model with fe.build\")\nif isinstance(model, tf.keras.Model):\nvariables = model.trainable_variables\nif defer:\ndeferred.setdefault(model.model_name,\n[]).append(lambda: model.current_optimizer.apply_gradients(zip(gradients, variables)))\nelse:\nmodel.current_optimizer.apply_gradients(zip(gradients, variables))\nelif isinstance(model, torch.nn.Module):\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\nfor gradient, parameter in zip(gradients, trainable_params):\nif gradient is None:\nglobal _ALREADY_GAVE_FE_GRAD_WARNING\nif not _ALREADY_GAVE_FE_GRAD_WARNING:\nprint(\"\\033[93m{}\\033[00m\".format(\"FastEstimator-Warn: 'None' detected in gradients. Some or all \"\n\"of your computation graph may not be connected to your loss.\"))\n_ALREADY_GAVE_FE_GRAD_WARNING = True\ncontinue\nif parameter.grad is not None:\nparameter.grad += gradient\nelse:\nparameter.grad = gradient.clone()\nif defer:\n# Only need to call once per model since gradients are getting accumulated\ndeferred[model.model_name] = [lambda: _torch_step(model.current_optimizer)]\nelse:\n_torch_step(model.current_optimizer)\nif deferred:\ndeferred.pop(model.model_name)  # Don't need those deferred steps anymore\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/_watch.html", "title": "_watch", "text": ""}, {"location": "fastestimator/backend/_watch.html#fastestimator.fastestimator.backend._watch.watch", "title": "<code>watch</code>", "text": "<p>Monitor the given <code>tensor</code> for later gradient computations.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.ones((3,28,28,1))\nwith tf.GradientTape(persistent=True) as tape:\nx = fe.backend.watch(x, tape=tape)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.ones((3,1,28,28))  # x.requires_grad == False\nx = fe.backend.watch(x)  # x.requires_grad == True\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be monitored.</p> required <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> or a copy of the <code>tensor</code> which is being tracked for gradient computations. This value is only</p> <code>Tensor</code> <p>needed if using PyTorch as the backend.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_watch.py</code> <pre><code>def watch(tensor: Tensor, tape: Optional[tf.GradientTape] = None) -&gt; Tensor:\n\"\"\"Monitor the given `tensor` for later gradient computations.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.ones((3,28,28,1))\n    with tf.GradientTape(persistent=True) as tape:\n        x = fe.backend.watch(x, tape=tape)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.ones((3,1,28,28))  # x.requires_grad == False\n    x = fe.backend.watch(x)  # x.requires_grad == True\n    ```\n    Args:\n        tensor: The tensor to be monitored.\n        tape: A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).\n    Returns:\n        The `tensor` or a copy of the `tensor` which is being tracked for gradient computations. This value is only\n        needed if using PyTorch as the backend.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\ntape.watch(tensor)\nreturn tensor\nelif isinstance(tensor, torch.Tensor):\nif tensor.requires_grad:\nreturn tensor\n# It is tempting to just do tensor.requires_grad = True here, but that will lead to trouble\nreturn tensor.detach().requires_grad_(True)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_zeros_like.html", "title": "_zeros_like", "text": ""}, {"location": "fastestimator/backend/_zeros_like.html#fastestimator.fastestimator.backend._zeros_like.zeros_like", "title": "<code>zeros_like</code>", "text": "<p>Generate zeros shaped like <code>tensor</code> with a specified <code>dtype</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. If None then the <code>tensor</code> dtype is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of zeros with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_zeros_like.py</code> <pre><code>def zeros_like(tensor: Tensor, dtype: Union[None, str] = None) -&gt; Tensor:\n\"\"\"Generate zeros shaped like `tensor` with a specified `dtype`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        dtype: The data type to be used when generating the resulting tensor. If None then the `tensor` dtype is used.\n    Returns:\n        A tensor of zeros with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(tensor):\nreturn tf.zeros_like(tensor, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.zeros_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype])\nelif isinstance(tensor, np.ndarray):\nreturn np.zeros_like(tensor, dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/_zscore.html", "title": "_zscore", "text": ""}, {"location": "fastestimator/backend/_zscore.html#fastestimator.fastestimator.backend._zscore.zscore", "title": "<code>zscore</code>", "text": "<p>Apply Zscore processing to a given tensor or array.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.zscore(n)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.zscore(t)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.zscore(p)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>The input tensor or array.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Data after substracting mean and divided by standard deviation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\_zscore.py</code> <pre><code>def zscore(data: Tensor, epsilon: float = 1e-7) -&gt; Tensor:\n\"\"\"Apply Zscore processing to a given tensor or array.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.zscore(n)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.zscore(t)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.zscore(p)  # [[-1.34164079, -0.4472136 ],[0.4472136 , 1.34164079]]\n    ```\n    Args:\n        data: The input tensor or array.\n    Returns:\n        Data after substracting mean and divided by standard deviation.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif tf.is_tensor(data):\ndata = tf.cast(data, tf.float32)\nmean = tf.reduce_mean(data)\nstd = tf.keras.backend.std(data)\nreturn (data - mean) / tf.maximum(std, epsilon)\nelif isinstance(data, torch.Tensor):\ndata = data.type(torch.float32)\nmean = torch.mean(data)\nstd = torch.std(data, unbiased=False)\nreturn (data - mean) / torch.max(std, torch.tensor(epsilon))\nelif isinstance(data, np.ndarray):\nmean = np.mean(data)\nstd = np.std(data)\nreturn (data - mean) / max(std, epsilon)\nelse:\nraise ValueError(\"Unrecognized data type {}\".format(type(data)))\n</code></pre>"}, {"location": "fastestimator/cli/history.html", "title": "history", "text": ""}, {"location": "fastestimator/cli/history.html#fastestimator.fastestimator.cli.history.configure_history_parser", "title": "<code>configure_history_parser</code>", "text": "<p>Add a history parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\history.py</code> <pre><code>def configure_history_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a history parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('history',\ndescription='View prior FE training histories',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('--limit', metavar='L', type=int, help=\"How many entries to return\", default=15)\nparser.add_argument('--interactive',\ndest='interactive',\nhelp=\"Whether to run an interactive session which lets you look up detailed information\",\naction='store_true',\ndefault=False)\nparser.add_argument('--args',\ndest='include_args',\nhelp=\"Whether to return a list of the args used to invoke the training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--errors',\ndest='errors',\nhelp=\"Whether to focus on failed trainings and include extra error information\",\naction='store_true',\ndefault=False)\nparser.add_argument('--features',\ndest='include_features',\nhelp=\"Whether to return a list of the FE features used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--datasets',\ndest='include_datasets',\nhelp=\"Whether to return a list of the datasets used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--pipeline',\ndest='include_pipeline',\nhelp=\"Whether to return a list of the pipeline ops used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--network',\ndest='include_network',\nhelp=\"Whether to return a list of the network ops used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--traces',\ndest='include_traces',\nhelp=\"Whether to return a list of the traces used during each training\",\naction='store_true',\ndefault=False)\nparser.add_argument('--pks',\ndest='include_pk',\nhelp=\"Whether to return the database primary keys of each entry\",\naction='store_true',\ndefault=False)\nparser.add_argument('--csv',\ndest='csv',\nhelp='Print the response as a csv rather than a formatted table',\naction='store_true',\ndefault=False)\nsp = parser.add_subparsers()\nsql_parser = sp.add_parser('sql',\ndescription='Perform a raw SQL query against the history database',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nsql_parser.add_argument('query',\nmetavar='&lt;Query&gt;',\ntype=str,\nhelp=\"ex: fastestimator history sql 'SELECT * FROM history'\")\nsql_parser.add_argument('--interactive',\ndest='interactive',\nhelp=\"Whether to run an interactive session which lets you look up detailed information\",\naction='store_true',\ndefault=False)\nsql_parser.add_argument('--csv',\ndest='csv',\nhelp='Print the response as a csv rather than a formatted table',\naction='store_true',\ndefault=False)\nsql_parser.set_defaults(func=history_sql)\nclear_parser = sp.add_parser('clear',\ndescription='Clear out old history entries to save space',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nclear_parser.add_argument('retain',\nmetavar='N',\nnargs='?',\ntype=int,\nhelp=\"How many of the most recent entries to keep\",\ndefault=20)\nclear_parser.set_defaults(func=clear_history)\nsettings_parser = sp.add_parser('settings',\ndescription=\"Modify history settings, such as how many logs to retain\",\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nsettings_parser.add_argument('--keep',\nmetavar='K',\ntype=int,\nhelp=\"How many of the most recent history entries to keep\")\nsettings_parser.add_argument('--keep_logs',\nmetavar='L',\ntype=int,\nhelp=\"How many of the most recent log entries to keep\")\nsettings_parser.set_defaults(func=settings)\nparser.set_defaults(func=history_basic)\n</code></pre>"}, {"location": "fastestimator/cli/history.html#fastestimator.fastestimator.cli.history.history_basic", "title": "<code>history_basic</code>", "text": "<p>A method to query FE history using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the read_sql() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the read_sql() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\history.py</code> <pre><code>def history_basic(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to query FE history using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the read_sql() method.\n        unknown: Any cli arguments not matching known inputs for the read_sql() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\nfrom fastestimator.summary.history import HistoryReader\nwith HistoryReader() as reader:\nreader.read_basic(limit=args['limit'],\ninteractive=args['interactive'],\ninclude_args=args['include_args'],\nerrors=args['errors'],\ninclude_pk=args['include_pk'],\ninclude_traces=args['include_traces'],\ninclude_features=args['include_features'],\ninclude_datasets=args['include_datasets'],\ninclude_pipeline=args['include_pipeline'],\ninclude_network=args['include_network'],\nas_csv=args['csv'])\n</code></pre>"}, {"location": "fastestimator/cli/logs.html", "title": "logs", "text": ""}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.configure_log_parser", "title": "<code>configure_log_parser</code>", "text": "<p>Add a logging parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def configure_log_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a logging parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('logs',\ndescription='Generates comparison graphs amongst one or more log files',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('log_dir',\nmetavar='&lt;Log Dir&gt;',\ntype=str,\nhelp=\"The path to a folder containing one or more log files\")\nparser.add_argument('--extension',\nmetavar='E',\ntype=str,\nhelp=\"The file type / extension of your logs\",\ndefault=\".txt\")\nparser.add_argument('--recursive', action='store_true', help=\"Recursively search sub-directories for log files\")\ngroup = parser.add_mutually_exclusive_group()\ngroup.add_argument('--ignore',\nmetavar='I',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to ignore though they may be present in the log files\")\ngroup.add_argument('--include',\nmetavar='Y',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to include. If provided, any other metrics will be ignored.\")\nparser.add_argument('--smooth',\nmetavar='&lt;float&gt;',\ntype=float,\nhelp=\"The amount of gaussian smoothing to apply (zero for no smoothing)\",\ndefault=0)\nparser.add_argument('--pretty_names', help=\"Clean up the metric names for display\", action='store_true')\nparser.add_argument('--group_by',\nmetavar='G',\ntype=str,\nnargs=1,\nhelp=\"A regex pattern to group different logs together and display their mean+-stdev. For \"\nr\"example, you could use --G '(.*)_[\\d]+\\.txt' to group files of the form \"\n\"&lt;name&gt;_&lt;number&gt;.txt by their &lt;name&gt;. We anticipate this being the common usecase, so you \"\n\"can use --G _n as a shortcut for that functionality.\")\nsave_group = parser.add_argument_group('output arguments')\nsave_x_group = save_group.add_mutually_exclusive_group(required=False)\nsave_x_group.add_argument(\n'--save',\nnargs='?',\nmetavar='&lt;Save Dir&gt;',\ndest='save',\naction=SaveAction,\ndefault=False,\nhelp=\"Save the output image. May be accompanied by a directory into \\\n                                              which the file is saved. If no output directory is specified, the log \\\n                                              directory will be used\")\nsave_x_group.add_argument('--display',\ndest='save',\naction='store_false',\nhelp=\"Render the image to the UI (rather than saving it)\",\ndefault=True)\nsave_x_group.set_defaults(save_dir=None)\nparser.set_defaults(func=logs)\n</code></pre>"}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.logs", "title": "<code>logs</code>", "text": "<p>A method to invoke the FE logging function using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the parse_log_dir() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the parse_log_dir() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def logs(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to invoke the FE logging function using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the parse_log_dir() method.\n        unknown: Any cli arguments not matching known inputs for the parse_log_dir() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\ngroup_by = args['group_by']\nif isinstance(group_by, list):\ngroup_by = group_by[0]\nif group_by == '_n':\ngroup_by = r'(.*)_[\\d]+' + '\\\\' + args['extension']\nfrom fastestimator.summary.logs import parse_log_dir\nparse_log_dir(args['log_dir'],\nargs['extension'],\nargs['recursive'],\nargs['smooth'],\nargs['save'],\nargs['save_dir'],\nargs['ignore'],\nargs['include'],\nargs['pretty_names'],\ngroup_by)\n</code></pre>"}, {"location": "fastestimator/cli/main.html", "title": "main", "text": ""}, {"location": "fastestimator/cli/main.html#fastestimator.fastestimator.cli.main.run_main", "title": "<code>run_main</code>", "text": "<p>A function which invokes the various argument parsers and then runs the requested subroutine.</p> Source code in <code>fastestimator\\fastestimator\\cli\\main.py</code> <pre><code>def run_main(argv) -&gt; None:\n\"\"\"A function which invokes the various argument parsers and then runs the requested subroutine.\n    \"\"\"\nparser = argparse.ArgumentParser(allow_abbrev=False)\nsubparsers = parser.add_subparsers()\n# In python 3.7 the following 2 lines could be put into the .add_subparsers() call\nsubparsers.required = True\nsubparsers.dest = 'mode'\nconfigure_train_parser(subparsers)\nconfigure_test_parser(subparsers)\nconfigure_run_parser(subparsers)\nconfigure_log_parser(subparsers)\nconfigure_plot_parser(subparsers)\nconfigure_history_parser(subparsers)\nargs, unknown = parser.parse_known_args(argv)\nargs.func(vars(args), unknown)\n</code></pre>"}, {"location": "fastestimator/cli/plot.html", "title": "plot", "text": ""}, {"location": "fastestimator/cli/plot.html#fastestimator.fastestimator.cli.plot.configure_plot_parser", "title": "<code>configure_plot_parser</code>", "text": "<p>Add a logging parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\plot.py</code> <pre><code>def configure_plot_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a logging parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('plot',\ndescription='Generates summary graph(s) from a saved search json file',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('search_path',\nmetavar='&lt;File Path&gt;',\ntype=str,\nhelp=\"The path to a json file summarizing a search object\")\nparser.add_argument('--ignore',\nmetavar='I',\ntype=str,\nnargs='+',\nhelp=\"The names of parameters or results to ignore when visualizing\")\nparser.add_argument('--title',\nmetavar='T',\ntype=str,\nhelp=\"A custom title for the generated plot\",\ndefault=None)\nparser.add_argument('--draw',\nmetavar='D',\nchoices=['cartesian', 'heatmap', 'parallel'],\nhelp=\"Force the system to attempt to draw a particular type of plot. This may raise an error if\"\n\" the given search instance is incompatible with the desired type of visualization. \"\n\"Choices are 'cartesian', 'heatmap', or 'parallel'.\",\ndefault=None)\nparser.add_argument('--color_by',\nmetavar='C',\ntype=str,\nhelp=\"Override the key used to color parallel coordinate plots\",\ndefault=None)\nparser.add_argument('-G',\n'--group',\naction='append',\ntype=str,\nnargs='+',\nhelp=\"Group multiple results onto the same cartesian plot\")\nsave_group = parser.add_argument_group('output arguments')\nsave_x_group = save_group.add_mutually_exclusive_group(required=False)\nsave_x_group.add_argument(\n'--save',\nnargs='?',\nmetavar='&lt;Save Dir&gt;',\ndest='save',\naction=SaveAction,\ndefault=False,\nhelp=\"Save the output image. May be accompanied by a directory into \\\n                                              which the file is saved. If no output directory is specified, the log \\\n                                              directory will be used\")\nsave_x_group.add_argument('--display',\ndest='save',\naction='store_false',\nhelp=\"Render the image to the UI (rather than saving it)\",\ndefault=True)\nsave_x_group.set_defaults(save_dir=None)\nparser.set_defaults(func=search)\n</code></pre>"}, {"location": "fastestimator/cli/plot.html#fastestimator.fastestimator.cli.plot.search", "title": "<code>search</code>", "text": "<p>A method to invoke the FE search logging function using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the visualize_search() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the visualize_search() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\plot.py</code> <pre><code>def search(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to invoke the FE search logging function using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the visualize_search() method.\n        unknown: Any cli arguments not matching known inputs for the visualize_search() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\nif args['draw'] == 'cartesian':\nfrom fastestimator.search.visualize.cartesian import visualize_cartesian\nfn = visualize_cartesian\nelif args['draw'] == 'heatmap':\nfrom fastestimator.search.visualize.heatmap import visualize_heatmap\nfn = visualize_heatmap\nelif args['draw'] == 'parallel':\nfrom fastestimator.search.visualize.parallel_coordinate_plot import visualize_parallel_coordinates\nfn = visualize_parallel_coordinates\nelse:\nfrom fastestimator.search.visualize.visualize import visualize_search\nfn = visualize_search\nkwargs = {'search': args['search_path'],\n'title': args['title'],\n'ignore_keys': args['ignore'],\n'save_path': args['save_dir']}\n# Only add the function specific args if the user provides them to avoid clashing with manually specified plot type\nif args['color_by']:\nkwargs['color_by'] = args['color_by']\nif args['group']:\nkwargs['groups'] = args['group']\nfn(**kwargs)\n</code></pre>"}, {"location": "fastestimator/cli/run.html", "title": "run", "text": ""}, {"location": "fastestimator/cli/run.html#fastestimator.fastestimator.cli.run.configure_run_parser", "title": "<code>configure_run_parser</code>", "text": "<p>Add a run parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\run.py</code> <pre><code>def configure_run_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a run parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('run',\ndescription='Execute fastestimator_run function',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the fastestimator_run() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=run)\n</code></pre>"}, {"location": "fastestimator/cli/run.html#fastestimator.fastestimator.cli.run.run", "title": "<code>run</code>", "text": "<p>Invoke the fastestimator_run function from a file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the fastestimator_run() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\run.py</code> <pre><code>def run(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Invoke the fastestimator_run function from a file.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the fastestimator_run() method.\n    \"\"\"\nentry_point = args['entry_point']\nhyperparameters = {}\nif args['hyperparameters_json']:\nhyperparameters = os.path.abspath(args['hyperparameters_json'])\nwith open(hyperparameters, 'r') as f:\nhyperparameters = json.load(f)\nhyperparameters.update(parse_cli_to_dictionary(unknown))\nmodule_name = os.path.splitext(os.path.basename(entry_point))[0]\ndir_name = os.path.abspath(os.path.dirname(entry_point))\nsys.path.insert(0, dir_name)\nspec_module = __import__(module_name, globals(), locals())\nif hasattr(spec_module, \"fastestimator_run\"):\nspec_module.fastestimator_run(**hyperparameters)\nelif hasattr(spec_module, \"get_estimator\"):\nest = spec_module.get_estimator(**hyperparameters)\nif \"train\" in est.pipeline.data:\nest.fit()\nif \"test\" in est.pipeline.data:\nest.test()\nelse:\nraise ValueError(\"The file {} does not contain 'fastestimator_run' or 'get_estimator'\".format(module_name))\n</code></pre>"}, {"location": "fastestimator/cli/train.html", "title": "train", "text": ""}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_test_parser", "title": "<code>configure_test_parser</code>", "text": "<p>Add a testing parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_test_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a testing parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('test',\ndescription='Test a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--eager',\ntype=literal_eval,\nhelp=\"Eager setting, can be True or False\",\nchoices=[True, False],\ndefault=False)\nparser.add_argument('--summary', type=str, help=\"Experiment name\", default=None)\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=test)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_train_parser", "title": "<code>configure_train_parser</code>", "text": "<p>Add a training parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_train_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a training parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('train',\ndescription='Train a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--warmup',\ntype=literal_eval,\nhelp=\"Warmup setting, can be True or False\",\nchoices=[True, False],\ndefault=True)\nparser.add_argument('--eager',\ntype=literal_eval,\nhelp=\"Eager setting, can be True or False\",\nchoices=[True, False],\ndefault=False)\nparser.add_argument('--summary', type=str, help=\"Experiment name\", default=None)\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=train)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.test", "title": "<code>test</code>", "text": "<p>Load an Estimator from a file and invoke its .test() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def test(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .test() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nestimator.test(summary=args['summary'], eager=args['eager'])\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.train", "title": "<code>train</code>", "text": "<p>Load an Estimator from a file and invoke its .fit() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def train(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .fit() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nestimator.fit(warmup=args['warmup'], eager=args['eager'], summary=args['summary'])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html", "title": "batch_dataset", "text": ""}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset", "title": "<code>BatchDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.</p> <p>This dataset helps to enable several use-cases: 1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.     <pre><code>ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\nds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\nunpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n# {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n</code></pre> 2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n</code></pre> 3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[FEDataset, Iterable[FEDataset]]</code> <p>The dataset(s) to use for batch sampling. While these should be FEDatasets, pytorch datasets will technically also work. If you use them, however, you will lose the .split() and .summary() methods.</p> required <code>num_samples</code> <code>Union[int, Iterable[int]]</code> <p>Number of samples to draw from the <code>datasets</code>. May be a single int if used in conjunction with <code>probability</code>, otherwise a list of ints of len(<code>datasets</code>) is required.</p> required <code>probability</code> <code>Optional[Iterable[float]]</code> <p>Probability to draw from each dataset. Only allowed if <code>num_samples</code> is an integer.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>@traceable()\nclass BatchDataset(FEDataset):\n\"\"\"BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.\n    This dataset helps to enable several use-cases:\n    1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.\n        ```python\n        ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\n        ds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\n        unpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n        # {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n        ```\n    2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n        ```\n    3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n        ```\n    Args:\n        datasets: The dataset(s) to use for batch sampling. While these should be FEDatasets, pytorch datasets will\n            technically also work. If you use them, however, you will lose the .split() and .summary() methods.\n        num_samples: Number of samples to draw from the `datasets`. May be a single int if used in conjunction with\n            `probability`, otherwise a list of ints of len(`datasets`) is required.\n        probability: Probability to draw from each dataset. Only allowed if `num_samples` is an integer.\n    \"\"\"\ndef __init__(self,\ndatasets: Union[FEDataset, Iterable[FEDataset]],\nnum_samples: Union[int, Iterable[int]],\nprobability: Optional[Iterable[float]] = None) -&gt; None:\nself.datasets = to_list(datasets)\nself.num_samples = to_list(num_samples)\nself.probability = to_list(probability)\nself.same_feature = False\nself.all_fe_datasets = False\nself._check_input()\nself.index_maps = []\nself.child_reset_fns = [dataset.fe_reset_ds for dataset in self.datasets if hasattr(dataset, 'fe_reset_ds')]\nself.fe_reset_ds(seed=0)\ndef _check_input(self) -&gt; None:\n\"\"\"Verify that the given input values are valid.\n        Raises:\n            AssertionError: If any of the parameters are found to by unacceptable for a variety of reasons.\n        \"\"\"\nassert len(self.datasets) &gt; 1, \"must provide multiple datasets as input\"\nfor num_sample in self.num_samples:\nassert isinstance(num_sample, int) and num_sample &gt; 0, \"only accept positive integer type as num_sample\"\n# check dataset keys\ndataset_keys = []\nnum_examples = self.num_samples * len(self.datasets) if len(\nself.num_samples) == 1 else [x for x in self.num_samples]\nfor idx, dataset in enumerate(self.datasets):\nsample_data = dataset[0]\nif isinstance(sample_data, list):\nkeys = [set(sample_data_element.keys()) for sample_data_element in sample_data]\nkeys = set.union(*keys)\nnum_examples[idx] *= len(sample_data)\nelse:\nkeys = set(sample_data.keys())\ndataset_keys.append(keys)\nfor key in dataset_keys:\nassert key, \"found no key in datasets\"\nis_same_key = all([dataset_keys[0] == key for key in dataset_keys])\nis_disjoint_key = sum([len(key) for key in dataset_keys]) == len(set.union(*dataset_keys))\nif len(self.datasets) &gt; 1:\nassert is_same_key != is_disjoint_key, \"dataset keys must be all same or all disjoint\"\nself.same_feature = is_same_key\nif self.probability:\nassert self.same_feature, \"keys must be exactly same among datasets when using probability distribution\"\nassert len(self.datasets) == len(self.probability), \"the length of dataset must match probability\"\nassert len(self.num_samples) == 1, \"num_sample must be scalar for probability mode\"\nassert len(self.datasets) &gt; 1, \"number of datasets must be more than one to use probability mode\"\nassert sum(self.probability) == 1, \"sum of probability must be 1\"\nfor p in self.probability:\nassert isinstance(p, float) and p &gt; 0, \"must provide positive float for probability distribution\"\nelse:\nassert len(self.datasets) == len(self.num_samples), \"the number of dataset must match num_samples\"\n# set up batch size\nif self.same_feature:\nif self.probability:\nself.fe_batch = round(sum([n * p for n, p in zip(num_examples, self.probability)]))\nelse:\nself.fe_batch = sum(num_examples)\nelse:\nassert len(set(num_examples)) == 1, \"the number of output samples must be the same for disjoint features\"\nself.fe_batch = num_examples[0]\nself.all_fe_datasets = all([isinstance(dataset, FEDataset) for dataset in self.datasets])\n# Check ExtendDataset\nfor idx, dataset in enumerate(self.datasets):\nassert not isinstance(dataset, ExtendDataset), \"Input Dataset cannot be an ExtendDataset object\"\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['BatchDataset']:\n\"\"\"This class overwrites the .split() method instead of _do_split().\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every element of the `splits` sequence.\n        Raises:\n            AssertionError: This method should never by invoked.\n        \"\"\"\nraise AssertionError(\"This method should not have been invoked. Please file a bug report\")\ndef split(self,\n*fractions: Union[float, int, Iterable[int]],\nseed: Optional[int] = None,\nstratify: Optional[str] = None) -&gt; Union['BatchDataset', List['BatchDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n            seed: The random seed to use when splitting the dataset. Useful if you want consistent splits across\n                multiple experiments. This isn't necessary if you are splitting by data index.\n            stratify: A class key within the dataset with which to stratify the split (to approximately maintain class\n                balance ratios before and after a split). Incompatible with data index splitting.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        Raises:\n            NotImplementedError: If the user created this dataset using one or more non-FEDataset inputs.\n        \"\"\"\nif not self.all_fe_datasets:\nraise NotImplementedError(\n\"BatchDataset.split() is not supported when BatchDataset contains non-FEDataset objects\")\n# Only pass the stratify argument to the dataset(s) which have the appropriate key\nnew_datasets = [\nto_list(ds.split(*fractions, seed=seed, stratify=stratify if stratify in ds[0] else None))\nfor ds in self.datasets\n]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\nif seed is not None:\n[ds.fe_reset_ds(seed=seed) for ds in results]\n# Re-compute personal variables\nself.fe_reset_ds(seed=seed)\nFEDataset.fix_split_traceabilty(self, results, fractions, seed, stratify)\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'datasets': [ds.__getstate__() if hasattr(ds, '__getstate__') else {} for ds in self.datasets]}\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nif not self.all_fe_datasets:\nprint(\"FastEstimator-Warn: BatchDataset summary will be incomplete since non-FEDatasets were used.\")\nreturn DatasetSummary(num_instances=len(self), keys={})\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\ndef __len__(self) -&gt; int:\n\"\"\"Compute the length of this dataset.\n        Returns:\n            How many batches of data can this dataset serve per epoch.\n        \"\"\"\nif len(self.num_samples) &gt; 1:\nlength = max([math.ceil(len(ds) / num_sample) for ds, num_sample in zip(self.datasets, self.num_samples)])\nelse:\nnum_sample = self.num_samples[0]\nlength = max([math.ceil(len(ds) / num_sample / p) for ds, p in zip(self.datasets, self.probability)])\nreturn length\ndef __getitem__(self, indices: Union[int, List[List[int]]]) -&gt; List[Dict[str, Any]]:\n\"\"\"Extract items from the underlying datasets based on the given `batch_idx`.\n        Args:\n            indices: Which indices to pull data from (or which batch_idx to query).\n        Returns:\n            A list of data instance dictionaries corresponding to the current `batch_idx`.\n        \"\"\"\nif isinstance(indices, int):\nindices = self.fe_batch_indices(indices)\nif self.same_feature:\nbatch = []\nfor dataset, idx_list in zip(self.datasets, indices):\nfor idx in idx_list:\nitem = dataset[idx]\nif isinstance(item, list):\nbatch.extend(item)\nelse:\nbatch.append(item)\nelse:\nunpaired_items = []\nfor dataset, idx_list in zip(self.datasets, indices):\nsingle_ds_items = []\nfor idx in idx_list:\nitem = dataset[idx]\nif isinstance(item, list):\nsingle_ds_items.extend(item)\nelse:\nsingle_ds_items.append(item)\nunpaired_items.append(single_ds_items)\nbatch = [{k: v for d in d_pair for k, v in d.items()} for d_pair in zip(*unpaired_items)]\nrandom.shuffle(batch)\nreturn batch\ndef fe_batch_indices(self, batch_idx: int) -&gt; List[List[int]]:\n\"\"\"Compute which internal dataset indices to use for a given batch.\n        This method is separate from the __getitem__ call so that multi-processing can work correctly when data is\n        filtered or extended.\n        Args:\n            batch_idx: Which batch is it.\n        Returns:\n            A list of data instance dictionaries corresponding to the current `batch_idx`.\n        \"\"\"\nif self.probability:\nindex = list(np.random.choice(range(self.n_datasets), size=self.num_samples, p=self.probability))\nnum_samples = [index.count(i) for i in range(self.n_datasets)]\nelse:\nnum_samples = self.num_samples\nindices = [[index_map[batch_idx * num_sample + idx] for idx in range(num_sample)] for num_sample, index_map\nin zip(num_samples, self.index_maps)]\nreturn indices\ndef fe_reset_ds(self, shuffle: bool = True, *, seed: Optional[int] = None) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n        Args:\n            shuffle: Whether to shuffle the dataset. If False the method will do nothing so long as index maps already\n                exist.\n            seed: A random seed to control the shuffling. This is provided for compatibility with the dataset.split\n                method random seed. It's not necessary from a training functionality perspective since shuffling is\n                performed every epoch, but if user wants to visualize a dataset element after the split this will help.\n        This method is invoked by the FEDataLoader which allows each epoch to have different random pairings of the\n        basis datasets.\n        \"\"\"\n# Reset any children who need resetting\nfor fn in self.child_reset_fns:\nfn(shuffle=shuffle, seed=seed)\n# Don't bother re-initializing if shuffle is False\nif shuffle is False and self.index_maps:\nreturn\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nif seed is not None:\nrandom.Random(seed).shuffle(mapping)\nelse:\nrandom.shuffle(mapping)\nif hasattr(dataset, \"fe_batch_indices\"):\nself.index_maps.append([dataset.fe_batch_indices(item) for sublist in index_map for item in sublist])\nelse:\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.fe_batch_indices", "title": "<code>fe_batch_indices</code>", "text": "<p>Compute which internal dataset indices to use for a given batch.</p> <p>This method is separate from the getitem call so that multi-processing can work correctly when data is filtered or extended.</p> <p>Parameters:</p> Name Type Description Default <code>batch_idx</code> <code>int</code> <p>Which batch is it.</p> required <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>A list of data instance dictionaries corresponding to the current <code>batch_idx</code>.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def fe_batch_indices(self, batch_idx: int) -&gt; List[List[int]]:\n\"\"\"Compute which internal dataset indices to use for a given batch.\n    This method is separate from the __getitem__ call so that multi-processing can work correctly when data is\n    filtered or extended.\n    Args:\n        batch_idx: Which batch is it.\n    Returns:\n        A list of data instance dictionaries corresponding to the current `batch_idx`.\n    \"\"\"\nif self.probability:\nindex = list(np.random.choice(range(self.n_datasets), size=self.num_samples, p=self.probability))\nnum_samples = [index.count(i) for i in range(self.n_datasets)]\nelse:\nnum_samples = self.num_samples\nindices = [[index_map[batch_idx * num_sample + idx] for idx in range(num_sample)] for num_sample, index_map\nin zip(num_samples, self.index_maps)]\nreturn indices\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.fe_reset_ds", "title": "<code>fe_reset_ds</code>", "text": "<p>Rearrange the index maps of this BatchDataset.</p> <p>Parameters:</p> Name Type Description Default <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the dataset. If False the method will do nothing so long as index maps already exist.</p> <code>True</code> <code>seed</code> <code>Optional[int]</code> <p>A random seed to control the shuffling. This is provided for compatibility with the dataset.split method random seed. It's not necessary from a training functionality perspective since shuffling is performed every epoch, but if user wants to visualize a dataset element after the split this will help.</p> <code>None</code> <p>This method is invoked by the FEDataLoader which allows each epoch to have different random pairings of the basis datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def fe_reset_ds(self, shuffle: bool = True, *, seed: Optional[int] = None) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n    Args:\n        shuffle: Whether to shuffle the dataset. If False the method will do nothing so long as index maps already\n            exist.\n        seed: A random seed to control the shuffling. This is provided for compatibility with the dataset.split\n            method random seed. It's not necessary from a training functionality perspective since shuffling is\n            performed every epoch, but if user wants to visualize a dataset element after the split this will help.\n    This method is invoked by the FEDataLoader which allows each epoch to have different random pairings of the\n    basis datasets.\n    \"\"\"\n# Reset any children who need resetting\nfor fn in self.child_reset_fns:\nfn(shuffle=shuffle, seed=seed)\n# Don't bother re-initializing if shuffle is False\nif shuffle is False and self.index_maps:\nreturn\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nif seed is not None:\nrandom.Random(seed).shuffle(mapping)\nelse:\nrandom.shuffle(mapping)\nif hasattr(dataset, \"fe_batch_indices\"):\nself.index_maps.append([dataset.fe_batch_indices(item) for sublist in index_map for item in sublist])\nelse:\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\nds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>*fractions</code> <code>Union[float, int, Iterable[int]]</code> <p>Floating point values will be interpreted as percentages, integers as an absolute number of datapoints, and an iterable of integers as the exact indices of the data that should be removed in order to create the new dataset.</p> <code>()</code> <code>seed</code> <code>Optional[int]</code> <p>The random seed to use when splitting the dataset. Useful if you want consistent splits across multiple experiments. This isn't necessary if you are splitting by data index.</p> <code>None</code> <code>stratify</code> <code>Optional[str]</code> <p>A class key within the dataset with which to stratify the split (to approximately maintain class balance ratios before and after a split). Incompatible with data index splitting.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[BatchDataset, List[BatchDataset]]</code> <p>One or more new datasets which are created by removing elements from the current dataset. The number of</p> <code>Union[BatchDataset, List[BatchDataset]]</code> <p>datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided</p> <code>Union[BatchDataset, List[BatchDataset]]</code> <p>then the return will be a single dataset rather than a list of datasets.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the user created this dataset using one or more non-FEDataset inputs.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def split(self,\n*fractions: Union[float, int, Iterable[int]],\nseed: Optional[int] = None,\nstratify: Optional[str] = None) -&gt; Union['BatchDataset', List['BatchDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n        seed: The random seed to use when splitting the dataset. Useful if you want consistent splits across\n            multiple experiments. This isn't necessary if you are splitting by data index.\n        stratify: A class key within the dataset with which to stratify the split (to approximately maintain class\n            balance ratios before and after a split). Incompatible with data index splitting.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    Raises:\n        NotImplementedError: If the user created this dataset using one or more non-FEDataset inputs.\n    \"\"\"\nif not self.all_fe_datasets:\nraise NotImplementedError(\n\"BatchDataset.split() is not supported when BatchDataset contains non-FEDataset objects\")\n# Only pass the stratify argument to the dataset(s) which have the appropriate key\nnew_datasets = [\nto_list(ds.split(*fractions, seed=seed, stratify=stratify if stratify in ds[0] else None))\nfor ds in self.datasets\n]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\nif seed is not None:\n[ds.fe_reset_ds(seed=seed) for ds in results]\n# Re-compute personal variables\nself.fe_reset_ds(seed=seed)\nFEDataset.fix_split_traceabilty(self, results, fractions, seed, stratify)\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nif not self.all_fe_datasets:\nprint(\"FastEstimator-Warn: BatchDataset summary will be incomplete since non-FEDatasets were used.\")\nreturn DatasetSummary(num_instances=len(self), keys={})\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\n</code></pre>"}, {"location": "fastestimator/dataset/csv_dataset.html", "title": "csv_dataset", "text": ""}, {"location": "fastestimator/dataset/csv_dataset.html#fastestimator.fastestimator.dataset.csv_dataset.CSVDataset", "title": "<code>CSVDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a CSV file.</p> <p>CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the CSV file.</p> required <code>delimiter</code> <code>str</code> <p>What delimiter is used by the file.</p> <code>','</code> <code>kwargs</code> <p>Other arguments to be passed through to pandas csv reader function. See the pandas docs for details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\dataset\\csv_dataset.py</code> <pre><code>@traceable()\nclass CSVDataset(InMemoryDataset):\n\"\"\"A dataset from a CSV file.\n    CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file\n    may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information\n    that you want to feed into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the CSV file.\n        delimiter: What delimiter is used by the file.\n        kwargs: Other arguments to be passed through to pandas csv reader function. See the pandas docs for details:\n            https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.\n    \"\"\"\ndef __init__(self, file_path: str, delimiter: str = \",\", **kwargs) -&gt; None:\ndf = pd.read_csv(file_path, delimiter=delimiter, **kwargs)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/dataloader.html", "title": "dataloader", "text": ""}, {"location": "fastestimator/dataset/dataloader.html#fastestimator.fastestimator.dataset.dataloader.FEDataLoader", "title": "<code>FEDataLoader</code>", "text": "<p>         Bases: <code>DataLoader</code></p> <p>A Data Loader that can handle filtering data.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[Dataset, Sized]</code> <p>The dataset to be drawn from. The dataset may optionally implement .fe_reset_ds(bool) and/or .fe_batch_indices(int) methods to modify the system's sampling behavior. See fe.dataset.BatchDataset for an example which uses both of these methods.</p> required <code>postprocess_fn</code> <code>Optional[Callable[[Dict[str, Any]], Union[Dict[str, Any], FilteredData]]]</code> <p>A function to run on a collated batch of data before returning it. This function can return a FilteredData object in order to drop the given batch.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use (or None if the dataset is already providing a batch).</p> <code>1</code> <code>steps_per_epoch</code> <code>Optional[int]</code> <p>How many steps to have per epoch. If None the loader will perform a single pass through the dataset (unless samples are filtered with replacement, in which case the dataset may be passed over multiple times). If <code>steps_per_epoch</code> is set, it will truncate or expand the dataset until the specified number of steps are reached. When expanding datasets, they will be exhausted in their entirety before being re-sampled, equivalent to running multiple epochs of training one after the other (unless you are also filtering data, in which case at most one batch of data might be seen after the re-shuffling occurs).</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the dataset.</p> <code>False</code> <code>num_workers</code> <code>int</code> <p>How many multiprocessing threads to use (unix/mac only).</p> <code>0</code> <code>collate_fn</code> <code>Callable</code> <p>What function to use to collate a list of data into a batch. This should take care of any desired padding.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch of data if that batch is incomplete. Note that this is meaningless for batched datasets, as well as when <code>steps_per_epoch</code> is set - in which case the dataset will be re-sampled as necessary until the specified number of steps has been completed in full.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dataloader.py</code> <pre><code>class FEDataLoader(DataLoader):\n\"\"\"A Data Loader that can handle filtering data.\n    This class is intentionally not @traceable.\n    Args:\n        dataset: The dataset to be drawn from. The dataset may optionally implement .fe_reset_ds(bool) and/or\n            .fe_batch_indices(int) methods to modify the system's sampling behavior. See fe.dataset.BatchDataset for an\n            example which uses both of these methods.\n        postprocess_fn: A function to run on a collated batch of data before returning it. This function can return a\n            FilteredData object in order to drop the given batch.\n        batch_size: The batch size to use (or None if the dataset is already providing a batch).\n        steps_per_epoch: How many steps to have per epoch. If None the loader will perform a single pass through the\n            dataset (unless samples are filtered with replacement, in which case the dataset may be passed over multiple\n            times). If `steps_per_epoch` is set, it will truncate or expand the dataset until the specified number of\n            steps are reached. When expanding datasets, they will be exhausted in their entirety before being\n            re-sampled, equivalent to running multiple epochs of training one after the other (unless you are also\n            filtering data, in which case at most one batch of data might be seen after the re-shuffling occurs).\n        shuffle: Whether to shuffle the dataset.\n        num_workers: How many multiprocessing threads to use (unix/mac only).\n        collate_fn: What function to use to collate a list of data into a batch. This should take care of any desired\n            padding.\n        drop_last: Whether to drop the last batch of data if that batch is incomplete. Note that this is meaningless for\n            batched datasets, as well as when `steps_per_epoch` is set - in which case the dataset will be re-sampled as\n            necessary until the specified number of steps has been completed in full.\n    \"\"\"\n_current_threads = []\nFE_LOADER_KIND = 7\n# The typing for 'dataset' should be an 'and' rather than 'or' but that feature is still under development:\n# https://github.com/python/typing/issues/213\ndef __init__(self,\ndataset: Union[Dataset, Sized],\npostprocess_fn: Optional[Callable[[Dict[str, Any]], Union[Dict[str, Any], FilteredData]]] = None,\nbatch_size: Optional[int] = 1,\nsteps_per_epoch: Optional[int] = None,\nshuffle: bool = False,\nnum_workers: int = 0,\ncollate_fn: Callable = None,\ndrop_last: bool = False):\nreset_fn = dataset.fe_reset_ds if hasattr(dataset, 'fe_reset_ds') else None\nconvert_fn = dataset.fe_batch_indices if hasattr(dataset, 'fe_batch_indices') else None\nsampler = InfiniteSampler(data_source=dataset, shuffle=shuffle, reset_fn=reset_fn, convert_fn=convert_fn)\nif batch_size is not None and batch_size &lt; 1:\nraise ValueError(f\"batch_size must be None or a positive integer, but got {batch_size}\")\n# Figure out the real batch size. This is already done in OpDataset, but if user manually instantiates this\n# loader without using an OpDataset we still want to know the batch size\nif not hasattr(dataset, \"fe_batch\"):\nsample_item = dataset[0]\ndataset.fe_batch = len(sample_item) if isinstance(sample_item, list) else 0\nif dataset.fe_batch:\n# The batch size where torch is concerned is probably None, but we know that it is secretly batched\nself.fe_batch_size = dataset.fe_batch\nelse:\nself.fe_batch_size = batch_size\n# Figure out how many samples should be returned during the course of 1 epoch\nif steps_per_epoch is not None:\nto_yield = steps_per_epoch * (batch_size or 1)\n# Note that drop_last is meaningless here since we will provide exactly the requested number of steps\nelse:\nif isinstance(dataset, OpDataset) and isinstance(dataset.dataset, ExtendDataset):\nto_yield = dataset.dataset.spoof_length\nelif isinstance(dataset, ExtendDataset):\nto_yield = dataset.spoof_length\nelse:\nto_yield = len(dataset)\nif drop_last:\nto_yield -= to_yield % (batch_size or 1)\nself.fe_samples_to_yield = to_yield\nself.fe_drop_last = drop_last\nself.fe_collate_fn = collate_fn or default_collate\nif self.fe_batch_size in (0, None) and batch_size is None and self.fe_collate_fn == default_collate:\n# The user did not provide a batch dataset nor a batch size, so default collate won't work. Have to try\n# convert instead.\nself.fe_collate_fn = default_convert\nself.fe_postprocess_fn = postprocess_fn\n# We could disable pre-collating when num_workers=0, but this would lead to inconsistent batch ordering between\n# single- and multi-processing.\nsuper().__init__(\ndataset=dataset,\nbatch_size=batch_size,\nsampler=sampler,\nnum_workers=num_workers,\npersistent_workers=False,\ncollate_fn=functools.partial(_pre_collate, try_fn=self.fe_collate_fn, postprocess_fn=postprocess_fn),\nworker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)))\nif self.batch_size is not None:\n# We need a special fetcher type later in order to build batches correctly\nself._dataset_kind = self.FE_LOADER_KIND\ndef shutdown(self) -&gt; None:\n\"\"\"Close the worker threads used by this iterator.\n        The hope is that this will prevent \"RuntimeError: DataLoader worker (pid(s) XXXX) exited unexpectedly\" during\n        the test suites.\n        \"\"\"\nif isinstance(self._iterator, _MultiProcessingDataLoaderIter):\nself._iterator._shutdown_workers()\nself._iterator = None\nFEDataLoader._current_threads.clear()\ndef __iter__(self) -&gt; _BaseDataLoaderIter:\n# Similar to the original iter method, but we remember iterators in order to manually close them when new ones\n# are created\nself.shutdown()\nself._iterator = self._get_fe_iterator()\nif isinstance(self._iterator, _MultiProcessingDataLoaderIter):\nFEDataLoader._current_threads.extend([w.pid for w in self._iterator._workers])\nreturn self._iterator\ndef _get_fe_iterator(self):\nif self.num_workers == 0:\nif self.batch_size is None:\n# We use 'fake' batch size here to identify datasets which perform their own batching\nreturn _SPPostBatchIter(self)\nreturn _SPPreBatchIter(self)\nelse:\nwith Suppressor(allow_pyprint=True):  # Prevent unnecessary warnings about resetting numbers of threads\nif self.batch_size is None:\n# We use 'fake' batch size here to identify datasets which perform their own batching\nreturn _MPPostBatchIter(self)\nreturn _MPPreBatchIter(self)\ndef __len__(self):\nreturn self.fe_samples_to_yield\ndef get_batch_size(self) -&gt; int:\nreturn self.fe_batch_size\n</code></pre>"}, {"location": "fastestimator/dataset/dataloader.html#fastestimator.fastestimator.dataset.dataloader.FEDataLoader.shutdown", "title": "<code>shutdown</code>", "text": "<p>Close the worker threads used by this iterator.</p> <p>The hope is that this will prevent \"RuntimeError: DataLoader worker (pid(s) XXXX) exited unexpectedly\" during the test suites.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataloader.py</code> <pre><code>def shutdown(self) -&gt; None:\n\"\"\"Close the worker threads used by this iterator.\n    The hope is that this will prevent \"RuntimeError: DataLoader worker (pid(s) XXXX) exited unexpectedly\" during\n    the test suites.\n    \"\"\"\nif isinstance(self._iterator, _MultiProcessingDataLoaderIter):\nself._iterator._shutdown_workers()\nself._iterator = None\nFEDataLoader._current_threads.clear()\n</code></pre>"}, {"location": "fastestimator/dataset/dataloader.html#fastestimator.fastestimator.dataset.dataloader.InfiniteSampler", "title": "<code>InfiniteSampler</code>", "text": "<p>         Bases: <code>Sampler</code></p> <p>A class which never stops sampling.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>Sized</code> <p>The dataset to be sampled.</p> required <code>shuffle</code> <code>bool</code> <p>Whether to shuffle when sampling.</p> <code>True</code> <code>reset_fn</code> <code>Optional[Callable[[bool], None]]</code> <p>A function to be invoked (using the provided <code>shuffle</code> arg) every time the dataset has been fully traversed.</p> <code>None</code> <code>convert_fn</code> <code>Optional[Callable[[int], Any]]</code> <p>A function to be invoked (using the current index) every sample in order to convert an integer index into some arbitrary alternative index representation.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dataloader.py</code> <pre><code>class InfiniteSampler(Sampler):\n\"\"\"A class which never stops sampling.\n    Args:\n        data_source: The dataset to be sampled.\n        shuffle: Whether to shuffle when sampling.\n        reset_fn: A function to be invoked (using the provided `shuffle` arg) every time the dataset has been fully\n            traversed.\n        convert_fn: A function to be invoked (using the current index) every sample in order to convert an integer index\n            into some arbitrary alternative index representation.\n    \"\"\"\ndef __init__(self,\ndata_source: Sized,\nshuffle: bool = True,\nreset_fn: Optional[Callable[[bool], None]] = None,\nconvert_fn: Optional[Callable[[int], Any]] = None):\nsuper().__init__(data_source=data_source)\nself.ds_len = len(data_source)\nif self.ds_len &lt; 1:\nraise ValueError(\"dataset length must be at least 1\")\nself.indices = [i for i in range(self.ds_len)]\nself.shuffle = shuffle\nself.reset_fn = reset_fn\nself.convert_fn = convert_fn\nself.idx = 0\ndef __len__(self):\nreturn self.ds_len\ndef __iter__(self):\nself.idx = 0\nif self.reset_fn:\nself.reset_fn(self.shuffle)\nif self.shuffle:\nrandom.shuffle(self.indices)\nreturn self\ndef __next__(self):\nif self.idx == self.ds_len:\nself.idx = 0\nif self.reset_fn:\nself.reset_fn(self.shuffle)\nif self.shuffle:\nrandom.shuffle(self.indices)\nelem = self.indices[self.idx]\nself.idx += 1\nif self.convert_fn:\nelem = self.convert_fn(elem)\nreturn elem\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html", "title": "dataset", "text": ""}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.DatasetSummary", "title": "<code>DatasetSummary</code>", "text": "<p>This class contains information summarizing a dataset object.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>num_instances</code> <code>int</code> <p>The number of data instances within the dataset (influences the size of an epoch).</p> required <code>num_classes</code> <code>Optional[int]</code> <p>How many different classes are present.</p> <code>None</code> <code>keys</code> <code>Dict[str, KeySummary]</code> <p>What keys does the dataset provide, along with summary information about each key.</p> required <code>class_key</code> <code>Optional[str]</code> <p>Which key corresponds to class information (if known).</p> <code>None</code> <code>class_key_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A mapping of the original class string values to the values which are output to the pipeline.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class DatasetSummary:\n\"\"\"This class contains information summarizing a dataset object.\n    This class is intentionally not @traceable.\n    Args:\n        num_instances: The number of data instances within the dataset (influences the size of an epoch).\n        num_classes: How many different classes are present.\n        keys: What keys does the dataset provide, along with summary information about each key.\n        class_key: Which key corresponds to class information (if known).\n        class_key_mapping: A mapping of the original class string values to the values which are output to the pipeline.\n    \"\"\"\nnum_instances: int\nnum_classes: Optional[int]\nclass_key: Optional[str]\nclass_key_mapping: Optional[Dict[str, Any]]\nkeys: Dict[str, KeySummary]\ndef __init__(self,\nnum_instances: int,\nkeys: Dict[str, KeySummary],\nnum_classes: Optional[int] = None,\nclass_key_mapping: Optional[Dict[str, Any]] = None,\nclass_key: Optional[str] = None):\nself.num_instances = num_instances\nself.class_key = class_key\nself.num_classes = num_classes\nself.class_key_mapping = class_key_mapping\nself.keys = keys\ndef __repr__(self):\nreturn \"&lt;DatasetSummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\ndef __str__(self):\nreturn jsonpickle.dumps(self, unpicklable=False)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset", "title": "<code>FEDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@traceable()\nclass FEDataset(Dataset):\ndef __len__(self) -&gt; int:\n\"\"\"Defines how many datapoints the dataset contains.\n        This is used for computing the number of datapoints available per epoch.\n        Returns:\n            The number of datapoints within the dataset.\n        \"\"\"\nraise NotImplementedError\ndef __getitem__(self, index: int) -&gt; Dict[str, Any]:\n\"\"\"Fetch a data instance at a specified index.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index.\n        \"\"\"\nraise NotImplementedError\n@classmethod\ndef fix_split_traceabilty(cls,\nparent: 'FEDataset',\nchildren: List['FEDataset'],\nfractions: Tuple[Union[float, int, Iterable[int]], ...],\nseed: Optional[int],\nstratify: Optional[str]) -&gt; None:\n\"\"\"A method to fix traceability information after invoking the dataset .split() method.\n        Note that the default implementation of the .split() function invokes this already, so this only needs to be\n        invoked if you override the .split() method when defining a subclass (ex. BatchDataset).\n        Args:\n            parent: The parent dataset on which .split() was invoked.\n            children: The datasets generated by performing the split.\n            fractions: The fraction arguments used to generate the children (should be one-to-one with the children).\n            seed: The random seed used to generate the split.\n            stratify: The stratify key used to generate the split.\n        \"\"\"\nif hasattr(parent, '_fe_traceability_summary'):\nparent_id = FEID(id(parent))\nfractions = [\nf\"range({frac.start}, {frac.stop}, {frac.step})\" if isinstance(frac, range) else f\"{frac}\"\nfor frac in fractions\n]\nfor child, frac in zip(children, fractions):\n# noinspection PyProtectedMember\ntables = deepcopy(child._fe_traceability_summary)\n# Update the ID if necessary\nchild_id = FEID(id(child))\nif child_id not in tables:\n# The child was created without invoking its __init__ method, so its internal summary will have the\n# wrong id\ntable = tables.pop(parent_id)\ntable.fe_id = child_id\ntables[child_id] = table\nelse:\ntable = tables[child_id]\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent=parent_id, fraction=frac, seed=seed, stratify=stratify)\ntable.fields['split'] = split_summary\nchild._fe_traceability_summary = tables\n# noinspection PyUnresolvedReferences\ntable = parent._fe_traceability_summary.get(parent_id)\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent='self',\nfraction=\", \".join([f\"-{frac}\" for frac in fractions]),\nseed=seed,\nstratify=stratify)\ntable.fields['split'] = split_summary\n# Put the new parent summary into the child table to ensure it will always exist in the final set of tables\nfor child in children:\nchild._fe_traceability_summary[parent_id] = deepcopy(table)\ndef split(self,\n*fractions: Union[float, int, Iterable[int]],\nseed: Optional[int] = None,\nstratify: Optional[str] = None) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n            seed: The random seed to use when splitting the dataset. Useful if you want consistent splits across\n                multiple experiments. This isn't necessary if you are splitting by data index.\n            stratify: A class key within the dataset with which to stratify the split (to approximately maintain class\n                balance ratios before and after a split). Incompatible with data index splitting.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        Raises:\n            AssertionError: If input arguments are unacceptable.\n        \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n            \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nif method == 'number':\nif stratify is not None:\nsplits = self._get_stratified_splits(n_samples, seed, stratify)\nelse:\nsplits = self._get_fractional_splits(n_samples, seed)\nelse:  # method == 'indices':\nassert stratify is None, \"Stratify may only be specified when splitting by count or fraction, not by index\"\nsplits = fractions\nsplits = self._do_split(splits)\nFEDataset.fix_split_traceabilty(self, splits, fractions, seed, stratify)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\ndef _get_stratified_splits(self, split_counts: List[int], seed: Optional[int],\nstratify: str) -&gt; Sequence[Iterable[int]]:\n\"\"\"Get sequence(s) of indices to split from the current dataset in order to generate new dataset(s).\n        Args:\n            split_counts: How many datapoints to include in each split.\n            seed: What random seed, if any, to use when generating the split(s).\n            stratify: A class key within the dataset with which to stratify the split (to approximately maintain class\n                balance ratios before and after a split).\n        Returns:\n            Which data indices to include in each split of data. len(return[i]) == split_counts[i].\n        \"\"\"\nsplits = []\noriginal_size = self._split_length()\nseed_offset = 0\n# Compute the distribution over the stratify key\ndistribution = defaultdict(list)\nfor idx in range(original_size):\nsample = self[idx]\nkey = sample[stratify]\nif hasattr(key, \"tobytes\"):\nkey = key.tobytes()  # Makes numpy arrays hashable\ndistribution[key].append(idx)\nsupply = {key: len(values) for key, values in distribution.items()}\nsplit_requests = [{key: (n_split * n_tot) / original_size\nfor key, n_tot in supply.items()} for n_split in split_counts]\ndef transfer(source: Dict[Any, int], sink: Dict[Any, int], key: Any, request: int) -&gt; int:\nallowance = min(request, source[key])\nsource[key] -= allowance\nsink[key] += allowance\nreturn allowance\n# Sample splits proportional to the computed distribution\nfor split_request, target in zip(split_requests, split_counts):\nsplit_actual = defaultdict(lambda: 0)\ntotal = 0\n# Step 1: Try to get as close to the target distribution as possible\nfor key, request in split_request.items():\nrequest = 1 if 0 &lt; request &lt; 1 else round(request)  # Always want at least 1 sample from a class\ntotal += transfer(source=supply, sink=split_actual, key=key, request=request)\n# Step 2: Correct the error in the total count at the expense of an optimal distribution\n# |total - target| may be &gt; n_keys due to rounding + supply shortage\nspare_last = True  # If we have drawn too many things, we will try not to reduce any classes beneath 1\nwhile total != target:\nold_total = total\n# Repeatedly add or shave 1 off of everything until we get the correct target number\nfor key, requested in sorted(split_actual.items(), key=lambda x: x[1], reverse=True):\n# reversed to start with the most abundant class\nif total &lt; target:\ntotal += transfer(source=supply, sink=split_actual, key=key, request=1)\nelif total &gt; target and (not spare_last or requested &gt; 1):\ntotal -= transfer(source=split_actual, sink=supply, key=key, request=1)\nif total == target:\nbreak\nif old_total == total:\nassert spare_last is True, \"Cannot stratify the requested split. Please file a bug report.\"\n# We weren't able to modify anything, so no choice but to reduce a 1-sample class to 0-sample\nspare_last = False\n# Step 3: Perform the actual sampling\nsplit_indices = []\nfor key, n_samples in split_actual.items():\n# Dicts have preserved insertion order since python 3.6, so we can increase the seed as we use it\n# to prevent any unintended patterns from emerging while still having consistency over multiple runs.\n# This wouldn't work if the order of encounter of a class can change (like in a generator), but in such\n# cases consistency would be impossible anyways.\nif seed is not None:\nindices = random.Random(seed + seed_offset).sample(distribution[key], n_samples)\nseed_offset += 1  # We'll use a different seed each time\nelse:\nindices = random.sample(distribution[key], n_samples)\nsplit_indices.extend(indices)\n# Sort to allow deterministic seed to work alongside sets\ndistribution[key] = sorted(list(set(distribution[key]) - set(indices)))\nif seed is not None:\nrandom.Random(seed + seed_offset).shuffle(split_indices)\nseed_offset += 1\nelse:\nrandom.shuffle(split_indices)\nsplits.append(split_indices)\nreturn splits\ndef _get_fractional_splits(self, split_counts: List[int], seed: Optional[int]) -&gt; Sequence[Iterable[int]]:\n\"\"\"Get sequence(s) of indices to split from the current dataset in order to generate new dataset(s).\n        Args:\n            split_counts: How many datapoints to include in each split.\n            seed: What random seed, if any, to use when generating the split(s).\n        Returns:\n            Which data indices to include in each split of data. len(return[i]) == split_counts[i].\n        \"\"\"\nsplits = []\noriginal_size = self._split_length()\nint_sum = sum(split_counts)\n# TODO - convert to a linear congruential generator for large datasets?\n# https://stackoverflow.com/questions/9755538/how-do-i-create-a-list-of-random-numbers-without-duplicates\nif seed is not None:\nindices = random.Random(seed).sample(range(original_size), int_sum)\nelse:\nindices = random.sample(range(original_size), int_sum)\nstart = 0\nfor stop in split_counts:\nsplits.append((indices[i] for i in range(start, start + stop)))\nstart += stop\nreturn splits\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        Useful if sub-classes want to split by something other than indices (see SiameseDirDataset for example).\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['FEDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nraise NotImplementedError\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nraise NotImplementedError\ndef __str__(self):\nreturn str(self.summary())\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.fix_split_traceabilty", "title": "<code>fix_split_traceabilty</code>  <code>classmethod</code>", "text": "<p>A method to fix traceability information after invoking the dataset .split() method.</p> <p>Note that the default implementation of the .split() function invokes this already, so this only needs to be invoked if you override the .split() method when defining a subclass (ex. BatchDataset).</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>FEDataset</code> <p>The parent dataset on which .split() was invoked.</p> required <code>children</code> <code>List[FEDataset]</code> <p>The datasets generated by performing the split.</p> required <code>fractions</code> <code>Tuple[Union[float, int, Iterable[int]], ...]</code> <p>The fraction arguments used to generate the children (should be one-to-one with the children).</p> required <code>seed</code> <code>Optional[int]</code> <p>The random seed used to generate the split.</p> required <code>stratify</code> <code>Optional[str]</code> <p>The stratify key used to generate the split.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@classmethod\ndef fix_split_traceabilty(cls,\nparent: 'FEDataset',\nchildren: List['FEDataset'],\nfractions: Tuple[Union[float, int, Iterable[int]], ...],\nseed: Optional[int],\nstratify: Optional[str]) -&gt; None:\n\"\"\"A method to fix traceability information after invoking the dataset .split() method.\n    Note that the default implementation of the .split() function invokes this already, so this only needs to be\n    invoked if you override the .split() method when defining a subclass (ex. BatchDataset).\n    Args:\n        parent: The parent dataset on which .split() was invoked.\n        children: The datasets generated by performing the split.\n        fractions: The fraction arguments used to generate the children (should be one-to-one with the children).\n        seed: The random seed used to generate the split.\n        stratify: The stratify key used to generate the split.\n    \"\"\"\nif hasattr(parent, '_fe_traceability_summary'):\nparent_id = FEID(id(parent))\nfractions = [\nf\"range({frac.start}, {frac.stop}, {frac.step})\" if isinstance(frac, range) else f\"{frac}\"\nfor frac in fractions\n]\nfor child, frac in zip(children, fractions):\n# noinspection PyProtectedMember\ntables = deepcopy(child._fe_traceability_summary)\n# Update the ID if necessary\nchild_id = FEID(id(child))\nif child_id not in tables:\n# The child was created without invoking its __init__ method, so its internal summary will have the\n# wrong id\ntable = tables.pop(parent_id)\ntable.fe_id = child_id\ntables[child_id] = table\nelse:\ntable = tables[child_id]\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent=parent_id, fraction=frac, seed=seed, stratify=stratify)\ntable.fields['split'] = split_summary\nchild._fe_traceability_summary = tables\n# noinspection PyUnresolvedReferences\ntable = parent._fe_traceability_summary.get(parent_id)\nsplit_summary = table.fields.get('split', FeSplitSummary())\nsplit_summary.add_split(parent='self',\nfraction=\", \".join([f\"-{frac}\" for frac in fractions]),\nseed=seed,\nstratify=stratify)\ntable.fields['split'] = split_summary\n# Put the new parent summary into the child table to ensure it will always exist in the final set of tables\nfor child in children:\nchild._fe_traceability_summary[parent_id] = deepcopy(table)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\nds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>*fractions</code> <code>Union[float, int, Iterable[int]]</code> <p>Floating point values will be interpreted as percentages, integers as an absolute number of datapoints, and an iterable of integers as the exact indices of the data that should be removed in order to create the new dataset.</p> <code>()</code> <code>seed</code> <code>Optional[int]</code> <p>The random seed to use when splitting the dataset. Useful if you want consistent splits across multiple experiments. This isn't necessary if you are splitting by data index.</p> <code>None</code> <code>stratify</code> <code>Optional[str]</code> <p>A class key within the dataset with which to stratify the split (to approximately maintain class balance ratios before and after a split). Incompatible with data index splitting.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[FEDataset, List[FEDataset]]</code> <p>One or more new datasets which are created by removing elements from the current dataset. The number of</p> <code>Union[FEDataset, List[FEDataset]]</code> <p>datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided</p> <code>Union[FEDataset, List[FEDataset]]</code> <p>then the return will be a single dataset rather than a list of datasets.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input arguments are unacceptable.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def split(self,\n*fractions: Union[float, int, Iterable[int]],\nseed: Optional[int] = None,\nstratify: Optional[str] = None) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n        seed: The random seed to use when splitting the dataset. Useful if you want consistent splits across\n            multiple experiments. This isn't necessary if you are splitting by data index.\n        stratify: A class key within the dataset with which to stratify the split (to approximately maintain class\n            balance ratios before and after a split). Incompatible with data index splitting.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    Raises:\n        AssertionError: If input arguments are unacceptable.\n    \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n        \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nif method == 'number':\nif stratify is not None:\nsplits = self._get_stratified_splits(n_samples, seed, stratify)\nelse:\nsplits = self._get_fractional_splits(n_samples, seed)\nelse:  # method == 'indices':\nassert stratify is None, \"Stratify may only be specified when splitting by count or fraction, not by index\"\nsplits = fractions\nsplits = self._do_split(splits)\nFEDataset.fix_split_traceabilty(self, splits, fractions, seed, stratify)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset", "title": "<code>InMemoryDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset abstraction to simplify the implementation of datasets which hold their data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[int, Dict[str, Any]]</code> <p>A dictionary like {data_index: {}}. required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>@traceable(blacklist=('data', '_summary'))\nclass InMemoryDataset(FEDataset):\n\"\"\"A dataset abstraction to simplify the implementation of datasets which hold their data in memory.\n    Args:\n        data: A dictionary like {data_index: {&lt;instance dictionary&gt;}}.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]  # Index-based data dictionary\ndef __init__(self, data: Dict[int, Dict[str, Any]]) -&gt; None:\nself.data = data\nself._summary = None\ndef __len__(self) -&gt; int:\nreturn len(self.data)\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        element = data[0]  # {\"x\": &lt;100&gt;}\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        ```\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nif isinstance(index, int):\nreturn self.data[index]\nelse:\nresult = [elem[index] for elem in self.data.values()]\nif isinstance(result[0], np.ndarray):\nreturn np.array(result)\nreturn result\ndef __setitem__(self, key: Union[int, str], value: Union[Dict[str, Any], Sequence[Any]]) -&gt; None:\n\"\"\"Modify data in the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        column = column - np.mean(column)\n        data[\"x\"] = column\n        ```\n        Args:\n            key: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be updated.\n            value: The value to be inserted for the given `key`. Must be a dictionary if `key` is an integer. Otherwise\n                must be a sequence with the same length as the current length of the dataset.\n        Raises:\n            AssertionError: If the `value` is inappropriate given the type of the `key`.\n        \"\"\"\nif isinstance(key, int):\nassert isinstance(value, Dict), \"if setting a value using an integer index, must provide a dictionary\"\nself.data[key] = value\nelse:\nassert len(value) == len(self.data), \\\n                \"input value must be of length {}, but had length {}\".format(len(self.data), len(value))\nfor i in range(len(self.data)):\nself.data[i][key] = value[i]\nself._summary = None\ndef _skip_init(self, data: Dict[int, Dict[str, Any]], **kwargs) -&gt; 'InMemoryDataset':\n\"\"\"A helper method to create new dataset instances without invoking their __init__ methods.\n        Args:\n            data: The data dictionary to be used in the new dataset.\n            **kwargs: Any other member variables to be assigned in the new dataset.\n        Returns:\n            A new dataset based on the given inputs.\n        \"\"\"\nobj = self.__class__.__new__(self.__class__)\nobj.data = data\nfor k, v in kwargs.items():\nobj.__setattr__(k, v)\nobj._summary = None\nreturn obj\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['InMemoryDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New Datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nresults = []\nfor split in splits:\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nresults.append(self._skip_init(data, **{k: v for k, v in self.__dict__.items() if k not in {'data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself._summary = None\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nif self._summary is not None:\nreturn self._summary\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nself._summary = DatasetSummary(num_instances=len(self), keys=key_summary)\nreturn self._summary\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nif self._summary is not None:\nreturn self._summary\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nself._summary = DatasetSummary(num_instances=len(self), keys=key_summary)\nreturn self._summary\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.KeySummary", "title": "<code>KeySummary</code>", "text": "<p>A summary of the dataset attributes corresponding to a particular key.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>num_unique_values</code> <code>Optional[int]</code> <p>The number of unique values corresponding to a particular key (if known).</p> <code>None</code> <code>shape</code> <code>List[Optional[int]]</code> <p>The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is ragged.</p> <code>()</code> <code>dtype</code> <code>str</code> <p>The data type of instances corresponding to the given key.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class KeySummary:\n\"\"\"A summary of the dataset attributes corresponding to a particular key.\n    This class is intentionally not @traceable.\n    Args:\n        num_unique_values: The number of unique values corresponding to a particular key (if known).\n        shape: The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is\n            ragged.\n        dtype: The data type of instances corresponding to the given key.\n    \"\"\"\nnum_unique_values: Optional[int]\nshape: List[Optional[int]]\ndtype: str\ndef __init__(self, dtype: str, num_unique_values: Optional[int] = None, shape: List[Optional[int]] = ()) -&gt; None:\nself.num_unique_values = num_unique_values\nself.shape = shape\nself.dtype = dtype\ndef __repr__(self):\nreturn \"&lt;KeySummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\n</code></pre>"}, {"location": "fastestimator/dataset/dir_dataset.html", "title": "dir_dataset", "text": ""}, {"location": "fastestimator/dataset/dir_dataset.html#fastestimator.fastestimator.dataset.dir_dataset.DirDataset", "title": "<code>DirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> <code>recursive_search</code> <code>bool</code> <p>Whether to search within subdirectories for files.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dir_dataset.py</code> <pre><code>@traceable()\nclass DirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/data.file.\n    Args:\n        root_dir: The path to the directory containing data.\n        data_key: What key to assign to the data values in the data dictionary.\n        file_extension: If provided then only files ending with the file_extension will be included.\n        recursive_search: Whether to search within subdirectories for files.\n    \"\"\"\ndata: Dict[int, Dict[str, str]]\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nfile_extension: Optional[str] = None,\nrecursive_search: bool = True) -&gt; None:\nroot_dir = os.path.normpath(root_dir)\nself.root_dir = root_dir\ndata = list_files(root_dir=root_dir, file_extension=file_extension, recursive_search=recursive_search)\n# Sort the data so that deterministic split will work properly\ndata.sort()\nsuper().__init__({i: {data_key: data[i]} for i in range(len(data))})\n</code></pre>"}, {"location": "fastestimator/dataset/extend_dataset.html", "title": "extend_dataset", "text": ""}, {"location": "fastestimator/dataset/extend_dataset.html#fastestimator.fastestimator.dataset.extend_dataset.ExtendDataset", "title": "<code>ExtendDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> <p>ExtendDataset either extends or contracts the length of provided Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The Original dataset(s) which need expansion or contraction.</p> required <code>spoof_length</code> <code>int</code> <p>Length to which original dataset must be expanded or contracted to. (New desired length)</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\extend_dataset.py</code> <pre><code>@traceable()\nclass ExtendDataset(Dataset):\n\"\"\"ExtendDataset either extends or contracts the length of provided Dataset.\n    Args:\n        dataset: The Original dataset(s) which need expansion or contraction.\n        spoof_length: Length to which original dataset must be expanded or contracted to. (New desired length)\n    \"\"\"\ndef __init__(self, dataset: Dataset, spoof_length: int) -&gt; None:\nself.dataset = dataset\nself.spoof_length = spoof_length\nself._check_input()\nif hasattr(dataset, \"fe_reset_ds\"):\nself.fe_reset_ds = dataset.fe_reset_ds\nif hasattr(dataset, \"fe_batch_indices\"):\nself.fe_batch_indices = dataset.fe_batch_indices\nif hasattr(dataset, \"fe_batch\"):\nself.fe_batch = dataset.fe_batch\ndef __len__(self):\nreturn len(self.dataset)\ndef _check_input(self) -&gt; None:\n\"\"\"Verify that the given input values are valid.\n        Raises:\n            AssertionError: If any of the parameters are found to by unacceptable for a variety of reasons.\n        \"\"\"\nassert isinstance(self.spoof_length, int), \"Only accept positive integer type as spoof_length\"\nassert self.spoof_length &gt; 0, \"Invalid spoof_length. Expand Length cannot be less than or equal to 0\"\nassert not isinstance(self.dataset, ExtendDataset), \"Input Dataset cannot be an ExtendDataset object\"\ndef __getitem__(self, index):\nreturn self.dataset[index]\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html", "title": "generator_dataset", "text": ""}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset", "title": "<code>GeneratorDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset from a generator function.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Generator[Dict[str, Any], int, None]</code> <p>The generator function to invoke in order to get a data sample.</p> required <code>samples_per_epoch</code> <code>int</code> <p>How many samples should be drawn from the generator during an epoch. Note that the generator function will actually be invoked more times than the number specified here due to backend validation routines.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>@traceable(blacklist='_summary')\nclass GeneratorDataset(FEDataset):\n\"\"\"A dataset from a generator function.\n    Args:\n        generator: The generator function to invoke in order to get a data sample.\n        samples_per_epoch: How many samples should be drawn from the generator during an epoch. Note that the generator\n            function will actually be invoked more times than the number specified here due to backend validation\n            routines.\n    \"\"\"\ndef __init__(self, generator: Generator[Dict[str, Any], int, None], samples_per_epoch: int) -&gt; None:\nself.generator = generator\nself.samples_per_epoch = samples_per_epoch\nnext(self.generator)  # Can't send non-none values to a new generator, so need to run a 'warm-up' first\nself._summary = None\ndef __len__(self):\nreturn self.samples_per_epoch\ndef __getitem__(self, index: int):\nreturn self.generator.send(index)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['GeneratorDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nprint(\"FastEstimator-Warn: You probably don't actually want to split a generator dataset\")\nself._summary = None\nresults = []\nfor split in splits:\nif isinstance(split, Sized):\nsize = len(split)\nelse:\n# TODO - make this efficient somehow\nsize = sum(1 for _ in split)\nresults.append(GeneratorDataset(self.generator, size))\nself.samples_per_epoch -= size\nreturn results\ndef _get_stratified_splits(self, split_counts: List[int], seed: Optional[int],\nstratify: str) -&gt; Sequence[Iterable[int]]:\nprint(\"\\033[93m {}\\033[00m\".format(\n\"Warning! GeneratorDataset does not support stratified splits. Falling back to classical split method.\"))\nreturn self._get_fractional_splits(split_counts=split_counts, seed=seed)\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nif self._summary is not None:\nreturn self._summary\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nself._summary = DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\nreturn self._summary\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nif self._summary is not None:\nreturn self._summary\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nself._summary = DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\nreturn self._summary\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html", "title": "labeled_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset", "title": "<code>LabeledDirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>@traceable()\nclass LabeledDirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key: What key to assign to the data values in the data dictionary.\n        label_key: What key to assign to the label values in the data dictionary.\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]\nmapping: Dict[str, Any]\nlabel_key: str\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nlabel_key: str = \"y\",\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None) -&gt; None:\n# Recursively find all the data\nroot_dir = os.path.normpath(root_dir)\ndata = {}\nkeys = deque([\"\"])\nfor _, dirs, entries in os.walk(root_dir):\nkey = keys.popleft()\ndirs = [os.path.join(key, d) for d in dirs]\ndirs.reverse()\nkeys.extendleft(dirs)\nentries = [\nos.path.join(key, e) for e in entries if not e.startswith(\".\") and e.endswith(file_extension or \"\")\n]\nif entries:\ndata[key] = entries\n# Compute label mappings\nself.mapping = label_mapping or {label: idx for idx, label in enumerate(sorted(data.keys()))}\nassert self.mapping.keys() &gt;= data.keys(), \\\n            \"Mapping provided to LabeledDirDataset is missing key(s): {}\".format(\ndata.keys() - self.mapping.keys())\n# Store the data by index\nparsed_data = {}\nidx = 0\nfor key, values in data.items():\nlabel = self.mapping[key]\n# Sort the values so that deterministic splitting works\nvalues.sort()\nfor value in values:\nparsed_data[idx] = {data_key: os.path.join(root_dir, value), label_key: label}\nidx += 1\nself.label_key = label_key\nsuper().__init__(parsed_data)\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/numpy_dataset.html", "title": "numpy_dataset", "text": ""}, {"location": "fastestimator/dataset/numpy_dataset.html#fastestimator.fastestimator.dataset.numpy_dataset.NumpyDataset", "title": "<code>NumpyDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset constructed from a dictionary of Numpy data or list of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Union[np.ndarray, List]]</code> <p>A dictionary of data like {\"key1\": , \"key2\": [list]}. required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the Numpy arrays or lists have differing numbers of elements.</p> <code>ValueError</code> <p>If any dictionary value is not instance of Numpy array or list.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\numpy_dataset.py</code> <pre><code>@traceable()\nclass NumpyDataset(InMemoryDataset):\n\"\"\"A dataset constructed from a dictionary of Numpy data or list of data.\n    Args:\n        data: A dictionary of data like {\"key1\": &lt;numpy array&gt;, \"key2\": [list]}.\n    Raises:\n        AssertionError: If any of the Numpy arrays or lists have differing numbers of elements.\n        ValueError: If any dictionary value is not instance of Numpy array or list.\n    \"\"\"\ndef __init__(self, data: Dict[str, Union[np.ndarray, List]]) -&gt; None:\nsize = None\nfor val in data.values():\nif isinstance(val, np.ndarray):\ncurrent_size = val.shape[0]\nelif isinstance(val, list):\ncurrent_size = len(val)\nelse:\nraise ValueError(\"Please ensure you are passing numpy array or list in the data dictionary.\")\nif size is not None:\nassert size == current_size, \"All data arrays must have the same number of elements\"\nelse:\nsize = current_size\nsuper().__init__({i: {k: v[i] for k, v in data.items()} for i in range(size)})\n</code></pre>"}, {"location": "fastestimator/dataset/op_dataset.html", "title": "op_dataset", "text": ""}, {"location": "fastestimator/dataset/op_dataset.html#fastestimator.fastestimator.dataset.op_dataset.OpDataset", "title": "<code>OpDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> <p>A wrapper for datasets which allows operators to be applied to them in a pipeline.</p> <p>This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets within an Op dataset as needed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The base dataset to wrap.</p> required <code>ops</code> <code>List[NumpyOp]</code> <p>A list of ops to be applied after the base <code>dataset</code> <code>__getitem__</code> is invoked.</p> required <code>mode</code> <code>str</code> <p>What mode the system is currently running in ('train', 'eval', 'test', or 'infer').</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys can be produced from pipeline. If None or empty, all keys will be considered.</p> <code>None</code> <code>deep_remainder</code> <code>bool</code> <p>Whether data which is not modified by Ops should be deep copied or not. This argument is used to help with RAM management, but end users can almost certainly ignore it.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\dataset\\op_dataset.py</code> <pre><code>@traceable(blacklist='lock')\nclass OpDataset(Dataset):\n\"\"\"A wrapper for datasets which allows operators to be applied to them in a pipeline.\n    This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets\n    within an Op dataset as needed.\n    Args:\n        dataset: The base dataset to wrap.\n        ops: A list of ops to be applied after the base `dataset` `__getitem__` is invoked.\n        mode: What mode the system is currently running in ('train', 'eval', 'test', or 'infer').\n        output_keys: What keys can be produced from pipeline. If None or empty, all keys will be considered.\n        deep_remainder: Whether data which is not modified by Ops should be deep copied or not. This argument is used to\n            help with RAM management, but end users can almost certainly ignore it.\n    \"\"\"\nto_warn: Set[str] = set()\nwarned: Set[str] = set()\ndef __init__(self,\ndataset: Dataset,\nops: List[NumpyOp],\nmode: str,\noutput_keys: Optional[Set[str]] = None,\ndeep_remainder: bool = True) -&gt; None:\n# Track whether this dataset returns batches or not (useful for pipeline and traceability)\nif not hasattr(dataset, \"fe_batch\"):\nsample_item = dataset[0]\ndataset.fe_batch = len(sample_item) if isinstance(sample_item, list) else 0\nself.dataset = dataset\nself.fe_batch = dataset.fe_batch\nif hasattr(dataset, \"fe_reset_ds\"):\nself.fe_reset_ds = dataset.fe_reset_ds\nif hasattr(dataset, \"fe_batch_indices\"):\nself.fe_batch_indices = dataset.fe_batch_indices\nself.ops = ops\nself.mode = mode\nself.output_keys = output_keys\nself.deep_remainder = deep_remainder\nself.lock = Lock()\ndef __getitem__(self, index: int) -&gt; Union[Mapping[str, Any], List[Mapping[str, Any]], FilteredData]:\n\"\"\"Fetch a data instance at a specified index, and apply transformations to it.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index, with transformations applied OR an indication that this index\n            should be thrown out.\n        \"\"\"\nitem = self.dataset[index]\nif isinstance(item, list):\n# BatchDataset may randomly sample the same elements multiple times, so need to avoid reprocessing\nunique_samples = {}  # id: idx\nresults = []\nfor idx, data in enumerate(item):\ndata_id = id(data)\nif data_id not in unique_samples:\ndata = _DelayedDeepDict(data)\nfilter_data = forward_numpyop(self.ops, data, {'mode': self.mode})\nif filter_data:\nresults.append(filter_data)\nelse:\ndata.finalize(retain=self.output_keys, deep_remainder=self.deep_remainder)\nresults.append(data)\nif data.warn:\nself.to_warn |= (data.to_warn - self.warned)\nunique_samples[data_id] = idx\nelse:\nresults.append(results[unique_samples[data_id]])\nelse:\nresults = _DelayedDeepDict(item)\nfilter_data = forward_numpyop(self.ops, results, {'mode': self.mode})\nif filter_data:\nreturn filter_data\nresults.finalize(retain=self.output_keys, deep_remainder=self.deep_remainder)\nif results.warn:\nself.to_warn |= (results.to_warn - self.warned)\nif self.to_warn and self.lock.acquire(block=False):\nself.warned.update(self.to_warn)\nprint(\"FastEstimator-Warn: The following key(s) are being pruned since they are unused outside of the \"\n\"Pipeline. To prevent this, you can declare the key(s) as inputs to Traces or TensorOps: \"\nf\"{', '.join(self.to_warn)}\")\nself.to_warn.clear()\n# We intentionally never release the lock so that during multi-threading only 1 message can be printed\nreturn results\ndef __len__(self):\nreturn len(self.dataset)\n</code></pre>"}, {"location": "fastestimator/dataset/pickle_dataset.html", "title": "pickle_dataset", "text": ""}, {"location": "fastestimator/dataset/pickle_dataset.html#fastestimator.fastestimator.dataset.pickle_dataset.PickleDataset", "title": "<code>PickleDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a pickle file.</p> <p>PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the pickle file.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\pickle_dataset.py</code> <pre><code>@traceable()\nclass PickleDataset(InMemoryDataset):\n\"\"\"A dataset from a pickle file.\n    PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed\n    using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed\n    into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the pickle file.\n    \"\"\"\ndef __init__(self, file_path: str) -&gt; None:\ndf = pd.read_pickle(file_path)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html", "title": "siamese_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset", "title": "<code>SiameseDirDataset</code>", "text": "<p>         Bases: <code>LabeledDirDataset</code></p> <p>A dataset which returns pairs of data.</p> <p>This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs, where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index rather than by data instance index.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key_left</code> <code>str</code> <p>What key to assign to the first data element in the pair.</p> <code>'x_a'</code> <code>data_key_right</code> <code>str</code> <p>What key to assign to the second data element in the pair.</p> <code>'x_b'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>percent_matching_data</code> <code>float</code> <p>What percentage of the time should data be paired by class (label value = 1).</p> <code>0.5</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>@traceable(blacklist=('data', 'class_data', '_summary'))\nclass SiameseDirDataset(LabeledDirDataset):\n\"\"\"A dataset which returns pairs of data.\n    This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs,\n    where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as\n    the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero\n    or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index\n    rather than by data instance index.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key_left: What key to assign to the first data element in the pair.\n        data_key_right: What key to assign to the second data element in the pair.\n        label_key: What key to assign to the label values in the data dictionary.\n        percent_matching_data: What percentage of the time should data be paired by class (label value = 1).\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\nclass_data: Dict[Any, Set[int]]\ndef __init__(self,\nroot_dir: str,\ndata_key_left: str = \"x_a\",\ndata_key_right: str = \"x_b\",\nlabel_key: str = \"y\",\npercent_matching_data: float = 0.5,\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None):\nsuper().__init__(root_dir, data_key_left, label_key, label_mapping, file_extension)\nself.class_data = self._data_to_class(self.data, label_key)\nself.percent_matching_data = percent_matching_data\nself.data_key_left = data_key_left\nself.data_key_right = data_key_right\nself.label_key = label_key\n@staticmethod\ndef _data_to_class(data: Dict[int, Dict[str, Any]], label_key: str) -&gt; Dict[Any, Set[int]]:\n\"\"\"A helper method to build a mapping from classes to their corresponding data indices.\n        Args:\n            data: A data dictionary like {&lt;index&gt;: {\"data_key\": &lt;data value&gt;}}.\n            label_key: Which key inside `data` corresponds to the label value for a given index entry.\n        Returns:\n            A mapping like {\"label1\": {&lt;indices with label1&gt;}, \"label2\": {&lt;indices with label2&gt;}}.\n        \"\"\"\nclass_data = {}\nfor idx, elem in data.items():\nclass_data.setdefault(elem[label_key], set()).add(idx)\nreturn class_data\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        In this case, splits are computed based on the number of classes rather than the number of instances per class.\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self.class_data)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['SiameseDirDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which classes to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing classes at the indices specified by `splits` from the current dataset.\n        \"\"\"\n# Splits in this context refer to class indices rather than the typical data indices\nresults = []\nfor split in splits:\n# Convert class indices to data indices\nint_class_keys = list(sorted(self.class_data.keys()))\nsplit = [item for i in split for item in self.class_data[int_class_keys[i]]]\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nclass_data = self._data_to_class(data, self.label_key)\nresults.append(\nself._skip_init(data,\nclass_data=class_data,\n**{k: v\nfor k, v in self.__dict__.items() if k not in {'data', 'class_data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself.class_data = self._data_to_class(self.data, self.label_key)\n# The summary function is being cached by a base class, so reset our cache here\nself._summary = None\nreturn results\ndef _get_stratified_splits(self, split_counts: List[int], seed: Optional[int],\nstratify: str) -&gt; Sequence[Iterable[int]]:\nprint(\"\\033[93m {}\\033[00m\".format(\n\"Warning! SiameseDirDataset does not support stratified splits. Falling back to classical split method.\"))\nreturn self._get_fractional_splits(split_counts=split_counts, seed=seed)\ndef __getitem__(self, index: int):\n\"\"\"Extract items from the dataset based on the given `batch_idx`.\n        Args:\n            index: Which data instance to use as the 'left' element.\n        Returns:\n            A datapoint for the given index.\n        \"\"\"\nbase_item = deepcopy(self.data[index])\nif np.random.uniform(0, 1) &lt; self.percent_matching_data:\n# Generate matching data\nclazz_items = self.class_data[base_item[self.label_key]]\nother = np.random.choice(list(clazz_items - {index}))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 1\nelse:\n# Generate non-matching data\nother_classes = self.class_data.keys() - {base_item[self.label_key]}\nother_class = np.random.choice(list(other_classes))\nother = np.random.choice(list(self.class_data[other_class]))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 0\nreturn base_item\ndef one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n        The similarity should be highest between the index 0 elements of the arrays.\n        Args:\n            n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n        Returns:\n            ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n            [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n        \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n            \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.one_shot_trial", "title": "<code>one_shot_trial</code>", "text": "<p>Generate one-shot trial data.</p> <p>The similarity should be highest between the index 0 elements of the arrays.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],</p> <code>List[str]</code> <p>[class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n    The similarity should be highest between the index 0 elements of the arrays.\n    Args:\n        n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n    Returns:\n        ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n        [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n    \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n        \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/data/breast_cancer.html", "title": "breast_cancer", "text": ""}, {"location": "fastestimator/dataset/data/breast_cancer.html#fastestimator.fastestimator.dataset.data.breast_cancer.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.</p> <p>For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.</p> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\breast_cancer.py</code> <pre><code>def load_data() -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.\n    For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x, y) = load_breast_cancer(return_X_y=True)\nx_train, x_eval, y_train, y_eval = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train, x_eval = np.float32(x_train), np.float32(x_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifair10.html", "title": "cifair10", "text": ""}, {"location": "fastestimator/dataset/data/cifair10.html#fastestimator.fastestimator.dataset.data.cifair10.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the ciFAIR10 dataset.</p> <p>This is the cifar10 dataset but with test set duplicates removed and replaced. See https://arxiv.org/pdf/1902.00423.pdf or https://cvjena.github.io/cifair/ for details. Cite the paper if you use the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifair10.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the ciFAIR10 dataset.\n    This is the cifar10 dataset but with test set duplicates removed and replaced. See\n    https://arxiv.org/pdf/1902.00423.pdf or https://cvjena.github.io/cifair/ for details. Cite the paper if you use the\n    dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\ndirname = 'ciFAIR-10'\narchive_name = 'ciFAIR-10.zip'\norigin = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-10.zip'\nmd5_hash = 'ca08fd390f0839693d3fc45c4e49585f'\npath = get_file(archive_name, origin=origin, file_hash=md5_hash, hash_algorithm='md5', extract=True,\narchive_format='zip')\npath = os.path.join(os.path.dirname(path), dirname)\nnum_train_samples = 50000\nx_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\ny_train = np.empty((num_train_samples,), dtype='uint8')\nfor i in range(1, 6):\nfpath = os.path.join(path, f'data_batch_{i}')\n(x_train[(i - 1) * 10000:i * 10000, :, :, :],\ny_train[(i - 1) * 10000:i * 10000]) = _load_batch(fpath)\nfpath = os.path.join(path, 'test_batch')\nx_test, y_test = _load_batch(fpath)\ny_train = np.reshape(y_train, (len(y_train), 1))\ny_test = np.reshape(y_test, (len(y_test), 1))\nx_train = x_train.transpose((0, 2, 3, 1))\nx_test = x_test.transpose((0, 2, 3, 1))\nx_test = x_test.astype(x_train.dtype)\ny_test = y_test.astype(y_train.dtype)\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\ntest_data = NumpyDataset({image_key: x_test, label_key: y_test})\nreturn train_data, test_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifair100.html", "title": "cifair100", "text": ""}, {"location": "fastestimator/dataset/data/cifair100.html#fastestimator.fastestimator.dataset.data.cifair100.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the ciFAIR100 dataset.</p> <p>This is the cifar100 dataset but with test set duplicates removed and replaced. See https://arxiv.org/pdf/1902.00423.pdf or https://cvjena.github.io/cifair/ for details. Cite the paper if you use the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <code>label_mode</code> <code>str</code> <p>Either \"fine\" for 100 classes or \"coarse\" for 20 classes.</p> <code>'fine'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, test_data)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the label_mode is invalid.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifair100.py</code> <pre><code>def load_data(image_key: str = \"x\",\nlabel_key: str = \"y\",\nlabel_mode: str = \"fine\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the ciFAIR100 dataset.\n    This is the cifar100 dataset but with test set duplicates removed and replaced. See\n    https://arxiv.org/pdf/1902.00423.pdf or https://cvjena.github.io/cifair/ for details. Cite the paper if you use the\n    dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n        label_mode: Either \"fine\" for 100 classes or \"coarse\" for 20 classes.\n    Returns:\n        (train_data, test_data)\n    Raises:\n        ValueError: If the label_mode is invalid.\n    \"\"\"\nif label_mode not in ['fine', 'coarse']:\nraise ValueError(\"label_mode must be one of either 'fine' or 'coarse'.\")\ndirname = 'ciFAIR-100'\narchive_name = 'ciFAIR-100.zip'\norigin = 'https://github.com/cvjena/cifair/releases/download/v1.0/ciFAIR-100.zip'\nmd5_hash = 'ddc236ab4b12eeb8b20b952614861a33'\npath = get_file(archive_name, origin=origin, file_hash=md5_hash, hash_algorithm='md5', extract=True,\narchive_format='zip')\npath = os.path.join(os.path.dirname(path), dirname)\nfpath = os.path.join(path, 'train')\nx_train, y_train = _load_batch(fpath, label_key=label_mode + '_labels')\nfpath = os.path.join(path, 'test')\nx_test, y_test = _load_batch(fpath, label_key=label_mode + '_labels')\ny_train = np.reshape(y_train, (len(y_train), 1))\ny_test = np.reshape(y_test, (len(y_test), 1))\nx_train = x_train.transpose((0, 2, 3, 1))\nx_test = x_test.transpose((0, 2, 3, 1))\nx_test = x_test.astype(x_train.dtype)\ny_test = y_test.astype(y_train.dtype)\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\ntest_data = NumpyDataset({image_key: x_test, label_key: y_test})\nreturn train_data, test_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifar10.html", "title": "cifar10", "text": ""}, {"location": "fastestimator/dataset/data/cifar10.html#fastestimator.fastestimator.dataset.data.cifar10.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the CIFAR10 dataset.</p> <p>Please consider using the ciFAIR10 dataset instead. CIFAR10 contains duplicates between its train and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifar10.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the CIFAR10 dataset.\n    Please consider using the ciFAIR10 dataset instead. CIFAR10 contains duplicates between its train and test sets.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nprint(\"\\033[93m {}\\033[00m\".format(\"FastEstimator-Warn: Consider using the ciFAIR10 dataset instead.\"))\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.cifar10.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifar100.html", "title": "cifar100", "text": ""}, {"location": "fastestimator/dataset/data/cifar100.html#fastestimator.fastestimator.dataset.data.cifar100.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the CIFAR100 dataset.</p> <p>Please consider using the ciFAIR100 dataset instead. CIFAR100 contains duplicates between its train and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <code>label_mode</code> <code>str</code> <p>Either \"fine\" for 100 classes or \"coarse\" for 20 classes.</p> <code>'fine'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the label_mode is invalid.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifar100.py</code> <pre><code>def load_data(image_key: str = \"x\",\nlabel_key: str = \"y\",\nlabel_mode: str = \"fine\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the CIFAR100 dataset.\n    Please consider using the ciFAIR100 dataset instead. CIFAR100 contains duplicates between its train and test sets.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n        label_mode: Either \"fine\" for 100 classes or \"coarse\" for 20 classes.\n    Returns:\n        (train_data, eval_data)\n    Raises:\n        ValueError: If the label_mode is invalid.\n    \"\"\"\nprint(\"\\033[93m {}\\033[00m\".format(\"FastEstimator-Warn: Consider using the ciFAIR100 dataset instead.\"))\nif label_mode not in ['fine', 'coarse']:\nraise ValueError(\"label_mode must be one of either 'fine' or 'coarse'.\")\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.cifar100.load_data(label_mode=label_mode)\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cub200.html", "title": "cub200", "text": ""}, {"location": "fastestimator/dataset/data/cub200.html#fastestimator.fastestimator.dataset.data.cub200.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.</p> <p>Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local     storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cub200.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.\n    Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local\n        storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'CUB200')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'CUB200')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, 'cub200.csv')\nimage_compressed_path = os.path.join(root_dir, 'images.tgz')\nannotation_compressed_path = os.path.join(root_dir, 'annotations.tgz')\nimage_extracted_path = os.path.join(root_dir, 'images')\nannotation_extracted_path = os.path.join(root_dir, 'annotations-mat')\nif not (os.path.exists(image_extracted_path) and os.path.exists(annotation_extracted_path)):\n# download\nif not (os.path.exists(image_compressed_path) and os.path.exists(annotation_compressed_path)):\nprint(\"Downloading data to {}\".format(root_dir))\n_download_file_from_google_drive('1GDr1OkoXdhaXWGA8S3MAq3a522Tak-nx', image_compressed_path)\n_download_file_from_google_drive('16NsbTpMs5L6hT4hUJAmpW2u7wH326WTR', annotation_compressed_path)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(image_compressed_path) as img_tar:\nimg_tar.extractall(root_dir)\nwith tarfile.open(annotation_compressed_path) as anno_tar:\nanno_tar.extractall(root_dir)\n# glob and generate csv\nif not os.path.exists(csv_path):\nimage_folder = os.path.join(root_dir, \"images\")\nclass_names = os.listdir(image_folder)\nlabel_map = {}\nimages = []\nlabels = []\nidx = 0\nfor class_name in class_names:\nif not class_name.startswith(\"._\"):\nimage_folder_class = os.path.join(image_folder, class_name)\nlabel_map[class_name] = idx\nidx += 1\nimage_names = os.listdir(image_folder_class)\nfor image_name in image_names:\nif not image_name.startswith(\"._\"):\nimages.append(os.path.join(image_folder_class, image_name))\nlabels.append(label_map[class_name])\nzipped_list = list(zip(images, labels))\nrandom.shuffle(zipped_list)\ndf = pd.DataFrame(zipped_list, columns=[\"image\", \"label\"])\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['annotation'] = df['image'].str.replace('images', 'annotations-mat').str.replace('jpg', 'mat')\ndf.to_csv(csv_path, index=False)\nprint(\"Data summary is saved at {}\".format(csv_path))\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/food101.html", "title": "food101", "text": ""}, {"location": "fastestimator/dataset/data/food101.html#fastestimator.fastestimator.dataset.data.food101.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Food-101 dataset.</p> <p>Food-101 dataset is a collection of images from 101 food categories. Sourced from http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[CSVDataset, CSVDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\food101.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[CSVDataset, CSVDataset]:\n\"\"\"Load and return the Food-101 dataset.\n    Food-101 dataset is a collection of images from 101 food categories.\n    Sourced from http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Food_101')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Food_101')\nos.makedirs(root_dir, exist_ok=True)\nimage_compressed_path = os.path.join(root_dir, 'food-101.tar.gz')\nimage_extracted_path = os.path.join(root_dir, 'food-101')\ntrain_csv_path = os.path.join(root_dir, 'train.csv')\ntest_csv_path = os.path.join(root_dir, 'test.csv')\nif not os.path.exists(image_extracted_path):\n# download\nif not os.path.exists(image_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(image_compressed_path) as img_tar:\nimg_tar.extractall(root_dir)\nlabels = open(os.path.join(root_dir, \"food-101/meta/classes.txt\"), \"r\").read().split()\nlabel_dict = {labels[i]: i for i in range(len(labels))}\nif not os.path.exists(train_csv_path):\ntrain_images = open(os.path.join(root_dir, \"food-101/meta/train.txt\"), \"r\").read().split()\nrandom.shuffle(train_images)\n_create_csv(train_images, label_dict, train_csv_path)\nif not os.path.exists(test_csv_path):\ntest_images = open(os.path.join(root_dir, \"food-101/meta/test.txt\"), \"r\").read().split()\nrandom.shuffle(test_images)\n_create_csv(test_images, label_dict, test_csv_path)\nreturn CSVDataset(train_csv_path), CSVDataset(test_csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/horse2zebra.html", "title": "horse2zebra", "text": ""}, {"location": "fastestimator/dataset/data/horse2zebra.html#fastestimator.fastestimator.dataset.data.horse2zebra.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the horse2zebra dataset.</p> <p>Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will     download the data to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The desired batch size.</p> required <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[BatchDataset, BatchDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\horse2zebra.py</code> <pre><code>def load_data(batch_size: int, root_dir: Optional[str] = None) -&gt; Tuple[BatchDataset, BatchDataset]:\n\"\"\"Load and return the horse2zebra dataset.\n    Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will\n        download the data to local storage if the data has not been previously downloaded.\n    Args:\n        batch_size: The desired batch size.\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'horse2zebra')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'horse2zebra')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'horse2zebra.zip')\ndata_folder_path = os.path.join(root_dir, 'images')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\nzip_file.extractall(root_dir)\nos.rename(os.path.join(root_dir, 'horse2zebra'), data_folder_path)\ntest_a = DirDataset(root_dir=os.path.join(data_folder_path, 'testA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntest_b = DirDataset(root_dir=os.path.join(data_folder_path, 'testB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_a = DirDataset(root_dir=os.path.join(data_folder_path, 'trainA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_b = DirDataset(root_dir=os.path.join(data_folder_path, 'trainB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\noutputs = (BatchDataset(datasets=[train_a, train_b], num_samples=[batch_size, batch_size]),\nBatchDataset(datasets=[test_a, test_b], num_samples=[batch_size, batch_size]))\nreturn outputs\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html", "title": "imdb_review", "text": ""}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the IMDB Movie review dataset.</p> <p>This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).</p> <p>Parameters:</p> Name Type Description Default <code>max_len</code> <code>int</code> <p>Maximum desired length of an input sequence.</p> required <code>vocab_size</code> <code>int</code> <p>Vocabulary size to learn word embeddings.</p> required <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def load_data(max_len: int, vocab_size: int) -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the IMDB Movie review dataset.\n    This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).\n    Args:\n        max_len: Maximum desired length of an input sequence.\n        vocab_size: Vocabulary size to learn word embeddings.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.imdb.load_data(maxlen=max_len, num_words=vocab_size)\n# pad the sequences to max length\nx_train = np.array([pad(x, max_len, 0) for x in x_train])\nx_eval = np.array([pad(x, max_len, 0) for x in x_eval])\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.pad", "title": "<code>pad</code>", "text": "<p>Pad an input_list to a given size.</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[int]</code> <p>The list to be padded.</p> required <code>padding_size</code> <code>int</code> <p>The desired length of the returned list.</p> required <code>padding_value</code> <code>int</code> <p>The value to be inserted for padding.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p><code>input_list</code> with <code>padding_value</code>s appended until the <code>padding_size</code> is reached.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def pad(input_list: List[int], padding_size: int, padding_value: int) -&gt; List[int]:\n\"\"\"Pad an input_list to a given size.\n    Args:\n        input_list: The list to be padded.\n        padding_size: The desired length of the returned list.\n        padding_value: The value to be inserted for padding.\n    Returns:\n        `input_list` with `padding_value`s appended until the `padding_size` is reached.\n    \"\"\"\nreturn input_list + [padding_value] * abs((len(input_list) - padding_size))\n</code></pre>"}, {"location": "fastestimator/dataset/data/mendeley.html", "title": "mendeley", "text": ""}, {"location": "fastestimator/dataset/data/mendeley.html#fastestimator.fastestimator.dataset.data.mendeley.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Mendeley dataset.</p> <p>Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2</p> <p>CC BY 4.0 licence: https://creativecommons.org/licenses/by/4.0/</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mendeley.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the Mendeley dataset.\n    Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray\n    Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2\n    CC BY 4.0 licence:\n    https://creativecommons.org/licenses/by/4.0/\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nurl = 'https://data.mendeley.com/public-files/datasets/rscbjbr9sj/files/f12eaf6d-6023-432f-acc9-80c9d7393433/' \\\n          'file_downloaded'\nuser_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/' \\\n                 '70.0.3538.77 Safari/537.36'\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Mendeley')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Mendeley')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'ChestXRay2017.zip')\ndata_folder_path = os.path.join(root_dir, 'chest_xray')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nstream = requests.get(url, stream=True, headers={'User-Agent':user_agent})  # python wget does not work\ntotal_size = int(stream.headers.get('content-length', 0))\nblock_size = int(1e6)  # 1 MB\nprogress = tqdm(total=total_size, unit='B', unit_scale=True)\nwith open(data_compressed_path, 'wb') as outfile:\nfor data in stream.iter_content(block_size):\nprogress.update(len(data))\noutfile.write(data)\nprogress.close()\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"chest_xray/\"), zip_file.namelist()))\nlabel_mapping = {'NORMAL': 0, 'PNEUMONIA': 1}\nreturn LabeledDirDataset(os.path.join(data_folder_path, \"train\"), label_mapping=label_mapping,\nfile_extension=\".jpeg\"), LabeledDirDataset(os.path.join(data_folder_path, \"test\"),\nlabel_mapping=label_mapping,\nfile_extension=\".jpeg\")\n</code></pre>"}, {"location": "fastestimator/dataset/data/mitmovie_ner.html", "title": "mitmovie_ner", "text": ""}, {"location": "fastestimator/dataset/data/mitmovie_ner.html#fastestimator.fastestimator.dataset.data.mitmovie_ner.get_sentences_and_labels", "title": "<code>get_sentences_and_labels</code>", "text": "<p>Combines tokens into sentences and create vocab set for train data and labels.</p> <p>For simplicity tokens with 'O' entity are omitted.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the downloaded dataset file.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[List[str]], Set[str], Set[str]]</code> <p>(sentences, labels, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mitmovie_ner.py</code> <pre><code>def get_sentences_and_labels(path: str) -&gt; Tuple[List[str], List[List[str]], Set[str], Set[str]]:\n\"\"\"Combines tokens into sentences and create vocab set for train data and labels.\n    For simplicity tokens with 'O' entity are omitted.\n    Args:\n        path: Path to the downloaded dataset file.\n    Returns:\n        (sentences, labels, train_vocab, label_vocab)\n    \"\"\"\nwords, tags = [], []\nword_vocab, label_vocab = set(), set()\nsentences, labels = [], []\ndata = open(path)\nfor line in data:\nif line != '\\n':\nline = line.split()\nwords.append(line[1])\ntags.append(line[0])\nword_vocab.add(line[1])\nlabel_vocab.add(line[0])\nelse:\nsentences.append(\" \".join([s for s in words]))\nlabels.append([t for t in tags])\nwords.clear()\ntags.clear()\nsentences = list(filter(None, sentences))\nlabels = list(filter(None, labels))\nreturn sentences, labels, word_vocab, label_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/mitmovie_ner.html#fastestimator.fastestimator.dataset.data.mitmovie_ner.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the MIT Movie dataset.</p> <p>MIT Movies dataset is a semantically tagged training and test corpus in BIO format. The sentence is encoded as one token per line with information provided in tab-seprated columns. Sourced from https://groups.csail.mit.edu/sls/downloads/movie/</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]</code> <p>(train_data, eval_data, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mitmovie_ner.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]:\n\"\"\"Load and return the MIT Movie dataset.\n    MIT Movies dataset is a semantically tagged training and test corpus in BIO format. The sentence is encoded as one\n    token per line with information provided in tab-seprated columns.\n    Sourced from https://groups.csail.mit.edu/sls/downloads/movie/\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data, train_vocab, label_vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'MITMovie')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'MITMovie')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data_path = os.path.join(root_dir, 'engtrain.bio')\ntest_data_path = os.path.join(root_dir, 'engtest.bio')\nfiles = [(train_data_path, 'https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio'),\n(test_data_path, 'https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio')]\nfor data_path, download_link in files:\nif not os.path.exists(data_path):\n# Download\nprint(\"Downloading data: {}\".format(data_path))\nwget.download(download_link, data_path, bar=bar_custom)\nx_train, y_train, x_vocab, y_vocab = get_sentences_and_labels(train_data_path)\nx_eval, y_eval, x_eval_vocab, y_eval_vocab = get_sentences_and_labels(test_data_path)\nx_vocab |= x_eval_vocab\ny_vocab |= y_eval_vocab\nx_train = np.array(x_train)\nx_eval = np.array(x_eval)\ny_train = np.array(y_train)\ny_eval = np.array(y_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data, x_vocab, y_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/mnist.html", "title": "mnist", "text": ""}, {"location": "fastestimator/dataset/data/mnist.html#fastestimator.fastestimator.dataset.data.mnist.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mnist.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the MNIST dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/montgomery.html", "title": "montgomery", "text": ""}, {"location": "fastestimator/dataset/data/montgomery.html#fastestimator.fastestimator.dataset.data.montgomery.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the montgomery dataset.</p> <p>Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data     to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\montgomery.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the montgomery dataset.\n    Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data\n        to local storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Montgomery')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Montgomery')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, \"montgomery.csv\")\ndata_compressed_path = os.path.join(root_dir, 'NLM-MontgomeryCXRSet.zip')\nextract_folder_path = os.path.join(root_dir, 'MontgomerySet')\nif not os.path.exists(extract_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"MontgomerySet/\"), zip_file.namelist()))\n# glob and generate csv\nif not os.path.exists(csv_path):\nimg_list = glob(os.path.join(extract_folder_path, 'CXR_png', '*.png'))\ndf = pd.DataFrame(data={'image': img_list})\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['mask_left'] = df['image'].apply(lambda x: x.replace('CXR_png', os.path.join('ManualMask', 'leftMask')))\ndf['mask_right'] = df['image'].apply(lambda x: x.replace('CXR_png', os.path.join('ManualMask', 'rightMask')))\ndf.to_csv(csv_path, index=False)\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html", "title": "mscoco", "text": ""}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.MSCOCODataset", "title": "<code>MSCOCODataset</code>", "text": "<p>         Bases: <code>DirDataset</code></p> <p>A specialized DirDataset to handle MSCOCO data.</p> <p>This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>The path the directory containing MSOCO images.</p> required <code>annotation_file</code> <code>str</code> <p>The path to the file containing annotation data.</p> required <code>caption_file</code> <code>str</code> <p>The path the file containing caption data.</p> required <code>include_bboxes</code> <code>bool</code> <p>Whether images should be paired with their associated bounding boxes. If true, images without bounding boxes will be ignored and other images may be oversampled in order to take their place.</p> <code>True</code> <code>include_masks</code> <code>bool</code> <p>Whether images should be paired with their associated masks. If true, images without masks will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>include_captions</code> <code>bool</code> <p>Whether images should be paired with their associated captions. If true, images without captions will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>min_bbox_area</code> <p>Bounding boxes with a total area less than <code>min_bbox_area</code> will be discarded.</p> <code>1.0</code> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>@traceable()\nclass MSCOCODataset(DirDataset):\n\"\"\"A specialized DirDataset to handle MSCOCO data.\n    This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.\n    Args:\n        image_dir: The path the directory containing MSOCO images.\n        annotation_file: The path to the file containing annotation data.\n        caption_file: The path the file containing caption data.\n        include_bboxes: Whether images should be paired with their associated bounding boxes. If true, images without\n            bounding boxes will be ignored and other images may be oversampled in order to take their place.\n        include_masks: Whether images should be paired with their associated masks. If true, images without masks will\n            be ignored and other images may be oversampled in order to take their place.\n        include_captions: Whether images should be paired with their associated captions. If true, images without\n            captions will be ignored and other images may be oversampled in order to take their place.\n        min_bbox_area: Bounding boxes with a total area less than `min_bbox_area` will be discarded.\n    \"\"\"\ninstances: Optional[COCO]\ncaptions: Optional[COCO]\ndef __init__(self,\nimage_dir: str,\nannotation_file: str,\ncaption_file: str,\ninclude_bboxes: bool = True,\ninclude_masks: bool = False,\ninclude_captions: bool = False,\nmin_bbox_area=1.0) -&gt; None:\nsuper().__init__(root_dir=image_dir, data_key=\"image\", recursive_search=False)\nif include_masks:\nassert include_bboxes, \"must include bboxes with mask data\"\nself.include_bboxes = include_bboxes\nself.include_masks = include_masks\nself.min_bbox_area = min_bbox_area\nwith Suppressor():\nself.instances = COCO(annotation_file)\nself.captions = COCO(caption_file) if include_captions else None\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned. If bboxes, masks, or captions are required and the data\n                at the desired index does not have one or more of these features, then data from a random index which\n                does have all necessary features will be fetched instead.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nhas_data = False\nresponse = {}\nwhile not has_data:\nhas_box, has_mask, has_caption = True, True, True\nresponse = self._get_single_item(index)\nif isinstance(index, str):\nreturn response\nif self.include_bboxes and not response[\"bbox\"]:\nhas_box = False\nif self.include_masks and not response[\"mask\"]:\nhas_mask = False\nif self.captions and not response[\"caption\"]:\nhas_caption = False\nhas_data = has_box and has_mask and has_caption\nindex = np.random.randint(len(self))\nreturn response\ndef _get_single_item(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nresponse = super().__getitem__(index)\nif isinstance(index, str):\nreturn response\nelse:\nresponse = deepcopy(response)\nimage = response[\"image\"]\nimage_id = int(os.path.splitext(os.path.basename(image))[0])\nresponse[\"image_id\"] = image_id\nif self.include_bboxes:\nself._populate_instance_data(response, image_id)\nif self.captions:\nself._populate_caption_data(response, image_id)\nreturn response\ndef _populate_instance_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add instance data to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find data.\n        \"\"\"\ndata[\"bbox\"] = []\nif self.include_masks:\ndata[\"mask\"] = []\nannotation_ids = self.instances.getAnnIds(imgIds=image_id, iscrowd=False)\nif annotation_ids:\nannotations = self.instances.loadAnns(annotation_ids)\nfor annotation in annotations:\nif annotation[\"bbox\"][2] * annotation[\"bbox\"][3] &gt; self.min_bbox_area:\ndata[\"bbox\"].append(tuple(annotation['bbox'] + [annotation['category_id']]))\nif self.include_masks:\ndata[\"mask\"].append(self.instances.annToMask(annotation))\ndef _populate_caption_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add captions to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find captions.\n        \"\"\"\ndata[\"caption\"] = []\nannotation_ids = self.captions.getAnnIds(imgIds=image_id)\nif annotation_ids:\nannotations = self.captions.loadAnns(annotation_ids)\nfor annotation in annotations:\ndata[\"caption\"].append(annotation['caption'])\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the COCO dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>load_bboxes</code> <code>bool</code> <p>Whether to load bbox-related data.</p> <code>True</code> <code>load_masks</code> <code>bool</code> <p>Whether to load mask data (in the form of an array of 1-hot images).</p> <code>False</code> <code>load_captions</code> <code>bool</code> <p>Whether to load caption-related data.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[MSCOCODataset, MSCOCODataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\nload_bboxes: bool = True,\nload_masks: bool = False,\nload_captions: bool = False) -&gt; Tuple[MSCOCODataset, MSCOCODataset]:\n\"\"\"Load and return the COCO dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        load_bboxes: Whether to load bbox-related data.\n        load_masks: Whether to load mask data (in the form of an array of 1-hot images).\n        load_captions: Whether to load caption-related data.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'MSCOCO2017')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'MSCOCO2017')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data = os.path.join(root_dir, \"train2017\")\neval_data = os.path.join(root_dir, \"val2017\")\nannotation_data = os.path.join(root_dir, \"annotations\")\nfiles = [(train_data, \"train2017.zip\", 'http://images.cocodataset.org/zips/train2017.zip'),\n(eval_data, \"val2017.zip\", 'http://images.cocodataset.org/zips/val2017.zip'),\n(annotation_data,\n\"annotations_trainval2017.zip\",\n'http://images.cocodataset.org/annotations/annotations_trainval2017.zip')]\nfor data_dir, zip_name, download_url in files:\nif not os.path.exists(data_dir):\nzip_path = os.path.join(root_dir, zip_name)\n# Download\nif not os.path.exists(zip_path):\nprint(\"Downloading {} to {}\".format(zip_name, root_dir))\nwget.download(download_url, zip_path, bar=bar_custom)\n# Extract\nprint(\"Extracting {}\".format(zip_name))\nwith zipfile.ZipFile(zip_path, 'r') as zip_file:\nzip_file.extractall(os.path.dirname(zip_path))\ntrain_annotation = os.path.join(annotation_data, \"instances_train2017.json\")\neval_annotation = os.path.join(annotation_data, \"instances_val2017.json\")\ntrain_captions = os.path.join(annotation_data, \"captions_train2017.json\")\neval_captions = os.path.join(annotation_data, \"captions_val2017.json\")\ntrain_ds = MSCOCODataset(train_data,\ntrain_annotation,\ntrain_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\neval_ds = MSCOCODataset(eval_data,\neval_annotation,\neval_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\nreturn train_ds, eval_ds\n</code></pre>"}, {"location": "fastestimator/dataset/data/nih_chestxray.html", "title": "nih_chestxray", "text": ""}, {"location": "fastestimator/dataset/data/nih_chestxray.html#fastestimator.fastestimator.dataset.data.nih_chestxray.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\nih_chestxray.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; DirDataset:\n\"\"\"Load and return the NIH Chest X-ray dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'NIH_Chestxray')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'NIH_Chestxray')\nos.makedirs(root_dir, exist_ok=True)\nimage_extracted_path = os.path.join(root_dir, 'images')\nif not os.path.exists(image_extracted_path):\n# download data\nlinks = [\n'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n]\ndata_paths = [os.path.join(root_dir, \"images_{}.tar.gz\".format(x)) for x in range(len(links))]\nfor idx, (link, data_path) in enumerate(zip(links, data_paths)):\n_download_data(link, data_path, idx, len(links))\n# extract data\nfor idx, data_path in enumerate(data_paths):\nprint(\"Extracting {}, file {} / {}\".format(data_path, idx + 1, len(links)))\nwith tarfile.open(data_path) as img_tar:\nimg_tar.extractall(root_dir)\nreturn DirDataset(image_extracted_path, file_extension='.png', recursive_search=False)\n</code></pre>"}, {"location": "fastestimator/dataset/data/omniglot.html", "title": "omniglot", "text": ""}, {"location": "fastestimator/dataset/data/omniglot.html#fastestimator.fastestimator.dataset.data.omniglot.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Omniglot dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[SiameseDirDataset, SiameseDirDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\omniglot.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[SiameseDirDataset, SiameseDirDataset]:\n\"\"\"Load and return the Omniglot dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'Omniglot')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Omniglot')\nos.makedirs(root_dir, exist_ok=True)\ntrain_path = os.path.join(root_dir, 'images_background')\neval_path = os.path.join(root_dir, 'images_evaluation')\ntrain_zip = os.path.join(root_dir, 'images_background.zip')\neval_zip = os.path.join(root_dir, 'images_evaluation.zip')\nfiles = [(train_path, train_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip'),\n(eval_path, eval_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip')]\nfor data_path, data_zip, download_link in files:\nif not os.path.exists(data_path):\n# Download\nif not os.path.exists(data_zip):\nprint(\"Downloading data: {}\".format(data_zip))\nwget.download(download_link, data_zip, bar=bar_custom)\n# Extract\nprint(\"Extracting data: {}\".format(data_path))\nwith zipfile.ZipFile(data_zip, 'r') as zip_file:\nzip_file.extractall(root_dir)\nreturn SiameseDirDataset(train_path), SiameseDirDataset(eval_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/penn_treebank.html", "title": "penn_treebank", "text": ""}, {"location": "fastestimator/dataset/data/penn_treebank.html#fastestimator.fastestimator.dataset.data.penn_treebank.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Penn TreeBank dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>seq_length</code> <code>int</code> <p>Length of data sequence.</p> <code>64</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, NumpyDataset, List[str]]</code> <p>(train_data, eval_data, test_data, vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\penn_treebank.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\nseq_length: int = 64) -&gt; Tuple[NumpyDataset, NumpyDataset, NumpyDataset, List[str]]:\n\"\"\"Load and return the Penn TreeBank dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        seq_length: Length of data sequence.\n    Returns:\n        (train_data, eval_data, test_data, vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'PennTreeBank')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'PennTreeBank')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data_path = os.path.join(root_dir, 'ptb.train.txt')\neval_data_path = os.path.join(root_dir, 'ptb.valid.txt')\ntest_data_path = os.path.join(root_dir, 'ptb.test.txt')\nfiles = [(train_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt'),\n(eval_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt'),\n(test_data_path, 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt')]\ntexts = []\nfor data_path, download_link in files:\nif not os.path.exists(data_path):\n# Download\nprint(\"Downloading data: {}\".format(data_path))\nwget.download(download_link, data_path, bar=bar_custom)\ntext = []\nwith open(data_path, 'r') as f:\nfor line in f:\ntext.extend(line.split() + ['&lt;eos&gt;'])\ntexts.append(text)\n# Build dictionary from training data\nvocab = sorted(set(texts[0]))\nword2idx = {u: i for i, u in enumerate(vocab)}\n#convert word to index and split the sequences and discard the last incomplete sequence\ndata = [[word2idx[word] for word in text[:-(len(text) % seq_length)]] for text in texts]\nx_train, x_eval, x_test = [np.array(d).reshape(-1, seq_length) for d in data]\ntrain_data = NumpyDataset(data={\"x\": x_train})\neval_data = NumpyDataset(data={\"x\": x_eval})\ntest_data = NumpyDataset(data={\"x\": x_test})\nreturn train_data, eval_data, test_data, vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/shakespeare.html", "title": "shakespeare", "text": ""}, {"location": "fastestimator/dataset/data/shakespeare.html#fastestimator.fastestimator.dataset.data.shakespeare.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Shakespeare dataset.</p> <p>Shakespeare dataset is a collection of texts written by Shakespeare. Sourced from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>seq_length</code> <code>int</code> <p>Length of data sequence.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, List[str]]</code> <p>(train_data, vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\shakespeare.py</code> <pre><code>def load_data(root_dir: Optional[str] = None, seq_length: int = 100) -&gt; Tuple[NumpyDataset, List[str]]:\n\"\"\"Load and return the Shakespeare dataset.\n    Shakespeare dataset is a collection of texts written by Shakespeare.\n    Sourced from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        seq_length: Length of data sequence.\n    Returns:\n        (train_data, vocab)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Shakespeare')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Shakespeare')\nos.makedirs(root_dir, exist_ok=True)\nfile_path = os.path.join(root_dir, 'shakespeare.txt')\ndownload_link = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\nif not os.path.exists(file_path):\n# Download\nprint(\"Downloading data: {}\".format(file_path))\nwget.download(download_link, file_path, bar=bar_custom)\nwith open(file_path, 'rb') as f:\ntext_data = f.read().decode(encoding='utf-8')\n# Build dictionary from training data\nvocab = sorted(set(text_data))\n# Creating a mapping from unique characters to indices\nchar2idx = {u: i for i, u in enumerate(vocab)}\ntext_data = [char2idx[c] for c in text_data] + [0] * (seq_length - len(text_data) % seq_length)\ntext_data = np.array(text_data).reshape(-1, seq_length)\ntrain_data = NumpyDataset(data={\"x\": text_data})\nreturn train_data, vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/skl_digits.html", "title": "skl_digits", "text": ""}, {"location": "fastestimator/dataset/data/skl_digits.html#fastestimator.fastestimator.dataset.data.skl_digits.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Sklearn digits dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>NumpyDataset</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\skl_digits.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; NumpyDataset:\n\"\"\"Load and return the Sklearn digits dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nds = datasets.load_digits()\nimages = ds.images\ntargets = ds.target\nreturn NumpyDataset({image_key: images, label_key: targets})\n</code></pre>"}, {"location": "fastestimator/dataset/data/svhn.html", "title": "svhn", "text": ""}, {"location": "fastestimator/dataset/data/svhn.html#fastestimator.fastestimator.dataset.data.svhn.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Street View House Numbers (SVHN) dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[PickleDataset, PickleDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\svhn.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[PickleDataset, PickleDataset]:\n\"\"\"Load and return the Street View House Numbers (SVHN) dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'SVHN')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'SVHN')\nos.makedirs(root_dir, exist_ok=True)\ntrain_file_path = os.path.join(root_dir, 'train.pickle')\ntest_file_path = os.path.join(root_dir, 'test.pickle')\ntrain_compressed_path = os.path.join(root_dir, \"train.tar.gz\")\ntest_compressed_path = os.path.join(root_dir, \"test.tar.gz\")\ntrain_folder_path = os.path.join(root_dir, \"train\")\ntest_folder_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_folder_path):\n# download\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/train.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(train_compressed_path) as tar:\ntar.extractall(root_dir)\nif not os.path.exists(test_folder_path):\n# download\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading eval data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/test.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(test_compressed_path) as tar:\ntar.extractall(root_dir)\n# glob and generate bbox files\nif not os.path.exists(train_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(train_folder_path, \"train\", train_file_path)\nif not os.path.exists(test_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(test_folder_path, \"test\", test_file_path)\nreturn PickleDataset(train_file_path), PickleDataset(test_file_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/svhn_cropped.html", "title": "svhn_cropped", "text": ""}, {"location": "fastestimator/dataset/data/svhn_cropped.html#fastestimator.fastestimator.dataset.data.svhn_cropped.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the SVHN Cropped digits dataset.</p> <p>For more information about this dataset please visit http://ufldl.stanford.edu/housenumbers/. Here, we are using Format 2 to get MNIST-like 32-by-32 images.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\svhn_cropped.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the SVHN Cropped digits dataset.\n    For more information about this dataset please visit http://ufldl.stanford.edu/housenumbers/. Here, we are using\n    Format 2 to get MNIST-like 32-by-32 images.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'SVHN_Cropped')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'SVHN_Cropped')\nos.makedirs(root_dir, exist_ok=True)\n# download data to memory\ntrain_path = os.path.join(root_dir, \"train_32x32.mat\")\ntest_path = os.path.join(root_dir, \"test_32x32.mat\")\nif not os.path.exists(train_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/train_32x32.mat', root_dir, bar=bar_custom)\nif not os.path.exists(test_path):\nprint(\"Downloading test data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/test_32x32.mat', root_dir, bar=bar_custom)\nxy_train = loadmat(train_path)\nxy_test = loadmat(test_path)\n# setting label of '0' digit from '10' to '0'\nxy_train['y'][xy_train['y'] == 10] = 0\nxy_test['y'][xy_test['y'] == 10] = 0\n# make datasets\ntrain_data = NumpyDataset({\"x\": np.transpose(xy_train['X'], (3, 0, 1, 2)), \"y\": xy_train['y']})\ntest_data = NumpyDataset({\"x\": np.transpose(xy_test['X'], (3, 0, 1, 2)), \"y\": xy_test['y']})\nreturn train_data, test_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/tednmt.html", "title": "tednmt", "text": ""}, {"location": "fastestimator/dataset/data/tednmt.html#fastestimator.fastestimator.dataset.data.tednmt.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the neural machine translation dataset from TED talks.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>translate_option</code> <code>str</code> <p>Options for translation languages. Available options are: \"az_to_en\", \"az_tr_to_en\", \"be_ru_to_en\", \"be_to_en\", \"es_to_pt\", \"fr_to_pt\", \"gl_pt_to_en\", \"gl_to_en\", \"he_to_pt\", \"it_to_pt\", \"pt_to_en\", \"ru_to_en\", \"ru_to_pt\", and \"tr_to_en\".</p> <code>'az_to_en'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\tednmt.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\ntranslate_option: str = \"az_to_en\") -&gt; Tuple[NumpyDataset, NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the neural machine translation dataset from TED talks.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        translate_option: Options for translation languages. Available options are: \"az_to_en\", \"az_tr_to_en\",\n            \"be_ru_to_en\", \"be_to_en\", \"es_to_pt\", \"fr_to_pt\", \"gl_pt_to_en\", \"gl_to_en\", \"he_to_pt\", \"it_to_pt\",\n            \"pt_to_en\", \"ru_to_en\", \"ru_to_pt\", and \"tr_to_en\".\n    Returns:\n        (train_data, eval_data, test_data)\n    \"\"\"\n# Set up path\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'tednmt')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'tednmt')\nos.makedirs(root_dir, exist_ok=True)\ncompressed_path = os.path.join(root_dir, 'qi18naacl-dataset.tar.gz')\nextracted_path = os.path.join(root_dir, 'datasets')\nif not os.path.exists(extracted_path):\n# Download\nif not os.path.exists(compressed_path):\nprint(\"Downloading data to {}\".format(compressed_path))\nwget.download('http://www.phontron.com/data/qi18naacl-dataset.tar.gz', compressed_path, bar=bar_custom)\n# Extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(compressed_path) as f:\nf.extractall(root_dir)\n# process data\ndata_path = os.path.join(extracted_path, translate_option)\nassert os.path.exists(data_path), \"folder {} does not exist, please verify translation options\".format(data_path)\ntrain_ds = _create_dataset(data_path=data_path, translate_option=translate_option, extension=\"train\")\neval_ds = _create_dataset(data_path=data_path, translate_option=translate_option, extension=\"dev\")\ntest_ds = _create_dataset(data_path=data_path, translate_option=translate_option, extension=\"test\")\nreturn train_ds, eval_ds, test_ds\n</code></pre>"}, {"location": "fastestimator/dataset/data/tiny_imagenet.html", "title": "tiny_imagenet", "text": ""}, {"location": "fastestimator/dataset/data/tiny_imagenet.html#fastestimator.fastestimator.dataset.data.tiny_imagenet.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Tiny ImageNet dataset. Sourced from http://cs231n.stanford.edu/tiny-imagenet-200.zip. This method will     download the data to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\tiny_imagenet.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the Tiny ImageNet dataset.\n    Sourced from http://cs231n.stanford.edu/tiny-imagenet-200.zip. This method will\n        download the data to local storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'tiny_imagenet')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'tiny_imagenet')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'tiny-imagenet-200.zip')\ntrain_file_path = os.path.join(root_dir, 'tiny-imagenet-200', 'train')\nval_file_path = os.path.join(root_dir, 'tiny-imagenet-200', 'val')\nif (os.path.exists(train_file_path) == False) or (os.path.exists(val_file_path) == False):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://cs231n.stanford.edu/tiny-imagenet-200.zip', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\nzip_file.extractall(root_dir)\n#os.rename(os.path.join(root_dir, 'horse2zebra'), data_folder_path)\ncurrent_dir = os.path.join(root_dir, 'tiny-imagenet-200')\n# Update Train Directory\nfor root, _, files in os.walk(os.path.join(current_dir, 'train')):\nfor filename in files:\n#print(os.path.join(root, filename))\nif filename.endswith('.txt'):\nos.remove(os.path.join(root, filename))\nelse:\np = Path(os.path.join(root, filename)).absolute()\nparent_dir = p.parents[1]\np.rename(parent_dir / p.name)\nif len(os.listdir(root)) == 0:\nos.rmdir(root)\n# Update Val Directory\nfor line in open(os.path.join(current_dir, 'val', 'val_annotations.txt')).readlines():\nfile_data = [n for n in line.split()]\nfolder_path = os.path.join(current_dir, 'val', file_data[1])\nif not os.path.exists(folder_path):\nos.makedirs(folder_path)\nfile_path = os.path.join(current_dir, 'val', 'images', file_data[0])\nos.rename(file_path, os.path.join(folder_path, file_data[0]))\nos.rmdir(os.path.join(current_dir, 'val', 'images'))\nos.remove(os.path.join(current_dir, 'val', 'val_annotations.txt'))\nroot_path = os.path.join(root_dir, 'tiny-imagenet-200')\ntrain_outputs = LabeledDirDataset(os.path.join(root_path, \"train\"), data_key='image', label_key='label')\neval_outputs = LabeledDirDataset(os.path.join(root_path, \"val\"), data_key='image', label_key='label')\nreturn (train_outputs, eval_outputs)\n</code></pre>"}, {"location": "fastestimator/dataset/data/usps.html", "title": "usps", "text": ""}, {"location": "fastestimator/dataset/data/usps.html#fastestimator.fastestimator.dataset.data.usps.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the USPS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\usps.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the USPS dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'USPS')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'USPS')\nos.makedirs(root_dir, exist_ok=True)\n# download data to memory\ntrain_compressed_path = os.path.join(root_dir, \"zip.train.gz\")\ntest_compressed_path = os.path.join(root_dir, \"zip.test.gz\")\ntrain_base_path = os.path.join(root_dir, \"train\")\ntest_base_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_base_path):\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz',\nroot_dir,\nbar=bar_custom)\ntrain_images, train_labels = _extract_images_labels(train_compressed_path)\n_write_data(train_images, train_labels, train_base_path, \"train\")\nif not os.path.exists(test_base_path):\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading test data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz',\nroot_dir,\nbar=bar_custom)\ntest_images, test_labels = _extract_images_labels(test_compressed_path)\n_write_data(test_images, test_labels, test_base_path, \"test\")\n# make datasets\nreturn LabeledDirDataset(train_base_path, file_extension=\".png\"), LabeledDirDataset(test_base_path,\nfile_extension=\".png\")\n</code></pre>"}, {"location": "fastestimator/layers/pytorch/cropping_2d.html", "title": "cropping_2d", "text": ""}, {"location": "fastestimator/layers/pytorch/cropping_2d.html#fastestimator.fastestimator.layers.pytorch.cropping_2d.Cropping2D", "title": "<code>Cropping2D</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A layer for cropping along height and width dimensions.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <pre><code>x = torch.tensor(list(range(100))).view((1,1,10,10))\nm = fe.layers.pytorch.Cropping2D(3)\ny = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\nm = fe.layers.pytorch.Cropping2D((3, 4))\ny = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\nm = fe.layers.pytorch.Cropping2D(((1, 4), 4))\ny = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cropping</code> <code>Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int, int]]]]</code> <p>Height and width cropping parameters. If a single int 'n' is specified, then the width and height of the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w') is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h' and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then 'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right (assuming the top left corner as the 0,0 origin).</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cropping</code> has an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\layers\\pytorch\\cropping_2d.py</code> <pre><code>class Cropping2D(nn.Module):\n\"\"\"A layer for cropping along height and width dimensions.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    ```python\n    x = torch.tensor(list(range(100))).view((1,1,10,10))\n    m = fe.layers.pytorch.Cropping2D(3)\n    y = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\n    m = fe.layers.pytorch.Cropping2D((3, 4))\n    y = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\n    m = fe.layers.pytorch.Cropping2D(((1, 4), 4))\n    y = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n    ```\n    Args:\n        cropping: Height and width cropping parameters. If a single int 'n' is specified, then the width and height of\n            the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w')\n            is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h'\n            and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then\n            'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right\n            (assuming the top left corner as the 0,0 origin).\n    Raises:\n        ValueError: If `cropping` has an unacceptable data type.\n    \"\"\"\ndef __init__(self, cropping: Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int,\nint]]]] = 0) -&gt; None:\nsuper().__init__()\nif isinstance(cropping, int):\nself.cropping = ((cropping, cropping), (cropping, cropping))\nelif hasattr(cropping, '__len__'):\nif len(cropping) != 2:\nraise ValueError(f\"'cropping' should have two elements, but found {len(cropping)}\")\nif isinstance(cropping[0], int):\nheight_cropping = (cropping[0], cropping[0])\nelif hasattr(cropping[0], '__len__') and len(cropping[0]) == 2:\nheight_cropping = (cropping[0][0], cropping[0][1])\nelse:\nraise ValueError(f\"'cropping' height should be an int or tuple of ints, but found {cropping[0]}\")\nif isinstance(cropping[1], int):\nwidth_cropping = (cropping[1], cropping[1])\nelif hasattr(cropping[1], '__len__') and len(cropping[1]) == 2:\nwidth_cropping = (cropping[1][0], cropping[1][1])\nelse:\nraise ValueError(f\"'cropping' width should be an int or tuple of ints, but found {cropping[1]}\")\nself.cropping = (height_cropping, width_cropping)\nelse:\nraise ValueError(\n\"cropping` should be either an int, a tuple of 2 ints or a tuple of two tuple of 2 ints. Found: \" +\nstr(cropping))\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn x[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:-self.cropping[1][1]]\n</code></pre>"}, {"location": "fastestimator/layers/pytorch/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/layers/pytorch/hadamard.html#fastestimator.fastestimator.layers.pytorch.hadamard.HadamardCode", "title": "<code>HadamardCode</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A layer for applying an error correcting code to your outputs.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial- robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code lookup.</p> <pre><code># Use as a drop-in replacement for your softmax layer:\ndef __init__(self, classes):\nself.fc1 = nn.Linear(1024, 64)\nself.fc2 = nn.Linear(64, classes)\ndef forward(self, x):\nx = fn.relu(self.fc1(x))\nx = fn.softmax(self.fc2(x), dim=-1)\n#   ----- vs ------\ndef __init__(self, classes):\nself.fc1 = nn.Linear(1024, 64)\nself.fc2 = HadamardCode(64, classes)\ndef forward(self, x):\nx = fn.relu(self.fc1(x))\nx = self.fc2(x)\n</code></pre> <pre><code># Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\ndef __init__(self, classes):\nself.fc1 = nn.ModuleList([nn.Linear(1024, 16) for _ in range(4)])\nself.fc2 = HadamardCode([16]*4, classes)\ndef forward(self, x):\nx = [fn.relu(fc(x)) for fc in self.fc1]\nx = self.fc2(x)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>Union[int, List[int]]</code> <p>How many input features there are (inputs should be of shape (Batch, N) or [(Batch, N), ...]).</p> required <code>n_classes</code> <code>int</code> <p>How many output classes to map onto.</p> required <code>code_length</code> <code>Optional[int]</code> <p>How long of an error correcting code to use. Should be a positive multiple of 2. If not provided, the smallest power of 2 which is &gt;= <code>n_outputs</code> will be used, or 16 if the latter is larger.</p> <code>None</code> <code>max_prob</code> <code>float</code> <p>The maximum probability that can be assigned to a class. For numeric stability this must be less than 1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should be noticeably less than 1, for example 0.95 or even 0.8.</p> <code>0.95</code> <code>power</code> <code>float</code> <p>The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give slightly faster convergence at the expense of adversarial performance. Must be greater than zero.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>code_length</code> is invalid.</p> Source code in <code>fastestimator\\fastestimator\\layers\\pytorch\\hadamard.py</code> <pre><code>class HadamardCode(nn.Module):\n\"\"\"A layer for applying an error correcting code to your outputs.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial-\n    robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be\n    split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code\n    lookup.\n    ```python\n    # Use as a drop-in replacement for your softmax layer:\n    def __init__(self, classes):\n        self.fc1 = nn.Linear(1024, 64)\n        self.fc2 = nn.Linear(64, classes)\n    def forward(self, x):\n        x = fn.relu(self.fc1(x))\n        x = fn.softmax(self.fc2(x), dim=-1)\n    #   ----- vs ------\n    def __init__(self, classes):\n        self.fc1 = nn.Linear(1024, 64)\n        self.fc2 = HadamardCode(64, classes)\n    def forward(self, x):\n        x = fn.relu(self.fc1(x))\n        x = self.fc2(x)\n    ```\n    ```python\n    # Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\n    def __init__(self, classes):\n        self.fc1 = nn.ModuleList([nn.Linear(1024, 16) for _ in range(4)])\n        self.fc2 = HadamardCode([16]*4, classes)\n    def forward(self, x):\n        x = [fn.relu(fc(x)) for fc in self.fc1]\n        x = self.fc2(x)\n    ```\n    Args:\n        in_features: How many input features there are (inputs should be of shape (Batch, N) or [(Batch, N), ...]).\n        n_classes: How many output classes to map onto.\n        code_length: How long of an error correcting code to use. Should be a positive multiple of 2. If not provided,\n            the smallest power of 2 which is &gt;= `n_outputs` will be used, or 16 if the latter is larger.\n        max_prob: The maximum probability that can be assigned to a class. For numeric stability this must be less than\n            1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should\n            be noticeably less than 1, for example 0.95 or even 0.8.\n        power: The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances\n            into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small\n            values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give\n            slightly faster convergence at the expense of adversarial performance. Must be greater than zero.\n    Raises:\n        ValueError: If `code_length` is invalid.\n    \"\"\"\nheads: Union[nn.ModuleList, nn.Module]\ndef __init__(self,\nin_features: Union[int, List[int]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmax_prob: float = 0.95,\npower: float = 1.0) -&gt; None:\nsuper().__init__()\nself.n_classes = n_classes\nif code_length is None:\ncode_length = max(16, 1 &lt;&lt; (n_classes - 1).bit_length())\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nif power &lt;= 0:\nraise ValueError(f\"power must be positive, but got {power}.\")\nself.power = nn.Parameter(torch.tensor(power), requires_grad=False)\nif not 0.0 &lt; max_prob &lt; 1.0:\nraise ValueError(f\"max_prob must be in the range (0, 1), but got {max_prob}\")\nself.eps = nn.Parameter(\ntorch.tensor(self.code_length * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / power)),\nrequires_grad=False)\nlabels = hadamard(self.code_length)\n# Cut off 0th column b/c it's constant. It would also be possible to make the column sign alternate, but that\n# would break the symmetry between rows in the code.\nlabels = labels[:self.n_classes, 1:]\nself.labels = nn.Parameter(torch.tensor(labels, dtype=torch.float32), requires_grad=False)\nin_features = to_list(in_features)\nif len(in_features) &gt; code_length - 1:\nraise ValueError(f\"Too many input heads {len(in_features)} for the given code length {self.code_length}.\")\nhead_sizes = [self.code_length // len(in_features) for _ in range(len(in_features))]\nhead_sizes[0] = head_sizes[0] + self.code_length - sum(head_sizes)\nhead_sizes[0] = head_sizes[0] - 1  # We're going to cut off the 0th column from the code\nself.heads = nn.ModuleList([\nnn.Linear(in_features=in_feat, out_features=out_feat) for in_feat, out_feat in zip(in_features, head_sizes)\n])\ndef forward(self, x: List[torch.Tensor]) -&gt; torch.Tensor:\n# can't have forward function call subfunctions otherwise will fail on multi-gpu\nif isinstance(x, list):\nx = [head(tensor) for head, tensor in zip(self.heads, x)]\nx = torch.cat(x, dim=-1)\nelse:\nx = self.heads[0](x)\nx = torch.tanh(x)\n# Compute L1 distance\nx = torch.max(torch.sum(torch.abs(torch.unsqueeze(x, dim=1) - self.labels), dim=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / torch.pow(x, self.power)\nx = torch.div(x, torch.sum(x, dim=-1).view(-1, 1))\nreturn x\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/layers/tensorflow/hadamard.html#fastestimator.fastestimator.layers.tensorflow.hadamard.HadamardCode", "title": "<code>HadamardCode</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for applying an error correcting code to your outputs.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial- robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code lookup.</p> <pre><code># Use as a drop-in replacement for your softmax layer:\nmodel = Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=input_shape))\nmodel.add(layers.Dense(10, activation='softmax'))\n#   ----- vs ------\nmodel = Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=input_shape))\nmodel.add(HadamardCode(10))\n</code></pre> <pre><code># Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\ninputs = Input(input_shape)\nfeatures = Dense(1024, activation='relu')(inputs)\nheads = [Dense(20)(features) for _ in range(5)]\noutputs = HadamardCode(10)(heads)\nmodel = Model(inputs, outputs)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_classes</code> <code>int</code> <p>How many output classes to map onto.</p> required <code>code_length</code> <code>Optional[int]</code> <p>How long of an error correcting code to use. Should be a positive multiple of 2. If not provided, the smallest power of 2 which is &gt;= <code>n_outputs</code> will be used, or 16 if the latter is larger.</p> <code>None</code> <code>max_prob</code> <code>float</code> <p>The maximum probability that can be assigned to a class. For numeric stability this must be less than 1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should be noticeably less than 1, for example 0.95 or even 0.8.</p> <code>0.95</code> <code>power</code> <code>float</code> <p>The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give slightly faster convergence at the expense of adversarial performance. Must be greater than zero.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>code_length</code>, <code>max_prob</code>, or <code>power</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\hadamard.py</code> <pre><code>class HadamardCode(layers.Layer):\n\"\"\"A layer for applying an error correcting code to your outputs.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    See 'https://papers.nips.cc/paper/9070-error-correcting-output-codes-improve-probability-estimation-and-adversarial-\n    robustness-of-deep-neural-networks'. Note that for best effectiveness, the model leading into this layer should be\n    split into multiple independent chunks, whose outputs this layer can combine together in order to perform the code\n    lookup.\n    ```python\n    # Use as a drop-in replacement for your softmax layer:\n    model = Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n    model.add(layers.Dense(10, activation='softmax'))\n    #   ----- vs ------\n    model = Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n    model.add(HadamardCode(10))\n    ```\n    ```python\n    # Use to combine multiple feature heads for a final output (biggest adversarial hardening benefit):\n    inputs = Input(input_shape)\n    features = Dense(1024, activation='relu')(inputs)\n    heads = [Dense(20)(features) for _ in range(5)]\n    outputs = HadamardCode(10)(heads)\n    model = Model(inputs, outputs)\n    ```\n    Args:\n        n_classes: How many output classes to map onto.\n        code_length: How long of an error correcting code to use. Should be a positive multiple of 2. If not provided,\n            the smallest power of 2 which is &gt;= `n_outputs` will be used, or 16 if the latter is larger.\n        max_prob: The maximum probability that can be assigned to a class. For numeric stability this must be less than\n            1.0. Intuitively it makes sense to keep this close to 1, but to get adversarial training benefits it should\n            be noticeably less than 1, for example 0.95 or even 0.8.\n        power: The power parameter to be used by Inverse Distance Weighting when transforming Hadamard class distances\n            into a class probability distribution. A value of 1.0 gives an intuitive mapping to probabilities, but small\n            values such as 0.25 appear to give slightly better adversarial benefits. Large values like 2 or 3 give\n            slightly faster convergence at the expense of adversarial performance. Must be greater than zero.\n    Raises:\n        ValueError: If `code_length`, `max_prob`, or `power` are invalid.\n    \"\"\"\nheads: Union[List[layers.Dense], layers.Dense]\ndef __init__(self, n_classes: int, code_length: Optional[int] = None, max_prob: float = 0.95,\npower: float = 1.0) -&gt; None:\nsuper().__init__()\nself.n_classes = n_classes\nif code_length is None:\ncode_length = max(16, 1 &lt;&lt; (n_classes - 1).bit_length())\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nif power &lt;= 0:\nraise ValueError(f\"power must be positive, but got {power}.\")\nself.power = power\nif not 0.0 &lt; max_prob &lt; 1.0:\nraise ValueError(f\"max_prob must be in the range (0, 1), but got {max_prob}\")\nself.eps = self.code_length * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / self.power)\nself.labels = None\nself.heads = []\nself._call_fn = None\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'n_classes': self.n_classes, 'code_length': self.code_length}\ndef build(self, input_shape: Union[Tuple[int, int], List[Tuple[int, int]]]) -&gt; None:\nsingle_input = not isinstance(input_shape, list)\ninput_shape = to_list(input_shape)\nbatch_size = input_shape[0][0]\nif len(input_shape) &gt; self.code_length - 1:\nraise ValueError(f\"Too many input heads {len(input_shape)} for the given code length {self.code_length}.\")\nhead_sizes = [self.code_length // len(input_shape) for _ in range(len(input_shape))]\nhead_sizes[0] = head_sizes[0] + self.code_length - sum(head_sizes)\nhead_sizes[0] = head_sizes[0] - 1  # We're going to cut off the 0th column from the code\nfor idx, shape in enumerate(input_shape):\nif len(shape) != 2:\nraise ValueError(\"ErrorCorrectingCode layer requires input like (batch, m) or [(batch, m), ...]\")\nif shape[0] != batch_size:\nraise ValueError(\"Inputs to ErrorCorrectingCode layer must have the same batch size\")\nself.heads.append(layers.Dense(units=head_sizes[idx]))\nlabels = hadamard(self.code_length)\n# Cut off 0th column b/c it's constant. It would also be possible to make the column sign alternate, but that\n# would break the symmetry between rows in the code.\nlabels = labels[:self.n_classes, 1:]\nself.labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n# Spare extra operations when they're not needed\nif single_input:\nself.heads = self.heads[0]\nself._call_fn = self._single_head_call\nelse:\nself._call_fn = self._multi_head_call\ndef _single_head_call(self, x: tf.Tensor) -&gt; tf.Tensor:\nx = self.heads(x)\nx = tf.tanh(x)\n# Compute L1 distance\nx = tf.maximum(tf.reduce_sum(tf.abs(tf.expand_dims(x, axis=1) - self.labels), axis=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / tf.pow(x, self.power)\nx = tf.math.divide(x, tf.reshape(tf.reduce_sum(x, axis=-1), (-1, 1)))\nreturn x\ndef _multi_head_call(self, x: List[tf.Tensor]) -&gt; tf.Tensor:\nx = [head(tensor) for head, tensor in zip(self.heads, x)]\nx = tf.concat(x, axis=-1)\nx = tf.tanh(x)\n# Compute L1 distance\nx = tf.maximum(tf.reduce_sum(tf.abs(tf.expand_dims(x, axis=1) - self.labels), axis=-1), self.eps)\n# Inverse Distance Weighting\nx = 1.0 / tf.pow(x, self.power)\nx = tf.math.divide(x, tf.reshape(tf.reduce_sum(x, axis=-1), (-1, 1)))\nreturn x\ndef call(self, x: Union[tf.Tensor, List[tf.Tensor]], **kwargs) -&gt; tf.Tensor:\nreturn self._call_fn(x)\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/instance_norm.html", "title": "instance_norm", "text": ""}, {"location": "fastestimator/layers/tensorflow/instance_norm.html#fastestimator.fastestimator.layers.tensorflow.instance_norm.InstanceNormalization", "title": "<code>InstanceNormalization</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing instance normalization.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.</p> <pre><code>n = tfp.distributions.Normal(loc=10, scale=2)\nx = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\nm = fe.layers.tensorflow.InstanceNormalization()\ny = m(x)  # mean ~= 0, stddev ~= 0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>A numerical stability constant added to the variance.</p> <code>1e-05</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\instance_norm.py</code> <pre><code>class InstanceNormalization(layers.Layer):\n\"\"\"A layer for performing instance normalization.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See\n    https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from\n    https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.\n    ```python\n    n = tfp.distributions.Normal(loc=10, scale=2)\n    x = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\n    m = fe.layers.tensorflow.InstanceNormalization()\n    y = m(x)  # mean ~= 0, stddev ~= 0\n    ```\n    Args:\n        epsilon: A numerical stability constant added to the variance.\n    \"\"\"\ndef __init__(self, epsilon: float = 1e-5) -&gt; None:\nsuper().__init__()\nself.epsilon = epsilon\nself.scale = None\nself.offset = None\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'epsilon': self.epsilon}\ndef build(self, input_shape: Tuple[int, int, int, int]) -&gt; None:\nself.scale = self.add_weight(name='scale',\nshape=input_shape[-1:],\ninitializer=tf.random_normal_initializer(0., 0.02),\ntrainable=True)\nself.offset = self.add_weight(name='offset', shape=input_shape[-1:], initializer='zeros', trainable=True)\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nmean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\ninv = tf.math.rsqrt(variance + self.epsilon)\nnormalized = (x - mean) * inv\nreturn self.scale * normalized + self.offset\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html", "title": "reflection_padding_2d", "text": ""}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D", "title": "<code>ReflectionPadding2D</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing Reflection Padding on 2D arrays.</p> <p>This class is intentionally not @traceable (models and layers are handled by a different process).</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.</p> <pre><code>x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\ny = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\ny = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>Tuple[int, int]</code> <p>padding size (Width, Height). The padding size must be less than the size of the corresponding dimension in the input tensor.</p> <code>(1, 1)</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>class ReflectionPadding2D(layers.Layer):\n\"\"\"A layer for performing Reflection Padding on 2D arrays.\n    This class is intentionally not @traceable (models and layers are handled by a different process).\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels).\n    The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.\n    ```python\n    x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\n    y = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\n    y = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n    ```\n    Args:\n        padding: padding size (Width, Height). The padding size must be less than the size of the corresponding\n            dimension in the input tensor.\n    \"\"\"\ndef __init__(self, padding: Tuple[int, int] = (1, 1)) -&gt; None:\nsuper().__init__()\nself.padding = tuple(padding)\nself.input_spec = [layers.InputSpec(ndim=4)]\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'padding': self.padding}\ndef compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nw_pad, h_pad = self.padding\nreturn tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D.compute_output_shape", "title": "<code>compute_output_shape</code>", "text": "<p>If you are using \"channels_last\" configuration</p> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>def compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n</code></pre>"}, {"location": "fastestimator/op/op.html", "title": "op", "text": ""}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.Op", "title": "<code>Op</code>", "text": "<p>A base class for FastEstimator Operators.</p> <p>Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three main variables: <code>inputs</code>, <code>outputs</code>, and <code>mode</code>. When FastEstimator executes, it holds all of its available data behind the scenes in a data dictionary. If an <code>Op</code> wants to interact with a piece of data from this dictionary, it lists the data's key as one of it's <code>inputs</code>. That data will then be passed to the <code>Op</code> when the <code>Op</code>s forward function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an <code>Op</code> wants to write data into the data dictionary, it can return values from its forward function. These values are then written into the data dictionary under the keys specified by the <code>Op</code>s <code>outputs</code>. An <code>Op</code> will only be run if its associated <code>mode</code> matches the current execution mode. For example, if an <code>Op</code> has a mode of 'eval' but FastEstimator is currently running in the 'train' mode, then the <code>Op</code>s forward function will not be called.</p> <p>Normally, if a single string \"key\" is passed as <code>inputs</code> then the value that is passed to the forward function will be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as <code>inputs</code> then the value passed to the forward function will be the element stored in the data dictionary, but wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an <code>Op</code> is anticipated to take one or more inputs and treat them all in the same way. In such cases the <code>in_list</code> member variable may be manually overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of whether <code>inputs</code> was a single string or a list of strings. For an example of when this is useful, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Similarly, by default, if an <code>Op</code> has a single <code>output</code> string \"key\" then that output R will be written into the data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as <code>outputs</code> then the return value for R from the <code>Op</code> is expected to be a list [X], where the inner value will be written to the data dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an <code>Op</code> wants to always return data in a list format without worrying about whether it had one input or more than one input. In such cases the <code>out_list</code> member variable may be manually overridden to True. This will cause the system to always assume that the response is in list format and unwrap the values before storing them into the data dictionary. For an example, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>@traceable()\nclass Op:\n\"\"\"A base class for FastEstimator Operators.\n    Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three\n    main variables: `inputs`, `outputs`, and `mode`. When FastEstimator executes, it holds all of its available data\n    behind the scenes in a data dictionary. If an `Op` wants to interact with a piece of data from this dictionary, it\n    lists the data's key as one of it's `inputs`. That data will then be passed to the `Op` when the `Op`s forward\n    function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an `Op` wants to\n    write data into the data dictionary, it can return values from its forward function. These values are then written\n    into the data dictionary under the keys specified by the `Op`s `outputs`. An `Op` will only be run if its associated\n    `mode` matches the current execution mode. For example, if an `Op` has a mode of 'eval' but FastEstimator is\n    currently running in the 'train' mode, then the `Op`s forward function will not be called.\n    Normally, if a single string \"key\" is passed as `inputs` then the value that is passed to the forward function will\n    be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as\n    `inputs` then the value passed to the forward function will be the element stored in the data dictionary, but\n    wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an `Op` is anticipated to take\n    one or more inputs and treat them all in the same way. In such cases the `in_list` member variable may be manually\n    overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of\n    whether `inputs` was a single string or a list of strings. For an example of when this is useful, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Similarly, by default, if an `Op` has a single `output` string \"key\" then that output R will be written into the\n    data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as `outputs` then the\n    return value for R from the `Op` is expected to be a list [X], where the inner value will be written to the data\n    dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an `Op` wants to always return data in a\n    list format without worrying about whether it had one input or more than one input. In such cases the `out_list`\n    member variable may be manually overridden to True. This will cause the system to always assume that the response is\n    in list format and unwrap the values before storing them into the data dictionary. For an example, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Args:\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ninputs: List[str]\noutputs: List[str]\nmode: Set[str]\nds_id: Set[str]\nin_list: bool  # Whether inputs should be presented as a list or an individual value\nout_list: bool  # Whether outputs will be returned as a list or an individual value\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = check_io_names(to_list(inputs))\nself.outputs = check_io_names(to_list(outputs))\nself.mode = parse_modes(to_set(mode))\nself.ds_id = check_ds_id(to_set(ds_id))\nself.in_list = not isinstance(inputs, (str, type(None)))\nself.out_list = not isinstance(outputs, (str, type(None)))\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.get_inputs_by_op", "title": "<code>get_inputs_by_op</code>", "text": "<p>Retrieve the necessary input data from the data dictionary in order to run an <code>op</code>.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The op to run.</p> required <code>store</code> <code>Mapping[str, Any]</code> <p>The system's data dictionary to draw inputs out of.</p> required <code>copy_on_write</code> <code>bool</code> <p>Whether to copy read-only data to make it writeable before returning it.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>Input data to be fed to the <code>op</code> forward function.</p> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def get_inputs_by_op(op: Op, store: Mapping[str, Any], copy_on_write: bool = False) -&gt; Any:\n\"\"\"Retrieve the necessary input data from the data dictionary in order to run an `op`.\n    Args:\n        op: The op to run.\n        store: The system's data dictionary to draw inputs out of.\n        copy_on_write: Whether to copy read-only data to make it writeable before returning it.\n    Returns:\n        Input data to be fed to the `op` forward function.\n    \"\"\"\nif op.in_list:\ndata = []\nelse:\ndata = None\nif op.inputs:\ndata = []\nfor key in op.inputs:\nelem = store[key]\nif copy_on_write and isinstance(elem, np.ndarray) and not elem.flags.writeable:\nelem = deepcopy(elem)\nstore[key] = elem\ndata.append(elem)\nif not op.in_list:\ndata = data[0]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.write_outputs_by_op", "title": "<code>write_outputs_by_op</code>", "text": "<p>Write <code>outputs</code> from an <code>op</code> forward function into the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The Op which generated <code>outputs</code>.</p> required <code>store</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary into which to write the <code>outputs</code>.</p> required <code>outputs</code> <code>Any</code> <p>The value(s) generated by the <code>op</code>s forward function.</p> required Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def write_outputs_by_op(op: Op, store: MutableMapping[str, Any], outputs: Any) -&gt; None:\n\"\"\"Write `outputs` from an `op` forward function into the data dictionary.\n    Args:\n        op: The Op which generated `outputs`.\n        store: The data dictionary into which to write the `outputs`.\n        outputs: The value(s) generated by the `op`s forward function.\n    \"\"\"\nif not op.out_list:\noutputs = [outputs]\nfor key, data in zip(op.outputs, outputs):\nstore[key] = data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html", "title": "numpyop", "text": ""}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.Batch", "title": "<code>Batch</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert data instances into a batch of data.</p> <p>Only one instance of a Batch Op can be present for a given epoch/mode/ds_id combination. Any Ops after this one will operate on batches of data rather than individual instances (using their batch_forward methods).</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use. If set, this will override any value specified by the Pipeline, allowing control of the batch size on a per-mode and per-ds_id level. Note that this value will be ignored when using a BatchDataset (or any dataset which decides on its own batch configuration).</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch if the last batch is incomplete. Note that setting this to True when using a BatchDataset (or any dataset which decides on its own batch configuration) won't do anything.</p> <code>False</code> <code>pad_value</code> <code>Optional[Union[int, float]]</code> <p>The padding value if batch padding is needed. None indicates that no padding is needed. Mutually exclusive with <code>collate_fn</code>.</p> <code>None</code> <code>collate_fn</code> <code>Optional[Callable[[List[Dict[str, Any]]], Dict[str, Any]]]</code> <p>A function to merge a list of data elements into a batch of data. Mutually exclusive with <code>pad_value</code>.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass Batch(NumpyOp):\n\"\"\"Convert data instances into a batch of data.\n    Only one instance of a Batch Op can be present for a given epoch/mode/ds_id combination. Any Ops after this one will\n    operate on batches of data rather than individual instances (using their batch_forward methods).\n    Args:\n        batch_size: The batch size to use. If set, this will override any value specified by the Pipeline, allowing\n            control of the batch size on a per-mode and per-ds_id level. Note that this value will be ignored when using\n            a BatchDataset (or any dataset which decides on its own batch configuration).\n        drop_last: Whether to drop the last batch if the last batch is incomplete. Note that setting this to True when\n            using a BatchDataset (or any dataset which decides on its own batch configuration) won't do anything.\n        pad_value: The padding value if batch padding is needed. None indicates that no padding is needed. Mutually\n            exclusive with `collate_fn`.\n        collate_fn: A function to merge a list of data elements into a batch of data. Mutually exclusive with\n            `pad_value`.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nbatch_size: Optional[int] = None,\ndrop_last: bool = False,\npad_value: Optional[Union[int, float]] = None,\ncollate_fn: Optional[Callable[[List[Dict[str, Any]]], Dict[str, Any]]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(mode=mode, ds_id=ds_id)\nif batch_size is not None:\nif not isinstance(batch_size, int):\nraise ValueError(f\"batch_size must be an integer, but got {type(batch_size)}\")\nif batch_size &lt; 0:\nraise ValueError(\"batch_size must be non-negative\")\nself.batch_size = batch_size\nself.drop_last = drop_last\nif pad_value is not None and collate_fn is not None:\nraise ValueError(\"Provide either a pad_value or collate_fn, but not both\")\nself._pad_value = pad_value\nself.collate_fn = collate_fn\nif pad_value is not None:\nself.collate_fn = self._pad_batch_collate\nif self.collate_fn is None:\n# Note that this might get ignored in favor of default_convert inside the FEDataLoader if it looks like the\n# user really doesn't want stuff to be batched.\nself.collate_fn = default_collate\ndef _pad_batch_collate(self, batch: List[MutableMapping[str, Any]]) -&gt; Dict[str, Any]:\n\"\"\"A collate function which pads a batch of data.\n        Args:\n            batch: The data to be batched and collated.\n        Returns:\n            A padded and collated batch of data.\n        \"\"\"\npad_batch(batch, self._pad_value)\nreturn default_collate(batch)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.Delete", "title": "<code>Delete</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Delete key(s) and their associated values from the data dictionary.</p> <p>The system has special logic to detect instances of this Op and delete its <code>inputs</code> from the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, List[str]]</code> <p>Existing key(s) to be deleted from the data dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass Delete(NumpyOp):\n\"\"\"Delete key(s) and their associated values from the data dictionary.\n    The system has special logic to detect instances of this Op and delete its `inputs` from the data dictionary.\n    Args:\n        keys: Existing key(s) to be deleted from the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nkeys: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=keys, mode=mode, ds_id=ds_id)\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]], state: Dict[str, Any]) -&gt; None:\npass\ndef forward_batch(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.LambdaOp", "title": "<code>LambdaOp</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Operator that performs any specified function as forward function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to be executed.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass LambdaOp(NumpyOp):\n\"\"\"An Operator that performs any specified function as forward function.\n    Args:\n        fn: The function to be executed.\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nfn: Callable,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.fn = fn\nself.in_list = True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\nreturn self.fn(*data)\ndef forward_batch(self, data: Union[Tensor, List[Tensor]], state: Dict[str,\nAny]) -&gt; Union[np.ndarray, List[np.ndarray]]:\nreturn self.forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.LambdaOp.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>A method which will be invoked by the RUA Op to adjust the augmentation intensity.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp", "title": "<code>NumpyOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns numpy data.</p> <p>These Operators are used in fe.Pipeline to perform data pre-processing / augmentation. They may also be used in fe.Network to perform postprocessing on data.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass NumpyOp(Op):\n\"\"\"An Operator class which takes and returns numpy data.\n    These Operators are used in fe.Pipeline to perform data pre-processing / augmentation. They may also be used in\n    fe.Network to perform postprocessing on data.\n    Args:\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n# in_place_edits tracks whether the .forward() method of this op will perform in-place edits of numpy arrays.\n# This is inferred automatically by the system and is used for memory management optimization. If you are\n# developing a NumpyOp which does in-place edits, the best practice is to set this to True in your init method.\nself.in_place_edits = False\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[None, FilteredData, np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n        Args:\n            data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\ndef forward_batch(self,\ndata: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[None, FilteredData, np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform a batch of data.\n        This method will be invoked on batches of data during network postprocessing. It should expect to receive\n        batched data and should itself return batched data.\n        Args:\n            data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nif isinstance(data, list):\ndata = [elem for elem in map(list, zip(*data))]\nelse:\ndata = [elem for elem in data]\nresults = [self.forward(elem, state) for elem in data]\nif self.out_list:\nresults = [np.array(col) for col in [[row[i] for row in results] for i in range(len(results[0]))]]\nelse:\nresults = np.array(results)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on individual elements of data before any batching / axis expansion is performed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The arrays from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[None, FilteredData, np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[None, FilteredData, np.ndarray, List[np.ndarray]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[None, FilteredData, np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n    Args:\n        data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp.forward_batch", "title": "<code>forward_batch</code>", "text": "<p>A method which will be invoked in order to transform a batch of data.</p> <p>This method will be invoked on batches of data during network postprocessing. It should expect to receive batched data and should itself return batched data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The arrays from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[None, FilteredData, np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[None, FilteredData, np.ndarray, List[np.ndarray]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward_batch(self,\ndata: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[None, FilteredData, np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform a batch of data.\n    This method will be invoked on batches of data during network postprocessing. It should expect to receive\n    batched data and should itself return batched data.\n    Args:\n        data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nif isinstance(data, list):\ndata = [elem for elem in map(list, zip(*data))]\nelse:\ndata = [elem for elem in data]\nresults = [self.forward(elem, state) for elem in data]\nif self.out_list:\nresults = [np.array(col) for col in [[row[i] for row in results] for i in range(len(results[0]))]]\nelse:\nresults = np.array(results)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.RemoveIf", "title": "<code>RemoveIf</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Operator which will remove a datapoint from the pipeline if the given criterion is satisfied.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., bool]</code> <p>A function taking any desired <code>inputs</code> and returning a boolean. If the return value is true, the current datapoint (or batch if using a batch dataset) will be removed and another will take its place.</p> required <code>replacement</code> <code>bool</code> <p>Whether to replace the filtered element with another (thus maintaining the number of steps in an epoch but potentially increasing data repetition) or else shortening the epoch by the number of filtered data points (fewer steps per epoch than expected, but no extra data repetition). Either way, the number of data points within an individual batch will remain the same. Even if <code>replacement</code> is true, data will not be repeated until all of the given epoch's data has been traversed (except for at most 1 batch of data which might not appear until after the re-shuffle has occurred).</p> <code>True</code> <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>@traceable()\nclass RemoveIf(NumpyOp):\n\"\"\"An Operator which will remove a datapoint from the pipeline if the given criterion is satisfied.\n    Args:\n        fn: A function taking any desired `inputs` and returning a boolean. If the return value is true, the current\n            datapoint (or batch if using a batch dataset) will be removed and another will take its place.\n        replacement: Whether to replace the filtered element with another (thus maintaining the number of steps in an\n            epoch but potentially increasing data repetition) or else shortening the epoch by the number of filtered\n            data points (fewer steps per epoch than expected, but no extra data repetition). Either way, the number of\n            data points within an individual batch will remain the same. Even if `replacement` is true, data will not be\n            repeated until all of the given epoch's data has been traversed (except for at most 1 batch of data which\n            might not appear until after the re-shuffle has occurred).\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nfn: Callable[..., bool],\nreplacement: bool = True,\ninputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode, ds_id=ds_id)\nself.filter_fn = fn\nself.in_list = True\nself.replacement = replacement\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Optional[FilteredData]:\nif self.filter_fn(*data):\nreturn FilteredData(replacement=self.replacement)\nreturn None\ndef forward_batch(self, data: Union[Tensor, List[Tensor]], state: Dict[str,\nAny]) -&gt; Optional[FilteredData]:\nreturn self.forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.forward_numpyop", "title": "<code>forward_numpyop</code>", "text": "<p>Call the forward function for list of NumpyOps, and modify the data dictionary in place.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>List[NumpyOp]</code> <p>A list of NumpyOps to execute.</p> required <code>data</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, ex. {\"mode\": \"train\"}. Must contain at least the mode.</p> required <code>batched</code> <code>Optional[str]</code> <p>Whether the <code>data</code> is batched or not. If it is batched, provide the string ('tf', 'torch', or 'np') indicating which type of tensors the batch contains.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward_numpyop(ops: List[NumpyOp],\ndata: MutableMapping[str, Any],\nstate: Dict[str, Any],\nbatched: Optional[str] = None) -&gt; Optional[FilteredData]:\n\"\"\"Call the forward function for list of NumpyOps, and modify the data dictionary in place.\n    Args:\n        ops: A list of NumpyOps to execute.\n        data: The data dictionary.\n        state: Information about the current execution context, ex. {\"mode\": \"train\"}. Must contain at least the mode.\n        batched: Whether the `data` is batched or not. If it is batched, provide the string ('tf', 'torch', or 'np')\n            indicating which type of tensors the batch contains.\n    \"\"\"\nif not ops:\n# Shortcut to prevent wasting time in to_tensor calls if there aren't any ops\nreturn None\nif batched:\n# Cast data to Numpy before performing batch forward\nfor key, val in data.items():\ndata[key] = to_tensor(val, target_type='np')\nfor op in ops:\nop_data = get_inputs_by_op(op, data, copy_on_write=op.in_place_edits)\ntry:\nop_data = op.forward_batch(op_data, state) if batched else op.forward(op_data, state)\nexcept ValueError as err:\nif err.args[0] == 'assignment destination is read-only':\n# If the numpy error text changes we'll need to make adjustments in the future\nop.in_place_edits = True\nop_data = get_inputs_by_op(op, data, copy_on_write=op.in_place_edits)\nop_data = op.forward_batch(op_data, state) if batched else op.forward(op_data, state)\nelse:\nraise err\nif isinstance(op_data, FilteredData):\nreturn op_data\nif isinstance(op, Delete):\nfor key in op.inputs:\ndel data[key]\nif op.outputs:\nwrite_outputs_by_op(op, data, op_data)\nif batched:\n# Cast data back to original tensor type after performing batch forward\nfor key, val in data.items():\ndata[key] = to_tensor(val, target_type=batched, shared_memory=True)\nreturn None\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/fuse.html", "title": "fuse", "text": ""}, {"location": "fastestimator/op/numpyop/meta/fuse.html#fastestimator.fastestimator.op.numpyop.meta.fuse.Fuse", "title": "<code>Fuse</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Run a sequence of NumpyOps as a single Op.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Union[NumpyOp, List[NumpyOp]]</code> <p>A sequence of NumpyOps to run. They must all share the same mode. It also doesn't support scheduled ops at the moment, though the Fuse itself may be scheduled.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code> or <code>ops</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\fuse.py</code> <pre><code>@traceable()\nclass Fuse(NumpyOp):\n\"\"\"Run a sequence of NumpyOps as a single Op.\n    Args:\n        ops: A sequence of NumpyOps to run. They must all share the same mode. It also doesn't support scheduled ops at\n            the moment, though the Fuse itself may be scheduled.\n    Raises:\n        ValueError: If `repeat` or `ops` are invalid.\n    \"\"\"\ndef __init__(self, ops: Union[NumpyOp, List[NumpyOp]]) -&gt; None:\nops = to_list(ops)\nif len(ops) &lt; 1:\nraise ValueError(\"Fuse requires at least one op\")\ninputs = []\noutputs = []\nmode = ops[0].mode\nds_id = ops[0].ds_id\nfor op in ops:\nif isinstance(op, Batch):\nraise ValueError(\"Cannot nest the Batch op inside of Fuse\")\nif op.mode != mode:\nraise ValueError(f\"All Fuse ops must share the same mode, but got {mode} and {op.mode}\")\nif op.ds_id != ds_id:\nraise ValueError(f\"All Fuse ops must share the same ds_id, but got {ds_id} and {op.ds_id}\")\nfor inp in op.inputs:\nif isinstance(op, Delete) and inp in outputs:\noutputs.remove(inp)\nelif inp not in inputs and inp not in outputs:\ninputs.append(inp)\nfor out in op.outputs:\nif out not in outputs:\noutputs.append(out)\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.ops = ops\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        Raises:\n            AttributeError: If ops don't have a 'set_rua_level' method.\n        \"\"\"\nfor op in self.ops:\nif hasattr(op, \"set_rua_level\") and inspect.ismethod(getattr(op, \"set_rua_level\")):\nop.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nop.__class__.__name__))\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nfiltered = forward_numpyop(self.ops, data, state)\nreturn filtered if filtered else [data[key] for key in self.outputs]\ndef forward_batch(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nfiltered = forward_numpyop(self.ops, data, state, batched=\"np\")\nreturn filtered if filtered else [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/fuse.html#fastestimator.fastestimator.op.numpyop.meta.fuse.Fuse.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If ops don't have a 'set_rua_level' method.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\fuse.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    Raises:\n        AttributeError: If ops don't have a 'set_rua_level' method.\n    \"\"\"\nfor op in self.ops:\nif hasattr(op, \"set_rua_level\") and inspect.ismethod(getattr(op, \"set_rua_level\")):\nop.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nop.__class__.__name__))\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html", "title": "one_of", "text": ""}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf", "title": "<code>OneOf</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform one of several possible NumpyOps.</p> <p>Parameters:</p> Name Type Description Default <code>*numpy_ops</code> <code>NumpyOp</code> <p>Ops to choose between with a specified (or uniform) probability.</p> <code>()</code> <code>probs</code> <code>Optional[List[float]]</code> <p>List of probabilities, must sum to 1. When None, the probabilities will be equally distributed.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>@traceable()\nclass OneOf(NumpyOp):\n\"\"\"Perform one of several possible NumpyOps.\n    Args:\n        *numpy_ops: Ops to choose between with a specified (or uniform) probability.\n        probs: List of probabilities, must sum to 1. When None, the probabilities will be equally distributed.\n    \"\"\"\ndef __init__(self, *numpy_ops: NumpyOp, probs: Optional[List[float]] = None) -&gt; None:\ninputs = numpy_ops[0].inputs\noutputs = numpy_ops[0].outputs\nmode = numpy_ops[0].mode\nds_id = numpy_ops[0].ds_id\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list = numpy_ops[0].in_list\nself.out_list = numpy_ops[0].out_list\nfor op in numpy_ops[1:]:\nassert not isinstance(op, Batch), \"Cannot nest the Batch op inside OneOf\"\nassert inputs == op.inputs, \"All ops within a OneOf must share the same inputs\"\nassert self.in_list == op.in_list, \"All ops within OneOf must share the same input configuration\"\nassert outputs == op.outputs, \"All ops within a OneOf must share the same outputs\"\nassert self.out_list == op.out_list, \"All ops within OneOf must share the same output configuration\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nassert ds_id == op.ds_id, \"All ops within a OneOf must share the same ds_id\"\nif probs:\nassert len(numpy_ops) == len(probs), \"The number of probabilities do not match with number of Operators\"\nassert abs(sum(probs) - 1) &lt; 1e-8, \"Probabilities must sum to 1\"\nself.ops = numpy_ops\nself.probs = probs\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        Raises:\n            AttributeError: If ops don't have a 'set_rua_level' method.\n        \"\"\"\nfor op in self.ops:\nif hasattr(op, \"set_rua_level\") and inspect.ismethod(getattr(op, \"set_rua_level\")):\nop.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nop.__class__.__name__))\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n        Args:\n            data: The information to be passed to one of the wrapped operators.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after application of one of the available numpyOps.\n        \"\"\"\nreturn np.random.choice(self.ops, p=self.probs).forward(data, state)\ndef forward_batch(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\nreturn np.random.choice(self.ops, p=self.probs).forward_batch(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf.forward", "title": "<code>forward</code>", "text": "<p>Execute a randomly selected op from the list of <code>numpy_ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The information to be passed to one of the wrapped operators.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after application of one of the available numpyOps.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n    Args:\n        data: The information to be passed to one of the wrapped operators.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after application of one of the available numpyOps.\n    \"\"\"\nreturn np.random.choice(self.ops, p=self.probs).forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef. This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If ops don't have a 'set_rua_level' method.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    Raises:\n        AttributeError: If ops don't have a 'set_rua_level' method.\n    \"\"\"\nfor op in self.ops:\nif hasattr(op, \"set_rua_level\") and inspect.ismethod(getattr(op, \"set_rua_level\")):\nop.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nop.__class__.__name__))\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/repeat.html", "title": "repeat", "text": ""}, {"location": "fastestimator/op/numpyop/meta/repeat.html#fastestimator.fastestimator.op.numpyop.meta.repeat.Repeat", "title": "<code>Repeat</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Repeat a NumpyOp several times in a row.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>NumpyOp</code> <p>A NumpyOp to be run one or more times in a row.</p> required <code>repeat</code> <code>Union[int, Callable[..., bool]]</code> <p>How many times to repeat the <code>op</code>. This can also be a function return, in which case the function input names will be matched to keys in the data dictionary, and the <code>op</code> will be repeated until the function evaluates to False. The function evaluation will happen at the end of a forward call, so the <code>op</code> will always be evaluated at least once.</p> <code>1</code> <code>max_iter</code> <code>Optional[int]</code> <p>A limit to how many iterations will be run (or None for no limit).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code>, <code>op</code>, or max_iter are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\repeat.py</code> <pre><code>@traceable()\nclass Repeat(NumpyOp):\n\"\"\"Repeat a NumpyOp several times in a row.\n    Args:\n        op: A NumpyOp to be run one or more times in a row.\n        repeat: How many times to repeat the `op`. This can also be a function return, in which case the function input\n            names will be matched to keys in the data dictionary, and the `op` will be repeated until the function\n            evaluates to False. The function evaluation will happen at the end of a forward call, so the `op` will\n            always be evaluated at least once.\n        max_iter: A limit to how many iterations will be run (or None for no limit).\n    Raises:\n        ValueError: If `repeat`, `op`, or max_iter are invalid.\n    \"\"\"\ndef __init__(self, op: NumpyOp, repeat: Union[int, Callable[..., bool]] = 1,\nmax_iter: Optional[int] = None) -&gt; None:\nif isinstance(op, Batch):\nraise ValueError(\"Cannot nest the Batch op inside Repeat\")\nself.repeat_inputs = []\nextra_reqs = []\nif max_iter is None:\nself.max_iter = max_iter\nelse:\nif max_iter &lt; 1:\nraise ValueError(f\"Repeat requires max_iter to be &gt;=1, but got {max_iter}\")\nself.max_iter = max_iter - 1  # -1 b/c the first invocation happens outside the while loop\nif isinstance(repeat, int):\nif repeat &lt; 1:\nraise ValueError(f\"Repeat requires repeat to be &gt;= 1, but got {repeat}\")\nif max_iter:\nraise ValueError(\"Do not set max_iter when repeat is an integer\")\nelse:\nself.repeat_inputs.extend(inspect.signature(repeat).parameters.keys())\nextra_reqs = list(set(self.repeat_inputs) - set(op.outputs))\nself.repeat = repeat\nsuper().__init__(inputs=op.inputs + extra_reqs, outputs=op.outputs, mode=op.mode, ds_id=op.ds_id)\nself.ops = [op]\n@property\ndef op(self) -&gt; NumpyOp:\nreturn self.ops[0]\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        Raises:\n            AttributeError: If the 'op' doesn't have a 'set_rua_level' method.\n        \"\"\"\nif hasattr(self.op, \"set_rua_level\") and inspect.ismethod(getattr(self.op, \"set_rua_level\")):\nself.op.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nself.op.__class__.__name__))\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Union[FilteredData, List[np.ndarray]]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nif isinstance(self.repeat, int):\nfor i in range(self.repeat):\nfiltered = forward_numpyop(self.ops, data, state)\nif filtered:\nreturn filtered\nelse:\nfiltered = forward_numpyop(self.ops, data, state)\nif filtered:\nreturn filtered\ni = 0\nwhile self.repeat(*[data[var_name] for var_name in self.repeat_inputs]):\nif self.max_iter and i &gt;= self.max_iter:\nbreak\nfiltered = forward_numpyop(self.ops, data, state)\nif filtered:\nreturn filtered\ni += 1\nreturn [data[key] for key in self.outputs]\ndef forward_batch(self,\ndata: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[FilteredData, np.ndarray, List[np.ndarray]]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nif isinstance(self.repeat, int):\nfor i in range(self.repeat):\nfiltered = forward_numpyop(self.ops, data, state, batched='np')\nif filtered:\nreturn filtered\nelse:\nfiltered = forward_numpyop(self.ops, data, state, batched='np')\nif filtered:\nreturn filtered\ni = 0\nwhile self.repeat(*[data[var_name] for var_name in self.repeat_inputs]):\nif self.max_iter and i &gt;= self.max_iter:\nbreak\nfiltered = forward_numpyop(self.ops, data, state, batched='np')\nif filtered:\nreturn filtered\ni += 1\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/repeat.html#fastestimator.fastestimator.op.numpyop.meta.repeat.Repeat.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the 'op' doesn't have a 'set_rua_level' method.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\repeat.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    Raises:\n        AttributeError: If the 'op' doesn't have a 'set_rua_level' method.\n    \"\"\"\nif hasattr(self.op, \"set_rua_level\") and inspect.ismethod(getattr(self.op, \"set_rua_level\")):\nself.op.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nself.op.__class__.__name__))\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html", "title": "sometimes", "text": ""}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes", "title": "<code>Sometimes</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform a NumpyOp with a given probability.</p> <p>Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking the Sometimes.</p> <p>Parameters:</p> Name Type Description Default <code>numpy_op</code> <code>NumpyOp</code> <p>The operator to be performed.</p> required <code>prob</code> <code>float</code> <p>The probability of execution, which should be in the range: [0-1).</p> <code>0.5</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>@traceable()\nclass Sometimes(NumpyOp):\n\"\"\"Perform a NumpyOp with a given probability.\n    Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data\n    dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but\n    Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before\n    invoking the Sometimes.\n    Args:\n        numpy_op: The operator to be performed.\n        prob: The probability of execution, which should be in the range: [0-1).\n    \"\"\"\ndef __init__(self, numpy_op: NumpyOp, prob: float = 0.5) -&gt; None:\n# We're going to try to collect any missing output keys from the data dictionary so that they don't get\n# overridden when Sometimes chooses not to execute.\ninps = set(numpy_op.inputs)\nouts = set(numpy_op.outputs)\nself.extra_inputs = list(outs - inps)  # Used by traceability\nself.inp_idx = len(numpy_op.inputs)\nsuper().__init__(inputs=numpy_op.inputs + self.extra_inputs,\noutputs=numpy_op.outputs,\nmode=numpy_op.mode,\nds_id=numpy_op.ds_id)\n# Note that in_list and out_list will always be true\nself.op = numpy_op\nif isinstance(numpy_op, Batch):\nraise ValueError(\"Cannot nest a Batch op inside of Sometimes\")\nself.prob = prob\ndef __getstate__(self) -&gt; Dict[str, Dict[Any, Any]]:\nreturn {'op': self.op.__getstate__() if hasattr(self.op, '__getstate__') else {}}\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        Raises:\n            AttributeError: If the 'op' doesn't have a 'set_rua_level' method.\n        \"\"\"\nif hasattr(self.op, \"set_rua_level\") and inspect.ismethod(getattr(self.op, \"set_rua_level\")):\nself.op.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nself.op.__class__.__name__))\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Union[FilteredData, List[np.ndarray]]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n        Args:\n            data: The information to be passed to the wrapped operator.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The original `data`, or the `data` after running it through the wrapped operator.\n        \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif isinstance(data, FilteredData):\nreturn data\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\ndef forward_batch(self,\ndata: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[FilteredData, List[np.ndarray]]:\nif self.prob &gt; np.random.uniform():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward_batch(data, state)\nif isinstance(data, FilteredData):\nreturn data\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes.forward", "title": "<code>forward</code>", "text": "<p>Execute the wrapped operator a certain fraction of the time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[np.ndarray]</code> <p>The information to be passed to the wrapped operator.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[FilteredData, List[np.ndarray]]</code> <p>The original <code>data</code>, or the <code>data</code> after running it through the wrapped operator.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; Union[FilteredData, List[np.ndarray]]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n    Args:\n        data: The information to be passed to the wrapped operator.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The original `data`, or the `data` after running it through the wrapped operator.\n    \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif isinstance(data, FilteredData):\nreturn data\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the 'op' doesn't have a 'set_rua_level' method.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    Raises:\n        AttributeError: If the 'op' doesn't have a 'set_rua_level' method.\n    \"\"\"\nif hasattr(self.op, \"set_rua_level\") and inspect.ismethod(getattr(self.op, \"set_rua_level\")):\nself.op.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nself.op.__class__.__name__))\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/affine.html", "title": "affine", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/affine.html#fastestimator.fastestimator.op.numpyop.multivariate.affine.Affine", "title": "<code>Affine</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Perform affine transformations on an image.</p> <p>Parameters:</p> Name Type Description Default <code>rotate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to rotate an image (in degrees). If a single value is given then images will be rotated by     a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated     by a value sampled from the range [a, b].</p> <code>0</code> <code>scale</code> <code>Union[float, Tuple[float, float]]</code> <p>How much to scale an image (in percentage). If a single value is given then all images will be scaled     by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled     based on a value drawn from the range [a,b].</p> <code>1.0</code> <code>shear</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to shear an image (in degrees). If a single value is given then all images will be sheared     on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will     be sheared on X and Y by two values randomly sampled from the range [a, b].</p> <code>0</code> <code>translate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to translate an image. If a single value is given then the translation extent will be     sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from     the range [a,b]. If integers are given then the translation will be in pixels. If a float then     it will be as a fraction of the image size.</p> <code>0</code> <code>border_handling</code> <code>Union[str, List[str]]</code> <p>What to do in order to fill newly created pixels. Options are 'constant', 'edge',     'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly     selected from the options in the list.</p> <code>'constant'</code> <code>fill_value</code> <code>Number</code> <p>What pixel value to insert when border_handling is 'constant'.</p> <code>0</code> <code>interpolation</code> <code>str</code> <p>What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',     'bilinear', 'bicubic', 'biquartic', and 'biquintic'.</p> <code>'bilinear'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\affine.py</code> <pre><code>@traceable()\nclass Affine(MultiVariateAlbumentation):\n\"\"\"Perform affine transformations on an image.\n    Args:\n        rotate: How much to rotate an image (in degrees). If a single value is given then images will be rotated by\n                a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated\n                by a value sampled from the range [a, b].\n        scale: How much to scale an image (in percentage). If a single value is given then all images will be scaled\n                by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled\n                based on a value drawn from the range [a,b].\n        shear: How much to shear an image (in degrees). If a single value is given then all images will be sheared\n                on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will\n                be sheared on X and Y by two values randomly sampled from the range [a, b].\n        translate: How much to translate an image. If a single value is given then the translation extent will be\n                sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from\n                the range [a,b]. If integers are given then the translation will be in pixels. If a float then\n                it will be as a fraction of the image size.\n        border_handling: What to do in order to fill newly created pixels. Options are 'constant', 'edge',\n                'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly\n                selected from the options in the list.\n        fill_value: What pixel value to insert when border_handling is 'constant'.\n        interpolation: What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',\n                'bilinear', 'bicubic', 'biquartic', and 'biquintic'.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nrotate: Union[Number, Tuple[Number, Number]] = 0,\nscale: Union[float, Tuple[float, float]] = 1.0,\nshear: Union[Number, Tuple[Number, Number]] = 0,\ntranslate: Union[Number, Tuple[Number, Number]] = 0,\nborder_handling: Union[str, List[str]] = \"constant\",\nfill_value: Number = 0,\ninterpolation: str = \"bilinear\",\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\norder = {'nearest_neighbor': 0, 'bilinear': 1, 'bicubic': 3, 'biquartic': 4, 'biquintic': 5}[interpolation]\nif isinstance(translate, int) or (isinstance(translate, Tuple) and isinstance(translate[0], int)):\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_px=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nelse:\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_percent=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nsuper().__init__(func,\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html", "title": "center_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.center_crop.CenterCrop", "title": "<code>CenterCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop the center of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32 (but uint8 is more efficient)</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\center_crop.py</code> <pre><code>@traceable()\nclass CenterCrop(MultiVariateAlbumentation):\n\"\"\"Crop the center of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32 (but uint8 is more efficient)\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CenterCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop.html", "title": "crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop.html#fastestimator.fastestimator.op.numpyop.multivariate.crop.Crop", "title": "<code>Crop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a region from the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>x_min</code> <code>int</code> <p>Minimum upper left x coordinate.</p> <code>0</code> <code>y_min</code> <code>int</code> <p>Minimum upper left y coordinate.</p> <code>0</code> <code>x_max</code> <code>int</code> <p>Maximum lower right x coordinate.</p> <code>1024</code> <code>y_max</code> <code>int</code> <p>Maximum lower right y coordinate.</p> <code>1024</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop.py</code> <pre><code>@traceable()\nclass Crop(MultiVariateAlbumentation):\n\"\"\"Crop a region from the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        x_min: Minimum upper left x coordinate.\n        y_min: Minimum upper left y coordinate.\n        x_max: Maximum lower right x coordinate.\n        y_max: Maximum lower right y coordinate.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nx_min: int = 0,\ny_min: int = 0,\nx_max: int = 1024,\ny_max: int = 1024,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CropAlb(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html", "title": "crop_non_empty_mask_if_exists", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html#fastestimator.fastestimator.op.numpyop.multivariate.crop_non_empty_mask_if_exists.CropNonEmptyMaskIfExists", "title": "<code>CropNonEmptyMaskIfExists</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop an area with mask if mask is non-empty, otherwise crop randomly.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Vertical size of crop in pixels.</p> required <code>width</code> <code>int</code> <p>Horizontal size of crop in pixels.</p> required <code>ignore_values</code> <code>Optional[List[int]]</code> <p>Values to ignore in mask, <code>0</code> values are always ignored (e.g. if background value is 5 set <code>ignore_values=[5]</code> to ignore).</p> <code>None</code> <code>ignore_channels</code> <code>Optional[List[int]]</code> <p>Channels to ignore in mask (e.g. if background is a first channel set <code>ignore_channels=[0]</code> to ignore).</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop_non_empty_mask_if_exists.py</code> <pre><code>@traceable()\nclass CropNonEmptyMaskIfExists(MultiVariateAlbumentation):\n\"\"\"Crop an area with mask if mask is non-empty, otherwise crop randomly.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Vertical size of crop in pixels.\n        width: Horizontal size of crop in pixels.\n        ignore_values: Values to ignore in mask, `0` values are always ignored (e.g. if background value is 5 set\n            `ignore_values=[5]` to ignore).\n        ignore_channels: Channels to ignore in mask (e.g. if background is a first channel set `ignore_channels=[0]` to\n            ignore).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nignore_values: Optional[List[int]] = None,\nignore_channels: Optional[List[int]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nCropNonEmptyMaskIfExistsAlb(height=height,\nwidth=width,\nignore_values=ignore_values,\nignore_channels=ignore_channels,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html", "title": "elastic_transform", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html#fastestimator.fastestimator.op.numpyop.multivariate.elastic_transform.ElasticTransform", "title": "<code>ElasticTransform</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Elastic deformation of images.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Scaling factor during point translation.</p> <code>34.0</code> <code>sigma</code> <code>float</code> <p>Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.</p> <code>4.0</code> <code>alpha_affine</code> <code>float</code> <p>The range will be (-alpha_affine, alpha_affine).</p> <code>50.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large (512x512) images.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\elastic_transform.py</code> <pre><code>@traceable()\nclass ElasticTransform(MultiVariateAlbumentation):\n\"\"\"Elastic deformation of images.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        alpha: Scaling factor during point translation.\n        sigma: Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.\n        alpha_affine: The range will be (-alpha_affine, alpha_affine).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        approximate: Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X\n            speedup on large (512x512) images.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nalpha: float = 34.0,\nsigma: float = 4.0,\nalpha_affine: float = 50.0,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\napproximate: bool = False,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nElasticTransformAlb(alpha=alpha,\nsigma=sigma,\nalpha_affine=alpha_affine,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\napproximate=approximate,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/flip.html", "title": "flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/flip.html#fastestimator.fastestimator.op.numpyop.multivariate.flip.Flip", "title": "<code>Flip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image either horizontally, vertically, or both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\flip.py</code> <pre><code>@traceable()\nclass Flip(MultiVariateAlbumentation):\n\"\"\"Flip an image either horizontally, vertically, or both.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html", "title": "grid_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.grid_distortion.GridDistortion", "title": "<code>GridDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Distort an image within a grid sub-division</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>count of grid cells on each side.</p> <code>5</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.3</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\grid_distortion.py</code> <pre><code>@traceable()\nclass GridDistortion(MultiVariateAlbumentation):\n\"\"\"Distort an image within a grid sub-division\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        num_steps: count of grid cells on each side.\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nnum_steps: int = 5,\ndistort_limit: Union[float, Tuple[float, float]] = 0.3,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nGridDistortionAlb(num_steps=num_steps,\ndistort_limit=distort_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html", "title": "horizontal_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.horizontal_flip.HorizontalFlip", "title": "<code>HorizontalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image horizontally.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\horizontal_flip.py</code> <pre><code>@traceable()\nclass HorizontalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image horizontally.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/iaa_crop_and_pad.html", "title": "iaa_crop_and_pad", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/iaa_crop_and_pad.html#fastestimator.fastestimator.op.numpyop.multivariate.iaa_crop_and_pad.IAACropAndPad", "title": "<code>IAACropAndPad</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop and pad images by pixel amounts or fractions of image sizes.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>px</code> <code>Union[None, int, Tuple[int, int]]</code> <p>he number of pixels to crop (negative values) or pad (positive values) on each side of the image. Either this or the parameter percent may be set, not both at the same time.</p> <code>None</code> <code>percent</code> <code>Union[None, float, Tuple[float, float]]</code> <p>The number of pixels to crop (negative values) or pad (positive values) on each side of the image given  as a fraction of the image height/width.</p> <code>None</code> <code>pad_mode</code> <code>Union[int, str]</code> <p>OpenCV border mode.</p> <code>'constant'</code> <code>pad_cval</code> <code>Union[int, Tuple[float], List[int]]</code> <p>The constant value to use if the pad mode is BORDER_CONSTANT.  If a tuple of two number s and at least  one of them is a float, then a random number will be uniformly sampled per image from the continuous  interval [a, b] and used as the value. If both number s are int s, the interval is discrete. If a list of  number, then a random value will be chosen from the elements of the list and used as the value.</p> <code>0</code> <code>keep_size</code> <code>bool</code> <p>Whether to keep the same size as original image.</p> <code>True</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\iaa_crop_and_pad.py</code> <pre><code>@traceable()\nclass IAACropAndPad(MultiVariateAlbumentation):\n\"\"\"Crop and pad images by pixel amounts or fractions of image sizes.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        px: he number of pixels to crop (negative values) or pad (positive values) on each side of the image.\n            Either this or the parameter percent may be set, not both at the same time.\n        percent: The number of pixels to crop (negative values) or pad (positive values) on each side of the image given\n             as a fraction of the image height/width.\n        pad_mode: OpenCV border mode.\n        pad_cval: The constant value to use if the pad mode is BORDER_CONSTANT.  If a tuple of two number s and at least\n             one of them is a float, then a random number will be uniformly sampled per image from the continuous\n             interval [a, b] and used as the value. If both number s are int s, the interval is discrete. If a list of\n             number, then a random value will be chosen from the elements of the list and used as the value.\n        keep_size: Whether to keep the same size as original image.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\npx: Union[None, int, Tuple[int, int]] = None,\npercent: Union[None, float, Tuple[float, float]] = None,\npad_mode: Union[int, str] = 'constant',\npad_cval: Union[int, Tuple[float], List[int]] = 0,\nkeep_size: bool = True,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nIAACropAndPadAlb(px=px,\npercent=percent,\npad_mode=pad_mode,\npad_cval=pad_cval,\nkeep_size=keep_size,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html", "title": "longest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.longest_max_size.LongestMaxSize", "title": "<code>LongestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\longest_max_size.py</code> <pre><code>@traceable()\nclass LongestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(LongestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html", "title": "mask_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html#fastestimator.fastestimator.op.numpyop.multivariate.mask_dropout.MaskDropout", "title": "<code>MaskDropout</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Zero out objects from an image + mask pair.</p> <p>An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask. The mask must be single-channel image, with zero values treated as background. The image can be any number of channels.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).     image_out: The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>max_objects</code> <code>Union[int, Tuple[int, int]]</code> <p>Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]</p> <code>1</code> <code>image_fill_value</code> <code>Union[int, float, str]</code> <p>Fill value to use when filling image. Can be 'inpaint' to apply in-painting (works only  for 3-channel images)</p> <code>0</code> <code>mask_fill_value</code> <code>Union[int, float]</code> <p>Fill value to use when filling mask.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\mask_dropout.py</code> <pre><code>@traceable()\nclass MaskDropout(MultiVariateAlbumentation):\n\"\"\"Zero out objects from an image + mask pair.\n    An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance\n    from mask. The mask must be single-channel image, with zero values treated as background. The image can be any\n    number of channels.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n                image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        max_objects: Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]\n        image_fill_value: Fill value to use when filling image.\n            Can be 'inpaint' to apply in-painting (works only  for 3-channel images)\n        mask_fill_value: Fill value to use when filling mask.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_objects: Union[int, Tuple[int, int]] = 1,\nimage_fill_value: Union[int, float, str] = 0,\nmask_fill_value: Union[int, float] = 0,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nMaskDropoutAlb(max_objects=max_objects,\nimage_fill_value=image_fill_value,\nmask_fill_value=mask_fill_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html", "title": "multivariate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html#fastestimator.fastestimator.op.numpyop.multivariate.multivariate.MultiVariateAlbumentation", "title": "<code>MultiVariateAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A base class for the DualTransform albumentation functions.</p> <p>DualTransforms are functions which apply simultaneously to images and corresponding information such as masks  and/or bounding boxes.</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>DualTransform</code> <p>An Albumentation function to be invoked.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If none of the various inputs such as <code>image_in</code> or <code>mask_in</code> are provided.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\multivariate.py</code> <pre><code>@traceable()\nclass MultiVariateAlbumentation(NumpyOp):\n\"\"\"A base class for the DualTransform albumentation functions.\n     DualTransforms are functions which apply simultaneously to images and corresponding information such as masks\n     and/or bounding boxes.\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Raises:\n        AssertionError: If none of the various inputs such as `image_in` or `mask_in` are provided.\n    \"\"\"\ndef __init__(self,\nfunc: DualTransform,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None,\nextra_in_keys: Optional[Dict[str, str]] = None,\nextra_out_keys: Optional[Dict[str, str]] = None):\nassert any((image_in, mask_in, masks_in, bbox_in, keypoints_in)), \"At least one input must be non-None\"\nimage_out = image_out or image_in\nmask_out = mask_out or mask_in\nmasks_out = masks_out or masks_in\nbbox_out = bbox_out or bbox_in\nkeypoints_out = keypoints_out or keypoints_in\nkeys = OrderedDict([(\"image\", image_in), (\"mask\", mask_in), (\"masks\", masks_in), (\"bboxes\", bbox_in),\n(\"keypoints\", keypoints_in)])\nif extra_in_keys:\nkeys.update(extra_in_keys)\nself.keys_in = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nkeys = OrderedDict([(\"image\", image_out), (\"mask\", mask_out), (\"masks\", masks_out), (\"bboxes\", bbox_out),\n(\"keypoints\", keypoints_out)])\nif extra_out_keys:\nkeys.update(extra_out_keys)\nself.keys_out = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nsuper().__init__(inputs=list(self.keys_in.values()),\noutputs=list(self.keys_out.values()),\nmode=mode,\nds_id=ds_id)\nif isinstance(bbox_params, str):\nbbox_params = BboxParams(bbox_params)\nif isinstance(keypoint_params, str):\nkeypoint_params = KeypointParams(keypoint_params)\nself.func = Compose(transforms=[func], bbox_params=bbox_params, keypoint_params=keypoint_params)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresult = self.func(**{k: v for k, v in zip(self.keys_in.keys(), data)})\nreturn [result[k] for k in self.keys_out.keys()]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html", "title": "optical_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.optical_distortion.OpticalDistortion", "title": "<code>OpticalDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Apply optical distortion to an image / mask.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.05</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit).</p> <code>0.05</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\optical_distortion.py</code> <pre><code>@traceable()\nclass OpticalDistortion(MultiVariateAlbumentation):\n\"\"\"Apply optical distortion to an image / mask.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        shift_limit: If shift_limit is a single float, the range will be (-shift_limit, shift_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ndistort_limit: Union[float, Tuple[float, float]] = 0.05,\nshift_limit: Union[float, Tuple[float, float]] = 0.05,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nOpticalDistortionAlb(distort_limit=distort_limit,\nshift_limit=shift_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html", "title": "pad_if_needed", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html#fastestimator.fastestimator.op.numpyop.multivariate.pad_if_needed.PadIfNeeded", "title": "<code>PadIfNeeded</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Pad the sides of an image / mask if size is less than a desired number.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_height</code> <code>int</code> <p>Minimal result image height.</p> <code>1024</code> <code>min_width</code> <code>int</code> <p>Minimal result image width.</p> <code>1024</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>cv2.BORDER_REFLECT_101</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value for mask if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\pad_if_needed.py</code> <pre><code>@traceable()\nclass PadIfNeeded(MultiVariateAlbumentation):\n\"\"\"Pad the sides of an image / mask if size is less than a desired number.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_height: Minimal result image height.\n        min_width: Minimal result image width.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_height: int = 1024,\nmin_width: int = 1024,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nPadIfNeededAlb(min_height=min_height,\nmin_width=min_width,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html", "title": "random_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop.RandomCrop", "title": "<code>RandomCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop.py</code> <pre><code>@traceable()\nclass RandomCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html", "title": "random_crop_near_bbox", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop_near_bbox.RandomCropNearBBox", "title": "<code>RandomCropNearBBox</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop bbox from an image with random shift by x,y coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>cropping_bbox_in</code> <code>str</code> <p>The key of the cropping box, in [x1, y1, x2, y2] format.</p> required <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_part_shift</code> <code>float</code> <p>Float value in the range (0.0, 1.0).</p> <code>0.3</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop_near_bbox.py</code> <pre><code>@traceable()\nclass RandomCropNearBBox(MultiVariateAlbumentation):\n\"\"\"Crop bbox from an image with random shift by x,y coordinates.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        cropping_bbox_in: The key of the cropping box, in [x1, y1, x2, y2] format.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_part_shift: Float value in the range (0.0, 1.0).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ncropping_bbox_in: str,\nmax_part_shift: float = 0.3,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropNearBBoxAlb(max_part_shift=max_part_shift, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id,\nextra_in_keys={\"cropping_bbox\": cropping_bbox_in})\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html", "title": "random_grid_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html#fastestimator.fastestimator.op.numpyop.multivariate.random_grid_shuffle.RandomGridShuffle", "title": "<code>RandomGridShuffle</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Divide an image into a grid and randomly shuffle the grid's cells.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>grid</code> <code>Tuple[int, int]</code> <p>size of grid for splitting image (height, width).</p> <code>(3, 3)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_grid_shuffle.py</code> <pre><code>@traceable()\nclass RandomGridShuffle(MultiVariateAlbumentation):\n\"\"\"Divide an image into a grid and randomly shuffle the grid's cells.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        grid: size of grid for splitting image (height, width).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ngrid: Tuple[int, int] = (3, 3),\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(RandomGridShuffleAlb(grid=grid, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html", "title": "random_resized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_resized_crop.RandomResizedCrop", "title": "<code>RandomResizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>scale</code> <code>Tuple[float, float]</code> <p>Range of size of the origin size cropped.</p> <code>(0.08, 1.0)</code> <code>ratio</code> <code>Tuple[float, float]</code> <p>Range of aspect ratio of the origin aspect ratio cropped.</p> <code>(0.75, 4 / 3)</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_resized_crop.py</code> <pre><code>@traceable()\nclass RandomResizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        scale: Range of size of the origin size cropped.\n        ratio: Range of aspect ratio of the origin aspect ratio cropped.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nscale: Tuple[float, float] = (0.08, 1.0),\nratio: Tuple[float, float] = (0.75, 4 / 3),\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomResizedCropAlb(height=height,\nwidth=width,\nscale=scale,\nratio=ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html", "title": "random_rotate_90", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html#fastestimator.fastestimator.op.numpyop.multivariate.random_rotate_90.RandomRotate90", "title": "<code>RandomRotate90</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate a given input randomly by 90 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_rotate_90.py</code> <pre><code>@traceable()\nclass RandomRotate90(MultiVariateAlbumentation):\n\"\"\"Rotate a given input randomly by 90 degrees.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RotateAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html", "title": "random_scale", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html#fastestimator.fastestimator.op.numpyop.multivariate.random_scale.RandomScale", "title": "<code>RandomScale</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit).</p> <code>0.1</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_scale.py</code> <pre><code>@traceable()\nclass RandomScale(MultiVariateAlbumentation):\n\"\"\"Randomly resize the input. Output image size is different from the input image size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (1 - scale_limit, 1 + scale_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomScaleAlb(scale_limit=scale_limit, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html", "title": "random_sized_bbox_safe_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_bbox_safe_crop.RandomSizedBBoxSafeCrop", "title": "<code>RandomSizedBBoxSafeCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size without loss of bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>erosion_rate</code> <code>float</code> <p>Erosion rate applied on input image height before crop.</p> <code>0.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_bbox_safe_crop.py</code> <pre><code>@traceable()\nclass RandomSizedBBoxSafeCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size without loss of bboxes.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        erosion_rate: Erosion rate applied on input image height before crop.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nerosion_rate: float = 0.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None):\nsuper().__init__(\nRandomSizedBBoxSafeCropAlb(height=height,\nwidth=width,\nerosion_rate=erosion_rate,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=None,\nbbox_params=bbox_params,\nkeypoint_params=None,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html", "title": "random_sized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_crop.RandomSizedCrop", "title": "<code>RandomSizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_max_height</code> <code>Tuple[int, int]</code> <p>Crop size limits.</p> required <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>w2h_ratio</code> <code>float</code> <p>Aspect ratio of crop.</p> <code>1.0</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_crop.py</code> <pre><code>@traceable()\nclass RandomSizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_max_height: Crop size limits.\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        w2h_ratio: Aspect ratio of crop.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_max_height: Tuple[int, int],\nheight: int,\nwidth: int,\nw2h_ratio: float = 1.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomSizedCropAlb(min_max_height=min_max_height,\nheight=height,\nwidth=width,\nw2h_ratio=w2h_ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html", "title": "read_mat", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html#fastestimator.fastestimator.op.numpyop.multivariate.read_mat.ReadMat", "title": "<code>ReadMat</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading .mat files from disk.</p> <p>This expects every sample to have a separate .mat file.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>str</code> <p>Dictionary key that contains the .mat path.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Keys to output from the mat file.</p> required <code>mat_keys</code> <code>Union[None, str, Iterable[str]]</code> <p>(Optional) Keys to read from the .mat file. Defaults to <code>outputs</code>, but to re-name keys you can provide the original name here and the new name in <code>outputs</code>.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given filepath.</p> <code>''</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\read_mat.py</code> <pre><code>@traceable()\nclass ReadMat(NumpyOp):\n\"\"\"A class for reading .mat files from disk.\n    This expects every sample to have a separate .mat file.\n    Args:\n        inputs: Dictionary key that contains the .mat path.\n        outputs: Keys to output from the mat file.\n        mat_keys: (Optional) Keys to read from the .mat file. Defaults to `outputs`, but to re-name keys you can provide\n            the original name here and the new name in `outputs`.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        parent_path: Parent path that will be prepended to a given filepath.\n    \"\"\"\ndef __init__(self,\ninputs: str,\noutputs: Union[str, Iterable[str]],\nmat_keys: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\"):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.parent_path = parent_path\nif mat_keys is None:\nself.mat_keys = self.outputs\nelse:\nself.mat_keys = to_list(mat_keys)\nself.out_list = True\nif isinstance(self.mat_keys, List) and isinstance(self.outputs, List):\nassert len(self.mat_keys) == len(self.outputs), \"keys and Output lengths must match\"\ndef forward(self, data: str, state: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\ninput_path = os.path.normpath(os.path.join(self.parent_path, data))\ndata = loadmat(input_path)\nresults = [data[key] for key in self.mat_keys]\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/resize.html", "title": "resize", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/resize.html#fastestimator.fastestimator.op.numpyop.multivariate.resize.Resize", "title": "<code>Resize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Desired height of the output.</p> required <code>width</code> <code>int</code> <p>Desired width of the output.</p> required <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\resize.py</code> <pre><code>@traceable()\nclass Resize(MultiVariateAlbumentation):\n\"\"\"Resize the input to the given height and width.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Desired height of the output.\n        width: Desired width of the output.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(ResizeAlb(height=height, width=width, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html", "title": "rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.rotate.Rotate", "title": "<code>Rotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit).</p> <code>90</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\rotate.py</code> <pre><code>@traceable()\nclass Rotate(MultiVariateAlbumentation):\n\"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        limit: Range from which a random angle is picked. If limit is a single int an angle is picked from\n            (-limit, limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nlimit: Union[int, Tuple[int, int]] = 90,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRotateAlb(limit=limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html", "title": "shift_scale_rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.shift_scale_rotate.ShiftScaleRotate", "title": "<code>ShiftScaleRotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].</p> <code>0.0625</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit).</p> <code>0.1</code> <code>rotate_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit).</p> <code>45</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\shift_scale_rotate.py</code> <pre><code>@traceable()\nclass ShiftScaleRotate(MultiVariateAlbumentation):\n\"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        shift_limit: Shift factor range for both height and width. If shift_limit is a single float value, the range\n            will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (-scale_limit, scale_limit).\n        rotate_limit: Rotation range. If rotate_limit is a single int value, the range will be\n            (-rotate_limit, rotate_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nshift_limit: Union[float, Tuple[float, float]] = 0.0625,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\nrotate_limit: Union[int, Tuple[int, int]] = 45,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nShiftScaleRotateAlb(shift_limit=shift_limit,\nscale_limit=scale_limit,\nrotate_limit=rotate_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html", "title": "smallest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.smallest_max_size.SmallestMaxSize", "title": "<code>SmallestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of smallest side of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\smallest_max_size.py</code> <pre><code>@traceable()\nclass SmallestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of smallest side of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(SmallestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html", "title": "transpose", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html#fastestimator.fastestimator.op.numpyop.multivariate.transpose.Transpose", "title": "<code>Transpose</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\transpose.py</code> <pre><code>@traceable()\nclass Transpose(MultiVariateAlbumentation):\n\"\"\"Transpose the input by swapping rows and columns.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(TransposeAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html", "title": "vertical_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.vertical_flip.VerticalFlip", "title": "<code>VerticalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image vertically (over x-axis).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\vertical_flip.py</code> <pre><code>@traceable()\nclass VerticalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image vertically (over x-axis).\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(VerticalFlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/autocontrast.html", "title": "autocontrast", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/autocontrast.html#fastestimator.fastestimator.op.numpyop.univariate.autocontrast.AutoContrast", "title": "<code>AutoContrast</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Adjust image contrast automatically.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\autocontrast.py</code> <pre><code>@traceable()\nclass AutoContrast(NumpyOp):\n\"\"\"Adjust image contrast automatically.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by RUA Op to adjust the augmentation intensity.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [AutoContrast._apply_autocontrast(elem) for elem in data]\n@staticmethod\ndef _apply_autocontrast(data: np.ndarray) -&gt; np.ndarray:\nim = Image.fromarray(data)\nim = ImageOps.autocontrast(im)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/autocontrast.html#fastestimator.fastestimator.op.numpyop.univariate.autocontrast.AutoContrast.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>A method which will be invoked by RUA Op to adjust the augmentation intensity.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\autocontrast.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by RUA Op to adjust the augmentation intensity.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/binarize.html", "title": "binarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/binarize.html#fastestimator.fastestimator.op.numpyop.univariate.binarize.Binarize", "title": "<code>Binarize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Binarization threshold.</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\binarize.py</code> <pre><code>@traceable()\nclass Binarize(NumpyOp):\n\"\"\"Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.\n    Args:\n        threshold: Binarization threshold.\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nthreshold: float,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.threshold = threshold\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [(dat &gt;= self.threshold).astype(np.float32) for dat in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/blur.html", "title": "blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/blur.html#fastestimator.fastestimator.op.numpyop.univariate.blur.Blur", "title": "<code>Blur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a randomly-sized kernel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\blur.py</code> <pre><code>@traceable()\nclass Blur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a randomly-sized kernel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(BlurAlb(blur_limit=blur_limit, always_apply=True), inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/brightness.html", "title": "brightness", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/brightness.html#fastestimator.fastestimator.op.numpyop.univariate.brightness.Brightness", "title": "<code>Brightness</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly change the brightness of an image.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>limit</code> <code>float</code> <p>Factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). A factor of 0.0 gives a black image and a factor of 1.0 gives the original image.</p> <code>0.54</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\brightness.py</code> <pre><code>@traceable()\nclass Brightness(NumpyOp):\n\"\"\"Randomly change the brightness of an image.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        limit: Factor range for changing brightness. If limit is a single float, the range will be (-limit, limit).\n            A factor of 0.0 gives a black image and a factor of 1.0 gives the original image.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nlimit: float = 0.54):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.limit = param_to_range(limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = 1.0 + random.uniform(self.limit[0], self.limit[1])\nreturn [Brightness._apply_brightness(elem, factor) for elem in data]\n@staticmethod\ndef _apply_brightness(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nim = ImageEnhance.Brightness(im).enhance(factor)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/brightness.html#fastestimator.fastestimator.op.numpyop.univariate.brightness.Brightness.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\brightness.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/calibate.html", "title": "calibate", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/calibate.html#fastestimator.fastestimator.op.numpyop.univariate.calibate.Calibrate", "title": "<code>Calibrate</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Calibrate model predictions using a given calibration function.</p> <p>This is often used in conjunction with the PBMCalibrator trace. It should be placed in the fe.Network postprocessing op list.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of predictions to be calibrated.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the calibrated predictions.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('test', 'infer')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>calibration_fn</code> <code>Union[str, Callable[[np.ndarray], np.ndarray]]</code> <p>The path to a dill-pickled calibration function, or an in-memory calibration function to apply. If a path is provided, it will be lazy-loaded and so the saved file does not need to exist already when training begins.</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\calibate.py</code> <pre><code>@traceable()\nclass Calibrate(NumpyOp):\n\"\"\"Calibrate model predictions using a given calibration function.\n    This is often used in conjunction with the PBMCalibrator trace. It should be placed in the fe.Network postprocessing\n    op list.\n    Args:\n        inputs: Key(s) of predictions to be calibrated.\n        outputs: Key(s) into which to write the calibrated predictions.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        calibration_fn: The path to a dill-pickled calibration function, or an in-memory calibration function to apply.\n            If a path is provided, it will be lazy-loaded and so the saved file does not need to exist already when\n            training begins.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\ncalibration_fn: Union[str, Callable[[np.ndarray], np.ndarray]],\nmode: Union[None, str, Iterable[str]] = ('test', 'infer'),\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nif isinstance(calibration_fn, str):\ncalibration_fn = os.path.abspath(os.path.normpath(calibration_fn))\nself.calibration_fn = calibration_fn\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [\nnp.squeeze(result, axis=0)\nfor result in self.forward_batch([np.expand_dims(elem, axis=0) for elem in data], state)\n]\ndef forward_batch(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nif isinstance(self.calibration_fn, str):\nif 'warmup' in state and state['warmup']:\n# Don't attempt to load the calibration_fn during warmup\nreturn data\nwith open(self.calibration_fn, 'rb') as f:\nnotice = f\"FastEstimator-Calibrate: calibration function loaded from {self.calibration_fn}\"\nself.calibration_fn = dill.load(f)\nprint(notice)\nreturn [self.calibration_fn(to_number(elem)) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html", "title": "channel_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.channel_dropout.ChannelDropout", "title": "<code>ChannelDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly drop channels from the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>channel_drop_range</code> <code>Tuple[int, int]</code> <p>Range from which we choose the number of channels to drop.</p> <code>(1, 1)</code> <code>fill_value</code> <code>Union[int, float]</code> <p>Pixel values for the dropped channel.</p> <code>0</code> Image types <p>int8, uint16, unit32, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_dropout.py</code> <pre><code>@traceable()\nclass ChannelDropout(ImageOnlyAlbumentation):\n\"\"\"Randomly drop channels from the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        channel_drop_range: Range from which we choose the number of channels to drop.\n        fill_value: Pixel values for the dropped channel.\n    Image types:\n        int8, uint16, unit32, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nchannel_drop_range: Tuple[int, int] = (1, 1),\nfill_value: Union[int, float] = 0):\nsuper().__init__(\nChannelDropoutAlb(channel_drop_range=channel_drop_range, fill_value=fill_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html", "title": "channel_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html#fastestimator.fastestimator.op.numpyop.univariate.channel_shuffle.ChannelShuffle", "title": "<code>ChannelShuffle</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly rearrange channels of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>int8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_shuffle.py</code> <pre><code>@traceable()\nclass ChannelShuffle(ImageOnlyAlbumentation):\n\"\"\"Randomly rearrange channels of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        int8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ChannelShuffleAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html", "title": "channel_transpose", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html#fastestimator.fastestimator.op.numpyop.univariate.channel_transpose.ChannelTranspose", "title": "<code>ChannelTranspose</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transpose the data (for example to make it channel-width-height instead of width-height-channel).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>axes</code> <code>List[int]</code> <p>The permutation order.</p> <code>(2, 0, 1)</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_transpose.py</code> <pre><code>@traceable()\nclass ChannelTranspose(NumpyOp):\n\"\"\"Transpose the data (for example to make it channel-width-height instead of width-height-channel).\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        axes: The permutation order.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\naxes: List[int] = (2, 0, 1)):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.axes = axes\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.transpose(elem, self.axes) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/clahe.html", "title": "clahe", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/clahe.html#fastestimator.fastestimator.op.numpyop.univariate.clahe.CLAHE", "title": "<code>CLAHE</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply contrast limited adaptive histogram equalization to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>clip_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit).</p> <code>4.0</code> <code>tile_grid_size</code> <code>Tuple[int, int]</code> <p>size of grid for histogram equalization.</p> <code>(8, 8)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\clahe.py</code> <pre><code>@traceable()\nclass CLAHE(ImageOnlyAlbumentation):\n\"\"\"Apply contrast limited adaptive histogram equalization to the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        clip_limit: upper threshold value for contrast limiting. If clip_limit is a single float value, the range will\n            be (1, clip_limit).\n        tile_grid_size: size of grid for histogram equalization.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nclip_limit: Union[float, Tuple[float, float]] = 4.0,\ntile_grid_size: Tuple[int, int] = (8, 8)):\nsuper().__init__(CLAHEAlb(clip_limit=clip_limit, tile_grid_size=tile_grid_size, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html", "title": "coarse_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.coarse_dropout.CoarseDropout", "title": "<code>CoarseDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Drop rectangular regions from an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>max_holes</code> <code>int</code> <p>Maximum number of regions to zero out.</p> <code>8</code> <code>max_height</code> <code>int</code> <p>Maximum height of the hole.</p> <code>8</code> <code>max_width</code> <code>int</code> <p>Maximum width of the hole.</p> <code>8</code> <code>min_holes</code> <code>Optional[int]</code> <p>Minimum number of regions to zero out. If <code>None</code>, <code>min_holes</code> is set to <code>max_holes</code>.</p> <code>None</code> <code>min_height</code> <code>Optional[int]</code> <p>Minimum height of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_height</code>.</p> <code>None</code> <code>min_width</code> <code>Optional[int]</code> <p>Minimum width of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_width</code>.</p> <code>None</code> <code>fill_value</code> <code>Union[int, float, List[int], List[float]]</code> <p>value for dropped pixels.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\coarse_dropout.py</code> <pre><code>@traceable()\nclass CoarseDropout(ImageOnlyAlbumentation):\n\"\"\"Drop rectangular regions from an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        max_holes: Maximum number of regions to zero out.\n        max_height: Maximum height of the hole.\n        max_width: Maximum width of the hole.\n        min_holes: Minimum number of regions to zero out. If `None`, `min_holes` is set to `max_holes`.\n        min_height: Minimum height of the hole. If `None`, `min_height` is set to `max_height`.\n        min_width: Minimum width of the hole. If `None`, `min_height` is set to `max_width`.\n        fill_value: value for dropped pixels.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmax_holes: int = 8,\nmax_height: int = 8,\nmax_width: int = 8,\nmin_holes: Optional[int] = None,\nmin_height: Optional[int] = None,\nmin_width: Optional[int] = None,\nfill_value: Union[int, float, List[int], List[float]] = 0):\nsuper().__init__(\nCoarseDropoutAlb(max_holes=max_holes,\nmax_height=max_height,\nmax_width=max_width,\nmin_holes=min_holes,\nmin_height=min_height,\nmin_width=min_width,\nfill_value=fill_value,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/color.html", "title": "color", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/color.html#fastestimator.fastestimator.op.numpyop.univariate.color.Color", "title": "<code>Color</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly change the color balance of an image.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>limit</code> <code>float</code> <p>Factor range for changing color balance. If limit is a single float, the range will be (-limit, limit). A factor of 0.0 gives a black and white image and a factor of 1.0 gives the original image.</p> <code>0.54</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\color.py</code> <pre><code>@traceable()\nclass Color(NumpyOp):\n\"\"\"Randomly change the color balance of an image.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        limit: Factor range for changing color balance. If limit is a single float, the range will be (-limit, limit).\n            A factor of 0.0 gives a black and white image and a factor of 1.0 gives the original image.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nlimit: float = 0.54):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.limit = param_to_range(limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = 1.0 + random.uniform(self.limit[0], self.limit[1])\nreturn [Color._apply_color(elem, factor) for elem in data]\n@staticmethod\ndef _apply_color(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nim = ImageEnhance.Color(im).enhance(factor)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/color.html#fastestimator.fastestimator.op.numpyop.univariate.color.Color.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\color.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/color_jitter.html", "title": "color_jitter", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/color_jitter.html#fastestimator.fastestimator.op.numpyop.univariate.color_jitter.ColorJitter", "title": "<code>ColorJitter</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly changes the brightness, contrast, saturation and hue of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>brightness</code> <code>Union[float, Tuple[float]]</code> <p>How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.</p> <code>0.2</code> <code>contrast</code> <code>Union[float, Tuple[float]]</code> <p>How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers.</p> <code>0.2</code> <code>saturation</code> <code>Union[float, Tuple[float]]</code> <p>How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers.</p> <code>0.2</code> <code>hue</code> <code>Union[float, Tuple[float]]</code> <p>How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0 &lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5.</p> <code>0.2</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\color_jitter.py</code> <pre><code>class ColorJitter(ImageOnlyAlbumentation):\n\"\"\"Randomly changes the brightness, contrast, saturation and hue of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        brightness: How much to jitter brightness. brightness_factor is chosen uniformly from\n            [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers.\n        contrast: How much to jitter contrast. contrast_factor is chosen uniformly from\n            [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers.\n        saturation: How much to jitter saturation. saturation_factor is chosen uniformly from\n            [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers.\n        hue: How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n            Should have 0 &lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nbrightness: Union[float, Tuple[float]] = 0.2,\ncontrast: Union[float, Tuple[float]] = 0.2,\nsaturation: Union[float, Tuple[float]] = 0.2,\nhue: Union[float, Tuple[float]] = 0.2):\nsuper().__init__(\nColorJitterAlb(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/contrast.html", "title": "contrast", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/contrast.html#fastestimator.fastestimator.op.numpyop.univariate.contrast.Contrast", "title": "<code>Contrast</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly change the contrast of an image.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>limit</code> <code>float</code> <p>Factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). A factor of 0.0 gives a solid grey image and a factor of 1.0 gives the original image.</p> <code>0.54</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\contrast.py</code> <pre><code>@traceable()\nclass Contrast(NumpyOp):\n\"\"\"Randomly change the contrast of an image.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        limit: Factor range for changing contrast. If limit is a single float, the range will be (-limit, limit).\n            A factor of 0.0 gives a solid grey image and a factor of 1.0 gives the original image.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nlimit: float = 0.54):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.limit = param_to_range(limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = 1.0 + random.uniform(self.limit[0], self.limit[1])\nreturn [Contrast._apply_contrast(elem, factor) for elem in data]\n@staticmethod\ndef _apply_contrast(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nim = ImageEnhance.Contrast(im).enhance(factor)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/contrast.html#fastestimator.fastestimator.op.numpyop.univariate.contrast.Contrast.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\contrast.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/downscale.html", "title": "downscale", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/downscale.html#fastestimator.fastestimator.op.numpyop.univariate.downscale.Downscale", "title": "<code>Downscale</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease image quality by downscaling and then upscaling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>scale_min</code> <code>float</code> <p>Lower bound on the image scale. Should be &lt; 1.</p> <code>0.25</code> <code>scale_max</code> <code>float</code> <p>Upper bound on the image scale. Should be &gt;= scale_min.</p> <code>0.25</code> <code>interpolation</code> <code>int</code> <p>cv2 interpolation method.</p> <code>cv2.INTER_NEAREST</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\downscale.py</code> <pre><code>@traceable()\nclass Downscale(ImageOnlyAlbumentation):\n\"\"\"Decrease image quality by downscaling and then upscaling.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        scale_min: Lower bound on the image scale. Should be &lt; 1.\n        scale_max:  Upper bound on the image scale. Should be &gt;= scale_min.\n        interpolation: cv2 interpolation method.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nscale_min: float = 0.25,\nscale_max: float = 0.25,\ninterpolation: int = cv2.INTER_NEAREST):\nsuper().__init__(\nDownscaleAlb(scale_min=scale_min, scale_max=scale_max, interpolation=interpolation, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/equalize.html", "title": "equalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/equalize.html#fastestimator.fastestimator.op.numpyop.univariate.equalize.Equalize", "title": "<code>Equalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Equalize the image histogram.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>eq_mode</code> <code>str</code> <p>{'cv', 'pil'}. Use OpenCV or Pillow equalization method.</p> <code>'cv'</code> <code>by_channels</code> <code>bool</code> <p>If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by <code>Y</code> channel.</p> <code>True</code> <code>mask</code> <code>Union[None, np.ndarray]</code> <p>If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel array. Function signature must include <code>image</code> argument.</p> <code>None</code> <code>mask_params</code> <code>List[str]</code> <p>Params for mask function.</p> <code>()</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\equalize.py</code> <pre><code>@traceable()\nclass Equalize(ImageOnlyAlbumentation):\n\"\"\"Equalize the image histogram.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        eq_mode: {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels: If True, use equalization by channels separately, else convert image to YCbCr representation and\n            use equalization by `Y` channel.\n        mask: If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel\n            array. Function signature must include `image` argument.\n        mask_params: Params for mask function.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\neq_mode: str = \"cv\",\nby_channels: bool = True,\nmask: Union[None, np.ndarray] = None,\nmask_params: List[str] = ()):\nsuper().__init__(\nEqualizeAlb(mode=eq_mode, by_channels=by_channels, mask=mask, mask_params=mask_params, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html", "title": "expand_dims", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html#fastestimator.fastestimator.op.numpyop.univariate.expand_dims.ExpandDims", "title": "<code>ExpandDims</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Expand the dimension of inputs by inserting a new axis to the specified axis position.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of inputs to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified inputs.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis to expand.</p> <code>-1</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\expand_dims.py</code> <pre><code>@traceable()\nclass ExpandDims(NumpyOp):\n\"\"\"Expand the dimension of inputs by inserting a new axis to the specified axis position.\n    Args:\n        inputs: Key(s) of inputs to be modified.\n        outputs: Key(s) into which to write the modified inputs.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        axis: The axis to expand.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\naxis: int = -1):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.expand_dims(elem, self.axis) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/from_float.html", "title": "from_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/from_float.html#fastestimator.fastestimator.op.numpyop.univariate.from_float.FromFloat", "title": "<code>FromFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Takes an input float image in range [0, 1.0] and then multiplies by <code>max_value</code> to get an int image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the multiplier. If None it will be inferred by dtype.</p> <code>None</code> <code>dtype</code> <code>Union[str, np.dtype]</code> <p>The data type to cast the output as.</p> <code>'uint16'</code> Image types <p>float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\from_float.py</code> <pre><code>@traceable()\nclass FromFloat(ImageOnlyAlbumentation):\n\"\"\"Takes an input float image in range [0, 1.0] and then multiplies by `max_value` to get an int image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        max_value: The maximum value to serve as the multiplier. If None it will be inferred by dtype.\n        dtype: The data type to cast the output as.\n    Image types:\n        float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None,\ndtype: Union[str, np.dtype] = \"uint16\"):\nsuper().__init__(FromFloatAlb(max_value=max_value, dtype=dtype, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html", "title": "gaussian_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_blur.GaussianBlur", "title": "<code>GaussianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a Gaussian filter of random kernel size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).</p> <code>7</code> <code>sigma_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Gaussian kernel standard deviation. Must be greater in range [0, inf). If set single value sigma_limit will be in range (0, sigma_limit). If set to 0 sigma will be computed as sigma = 0.3((ksize-1)0.5 - 1)</p> <code>0.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_blur.py</code> <pre><code>@traceable()\nclass GaussianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a Gaussian filter of random kernel size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        blur_limit: Maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).\n        sigma_limit: Gaussian kernel standard deviation. Must be greater in range [0, inf). If set single value\n            sigma_limit will be in range (0, sigma_limit). If set to 0 sigma will be computed as sigma =\n            0.3*((ksize-1)*0.5 - 1)\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7,\nsigma_limit: Union[float, Tuple[float, float]] = 0.0):\nsuper().__init__(GaussianBlurAlb(blur_limit=blur_limit, always_apply=True, sigma_limit=sigma_limit),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html", "title": "gaussian_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_noise.GaussianNoise", "title": "<code>GaussianNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply gaussian noise to the image.</p> <p>WARNING: This assumes that floating point images are in the range [0,1] and will trim the output to that range. If your image is in a range like [-0.5, 0.5] then you do not want to use this Op.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>var_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).</p> <code>(10.0, 50.0)</code> <code>mean</code> <code>float</code> <p>Mean of the noise.</p> <code>0.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_noise.py</code> <pre><code>@traceable()\nclass GaussianNoise(ImageOnlyAlbumentation):\n\"\"\"Apply gaussian noise to the image.\n    WARNING: This assumes that floating point images are in the range [0,1] and will trim the output to that range. If\n    your image is in a range like [-0.5, 0.5] then you do not want to use this Op.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        var_limit: Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).\n        mean: Mean of the noise.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nvar_limit: Union[float, Tuple[float, float]] = (10.0, 50.0),\nmean: float = 0.0):\nsuper().__init__(GaussNoiseAlb(var_limit=var_limit, mean=mean, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/hadamard.html", "title": "hadamard", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/hadamard.html#fastestimator.fastestimator.op.numpyop.univariate.hadamard.Hadamard", "title": "<code>Hadamard</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert integer labels into hadamard code representations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor(s) to be converted.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor(s) in hadamard code representation.</p> required <code>n_classes</code> <code>int</code> <p>How many classes are there in the inputs.</p> required <code>code_length</code> <code>Optional[int]</code> <p>What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unequal number of <code>inputs</code> and <code>outputs</code> are provided, or if <code>code_length</code> is invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\hadamard.py</code> <pre><code>@traceable()\nclass Hadamard(NumpyOp):\n\"\"\"Convert integer labels into hadamard code representations.\n    Args:\n        inputs: Key of the input tensor(s) to be converted.\n        outputs: Key of the output tensor(s) in hadamard code representation.\n        n_classes: How many classes are there in the inputs.\n        code_length: What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Raises:\n        ValueError: If an unequal number of `inputs` and `outputs` are provided, or if `code_length` is invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nif len(self.inputs) != len(self.outputs):\nraise ValueError(\"Hadamard requires the same number of input and output keys.\")\nself.n_classes = n_classes\nif code_length is None:\ncode_length = 1 &lt;&lt; (n_classes - 1).bit_length()\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}.\")\nself.code_length = code_length\nlabels = hadamard(self.code_length).astype(np.float32)\nlabels[np.arange(0, self.code_length, 2), 0] = -1  # Make first column alternate\nlabels = labels[:self.n_classes]\nself.labels = labels\ndef forward(self, data: List[Union[int, np.ndarray]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n# TODO - also support one hot with smoothed labels?\nreturn [gather(tensor=self.labels, indices=np.array(inp)) for inp in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html", "title": "hue_saturation_value", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html#fastestimator.fastestimator.op.numpyop.univariate.hue_saturation_value.HueSaturationValue", "title": "<code>HueSaturationValue</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly modify the hue, saturation, and value of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>hue_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit).</p> <code>20</code> <code>sat_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit).</p> <code>30</code> <code>val_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\hue_saturation_value.py</code> <pre><code>@traceable()\nclass HueSaturationValue(ImageOnlyAlbumentation):\n\"\"\"Randomly modify the hue, saturation, and value of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        hue_shift_limit: Range for changing hue. If hue_shift_limit is a single int, the range will be\n            (-hue_shift_limit, hue_shift_limit).\n        sat_shift_limit: Range for changing saturation. If sat_shift_limit is a single int, the range will be\n            (-sat_shift_limit, sat_shift_limit).\n        val_shift_limit: range for changing value. If val_shift_limit is a single int, the range will be\n            (-val_shift_limit, val_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nhue_shift_limit: Union[int, Tuple[int, int]] = 20,\nsat_shift_limit: Union[int, Tuple[int, int]] = 30,\nval_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nHueSaturationValueAlb(hue_shift_limit=hue_shift_limit,\nsat_shift_limit=sat_shift_limit,\nval_shift_limit=val_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/iaa_additive_gaussian_noise.html", "title": "iaa_additive_gaussian_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/iaa_additive_gaussian_noise.html#fastestimator.fastestimator.op.numpyop.univariate.iaa_additive_gaussian_noise.IAAAdditiveGaussianNoise", "title": "<code>IAAAdditiveGaussianNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add gaussian noise to the input image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>loc</code> <code>int</code> <p>mean of the normal distribution that generates the noise. Default: 0.</p> <code>0</code> <code>scale</code> <code>Tuple[float, float]</code> <p>standard deviation of the normal distribution that generates the noise. Default: (0.01 * 255, 0.05 * 255)</p> <code>(2.5500000000000003, 12.75)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\iaa_additive_gaussian_noise.py</code> <pre><code>@traceable()\nclass IAAAdditiveGaussianNoise(ImageOnlyAlbumentation):\n\"\"\"Add gaussian noise to the input image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        loc: mean of the normal distribution that generates the noise. Default: 0.\n        scale: standard deviation of the normal distribution that generates the noise. Default: (0.01 * 255, 0.05 * 255)\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nloc: int = 0,\nscale:Tuple[float, float] =(2.5500000000000003, 12.75)):\nsuper().__init__(\nIAAAdditiveGaussianNoiseAlb(loc=loc, scale=scale, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html", "title": "image_compression", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html#fastestimator.fastestimator.op.numpyop.univariate.image_compression.ImageCompression", "title": "<code>ImageCompression</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease compression of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>quality_lower</code> <code>float</code> <p>Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>99</code> <code>quality_upper</code> <code>float</code> <p>Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>100</code> <code>compression_type</code> <code>ImgCmpAlb.ImageCompressionType</code> <p>should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.</p> <code>ImgCmpAlb.ImageCompressionType.JPEG</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\image_compression.py</code> <pre><code>@traceable()\nclass ImageCompression(ImageOnlyAlbumentation):\n\"\"\"Decrease compression of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        quality_lower: Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        quality_upper: Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        compression_type: should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nquality_lower: float = 99,\nquality_upper: float = 100,\ncompression_type: ImgCmpAlb.ImageCompressionType = ImgCmpAlb.ImageCompressionType.JPEG):\nsuper().__init__(\nImgCmpAlb(quality_lower=quality_lower,\nquality_upper=quality_upper,\ncompression_type=compression_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html", "title": "invert_img", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html#fastestimator.fastestimator.op.numpyop.univariate.invert_img.InvertImg", "title": "<code>InvertImg</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert an image by subtracting its pixel values from 255.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>int8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\invert_img.py</code> <pre><code>@traceable()\nclass InvertImg(ImageOnlyAlbumentation):\n\"\"\"Invert an image by subtracting its pixel values from 255.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        int8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(InvertImgAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html", "title": "iso_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html#fastestimator.fastestimator.op.numpyop.univariate.iso_noise.ISONoise", "title": "<code>ISONoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply camera sensor noise.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>color_shift</code> <code>Tuple[float, float]</code> <p>Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS colorspace.</p> <code>(0.01, 0.05)</code> <code>intensity</code> <code>Tuple[float, float]</code> <p>Multiplicative factor that controls the strength of color and luminace noise.</p> <code>(0.1, 0.5)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\iso_noise.py</code> <pre><code>@traceable()\nclass ISONoise(ImageOnlyAlbumentation):\n\"\"\"Apply camera sensor noise.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        color_shift: Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS\n            colorspace.\n        intensity: Multiplicative factor that controls the strength of color and luminace noise.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\ncolor_shift: Tuple[float, float] = (0.01, 0.05),\nintensity: Tuple[float, float] = (0.1, 0.5)):\nsuper().__init__(ISONoiseAlb(color_shift=color_shift, intensity=intensity, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html", "title": "median_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html#fastestimator.fastestimator.op.numpyop.univariate.median_blur.MedianBlur", "title": "<code>MedianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with median filter of random aperture size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf). If image is a float type then only 3 and 5 are valid sizes.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\median_blur.py</code> <pre><code>@traceable()\nclass MedianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with median filter of random aperture size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        blur_limit: maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf).\n            If image is a float type then only 3 and 5 are valid sizes.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 5):\nsuper().__init__(MedianBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/minmax.html", "title": "minmax", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/minmax.html#fastestimator.fastestimator.op.numpyop.univariate.minmax.Minmax", "title": "<code>Minmax</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Normalize data using the minmax method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>A small value to prevent numeric instability in the division.</p> <code>1e-07</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\minmax.py</code> <pre><code>@traceable()\nclass Minmax(NumpyOp):\n\"\"\"Normalize data using the minmax method.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        epsilon: A small value to prevent numeric instability in the division.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nepsilon: float = 1e-7):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.epsilon = epsilon\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_minmax(elem) for elem in data]\ndef _apply_minmax(self, data: np.ndarray) -&gt; np.ndarray:\ndata_max = np.max(data)\ndata_min = np.min(data)\ndata = (data - data_min) / max((data_max - data_min), self.epsilon)\nreturn data.astype(np.float32)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html", "title": "motion_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html#fastestimator.fastestimator.op.numpyop.univariate.motion_blur.MotionBlur", "title": "<code>MotionBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Motion Blur the image with a randomly-sized kernel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in the range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\motion_blur.py</code> <pre><code>@traceable()\nclass MotionBlur(ImageOnlyAlbumentation):\n\"\"\"Motion Blur the image with a randomly-sized kernel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in the range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(MotionBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html", "title": "multiplicative_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html#fastestimator.fastestimator.op.numpyop.univariate.multiplicative_noise.MultiplicativeNoise", "title": "<code>MultiplicativeNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Multiply an image with random perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>multiplier</code> <code>Union[float, Tuple[float, float]]</code> <p>If a single float, the image will be multiplied by this number. If tuple of floats then <code>multiplier</code> will be in the range [multiplier[0], multiplier[1]).</p> <code>(0.9, 1.1)</code> <code>per_channel</code> <code>bool</code> <p>Whether to sample different multipliers for each channel of the image.</p> <code>False</code> <code>elementwise</code> <code>bool</code> <p>If <code>False</code> multiply multiply all pixels in an image with a random value sampled once. If <code>True</code> Multiply image pixels with values that are pixelwise randomly sampled.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\multiplicative_noise.py</code> <pre><code>@traceable()\nclass MultiplicativeNoise(ImageOnlyAlbumentation):\n\"\"\"Multiply an image with random perturbations.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        multiplier: If a single float, the image will be multiplied by this number. If tuple of floats then `multiplier`\n            will be in the range [multiplier[0], multiplier[1]).\n        per_channel: Whether to sample different multipliers for each channel of the image.\n        elementwise: If `False` multiply multiply all pixels in an image with a random value sampled once.\n            If `True` Multiply image pixels with values that are pixelwise randomly sampled.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmultiplier: Union[float, Tuple[float, float]] = (0.9, 1.1),\nper_channel: bool = False,\nelementwise: bool = False):\nsuper().__init__(\nMultiplicativeNoiseAlb(multiplier=multiplier,\nper_channel=per_channel,\nelementwise=elementwise,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/normalize.html", "title": "normalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/normalize.html#fastestimator.fastestimator.op.numpyop.univariate.normalize.Normalize", "title": "<code>Normalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>mean</code> <code>Union[float, Tuple[float, ...]]</code> <p>Mean values to subtract.</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Union[float, Tuple[float, ...]]</code> <p>The divisor.</p> <code>(0.229, 0.224, 0.225)</code> <code>max_pixel_value</code> <code>float</code> <p>Maximum possible pixel value.</p> <code>255.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\normalize.py</code> <pre><code>@traceable()\nclass Normalize(ImageOnlyAlbumentation):\n\"\"\"Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        mean: Mean values to subtract.\n        std: The divisor.\n        max_pixel_value: Maximum possible pixel value.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmean: Union[float, Tuple[float, ...]] = (0.485, 0.456, 0.406),\nstd: Union[float, Tuple[float, ...]] = (0.229, 0.224, 0.225),\nmax_pixel_value: float = 255.0):\nsuper().__init__(NormalizeAlb(mean=mean, std=std, max_pixel_value=max_pixel_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/onehot.html", "title": "onehot", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/onehot.html#fastestimator.fastestimator.op.numpyop.univariate.onehot.Onehot", "title": "<code>Onehot</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transform an integer label to one-hot-encoding.</p> <p>This can be desirable for increasing robustness against incorrect labels: https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Input key(s) of labels to be onehot encoded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Output key(s) of labels.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes.</p> required <code>label_smoothing</code> <code>float</code> <p>Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing / num_classes, the other class output is: label_smoothing / num_classes.</p> <code>0.0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\onehot.py</code> <pre><code>@traceable()\nclass Onehot(NumpyOp):\n\"\"\"Transform an integer label to one-hot-encoding.\n    This can be desirable for increasing robustness against incorrect labels:\n    https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n    Args:\n        inputs: Input key(s) of labels to be onehot encoded.\n        outputs: Output key(s) of labels.\n        num_classes: Total number of classes.\n        label_smoothing: Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing\n            / num_classes, the other class output is: label_smoothing / num_classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nnum_classes: int,\nlabel_smoothing: float = 0.0,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.num_classes = num_classes\nself.label_smoothing = label_smoothing\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Union[int, np.ndarray]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_onehot(elem) for elem in data]\ndef _apply_onehot(self, data: Union[int, np.ndarray]) -&gt; np.ndarray:\nclass_index = np.array(data)\nassert \"int\" in str(class_index.dtype)\nassert class_index.size == 1, \"data must have only one item\"\nclass_index = class_index.item()\nassert class_index &lt; self.num_classes, \"label value should be smaller than num_classes\"\noutput = np.full((self.num_classes), fill_value=self.label_smoothing / self.num_classes, dtype=\"float32\")\noutput[class_index] = 1.0 - self.label_smoothing + self.label_smoothing / self.num_classes\nreturn output\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html", "title": "pad_sequence", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html#fastestimator.fastestimator.op.numpyop.univariate.pad_sequence.PadSequence", "title": "<code>PadSequence</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Pad sequences to the same length with provided value.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be padded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are padded.</p> required <code>max_len</code> <code>int</code> <p>Maximum length of all sequences.</p> required <code>value</code> <code>Union[str, int]</code> <p>Padding value.</p> <code>0</code> <code>append</code> <code>bool</code> <p>Pad before or after the sequences. True for padding the values after the sequence, False otherwise.</p> <code>True</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\pad_sequence.py</code> <pre><code>@traceable()\nclass PadSequence(NumpyOp):\n\"\"\"Pad sequences to the same length with provided value.\n    Args:\n        inputs: Key(s) of sequences to be padded.\n        outputs: Key(s) of sequences that are padded.\n        max_len: Maximum length of all sequences.\n        value: Padding value.\n        append: Pad before or after the sequences. True for padding the values after the sequence, False otherwise.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmax_len: int,\nvalue: Union[str, int] = 0,\nappend: bool = True,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nself.max_len = max_len\nself.value = value\nself.append = append\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._pad_sequence(elem) for elem in data]\ndef _pad_sequence(self, data: np.ndarray) -&gt; np.ndarray:\n\"\"\"Pad the input sequence to the maximum length. Sequences longer than `max_len` are truncated.\n        Args:\n            data: input sequence in the data.\n        Returns:\n            Padded sequence\n        \"\"\"\nif len(data) &lt; self.max_len:\npad_len = self.max_len - len(data)\npad_arr = np.full(pad_len, self.value)\nif self.append:\ndata = np.append(data, pad_arr)\nelse:\ndata = np.append(pad_arr, data)\nelse:\ndata = data[:self.max_len]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/posterize.html", "title": "posterize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/posterize.html#fastestimator.fastestimator.op.numpyop.univariate.posterize.Posterize", "title": "<code>Posterize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Reduce the number of bits for each color channel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>num_bits</code> <code>Union[int, Tuple[int, int], Tuple[int, int, int], Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]]</code> <p>Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be in the range [0, 8].</p> <code>4</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\posterize.py</code> <pre><code>@traceable()\nclass Posterize(ImageOnlyAlbumentation):\n\"\"\"Reduce the number of bits for each color channel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        num_bits: Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet\n            of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be\n            in the range [0, 8].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nnum_bits: Union[int,\nTuple[int, int],\nTuple[int, int, int],\nTuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]] = 4):\nsuper().__init__(PosterizeAlb(num_bits=num_bits, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html", "title": "random_brightness_contrast", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html#fastestimator.fastestimator.op.numpyop.univariate.random_brightness_contrast.RandomBrightnessContrast", "title": "<code>RandomBrightnessContrast</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly change the brightness and contrast of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>brightness_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing brightness. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>contrast_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing contrast. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>brightness_by_max</code> <code>bool</code> <p>If True adjust contrast by image dtype maximum, else adjust contrast by image mean.</p> <code>True</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_brightness_contrast.py</code> <pre><code>@traceable()\nclass RandomBrightnessContrast(ImageOnlyAlbumentation):\n\"\"\"Randomly change the brightness and contrast of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        brightness_limit: Factor range for changing brightness.\n            If limit is a single float, the range will be (-limit, limit).\n        contrast_limit: Factor range for changing contrast.\n            If limit is a single float, the range will be (-limit, limit).\n        brightness_by_max: If True adjust contrast by image dtype maximum, else adjust contrast by image mean.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nbrightness_limit: Union[float, Tuple[float, float]] = 0.2,\ncontrast_limit: Union[float, Tuple[float, float]] = 0.2,\nbrightness_by_max: bool = True):\nsuper().__init__(\nRandomBrightnessContrastAlb(brightness_limit=brightness_limit,\ncontrast_limit=contrast_limit,\nbrightness_by_max=brightness_by_max,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html", "title": "random_fog", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html#fastestimator.fastestimator.op.numpyop.univariate.random_fog.RandomFog", "title": "<code>RandomFog</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add fog to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>fog_coef_lower</code> <code>float</code> <p>Lower limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>0.3</code> <code>fog_coef_upper</code> <code>float</code> <p>Upper limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>1.0</code> <code>alpha_coef</code> <code>float</code> <p>Transparency of the fog circles. Should be in the range [0, 1].</p> <code>0.08</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_fog.py</code> <pre><code>@traceable()\nclass RandomFog(ImageOnlyAlbumentation):\n\"\"\"Add fog to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        fog_coef_lower: Lower limit for fog intensity coefficient. Should be in the range [0, 1].\n        fog_coef_upper: Upper limit for fog intensity coefficient. Should be in the range [0, 1].\n        alpha_coef: Transparency of the fog circles. Should be in the range [0, 1].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nfog_coef_lower: float = 0.3,\nfog_coef_upper: float = 1.0,\nalpha_coef: float = 0.08):\nsuper().__init__(\nRandomFogAlb(fog_coef_lower=fog_coef_lower,\nfog_coef_upper=fog_coef_upper,\nalpha_coef=alpha_coef,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html", "title": "random_gamma", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html#fastestimator.fastestimator.op.numpyop.univariate.random_gamma.RandomGamma", "title": "<code>RandomGamma</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply a gamma transform to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>gamma_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).</p> <code>(80, 120)</code> <code>eps</code> <code>float</code> <p>A numerical stability constant to avoid division by zero.</p> <code>1e-07</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_gamma.py</code> <pre><code>@traceable()\nclass RandomGamma(ImageOnlyAlbumentation):\n\"\"\"Apply a gamma transform to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        gamma_limit: If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).\n        eps: A numerical stability constant to avoid division by zero.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\ngamma_limit: Union[float, Tuple[float, float]] = (80, 120),\neps: float = 1e-7):\nsuper().__init__(RandomGammaAlb(gamma_limit=gamma_limit, eps=eps, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html", "title": "random_rain", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html#fastestimator.fastestimator.op.numpyop.univariate.random_rain.RandomRain", "title": "<code>RandomRain</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add rain to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>slant_lower</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>-10</code> <code>slant_upper</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>10</code> <code>drop_length</code> <code>int</code> <p>Should be in range [0, 100].</p> <code>20</code> <code>drop_width</code> <code>int</code> <p>Should be in range [1, 5].</p> <code>1</code> <code>drop_color</code> <code>Tuple[int, int, int]</code> <p>Rain lines color (r, g, b).</p> <code>(200, 200, 200)</code> <code>blur_value</code> <code>int</code> <p>How blurry to make the rain.</p> <code>7</code> <code>brightness_coefficient</code> <code>float</code> <p>Rainy days are usually shady. Should be in range [0, 1].</p> <code>0.7</code> <code>rain_type</code> <code>Optional[str]</code> <p>One of [None, \"drizzle\", \"heavy\", \"torrential\"].</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_rain.py</code> <pre><code>@traceable()\nclass RandomRain(ImageOnlyAlbumentation):\n\"\"\"Add rain to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        slant_lower: Should be in range [-20, 20].\n        slant_upper: Should be in range [-20, 20].\n        drop_length: Should be in range [0, 100].\n        drop_width: Should be in range [1, 5].\n        drop_color: Rain lines color (r, g, b).\n        blur_value: How blurry to make the rain.\n        brightness_coefficient: Rainy days are usually shady. Should be in range [0, 1].\n        rain_type: One of [None, \"drizzle\", \"heavy\", \"torrential\"].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nslant_lower: int = -10,\nslant_upper: int = 10,\ndrop_length: int = 20,\ndrop_width: int = 1,\ndrop_color: Tuple[int, int, int] = (200, 200, 200),\nblur_value: int = 7,\nbrightness_coefficient: float = 0.7,\nrain_type: Optional[str] = None):\nsuper().__init__(\nRandomRainAlb(slant_lower=slant_lower,\nslant_upper=slant_upper,\ndrop_length=drop_length,\ndrop_width=drop_width,\ndrop_color=drop_color,  # Their docstring type hint doesn't match the real code\nblur_value=blur_value,\nbrightness_coefficient=brightness_coefficient,\nrain_type=rain_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html", "title": "random_shadow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html#fastestimator.fastestimator.op.numpyop.univariate.random_shadow.RandomShadow", "title": "<code>RandomShadow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add shadows to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>shadow_roi</code> <code>Tuple[float, float, float, float]</code> <p>Region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0.0, 0.5, 1.0, 1.0)</code> <code>num_shadows_lower</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [0, <code>num_shadows_upper</code>].</p> <code>1</code> <code>num_shadows_upper</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [<code>num_shadows_lower</code>, inf].</p> <code>2</code> <code>shadow_dimension</code> <code>int</code> <p>Number of edges in the shadow polygons.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_shadow.py</code> <pre><code>@traceable()\nclass RandomShadow(ImageOnlyAlbumentation):\n\"\"\"Add shadows to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        shadow_roi: Region of the image where shadows will appear (x_min, y_min, x_max, y_max).\n            All values should be in range [0, 1].\n        num_shadows_lower: Lower limit for the possible number of shadows. Should be in range [0, `num_shadows_upper`].\n        num_shadows_upper: Lower limit for the possible number of shadows.\n            Should be in range [`num_shadows_lower`, inf].\n        shadow_dimension: Number of edges in the shadow polygons.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nshadow_roi: Tuple[float, float, float, float] = (0.0, 0.5, 1.0, 1.0),\nnum_shadows_lower: int = 1,\nnum_shadows_upper: int = 2,\nshadow_dimension: int = 5):\nprint(\"\\033[93m {}\\033[00m\".format(\n\"Warning! RandomShadow does not work with multi-threaded Pipelines. Either do not use this Op or else \" +\n\"set your Pipeline num_process=0\"))\n# TODO - Have pipeline look for bad ops and auto-magically set num_process correctly\nsuper().__init__(\nRandomShadowAlb(shadow_roi=shadow_roi,\nnum_shadows_lower=num_shadows_lower,\nnum_shadows_upper=num_shadows_upper,\nshadow_dimension=shadow_dimension,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_shapes.html", "title": "random_shapes", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_shapes.html#fastestimator.fastestimator.op.numpyop.univariate.random_shapes.RandomShapes", "title": "<code>RandomShapes</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Add random shapes to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>max_shapes</code> <code>int</code> <p>The maximum number of shapes to add to the image.</p> <code>3</code> <code>max_size</code> <code>Optional[int]</code> <p>The maximum size of the shapes to generate.</p> <code>None</code> <code>intensity_range</code> <code>Tuple[int, int]</code> <p>The allowable pixel values for the shapes.</p> <code>(0, 254)</code> <code>transparency_range</code> <code>Tuple[float, float]</code> <p>The range of transparency values to be randomly sampled from. 0 means that shapes are completely transparent, and 1 means that shapes are completely opaque.</p> <code>(0.1, 0.9)</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>intensity_range</code> or <code>transparency_range</code> arguments are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_shapes.py</code> <pre><code>@traceable()\nclass RandomShapes(NumpyOp):\n\"\"\"Add random shapes to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        max_shapes: The maximum number of shapes to add to the image.\n        max_size: The maximum size of the shapes to generate.\n        intensity_range: The allowable pixel values for the shapes.\n        transparency_range: The range of transparency values to be randomly sampled from. 0 means that shapes are\n            completely transparent, and 1 means that shapes are completely opaque.\n    Raises:\n        AssertionError: If the `intensity_range` or `transparency_range` arguments are invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmax_shapes: int = 3,\nmax_size: Optional[int] = None,\nintensity_range: Tuple[int, int] = (0, 254),\ntransparency_range: Tuple[float, float] = (0.1, 0.9)):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.max_shapes = max_shapes\nself.max_size = max_size\nassert 0 &lt;= intensity_range[0] &lt;= intensity_range[1] &lt;= 254, \"Intensity_range should be in [0, 254]\"\nself.intensity_range = intensity_range\nassert 0 &lt;= transparency_range[0] &lt;= transparency_range[1] &lt;= 1, \"Transparency_range should be in [0,1]\"\nself.transparency_range = transparency_range\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_shapes(elem) for elem in data]\ndef _apply_shapes(self, data: np.ndarray) -&gt; np.ndarray:\nchannels, height, width = get_image_dims(data)\n# Wrap intensity_range in another tuple for color images. User might have provided nested tuple already though,\n#  so don't wrap those again.\nintensity_range = self.intensity_range\nif channels &gt; 1 and not isinstance(intensity_range[0], tuple):\nintensity_range = (intensity_range, )\nshapes, _ = random_shapes(image_shape=(height, width), max_shapes=self.max_shapes, max_size=self.max_size,\nnum_channels=channels,\nintensity_range=intensity_range,\nallow_overlap=True)\nalpha = np.random.uniform(self.transparency_range[0], self.transparency_range[1])\n# Convert shapes to range [0,1] if image is floating point\nnormalized_shapes = shapes / 255.0 if np.issubdtype(data.dtype, np.floating) else shapes\nblend = normalized_shapes * alpha + data * (1.0 - alpha)\n# Combine images without whitening non-shape regions\noverlay = np.where(shapes &lt; 255, blend, data)\nreturn overlay.astype(data.dtype)\ndef set_rua_level(self, magnitude_coef: Union[int, float]) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nself.max_shapes = math.ceil(magnitude_coef * self.max_shapes)\nself.max_size = None if self.max_size is None else math.ceil(magnitude_coef * self.max_size)\n# Transparency range will keep the same minimum value, but reduce the maximum opacity based on the level.\ntransparency_diff = self.transparency_range[1] - self.transparency_range[0]\ntransparency_diff = transparency_diff * magnitude_coef\nself.transparency_range = (self.transparency_range[0], self.transparency_range[0] + transparency_diff)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_shapes.html#fastestimator.fastestimator.op.numpyop.univariate.random_shapes.RandomShapes.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>Union[int, float]</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_shapes.py</code> <pre><code>def set_rua_level(self, magnitude_coef: Union[int, float]) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nself.max_shapes = math.ceil(magnitude_coef * self.max_shapes)\nself.max_size = None if self.max_size is None else math.ceil(magnitude_coef * self.max_size)\n# Transparency range will keep the same minimum value, but reduce the maximum opacity based on the level.\ntransparency_diff = self.transparency_range[1] - self.transparency_range[0]\ntransparency_diff = transparency_diff * magnitude_coef\nself.transparency_range = (self.transparency_range[0], self.transparency_range[0] + transparency_diff)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html", "title": "random_snow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html#fastestimator.fastestimator.op.numpyop.univariate.random_snow.RandomSnow", "title": "<code>RandomSnow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Bleach out some pixels to simulate snow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>snow_point_lower</code> <code>float</code> <p>Lower bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.1</code> <code>snow_point_upper</code> <code>float</code> <p>Upper bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.3</code> <code>brightness_coeff</code> <code>float</code> <p>A larger number will lead to a more snow on the image. Should be &gt;= 0.</p> <code>2.5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_snow.py</code> <pre><code>@traceable()\nclass RandomSnow(ImageOnlyAlbumentation):\n\"\"\"Bleach out some pixels to simulate snow.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        snow_point_lower: Lower bound of the amount of snow. Should be in the range [0, 1].\n        snow_point_upper: Upper bound of the amount of snow. Should be in the range [0, 1].\n        brightness_coeff: A larger number will lead to a more snow on the image. Should be &gt;= 0.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nsnow_point_lower: float = 0.1,\nsnow_point_upper: float = 0.3,\nbrightness_coeff: float = 2.5):\nsuper().__init__(\nRandomSnowAlb(snow_point_lower=snow_point_lower,\nsnow_point_upper=snow_point_upper,\nbrightness_coeff=brightness_coeff,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html", "title": "random_sun_flare", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html#fastestimator.fastestimator.op.numpyop.univariate.random_sun_flare.RandomSunFlare", "title": "<code>RandomSunFlare</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add a sun flare to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>flare_roi</code> <code>Tuple[float, float, float, float]</code> <p>region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0, 0, 1, 0.5)</code> <code>angle_lower</code> <code>float</code> <p>should be in range [0, <code>angle_upper</code>].</p> <code>0.0</code> <code>angle_upper</code> <code>float</code> <p>should be in range [<code>angle_lower</code>, 1].</p> <code>1.0</code> <code>num_flare_circles_lower</code> <code>int</code> <p>lower limit for the number of flare circles. Should be in range [0, <code>num_flare_circles_upper</code>].</p> <code>6</code> <code>num_flare_circles_upper</code> <code>int</code> <p>upper limit for the number of flare circles. Should be in range [<code>num_flare_circles_lower</code>, inf].</p> <code>10</code> <code>src_radius</code> <code>int</code> <p>Radius of the flare.</p> <code>400</code> <code>src_color</code> <code>Tuple[int, int, int]</code> <p>Color of the flare (R,G,B).</p> <code>(255, 255, 255)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_sun_flare.py</code> <pre><code>@traceable()\nclass RandomSunFlare(ImageOnlyAlbumentation):\n\"\"\"Add a sun flare to the image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        flare_roi: region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be\n            in range [0, 1].\n        angle_lower: should be in range [0, `angle_upper`].\n        angle_upper: should be in range [`angle_lower`, 1].\n        num_flare_circles_lower: lower limit for the number of flare circles.\n            Should be in range [0, `num_flare_circles_upper`].\n        num_flare_circles_upper: upper limit for the number of flare circles.\n            Should be in range [`num_flare_circles_lower`, inf].\n        src_radius: Radius of the flare.\n        src_color: Color of the flare (R,G,B).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nflare_roi: Tuple[float, float, float, float] = (0, 0, 1, 0.5),\nangle_lower: float = 0.0,\nangle_upper: float = 1.0,\nnum_flare_circles_lower: int = 6,\nnum_flare_circles_upper: int = 10,\nsrc_radius: int = 400,\nsrc_color: Tuple[int, int, int] = (255, 255, 255)):\nsuper().__init__(\nRandomSunFlareAlb(flare_roi=flare_roi,\nangle_lower=angle_lower,\nangle_upper=angle_upper,\nnum_flare_circles_lower=num_flare_circles_lower,\nnum_flare_circles_upper=num_flare_circles_upper,\nsrc_radius=src_radius,\nsrc_color=src_color,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/read_image.html", "title": "read_image", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/read_image.html#fastestimator.fastestimator.op.numpyop.univariate.read_image.ReadImage", "title": "<code>ReadImage</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading png or jpg images from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of paths to images to be loaded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given path.</p> <code>''</code> <code>color_flag</code> <code>Union[str, int]</code> <p>Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.</p> <code>cv2.IMREAD_COLOR</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>inputs</code> and <code>outputs</code> have mismatched lengths, or the <code>color_flag</code> is unacceptable.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\read_image.py</code> <pre><code>@traceable()\nclass ReadImage(NumpyOp):\n\"\"\"A class for reading png or jpg images from disk.\n    Args:\n        inputs: Key(s) of paths to images to be loaded.\n        outputs: Key(s) of images to be output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        parent_path: Parent path that will be prepended to a given path.\n        color_flag: Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.\n    Raises:\n        AssertionError: If `inputs` and `outputs` have mismatched lengths, or the `color_flag` is unacceptable.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\",\ncolor_flag: Union[str, int] = cv2.IMREAD_COLOR):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nif isinstance(self.inputs, List) and isinstance(self.outputs, List):\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.parent_path = parent_path\nassert isinstance(color_flag, int) or color_flag in {'color', 'gray', 'grey'}, \\\n            f\"Unacceptable color_flag value: {color_flag}\"\nself.color_flag = color_flag\nif self.color_flag == \"color\":\nself.color_flag = cv2.IMREAD_COLOR\nelif self.color_flag in {\"gray\", \"grey\"}:\nself.color_flag = cv2.IMREAD_GRAYSCALE\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._read(elem) for elem in data]\ndef _read(self, path: str) -&gt; np.ndarray:\npath = os.path.normpath(os.path.join(self.parent_path, path))\nimg = cv2.imread(path, self.color_flag)\nif self.color_flag in {\ncv2.IMREAD_COLOR, cv2.IMREAD_REDUCED_COLOR_2, cv2.IMREAD_REDUCED_COLOR_4, cv2.IMREAD_REDUCED_COLOR_8\n}:\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nif not isinstance(img, np.ndarray):\nraise ValueError('cv2 did not read correctly for file \"{}\"'.format(path))\nif img.ndim == 2:\nimg = np.expand_dims(img, -1)\nreturn img\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/reshape.html#fastestimator.fastestimator.op.numpyop.univariate.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Op which re-shapes data to a target shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of the data to be reshaped.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\reshape.py</code> <pre><code>@traceable()\nclass Reshape(NumpyOp):\n\"\"\"An Op which re-shapes data to a target shape.\n    Args:\n        shape: The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.\n        inputs: Key(s) of the data to be reshaped.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nshape: Union[int, Tuple[int, ...]],\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shape = shape\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_reshape(elem) for elem in data]\ndef _apply_reshape(self, data):\ndata = np.reshape(data, self.shape)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html", "title": "rgb_shift", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html#fastestimator.fastestimator.op.numpyop.univariate.rgb_shift.RGBShift", "title": "<code>RGBShift</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly shift the channel values for an input RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the normalized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>r_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit).</p> <code>20</code> <code>g_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the green channel. If g_shift_limit is a single int, the range will be (-g_shift_limit, g_shift_limit).</p> <code>20</code> <code>b_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rgb_shift.py</code> <pre><code>@traceable()\nclass RGBShift(ImageOnlyAlbumentation):\n\"\"\"Randomly shift the channel values for an input RGB image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the normalized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        r_shift_limit: range for changing values for the red channel. If r_shift_limit is a single int, the range\n            will be (-r_shift_limit, r_shift_limit).\n        g_shift_limit: range for changing values for the green channel. If g_shift_limit is a single int, the range\n            will be (-g_shift_limit, g_shift_limit).\n        b_shift_limit: range for changing values for the blue channel. If b_shift_limit is a single int, the range\n            will be (-b_shift_limit, b_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nr_shift_limit: Union[int, Tuple[int, int]] = 20,\ng_shift_limit: Union[int, Tuple[int, int]] = 20,\nb_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nRGBShiftAlb(r_shift_limit=r_shift_limit,\ng_shift_limit=g_shift_limit,\nb_shift_limit=b_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html", "title": "rua", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Equalize", "title": "<code>Equalize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Equalize the image histogram.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass Equalize(NumpyOp):\n\"\"\"Equalize the image histogram.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=to_list(inputs), outputs=to_list(outputs), mode=mode, ds_id=ds_id)\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [Equalize._apply_equalize(elem) for elem in data]\n@staticmethod\ndef _apply_equalize(data: np.ndarray) -&gt; np.ndarray:\n\"\"\"Equalize the image histogram.\n        Args:\n            data: The image to be modified.\n        Returns:\n            The image after applying equalize.\n        \"\"\"\nim = Image.fromarray(data)\nim = ImageOps.equalize(im)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Equalize.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>A method which will be invoked by the RUA Op to adjust the augmentation intensity.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Identity", "title": "<code>Identity</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Pass the input as-is.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass Identity(NumpyOp):\n\"\"\"Pass the input as-is.\n    Args:\n        inputs: Key(s) of images.\n        outputs: Key(s) into which to write the images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=to_list(inputs), outputs=to_list(outputs), mode=mode, ds_id=ds_id)\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Identity.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>A method which will be invoked by the RUA Op to adjust the augmentation intensity.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"A method which will be invoked by the RUA Op to adjust the augmentation intensity.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.OneOfMultiVar", "title": "<code>OneOfMultiVar</code>", "text": "<p>         Bases: <code>OneOf</code></p> <p>Perform one of several possible NumpyOps.</p> <p>Note that OneOfMultiVar accepts both univariate and multivariate ops and allows the list of passed NumpyOps to have different input and output keys. OneOfMultiVar should not be used to wrap an op whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but OneOfMultiVar declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking the OneOfMultiVar.</p> <p>Parameters:</p> Name Type Description Default <code>*numpy_ops</code> <code>NumpyOp</code> <p>A list of ops to choose between with uniform probability.</p> <code>()</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass OneOfMultiVar(OneOf):\n\"\"\"Perform one of several possible NumpyOps.\n    Note that OneOfMultiVar accepts both univariate and multivariate ops and allows the list of passed NumpyOps to have\n    different input and output keys. OneOfMultiVar should not be used to wrap an op whose output key(s) do not already\n    exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the\n    output key, but OneOfMultiVar declined to generate it. If you want to create a default value for a new key, simply\n    use a LambdaOp before invoking the OneOfMultiVar.\n    Args:\n        *numpy_ops: A list of ops to choose between with uniform probability.\n    \"\"\"\ndef __init__(self, *numpy_ops: NumpyOp) -&gt; None:\ninputs = to_set(numpy_ops[0].inputs)\noutputs = to_set(numpy_ops[0].outputs)\nmode = numpy_ops[0].mode\nds_id = numpy_ops[0].ds_id\nself.in_list = numpy_ops[0].in_list\nself.out_list = numpy_ops[0].out_list\nfor op in numpy_ops[1:]:\nassert self.in_list == op.in_list, \"All ops within OneOf must share the same input configuration\"\nassert self.out_list == op.out_list, \"All ops within OneOf must share the same output configuration\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nfor inp in op.inputs:\ninputs.add(inp)\nfor out in op.outputs:\noutputs.add(out)\n# Bypassing OneOf Op's restriction of same input and output key(s) on the list of passed NumpyOps.\nsuper(OneOf, self).__init__(inputs=inputs.union(outputs), outputs=outputs, mode=mode, ds_id=ds_id)\nself.ops = numpy_ops\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nforward_numpyop([random.choice(self.ops)], data, state)\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Posterize", "title": "<code>Posterize</code>", "text": "<p>         Bases: <code>PosterizeAug</code></p> <p>Reduce the number of bits for the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>num_bits</code> <code>Union[int, Tuple[int, int], Tuple[int, int, int], Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]]</code> <p>Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be in the range [0, 8].</p> <code>7</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass Posterize(PosterizeAug):\n\"\"\"Reduce the number of bits for the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        num_bits: Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet\n            of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be\n            in the range [0, 8].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nnum_bits: Union[int,\nTuple[int, int],\nTuple[int, int, int],\nTuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]] = 7):\nself.num_bits = num_bits\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id, num_bits=num_bits)\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nif isinstance(self.num_bits, tuple) and len(self.num_bits) == 3:\nnum_bits = []\nfor i in self.num_bits:\nnum_bits.append(Posterize._range_tuple(num_bits=i, magnitude_coef=magnitude_coef))\nself.num_bits = tuple(num_bits)\nelse:\nself.num_bits = Posterize._range_tuple(num_bits=self.num_bits, magnitude_coef=magnitude_coef)\nsuper().__init__(inputs=self.inputs,\noutputs=self.outputs,\nmode=self.mode,\nds_id=self.ds_id,\nnum_bits=self.num_bits)\n@staticmethod\ndef _range_tuple(num_bits: Union[int, Tuple[int, int]], magnitude_coef: float) -&gt; Tuple[int, int]:\n\"\"\"Process num_bits for posterization based on augmentation intensity.\n        Args:\n            num_bits: Number of high bits.\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        Returns:\n            The range of high bits after adjusting augmentation intensity.\n        \"\"\"\nif isinstance(num_bits, tuple):\nparam_mid = (num_bits[0] + num_bits[1])/2\nparam_extent = magnitude_coef * ((num_bits[1] - num_bits[0])/2)\nbits_range = (round(param_mid - param_extent), round(param_mid + param_extent))\nelse:\nbits_range = (round(8-(magnitude_coef*num_bits)), 8)\nreturn bits_range\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Posterize.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nif isinstance(self.num_bits, tuple) and len(self.num_bits) == 3:\nnum_bits = []\nfor i in self.num_bits:\nnum_bits.append(Posterize._range_tuple(num_bits=i, magnitude_coef=magnitude_coef))\nself.num_bits = tuple(num_bits)\nelse:\nself.num_bits = Posterize._range_tuple(num_bits=self.num_bits, magnitude_coef=magnitude_coef)\nsuper().__init__(inputs=self.inputs,\noutputs=self.outputs,\nmode=self.mode,\nds_id=self.ds_id,\nnum_bits=self.num_bits)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.RUA", "title": "<code>RUA</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Apply RUA augmentation strategy.</p> <p>Note that all augmentation ops passed to RUA should have a set_rua_level method to modify their strength based on the level. Custom NumpyOps can be passed to the <code>choices</code> argument along with names of augmentations to add. Passing 'defaults' adds the default list of augmentations along with any custom NumpyOps specified by the user. The default augmentations are: 'Rotate', 'Identity', 'AutoContrast', 'Equalize', 'Posterize', 'Solarize', 'Sharpness', 'Contrast', 'Color', 'Brightness', 'ShearX', 'ShearY', 'TranslateX' and 'TranslateY'. To add specific augmentations from the default list, their names can be passed. Ex: 'Rotate'. To remove specific augmentations from the list, you can negate their names. Ex: '!Rotate' will load all the augmentations except 'Rotate'.</p> <p>Example combinations which are not allowed: choices = ['defaults', 'Rotate']        # augmentations from the default list are redundant with 'defaults'. choices = ['defaults', '!Rotate']       # negated augmentations automatically load the default list. choices = ['!Solarize', 'Rotate']       # Cannot mix negated and normal augmentations.</p> <p>RUA should not have augmentation ops whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but RUA declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking RUA.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>choices</code> <code>Union[str, NumpyOp, List[Union[str, NumpyOp]]]</code> <p>List of augmentations to apply.</p> <code>'defaults'</code> <code>level</code> <code>Union[int, float]</code> <p>Factor to set the range for magnitude of augmentation. Must be in the range [0, 30].</p> <code>18</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass RUA(NumpyOp):\n\"\"\"Apply RUA augmentation strategy.\n    Note that all augmentation ops passed to RUA should have a set_rua_level method to modify their strength based on\n    the level. Custom NumpyOps can be passed to the `choices` argument along with names of augmentations to add. Passing\n    'defaults' adds the default list of augmentations along with any custom NumpyOps specified by the user.\n    The default augmentations are: 'Rotate', 'Identity', 'AutoContrast', 'Equalize', 'Posterize', 'Solarize',\n    'Sharpness', 'Contrast', 'Color', 'Brightness', 'ShearX', 'ShearY', 'TranslateX' and 'TranslateY'.\n    To add specific augmentations from the default list, their names can be passed. Ex: 'Rotate'.\n    To remove specific augmentations from the list, you can negate their names. Ex: '!Rotate' will load all the\n    augmentations except 'Rotate'.\n    Example combinations which are not allowed:\n    choices = ['defaults', 'Rotate']        # augmentations from the default list are redundant with 'defaults'.\n    choices = ['defaults', '!Rotate']       # negated augmentations automatically load the default list.\n    choices = ['!Solarize', 'Rotate']       # Cannot mix negated and normal augmentations.\n    RUA should not have augmentation ops whose output key(s) do not already exist in the data dictionary. This would\n    result in a problem when future ops / traces attempt to reference the output key, but RUA declined to generate it.\n    If you want to create a default value for a new key, simply use a LambdaOp before invoking RUA.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        choices: List of augmentations to apply.\n        level: Factor to set the range for magnitude of augmentation. Must be in the range [0, 30].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nchoices: Union[str, NumpyOp, List[Union[str, NumpyOp]]] = \"defaults\",\nlevel: Union[int, float] = 18):\nself.default_aug_dict = {\n\"Rotate\": Rotate(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,limit=90),\n\"Identity\": Identity(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id),\n\"AutoContrast\": AutoContrast(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id),\n\"Equalize\": Equalize(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id),\n\"Posterize\": Posterize(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,num_bits=7),\n\"Solarize\": Solarize(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,threshold=256),\n\"Sharpness\": Sharpness(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,limit=0.9),\n\"Contrast\": Contrast(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,limit=0.9),\n\"Color\": Color(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,limit=0.9),\n\"Brightness\": Brightness(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,limit=0.9),\n\"ShearX\": ShearX(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,shear_coef=0.5),\n\"ShearY\": ShearY(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,shear_coef=0.5),\n\"TranslateX\": TranslateX(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,shift_limit=0.33),\n\"TranslateY\": TranslateY(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id,shift_limit=0.33)\n}\naug_options = self._parse_aug_choices(magnitude_coef=(level / 30.), choices=to_list(choices))\ninputs, outputs = to_set(inputs), to_set(outputs)\nfor op in aug_options:\nfor inp in op.inputs:\ninputs.add(inp)\nfor out in op.outputs:\noutputs.add(out)\nsuper().__init__(inputs=inputs.union(outputs), outputs=outputs, mode=mode, ds_id=ds_id)\n# Calculating number of augmentation to apply at each training iteration\nN_min = 1\nN_max = min(len(aug_options), 5)\nN = level * (N_max - N_min) / 30 + N_min\nN_guarantee, N_p = int(N), N % 1\nself.ops = [OneOfMultiVar(*aug_options) for _ in range(N_guarantee)]\nif N_p &gt; 0:\nself.ops.append(Sometimes(OneOfMultiVar(*aug_options), prob=N_p))\ndef _parse_aug_choices(self, magnitude_coef: float, choices: List[Union[str, NumpyOp]]) -&gt; List[NumpyOp]:\n\"\"\"Parse the augmentation choices to determine the final list of augmentations to apply.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n            choices: List of augmentations to apply.\n        Returns:\n            List of augmentations to apply.\n        Raises:\n            AssertionError: If augmentations to add and remove are mixed.\n            AttributeError: If augmentation choices don't have a 'set_rua_level' method.\n            ValueError: If 'defaults' is provided with augmentation strings to add or remove, or wrong names are\n                provided.\n        \"\"\"\ncustom_ops = [op for op in choices if not isinstance(op, str)]\nremove_ops = [op for op in choices if isinstance(op, str) and op.startswith(\"!\")]\nadd_ops = [op for op in choices if isinstance(op, str) and not (op.startswith(\"!\") or (op == \"defaults\"))]\naug_names = list(self.default_aug_dict.keys())\nassert len(remove_ops)==0 or len(add_ops)==0, \\\n            \"RUA supports either add or remove ops, but not both. Found {} and {}\".format(add_ops, remove_ops)\nif len(remove_ops) &gt; 0:\nif \"defaults\" in choices:\nraise ValueError(\"Can't provide 'defaults' value with ops to remove, found: {}\".format(remove_ops))\nremove_ops = [op[1:] for op in remove_ops]\nfor op in remove_ops:\nif op not in aug_names:\nraise ValueError(\"Unable to remove {}, list of augmentations available: {}\".format(op, aug_names))\naug_list = [aug for aug_name, aug in self.default_aug_dict.items() if aug_name not in remove_ops]\nelse:\nif \"defaults\" in choices:\nif len(add_ops) &gt; 0:\nraise ValueError(\"Can't pass 'defaults' value with default list's ops, found: {}\".format(add_ops))\naug_list = list(self.default_aug_dict.values())\nelif len(add_ops) &gt; 0:\nfor op in add_ops:\nif op not in aug_names:\nraise ValueError(\"Unable to add {}, list of augmentations available: {}\".format(op, aug_names))\naug_list = [self.default_aug_dict[aug_name] for aug_name in add_ops]\nelse:\naug_list = []\naug_list = aug_list + custom_ops\nfor op in aug_list:\nif hasattr(op, \"set_rua_level\") and inspect.ismethod(getattr(op, \"set_rua_level\")):\nop.set_rua_level(magnitude_coef=magnitude_coef)\nelse:\nraise AttributeError(\n\"RUA Augmentations should have a 'set_rua_level' method but it's not present in Op: {}\".format(\nop.__class__.__name__))\nreturn aug_list\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nforward_numpyop(self.ops, data, state)\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Rotate", "title": "<code>Rotate</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Rotate the input by an angle selected randomly.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range from which the angle can be picked. If limit is a single int the range is considered from (0, limit).</p> <code>30</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass Rotate(NumpyOp):\n\"\"\"Rotate the input by an angle selected randomly.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        limit: Range from which the angle can be picked. If limit is a single int the range is considered from\n            (0, limit).\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nlimit: Union[int, Tuple[int, int]] = 30):\nsuper().__init__(inputs=to_list(inputs), outputs=to_list(outputs), mode=mode, ds_id=ds_id)\nself.limit = param_to_range(limit)\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\ndegree = random.uniform(self.limit[0], self.limit[1])\nreturn [Rotate._apply_rotate(elem, degree) for elem in data]\n@staticmethod\ndef _apply_rotate(data: np.ndarray, degree: float) -&gt; np.ndarray:\n\"\"\"Rotate the image.\n        Args:\n            data: The image to be modified.\n            degree: Angle for image rotation.\n        Returns:\n            The image after applying rotation.\n        \"\"\"\nim = Image.fromarray(data)\nim = im.rotate(degree)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Rotate.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Solarize", "title": "<code>Solarize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>threshold</code> <code>Union[int, Tuple[int, int], float, Tuple[float, float]]</code> <p>Range for the solarizing threshold. If threshold is a single value 't', the range will be [0, t].</p> <code>256</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>@traceable()\nclass Solarize(NumpyOp):\n\"\"\"Invert all pixel values above a threshold.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        threshold: Range for the solarizing threshold. If threshold is a single value 't', the range will be [0, t].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nthreshold: Union[int, Tuple[int, int], float, Tuple[float, float]] = 256):\nsuper().__init__(inputs=to_list(inputs), outputs=to_list(outputs), mode=mode, ds_id=ds_id)\nself.threshold = threshold\ndef set_rua_level(self, magnitude_coef: Union[int, float]) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nif isinstance(self.threshold, tuple):\nself.threshold = magnitude_coef * (self.threshold[1] - self.threshold[0]) + self.threshold[0]\nelse:\nself.threshold = magnitude_coef * self.threshold\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nif isinstance(self.threshold, tuple):\nthreshold = 256 - round(random.uniform(self.threshold[0], self.threshold[1]))\nelse:\nthreshold = 256 - round(random.uniform(0, self.threshold))\nreturn [Solarize._apply_solarize(elem, threshold) for elem in data]\n@staticmethod\ndef _apply_solarize(data: np.ndarray, threshold: int) -&gt; np.ndarray:\n\"\"\"Invert all pixel values of the image above a threshold.\n        Args:\n            data: The image to be modified.\n            threshold: Solarizing threshold.\n        Returns:\n            The image after applying solarize.\n        \"\"\"\ndata = np.where(data &lt; threshold, data, 255 - data)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rua.html#fastestimator.fastestimator.op.numpyop.univariate.rua.Solarize.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>Union[int, float]</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rua.py</code> <pre><code>def set_rua_level(self, magnitude_coef: Union[int, float]) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nif isinstance(self.threshold, tuple):\nself.threshold = magnitude_coef * (self.threshold[1] - self.threshold[0]) + self.threshold[0]\nelse:\nself.threshold = magnitude_coef * self.threshold\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/sharpness.html", "title": "sharpness", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/sharpness.html#fastestimator.fastestimator.op.numpyop.univariate.sharpness.Sharpness", "title": "<code>Sharpness</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly change the sharpness of an image.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>limit</code> <code>float</code> <p>Factor range for changing sharpness. If limit is a single float, the range will be (-limit, limit). A factor of 0.0 gives a blurred image, a factor of 1.0 gives the original image, and a factor of 2.0 gives a sharpened image.</p> <code>0.54</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\sharpness.py</code> <pre><code>@traceable()\nclass Sharpness(NumpyOp):\n\"\"\"Randomly change the sharpness of an image.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        limit: Factor range for changing sharpness. If limit is a single float, the range will be (-limit, limit).\n            A factor of 0.0 gives a blurred image, a factor of 1.0 gives the original image, and a factor of 2.0 gives\n            a sharpened image.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nlimit: float = 0.54):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.limit = param_to_range(limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = 1.0 + random.uniform(self.limit[0], self.limit[1])\nreturn [Sharpness._apply_sharpness(elem, factor) for elem in data]\n@staticmethod\ndef _apply_sharpness(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nim = ImageEnhance.Sharpness(im).enhance(factor)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/sharpness.html#fastestimator.fastestimator.op.numpyop.univariate.sharpness.Sharpness.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\sharpness.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.limit[1] + self.limit[0]) / 2\nparam_extent = magnitude_coef * ((self.limit[1] - self.limit[0]) / 2)\nself.limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/shear_x.html", "title": "shear_x", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/shear_x.html#fastestimator.fastestimator.op.numpyop.univariate.shear_x.ShearX", "title": "<code>ShearX</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly shear the image along the X axis.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>shear_coef</code> <code>float</code> <p>Factor range for shear. If shear_coef is a single float, the range will be (-shear_coef, shear_coef)</p> <code>0.3</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\shear_x.py</code> <pre><code>@traceable()\nclass ShearX(NumpyOp):\n\"\"\"Randomly shear the image along the X axis.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        shear_coef: Factor range for shear. If shear_coef is a single float, the range will be (-shear_coef, shear_coef)\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nshear_coef: float = 0.3):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shear_coef = param_to_range(shear_coef)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.shear_coef[1] + self.shear_coef[0]) / 2\nparam_extent = magnitude_coef * ((self.shear_coef[1] - self.shear_coef[0]) / 2)\nself.shear_coef = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nshear_coeff = random.uniform(self.shear_coef[0], self.shear_coef[1])\nreturn [ShearX._apply_shearx(elem, shear_coeff) for elem in data]\n@staticmethod\ndef _apply_shearx(data: np.ndarray, shear_coeff: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nwidth, height = im.size\nxshift = round(abs(shear_coeff) * width)\nnew_width = width + xshift\nim = im.transform((new_width, height),\nImageTransform.AffineTransform(\n(1.0, shear_coeff, -xshift if shear_coeff &gt; 0 else 0.0, 0.0, 1.0, 0.0)),\nresample=Image.BICUBIC)\nim = im.resize((width, height))\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/shear_x.html#fastestimator.fastestimator.op.numpyop.univariate.shear_x.ShearX.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\shear_x.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.shear_coef[1] + self.shear_coef[0]) / 2\nparam_extent = magnitude_coef * ((self.shear_coef[1] - self.shear_coef[0]) / 2)\nself.shear_coef = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/shear_y.html", "title": "shear_y", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/shear_y.html#fastestimator.fastestimator.op.numpyop.univariate.shear_y.ShearY", "title": "<code>ShearY</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly shear the image along the Y axis.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>shear_coef</code> <code>float</code> <p>Factor range for shear. If shear_coef is a single float, the range will be (-shear_coef, shear_coef)</p> <code>0.3</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\shear_y.py</code> <pre><code>@traceable()\nclass ShearY(NumpyOp):\n\"\"\"Randomly shear the image along the Y axis.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        shear_coef: Factor range for shear. If shear_coef is a single float, the range will be (-shear_coef, shear_coef)\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nshear_coef: float = 0.3):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shear_coef = param_to_range(shear_coef)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.shear_coef[1] + self.shear_coef[0]) / 2\nparam_extent = magnitude_coef * ((self.shear_coef[1] - self.shear_coef[0]) / 2)\nself.shear_coef = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nshear_coeff = random.uniform(self.shear_coef[0], self.shear_coef[1])\nreturn [ShearY._apply_sheary(elem, shear_coeff) for elem in data]\n@staticmethod\ndef _apply_sheary(data: np.ndarray, shear_coeff: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nwidth, height = im.size\nyshift = round(abs(shear_coeff) * height)\nnewheight = height + yshift\nim = im.transform((width, newheight),\nImageTransform.AffineTransform(\n(1.0, 0.0, 0.0, shear_coeff, 1.0, -yshift if shear_coeff &gt; 0 else 0.0)),\nresample=Image.BICUBIC)\nim = im.resize((width, height))\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/shear_y.html#fastestimator.fastestimator.op.numpyop.univariate.shear_y.ShearY.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\shear_y.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.shear_coef[1] + self.shear_coef[0]) / 2\nparam_extent = magnitude_coef * ((self.shear_coef[1] - self.shear_coef[0]) / 2)\nself.shear_coef = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/solarize.html", "title": "solarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/solarize.html#fastestimator.fastestimator.op.numpyop.univariate.solarize.Solarize", "title": "<code>Solarize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be solarized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the solarized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>threshold</code> <code>Union[int, Tuple[int, int], float, Tuple[float, float]]</code> <p>Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].</p> <code>128</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\solarize.py</code> <pre><code>@traceable()\nclass Solarize(ImageOnlyAlbumentation):\n\"\"\"Invert all pixel values above a threshold.\n    Args:\n        inputs: Key(s) of images to be solarized.\n        outputs: Key(s) into which to write the solarized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        threshold: Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nthreshold: Union[int, Tuple[int, int], float, Tuple[float, float]] = 128):\nsuper().__init__(SolarizeAlb(threshold=threshold, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_array.html", "title": "to_array", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_array.html#fastestimator.fastestimator.op.numpyop.univariate.to_array.ToArray", "title": "<code>ToArray</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert data to a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of the data to be converted.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype to apply to the output array, or None to infer the type.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_array.py</code> <pre><code>@traceable()\nclass ToArray(NumpyOp):\n\"\"\"Convert data to a numpy array.\n    Args:\n        inputs: Key(s) of the data to be converted.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        dtype: The dtype to apply to the output array, or None to infer the type.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\ndtype: Optional[str] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.dtype = dtype\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Any], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_transform(elem) for elem in data]\ndef _apply_transform(self, data: Any) -&gt; np.ndarray:\nreturn np.array(data, dtype=self.dtype)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_float.html", "title": "to_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_float.html#fastestimator.fastestimator.op.numpyop.univariate.to_float.ToFloat", "title": "<code>ToFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divides an input by max_value to give a float image in range [0,1].</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to floating point representation.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the divisor. If None it will be inferred by dtype.</p> <code>None</code> Image types <p>Any</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_float.py</code> <pre><code>@traceable()\nclass ToFloat(ImageOnlyAlbumentation):\n\"\"\"Divides an input by max_value to give a float image in range [0,1].\n    Args:\n        inputs: Key(s) of images to be converted to floating point representation.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        max_value: The maximum value to serve as the divisor. If None it will be inferred by dtype.\n    Image types:\n        Any\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None):\nsuper().__init__(ToFloatAlb(max_value=max_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode,\nds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html", "title": "to_gray", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html#fastestimator.fastestimator.op.numpyop.univariate.to_gray.ToGray", "title": "<code>ToGray</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to grayscale.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_gray.py</code> <pre><code>@traceable()\nclass ToGray(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.\n    Args:\n        inputs: Key(s) of images to be converted to grayscale.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToGrayAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html", "title": "to_sepia", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html#fastestimator.fastestimator.op.numpyop.univariate.to_sepia.ToSepia", "title": "<code>ToSepia</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to sepia.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be converted to sepia.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the sepia images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_sepia.py</code> <pre><code>@traceable()\nclass ToSepia(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to sepia.\n    Args:\n        inputs: Key(s) of images to be converted to sepia.\n        outputs: Key(s) into which to write the sepia images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToSepiaAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html", "title": "tokenize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html#fastestimator.fastestimator.op.numpyop.univariate.tokenize.Tokenize", "title": "<code>Tokenize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Split the sequences into tokens.</p> <p>Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if defined in the passed function object. By default, tokenize only splits the sequences into tokens.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be tokenized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are tokenized.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>tokenize_fn</code> <code>Union[None, Callable[[str], List[str]]]</code> <p>Tokenization function object.</p> <code>None</code> <code>to_lower_case</code> <code>bool</code> <p>Whether to convert tokens to lowercase.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\tokenize.py</code> <pre><code>@traceable()\nclass Tokenize(NumpyOp):\n\"\"\"Split the sequences into tokens.\n    Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if\n    defined in the passed function object. By default, tokenize only splits the sequences into tokens.\n    Args:\n        inputs: Key(s) of sequences to be tokenized.\n        outputs: Key(s) of sequences that are tokenized.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        tokenize_fn: Tokenization function object.\n        to_lower_case: Whether to convert tokens to lowercase.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\ntokenize_fn: Union[None, Callable[[str], List[str]]] = None,\nto_lower_case: bool = False) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nself.tokenize_fn = tokenize_fn\nself.to_lower_case = to_lower_case\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[List[str]]:\nreturn [self._apply_tokenization(seq) for seq in data]\ndef _apply_tokenization(self, data: str) -&gt; List[str]:\n\"\"\"Split the sequence into tokens and apply lowercase if `do_lower_case` is set.\n        Args:\n            data: Input sequence.\n        Returns:\n            A list of tokens.\n        \"\"\"\nif self.tokenize_fn:\ndata = self.tokenize_fn(data)\nelse:\ndata = data.split()\nif self.to_lower_case:\ndata = list(map(lambda x: x.lower(), data))\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/translate_x.html", "title": "translate_x", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/translate_x.html#fastestimator.fastestimator.op.numpyop.univariate.translate_x.TranslateX", "title": "<code>TranslateX</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly shift the image along the X axis.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>shift_limit</code> <code>float</code> <p>Shift factor range as a fraction of image width. If shift_limit is a single float, the range will be (-shift_limit, shift_limit).</p> <code>0.2</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\translate_x.py</code> <pre><code>@traceable()\nclass TranslateX(NumpyOp):\n\"\"\"Randomly shift the image along the X axis.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        shift_limit: Shift factor range as a fraction of image width. If shift_limit is a single float, the range will\n            be (-shift_limit, shift_limit).\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nshift_limit: float = 0.2):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shift_limit = param_to_range(shift_limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.shift_limit[1] + self.shift_limit[0]) / 2\nparam_extent = magnitude_coef * ((self.shift_limit[1] - self.shift_limit[0]) / 2)\nself.shift_limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = random.uniform(self.shift_limit[0], self.shift_limit[1])\nreturn [TranslateX._apply_translatex(elem, factor) for elem in data]\n@staticmethod\ndef _apply_translatex(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nwidth, height = im.size\ndisplacement = factor * width\nim = im.transform((width, height),\nImageTransform.AffineTransform((1.0, 0.0, displacement, 0.0, 1.0, 0.0)),\nresample=Image.BICUBIC)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/translate_x.html#fastestimator.fastestimator.op.numpyop.univariate.translate_x.TranslateX.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\translate_x.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.shift_limit[1] + self.shift_limit[0]) / 2\nparam_extent = magnitude_coef * ((self.shift_limit[1] - self.shift_limit[0]) / 2)\nself.shift_limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/translate_y.html", "title": "translate_y", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/translate_y.html#fastestimator.fastestimator.op.numpyop.univariate.translate_y.TranslateY", "title": "<code>TranslateY</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Randomly shift the image along the Y axis.</p> <p>This is a wrapper for functionality provided by the PIL library: https://github.com/python-pillow/Pillow/tree/master/src/PIL.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>shift_limit</code> <code>float</code> <p>Shift factor range as a fraction of image height. If shift_limit is a single float, the range will be (-shift_limit, shift_limit).</p> <code>0.2</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\translate_y.py</code> <pre><code>@traceable()\nclass TranslateY(NumpyOp):\n\"\"\"Randomly shift the image along the Y axis.\n    This is a wrapper for functionality provided by the PIL library:\n    https://github.com/python-pillow/Pillow/tree/master/src/PIL.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        shift_limit: Shift factor range as a fraction of image height. If shift_limit is a single float, the range will\n            be (-shift_limit, shift_limit).\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\nshift_limit: float = 0.2):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shift_limit = param_to_range(shift_limit)\nself.in_list, self.out_list = True, True\ndef set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n        This method is specifically designed to be invoked by the RUA Op.\n        Args:\n            magnitude_coef: The desired augmentation intensity (range [0-1]).\n        \"\"\"\nparam_mid = (self.shift_limit[1] + self.shift_limit[0]) / 2\nparam_extent = magnitude_coef * ((self.shift_limit[1] - self.shift_limit[0]) / 2)\nself.shift_limit = (param_mid - param_extent, param_mid + param_extent)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nfactor = random.uniform(self.shift_limit[0], self.shift_limit[1])\nreturn [TranslateY._apply_translatey(elem, factor) for elem in data]\n@staticmethod\ndef _apply_translatey(data: np.ndarray, factor: float) -&gt; np.ndarray:\nim = Image.fromarray(data)\nwidth, height = im.size\ndisplacement = factor * height\nim = im.transform((width, height),\nImageTransform.AffineTransform((1.0, 0.0, 0.0, 0.0, 1.0, displacement)),\nresample=Image.BICUBIC)\nreturn np.array(im)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/translate_y.html#fastestimator.fastestimator.op.numpyop.univariate.translate_y.TranslateY.set_rua_level", "title": "<code>set_rua_level</code>", "text": "<p>Set the augmentation intensity based on the magnitude_coef.</p> <p>This method is specifically designed to be invoked by the RUA Op.</p> <p>Parameters:</p> Name Type Description Default <code>magnitude_coef</code> <code>float</code> <p>The desired augmentation intensity (range [0-1]).</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\translate_y.py</code> <pre><code>def set_rua_level(self, magnitude_coef: float) -&gt; None:\n\"\"\"Set the augmentation intensity based on the magnitude_coef.\n    This method is specifically designed to be invoked by the RUA Op.\n    Args:\n        magnitude_coef: The desired augmentation intensity (range [0-1]).\n    \"\"\"\nparam_mid = (self.shift_limit[1] + self.shift_limit[0]) / 2\nparam_extent = magnitude_coef * ((self.shift_limit[1] - self.shift_limit[0]) / 2)\nself.shift_limit = (param_mid - param_extent, param_mid + param_extent)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/univariate.html", "title": "univariate", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/univariate.html#fastestimator.fastestimator.op.numpyop.univariate.univariate.ImageOnlyAlbumentation", "title": "<code>ImageOnlyAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Operators which apply to single images (as opposed to images + masks or images + bounding boxes).</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ImageOnlyTransform</code> <p>An Albumentation function to be invoked.</p> required <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the <code>func</code> will be run in replay mode so that the exact same augmentation is applied to each value.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\univariate.py</code> <pre><code>@traceable()\nclass ImageOnlyAlbumentation(NumpyOp):\n\"\"\"Operators which apply to single images (as opposed to images + masks or images + bounding boxes).\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        inputs: Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the\n            `func` will be run in replay mode so that the exact same augmentation is applied to each value.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nfunc: ImageOnlyTransform,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.func = Compose(transforms=[func])\nself.replay_func = ReplayCompose(transforms=[deepcopy(func)])\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresults = [self.replay_func(image=data[0]) if len(data) &gt; 1 else self.func(image=data[0])]\nfor i in range(1, len(data)):\nresults.append(self.replay_func.replay(results[0]['replay'], image=data[i]))\nreturn [result[\"image\"] for result in results]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html", "title": "word_to_id", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html#fastestimator.fastestimator.op.numpyop.univariate.word_to_id.WordtoId", "title": "<code>WordtoId</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Converts words to their corresponding id using mapper function or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Union[Dict[str, int], Callable[[List[str]], List[int]]]</code> <p>Mapper function or dictionary</p> required <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences to be converted to ids.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences are converted to ids.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\word_to_id.py</code> <pre><code>@traceable()\nclass WordtoId(NumpyOp):\n\"\"\"Converts words to their corresponding id using mapper function or dictionary.\n    Args:\n        mapping: Mapper function or dictionary\n        inputs: Key(s) of sequences to be converted to ids.\n        outputs: Key(s) of sequences are converted to ids.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nmapping: Union[Dict[str, int], Callable[[List[str]], List[int]]],\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nassert callable(mapping) or isinstance(mapping, dict), \\\n            \"Incorrect data type provided for `mapping`. Please provide a function or a dictionary.\"\nself.mapping = mapping\ndef forward(self, data: List[List[str]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._convert_to_id(elem) for elem in data]\ndef _convert_to_id(self, data: List[str]) -&gt; np.ndarray:\n\"\"\"Flatten the input list and map the token to ids using mapper function or lookup table.\n        Args:\n            data: Input array of tokens\n        Raises:\n            Exception: If neither of the mapper function or dictionary object is passed\n        Returns:\n            Array of token ids\n        \"\"\"\nif callable(self.mapping):\ndata = self.mapping(data)\nelse:\ndata = [self.mapping.get(token) for token in data]\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/argmax.html", "title": "argmax", "text": ""}, {"location": "fastestimator/op/tensorop/argmax.html#fastestimator.fastestimator.op.tensorop.argmax.Argmax", "title": "<code>Argmax</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Get the argmax from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>axis</code> <code>int</code> <p>The axis along which to collect the argmax.</p> <code>0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\argmax.py</code> <pre><code>@traceable()\nclass Argmax(TensorOp):\n\"\"\"Get the argmax from a tensor.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        outputs: The key(s) under which to save the output.\n        axis: The axis along which to collect the argmax.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\naxis: int = 0,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nreturn [argmax(tensor=tensor, axis=self.axis) for tensor in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/average.html", "title": "average", "text": ""}, {"location": "fastestimator/op/tensorop/average.html#fastestimator.fastestimator.op.tensorop.average.Average", "title": "<code>Average</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Compute the average across tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Keys of tensors to be averaged.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\average.py</code> <pre><code>@traceable()\nclass Average(TensorOp):\n\"\"\"Compute the average across tensors.\n    Args:\n        inputs: Keys of tensors to be averaged.\n        outputs: The key under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, False\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\nresult = zeros_like(data[0])\nfor tensor in data:\nresult += tensor\nreturn result / len(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/dice.html", "title": "dice", "text": ""}, {"location": "fastestimator/op/tensorop/dice.html#fastestimator.fastestimator.op.tensorop.dice.Dice", "title": "<code>Dice</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Calculate Element-Wise Dice Score.</p> <pre><code>Args:\ninputs: A tuple or list of keys representing prediction and ground truth, like: (\"y_pred\", \"y_true\").\noutputs: The key under which to save the output.\nmode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\nregardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\nlike \"!infer\" or \"!train\".\nds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\nds_ids except for a particular one, you can pass an argument like \"!ds1\".\nsoft_dice: Whether to square elements. If True, square of elements is added.\nsample_average: Whether to average the element-wise dice score.\nchannel_average: Whether to average the channel wise dice score.\nnegate: Whether to negate dice score.\nepsilon: A small value to prevent numeric instability in the division.\n\nReturns:\nThe dice loss between `y_pred` and `y_true`. A scalar if `average_sample` is True, else a\ntensor with the shape (Batch).\n\nRaises:\nAssertionError: If `y_true` or `y_pred` are unacceptable data types.\n</code></pre> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\dice.py</code> <pre><code>class Dice(TensorOp):\n\"\"\"\n    Calculate Element-Wise Dice Score.\n        Args:\n            inputs: A tuple or list of keys representing prediction and ground truth, like: (\"y_pred\", \"y_true\").\n            outputs: The key under which to save the output.\n            mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n                regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n                like \"!infer\" or \"!train\".\n            ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n                ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n            soft_dice: Whether to square elements. If True, square of elements is added.\n            sample_average: Whether to average the element-wise dice score.\n            channel_average: Whether to average the channel wise dice score.\n            negate: Whether to negate dice score.\n            epsilon: A small value to prevent numeric instability in the division.\n        Returns:\n            The dice loss between `y_pred` and `y_true`. A scalar if `average_sample` is True, else a\n            tensor with the shape (Batch).\n        Raises:\n            AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\nsoft_dice: bool = False,\nsample_average: bool = False,\nchannel_average: bool = False,\nnegate: bool = False,\nepsilon: float = 1e-6):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.channel_average = channel_average\nself.soft_dice = soft_dice\nself.epsilon = epsilon\nself.sample_average = sample_average\nself.negate = negate\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\ndice = dice_score(y_pred, y_true, self.soft_dice, self.sample_average, self.channel_average, self.epsilon)\nreturn -dice if self.negate else dice\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gather.html", "title": "gather", "text": ""}, {"location": "fastestimator/op/tensorop/gather.html#fastestimator.fastestimator.op.tensorop.gather.Gather", "title": "<code>Gather</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Gather values from an input tensor.</p> <p>If indices are not provided, the maximum values along the batch dimension will be collected.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>indices</code> <code>Union[None, str, List[str]]</code> <p>A tensor containing target indices to gather.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gather.py</code> <pre><code>@traceable()\nclass Gather(TensorOp):\n\"\"\"Gather values from an input tensor.\n    If indices are not provided, the maximum values along the batch dimension will be collected.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        indices: A tensor containing target indices to gather.\n        outputs: The key(s) under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nindices: Union[None, str, List[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nindices = to_list(indices)\nself.num_indices = len(indices)\ncombined_inputs = indices\ncombined_inputs.extend(to_list(inputs))\nsuper().__init__(inputs=combined_inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nindices = data[:self.num_indices]\ninputs = data[self.num_indices:]\nresults = []\nfor idx, tensor in enumerate(inputs):\n# Check len(indices[0]) since an empty indices element is used to trigger the else\nif tf.is_tensor(indices[0]) or isinstance(indices[0], torch.Tensor):\nelem_len = indices[0].shape[0]\nelse:\nelem_len = len(indices[0])\nif len(indices) &gt; idx and elem_len &gt; 0:\nresults.append(gather_from_batch(tensor, indices=indices[idx]))\nelif len(indices) == 1 and elem_len &gt; 0:\n# One set of indices for all outputs\nresults.append(gather_from_batch(tensor, indices=indices[0]))\nelse:\nresults.append(reduce_max(tensor, 1))  # The maximum value within each batch element\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/normalize.html", "title": "normalize", "text": ""}, {"location": "fastestimator/op/tensorop/normalize.html#fastestimator.fastestimator.op.tensorop.normalize.Normalize", "title": "<code>Normalize</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Normalize a input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the input tensor that is to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the output tensor that has been normalized.</p> required <code>mean</code> <code>Union[float, Sequence[float]]</code> <p>The mean which needs to applied (eg: None, 0.54, (0.24, 0.34, 0.35))</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Union[float, Sequence[float]]</code> <p>The standard deviation which needs to applied (eg: None, 0.4, (0.1, 0.25, 0.45))</p> <code>(0.229, 0.224, 0.225)</code> <code>max_pixel_value</code> <code>float</code> <p>The max value of the input data(eg: 255, 65025) to be multipled with mean and std to get actual mean and std.                 To directly use the mean and std provide set max_pixel_value as 1.</p> <code>255.0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\normalize.py</code> <pre><code>@traceable()\nclass Normalize(TensorOp):\n\"\"\"Normalize a input tensor.\n    Args:\n        inputs: Key of the input tensor that is to be normalized.\n        outputs: Key of the output tensor that has been normalized.\n        mean: The mean which needs to applied (eg: None, 0.54, (0.24, 0.34, 0.35))\n        std: The standard deviation which needs to applied (eg: None, 0.4, (0.1, 0.25, 0.45))\n        max_pixel_value: The max value of the input data(eg: 255, 65025) to be multipled with mean and std to get actual mean and std.\n                            To directly use the mean and std provide set max_pixel_value as 1.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\nmean: Union[float, Sequence[float]] = (0.485, 0.456, 0.406),\nstd: Union[float, Sequence[float]] = (0.229, 0.224, 0.225),\nmax_pixel_value: float = 255.0,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.mean = mean\nself.std = std\nself.max_pixel_value = max_pixel_value\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nreturn normalize(data, self.mean, self.std, self.max_pixel_value)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/permute.html", "title": "permute", "text": ""}, {"location": "fastestimator/op/tensorop/permute.html#fastestimator.fastestimator.op.tensorop.permute.Permute", "title": "<code>Permute</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Permute a input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the input tensor that is to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the output tensor that has been normalized.</p> required <code>permutation</code> <code>Sequence[int]</code> <p>Sequence[int]</p> <code>(0, 3, 1, 2)</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\permute.py</code> <pre><code>@traceable()\nclass Permute(TensorOp):\n\"\"\"Permute a input tensor.\n    Args:\n        inputs: Key of the input tensor that is to be normalized.\n        outputs: Key of the output tensor that has been normalized.\n        permutation: Sequence[int]\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\npermutation: Sequence[int] = (0, 3, 1, 2),\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.permutation = permutation\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nreturn permute(data, self.permutation)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/tensorop/reshape.html#fastestimator.fastestimator.op.tensorop.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Reshape a input tensor to conform to a given shape.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor that is to be reshaped.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor that has been reshaped.</p> required <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>Target shape.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\reshape.py</code> <pre><code>@traceable()\nclass Reshape(TensorOp):\n\"\"\"Reshape a input tensor to conform to a given shape.\n    Args:\n        inputs: Key of the input tensor that is to be reshaped.\n        outputs: Key of the output tensor that has been reshaped.\n        shape: Target shape.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nshape: Union[int, Tuple[int, ...]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.shape = list(shape)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nreturn [reshape(elem, self.shape) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/resize3d.html", "title": "resize3d", "text": ""}, {"location": "fastestimator/op/tensorop/resize3d.html#fastestimator.fastestimator.op.tensorop.resize3d.Resize3D", "title": "<code>Resize3D</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Resize a 3D tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the input tensor.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key of the output tensor.</p> required <code>output_shape</code> <code>Sequence[int]</code> <p>The desired output shape for the input tensor.</p> required <code>resize_mode</code> <code>str</code> <p>The resize mode of the operation ('area' or 'nearest').</p> <code>'nearest'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\resize3d.py</code> <pre><code>@traceable()\nclass Resize3D(TensorOp):\n\"\"\"Resize a 3D tensor.\n        Args:\n            inputs: Key of the input tensor.\n            outputs: Key of the output tensor.\n            output_shape: The desired output shape for the input tensor.\n            resize_mode: The resize mode of the operation ('area' or 'nearest').\n            mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n                regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n                like \"!infer\" or \"!train\".\n            ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n                ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str]],\noutputs: Union[str, Iterable[str]],\noutput_shape: Sequence[int],\nresize_mode: str = 'nearest',\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=to_list(inputs), outputs=to_list(outputs), mode=mode)\nassert resize_mode in ['nearest', 'area'], \"Only following resize modes are supported: 'nearest', 'area' \"\nself.output_shape = output_shape\nself.reize_mode = resize_mode\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nreturn [resize_3d(elem, self.output_shape, self.resize_mode) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html", "title": "tensorop", "text": ""}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.LambdaOp", "title": "<code>LambdaOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>An Operator that performs any specified function as forward function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable</code> <p>The function to be executed.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>@traceable()\nclass LambdaOp(TensorOp):\n\"\"\"An Operator that performs any specified function as forward function.\n    Args:\n        fn: The function to be executed.\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nfn: Callable,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.fn = fn\nself.in_list = True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nreturn self.fn(*data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp", "title": "<code>TensorOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns tensor data.</p> <p>These Operators are used in fe.Network to perform graph-based operations like neural network training.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>@traceable()\nclass TensorOp(Op):\n\"\"\"An Operator class which takes and returns tensor data.\n    These Operators are used in fe.Network to perform graph-based operations like neural network training.\n    \"\"\"\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on batches of data.\n        Args:\n            data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\n\"\"\"A method which will be invoked during Network instantiation.\n        This method can be used to augment the natural __init__ method of the TensorOp once the desired backend\n        framework is known.\n        Args:\n            framework: Which framework this Op will be executing in. One of 'tf' or 'torch'.\n            device: Which device this Op will execute on. Usually 'cuda:0' or 'cpu'. Only populated when the `framework`\n                is 'torch'.\n        \"\"\"\npass\n# ###########################################################################\n# The methods below this point can be ignored by most non-FE developers\n# ###########################################################################\n# noinspection PyMethodMayBeStatic\ndef get_fe_models(self) -&gt; Set[Model]:\n\"\"\"A method to get any models held by this Op.\n        All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate\n        models, for example by the Network during load_epoch().\n        Returns:\n            Any models held by this Op.\n        \"\"\"\nreturn set()\n# noinspection PyMethodMayBeStatic\ndef get_fe_loss_keys(self) -&gt; Set[str]:\n\"\"\"A method to get any loss keys held by this Op.\n        All users and most developers can safely ignore this method. This method may be invoked to gather information\n        about losses, for example by the Network in get_loss_keys().\n        Returns:\n            Any loss keys held by this Op.\n        \"\"\"\nreturn set()\n# noinspection PyMethodMayBeStatic\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\n\"\"\"A method to get / set whether this Op should retain network gradients after computing them.\n        All users and most developers can safely ignore this method. Ops which do not compute gradients should leave\n        this method alone. If this method is invoked with `retain` as True or False, then the gradient computations\n        performed by this Op should retain or discard the graph respectively afterwards.\n        Args:\n            retain: If None, then return the current retain_graph status of the Op. If True or False, then set the\n                retain_graph status of the op to the new status and return the new status.\n        Returns:\n            Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not\n            compute backward gradients.\n        \"\"\"\nreturn None\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.build", "title": "<code>build</code>", "text": "<p>A method which will be invoked during Network instantiation.</p> <p>This method can be used to augment the natural init method of the TensorOp once the desired backend framework is known.</p> <p>Parameters:</p> Name Type Description Default <code>framework</code> <code>str</code> <p>Which framework this Op will be executing in. One of 'tf' or 'torch'.</p> required <code>device</code> <code>Optional[torch.device]</code> <p>Which device this Op will execute on. Usually 'cuda:0' or 'cpu'. Only populated when the <code>framework</code> is 'torch'.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\n\"\"\"A method which will be invoked during Network instantiation.\n    This method can be used to augment the natural __init__ method of the TensorOp once the desired backend\n    framework is known.\n    Args:\n        framework: Which framework this Op will be executing in. One of 'tf' or 'torch'.\n        device: Which device this Op will execute on. Usually 'cuda:0' or 'cpu'. Only populated when the `framework`\n            is 'torch'.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.fe_retain_graph", "title": "<code>fe_retain_graph</code>", "text": "<p>A method to get / set whether this Op should retain network gradients after computing them.</p> <p>All users and most developers can safely ignore this method. Ops which do not compute gradients should leave this method alone. If this method is invoked with <code>retain</code> as True or False, then the gradient computations performed by this Op should retain or discard the graph respectively afterwards.</p> <p>Parameters:</p> Name Type Description Default <code>retain</code> <code>Optional[bool]</code> <p>If None, then return the current retain_graph status of the Op. If True or False, then set the retain_graph status of the op to the new status and return the new status.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[bool]</code> <p>Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not</p> <code>Optional[bool]</code> <p>compute backward gradients.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\n\"\"\"A method to get / set whether this Op should retain network gradients after computing them.\n    All users and most developers can safely ignore this method. Ops which do not compute gradients should leave\n    this method alone. If this method is invoked with `retain` as True or False, then the gradient computations\n    performed by this Op should retain or discard the graph respectively afterwards.\n    Args:\n        retain: If None, then return the current retain_graph status of the Op. If True or False, then set the\n            retain_graph status of the op to the new status and return the new status.\n    Returns:\n        Whether this Op will retain the backward gradient graph after it's forward pass, or None if this Op does not\n        compute backward gradients.\n    \"\"\"\nreturn None\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[Tensor]]</code> <p>The batch from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[Tensor, List[Tensor]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on batches of data.\n    Args:\n        data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.get_fe_loss_keys", "title": "<code>get_fe_loss_keys</code>", "text": "<p>A method to get any loss keys held by this Op.</p> <p>All users and most developers can safely ignore this method. This method may be invoked to gather information about losses, for example by the Network in get_loss_keys().</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Any loss keys held by this Op.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def get_fe_loss_keys(self) -&gt; Set[str]:\n\"\"\"A method to get any loss keys held by this Op.\n    All users and most developers can safely ignore this method. This method may be invoked to gather information\n    about losses, for example by the Network in get_loss_keys().\n    Returns:\n        Any loss keys held by this Op.\n    \"\"\"\nreturn set()\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.get_fe_models", "title": "<code>get_fe_models</code>", "text": "<p>A method to get any models held by this Op.</p> <p>All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate models, for example by the Network during load_epoch().</p> <p>Returns:</p> Type Description <code>Set[Model]</code> <p>Any models held by this Op.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def get_fe_models(self) -&gt; Set[Model]:\n\"\"\"A method to get any models held by this Op.\n    All users and most developers can safely ignore this method. This method may be invoked to gather and manipulate\n    models, for example by the Network during load_epoch().\n    Returns:\n        Any models held by this Op.\n    \"\"\"\nreturn set()\n</code></pre>"}, {"location": "fastestimator/op/tensorop/un_hadamard.html", "title": "un_hadamard", "text": ""}, {"location": "fastestimator/op/tensorop/un_hadamard.html#fastestimator.fastestimator.op.tensorop.un_hadamard.UnHadamard", "title": "<code>UnHadamard</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Convert hadamard encoded class representations into onehot probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor(s) to be converted.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor(s) as class probabilities.</p> required <code>n_classes</code> <code>int</code> <p>How many classes are there in the inputs.</p> required <code>code_length</code> <code>Optional[int]</code> <p>What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\un_hadamard.py</code> <pre><code>@traceable()\nclass UnHadamard(TensorOp):\n\"\"\"Convert hadamard encoded class representations into onehot probabilities.\n    Args:\n        inputs: Key of the input tensor(s) to be converted.\n        outputs: Key of the output tensor(s) as class probabilities.\n        n_classes: How many classes are there in the inputs.\n        code_length: What code length to use. Will default to the smallest power of 2 which is &gt;= the number of classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nn_classes: int,\ncode_length: Optional[int] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nself.n_classes = n_classes\nif code_length is None:\ncode_length = 1 &lt;&lt; (n_classes - 1).bit_length()\nif code_length &lt;= 0 or (code_length &amp; (code_length - 1) != 0):\nraise ValueError(f\"code_length must be a positive power of 2, but got {code_length}.\")\nif code_length &lt; n_classes:\nraise ValueError(f\"code_length must be &gt;= n_classes, but got {code_length} and {n_classes}\")\nself.code_length = code_length\nself.labels = None\nself.eps = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nlabels = hadamard(self.code_length).astype(np.float32)\nlabels[np.arange(0, self.code_length, 2), 0] = -1  # Make first column alternate\nlabels = labels[:self.n_classes]\nself.labels = to_tensor(labels, target_type=framework)\nmax_prob = 0.99999  # This will only be approximate since the first column is alternating\npower = 1.0\nself.eps = to_tensor(\nnp.array((self.code_length + 1) * math.pow((1.0 - max_prob) / (max_prob * (self.n_classes - 1)), 1 / power),\ndtype=np.float32),\ntarget_type=framework)\nif framework == \"torch\":\nself.labels = self.labels.to(device)\nself.eps = self.eps.to(device)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nresults = []\nfor elem in data:\n# L1 Distance\nx = reduce_sum(abs(expand_dims(elem, axis=1) - self.labels), axis=-1)\nx = iwd(x, power=1.0, eps=self.eps)\nresults.append(x)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/augmentation/cutmix_batch.html", "title": "cutmix_batch", "text": ""}, {"location": "fastestimator/op/tensorop/augmentation/cutmix_batch.html#fastestimator.fastestimator.op.tensorop.augmentation.cutmix_batch.CutMixBatch", "title": "<code>CutMixBatch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs cutmix augmentation on a batch of tensors.</p> <p>In this augmentation technique patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. This class should be used in conjunction with MixLoss to perform CutMix training, which helps to reduce over-fitting, perform object detection, and against adversarial attacks (https://arxiv.org/pdf/1905.04899.pdf).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Iterable[str]</code> <p>Keys of the image batch and label batch to be cut-mixed.</p> required <code>outputs</code> <code>Iterable[str]</code> <p>Keys under which to store the cut-mixed images and cut-mixed label.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'train'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>alpha</code> <code>Union[float, Tensor]</code> <p>The alpha value defining the beta distribution to be drawn from during training which controls the combination ratio between image pairs.</p> <code>1.0</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided inputs are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\augmentation\\cutmix_batch.py</code> <pre><code>class CutMixBatch(TensorOp):\n\"\"\"This class performs cutmix augmentation on a batch of tensors.\n    In this augmentation technique patches are cut and pasted among training images where the ground truth labels are\n    also mixed proportionally to the area of the patches. This class should be used in conjunction with MixLoss to\n    perform CutMix training, which helps to reduce over-fitting, perform object detection, and against adversarial\n    attacks (https://arxiv.org/pdf/1905.04899.pdf).\n    Args:\n        inputs: Keys of the image batch and label batch to be cut-mixed.\n        outputs: Keys under which to store the cut-mixed images and cut-mixed label.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        alpha: The alpha value defining the beta distribution to be drawn from during training which controls the\n            combination ratio between image pairs.\n    Raises:\n        AssertionError: If the provided inputs are invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Iterable[str],\noutputs: Iterable[str],\nmode: Union[None, str, Iterable[str]] = 'train',\nds_id: Union[None, str, Iterable[str]] = None,\nalpha: Union[float, Tensor] = 1.0) -&gt; None:\nassert alpha &gt; 0, \"Alpha value must be greater than zero\"\nassert len(inputs) == 2, \"Cut-Mix must have exactly 2 inputs\"\nassert len(outputs) == 2, \"Cut-Mix must have exactly 2 outputs\"\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.alpha = alpha\nself.beta = None\nself.uniform = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nif framework == 'tf':\nself.beta = tfp.distributions.Beta(self.alpha, self.alpha)\nself.uniform = tfp.distributions.Uniform()\nelif framework == 'torch':\nself.beta = torch.distributions.beta.Beta(self.alpha, self.alpha)\nself.uniform = torch.distributions.uniform.Uniform(low=0, high=1)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\n@staticmethod\ndef _get_patch_coordinates(tensor: Tensor, x: Tensor, y: Tensor,\nlam: Tensor) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n\"\"\"Randomly cut the patches from input images.\n        If patches are going to be pasted in other image, combination ratio between two images is defined by `lam`.\n        Cropping region indicates where to drop out from the image and `cut_x` &amp; `cut_y` are used to calculate cropping\n        region whose aspect ratio is proportional to the original image.\n        Args:\n            tensor: The input value.\n            lam: Combination ratio between two images. Larger the lambda value is smaller the patch would be. A\n                scalar tensor containing value between 0 and 1.\n            x: X-coordinate in image from which patch needs to be cropped. A scalar tensor containing value between 0\n                and 1 which in turn is transformed in the range of image width.\n            y: Y-coordinate in image from which patch needs to be cropped. A scalar tensor containing value between 0\n                and 1 which in turn is transformed in the range of image height.\n        Returns:\n            The X and Y coordinates of the cropped patch along with width and height.\n        \"\"\"\n_, img_height, img_width = get_image_dims(tensor)\nif tf.is_tensor(tensor):\nht = cast(img_height, \"float32\")\nwd = cast(img_width, \"float32\")\nelse:\nht = img_height\nwd = img_width\ncut_x = wd * x\ncut_y = ht * y\ncut_w = wd * tensor_sqrt(1 - lam)\ncut_h = ht * tensor_sqrt(1 - lam)\nbbox_x1 = cast(tensor_round(clip_by_value(cut_x - cut_w / 2, min_value=0)), \"int32\")\nbbox_x2 = cast(tensor_round(clip_by_value(cut_x + cut_w / 2, max_value=wd)), \"int32\")\nbbox_y1 = cast(tensor_round(clip_by_value(cut_y - cut_h / 2, min_value=0)), \"int32\")\nbbox_y2 = cast(tensor_round(clip_by_value(cut_y + cut_h / 2, max_value=ht)), \"int32\")\nreturn bbox_x1, bbox_x2, bbox_y1, bbox_y2, img_width, img_height\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tuple[Tensor, Tensor]:\nx, y = data\nlam = self.beta.sample()\nlam = maximum(lam, (1 - lam))\ncut_x = self.uniform.sample()\ncut_y = self.uniform.sample()\nbbox_x1, bbox_x2, bbox_y1, bbox_y2, width, height = self._get_patch_coordinates(x, cut_x, cut_y, lam=lam)\nif tf.is_tensor(x):\nrolled_x = roll(x, shift=1, axis=0)\npatches = rolled_x[:, bbox_y1:bbox_y2, bbox_x1:bbox_x2, :] - x[:, bbox_y1:bbox_y2, bbox_x1:bbox_x2, :]\npatches = tf.pad(patches, [[0, 0], [bbox_y1, height - bbox_y2], [bbox_x1, width - bbox_x2], [0, 0]],\nmode=\"CONSTANT\",\nconstant_values=0)\nx = x + patches\nelse:\nrolled_x = roll(x, shift=1, axis=0)\nx[:, :, bbox_y1:bbox_y2, bbox_x1:bbox_x2] = rolled_x[:, :, bbox_y1:bbox_y2, bbox_x1:bbox_x2]\n# adjust lambda to match pixel ratio\nlam = 1 - cast(((bbox_x2 - bbox_x1) * (bbox_y2 - bbox_y1)), dtype=\"float32\") / cast((width * height), \"float32\")\nrolled_y = roll(y, shift=1, axis=0) * (1. - lam)\nmixed_y = (y * lam) + rolled_y\nreturn x, mixed_y\n</code></pre>"}, {"location": "fastestimator/op/tensorop/augmentation/mixup_batch.html", "title": "mixup_batch", "text": ""}, {"location": "fastestimator/op/tensorop/augmentation/mixup_batch.html#fastestimator.fastestimator.op.tensorop.augmentation.mixup_batch.MixUpBatch", "title": "<code>MixUpBatch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>MixUp augmentation for tensors.</p> <p>This class should be used in conjunction with MixLoss to perform mix-up training, which helps to reduce over-fitting, stabilize GAN training, and against adversarial attacks (https://arxiv.org/abs/1710.09412).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Iterable[str]</code> <p>Keys of the image batch and label batch to be mixed up.</p> required <code>outputs</code> <code>Iterable[str]</code> <p>Keys under which to store the mixed up images and mixed up label.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode to execute in. Probably 'train'.</p> <code>'train'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>alpha</code> <code>float</code> <p>The alpha value defining the beta distribution to be drawn from during training.</p> <code>1.0</code> <code>shared_beta</code> <code>bool</code> <p>Sample a single beta for a batch or element wise beta for each image.</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input arguments are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\augmentation\\mixup_batch.py</code> <pre><code>class MixUpBatch(TensorOp):\n\"\"\"MixUp augmentation for tensors.\n    This class should be used in conjunction with MixLoss to perform mix-up training, which helps to reduce\n    over-fitting, stabilize GAN training, and against adversarial attacks (https://arxiv.org/abs/1710.09412).\n    Args:\n        inputs: Keys of the image batch and label batch to be mixed up.\n        outputs: Keys under which to store the mixed up images and mixed up label.\n        mode: What mode to execute in. Probably 'train'.\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        alpha: The alpha value defining the beta distribution to be drawn from during training.\n        shared_beta: Sample a single beta for a batch or element wise beta for each image.\n    Raises:\n        AssertionError: If input arguments are invalid.\n    \"\"\"\ndef __init__(self,\ninputs: Iterable[str],\noutputs: Iterable[str],\nmode: Union[None, str, Iterable[str]] = 'train',\nds_id: Union[None, str, Iterable[str]] = None,\nalpha: float = 1.0,\nshared_beta: bool = False):\nassert alpha &gt; 0, \"MixUp alpha value must be greater than zero\"\nassert len(inputs) == 2, \"MixUp must have exactly 2 inputs\"\nassert len(outputs) == 2, \"MixUp must have exactly 2 outputs\"\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.alpha = alpha\nself.beta = None\nself.shared_beta = shared_beta\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nif framework == 'tf':\nself.beta = tfp.distributions.Beta(self.alpha, self.alpha)\nelif framework == 'torch':\nself.beta = torch.distributions.beta.Beta(\ntorch.tensor([self.alpha]).to(device), torch.tensor([self.alpha]).to(device))\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tuple[Tensor, Tensor]:\nx, y = data\nif self.shared_beta:\nlam = self.beta.sample()\nlam_y = lam\nelse:\nshp = get_shape(x)\nlam = self.beta.sample(sample_shape=(shp[0], ))\nshape = [-1] + [1] * (len(shp) - 1)\nlam = reshape(lam, shape)\n# Shape for labels\nshape_y = [-1] + [1] * (len(y.shape) - 1)\nlam_y = reshape(lam, shape_y)\n# To ensure we are not merging the same images\nlam = maximum(lam, (1 - lam))\n# Merge Images\nrolled_x = roll(x, shift=1, axis=0) * (1. - lam)\nmixed_x = (x * lam) + rolled_x\n# Merge Labels\nrolled_y = roll(y, shift=1, axis=0) * (1. - lam_y)\nmixed_y = (y * lam_y) + rolled_y\nreturn mixed_x, mixed_y\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html", "title": "fgsm", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html#fastestimator.fastestimator.op.tensorop.gradient.fgsm.FGSM", "title": "<code>FGSM</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Create an adversarial sample from input data using the Fast Gradient Sign Method.</p> <p>See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Key of the input to be attacked.</p> required <code>loss</code> <code>str</code> <p>Key of the loss value to use for gradient computation.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>epsilon</code> <code>float</code> <p>The strength of the perturbation to use in the attack.</p> <code>0.01</code> <code>clip_low</code> <code>Optional[float]</code> <p>a minimum value to clip the output by (defaults to min value of data when set to None).</p> <code>None</code> <code>clip_high</code> <code>Optional[float]</code> <p>a maximum value to clip the output by (defaults to max value of data when set to None).</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\fgsm.py</code> <pre><code>@traceable()\nclass FGSM(TensorOp):\n\"\"\"Create an adversarial sample from input data using the Fast Gradient Sign Method.\n    See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.\n    Args:\n        data: Key of the input to be attacked.\n        loss: Key of the loss value to use for gradient computation.\n        outputs: The key under which to save the output.\n        epsilon: The strength of the perturbation to use in the attack.\n        clip_low: a minimum value to clip the output by (defaults to min value of data when set to None).\n        clip_high: a maximum value to clip the output by (defaults to max value of data when set to None).\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ndata: str,\nloss: str,\noutputs: str,\nepsilon: float = 0.01,\nclip_low: Optional[float] = None,\nclip_high: Optional[float] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=[data, loss], outputs=outputs, mode=mode, ds_id=ds_id)\nself.epsilon = epsilon\nself.clip_low = clip_low\nself.clip_high = clip_high\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ndata, loss = data\ngrad = get_gradient(target=loss, sources=data, tape=state['tape'], retain_graph=self.retain_graph)\nadverse_data = clip_by_value(data + self.epsilon * sign(grad),\nmin_value=self.clip_low or reduce_min(data),\nmax_value=self.clip_high or reduce_max(data))\nreturn adverse_data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/gradient.html", "title": "gradient", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/gradient.html#fastestimator.fastestimator.op.tensorop.gradient.gradient.GradientOp", "title": "<code>GradientOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Return the gradients of finals w.r.t. inputs.</p> <p>Parameters:</p> Name Type Description Default <code>finals</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to compute gradients from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the gradients.</p> required <code>inputs</code> <code>Union[None, str, List[str]]</code> <p>The tensor(s) to compute gradients with respect to, mutually exclusive with <code>model</code>.</p> <code>None</code> <code>model</code> <code>Union[None, tf.keras.Model, torch.nn.Module]</code> <p>The model instance to compute gradients with respect to, mutually exclusive with <code>inputs</code>.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\gradient.py</code> <pre><code>@traceable()\nclass GradientOp(TensorOp):\n\"\"\"Return the gradients of finals w.r.t. inputs.\n    Args:\n        finals: The tensor(s) to compute gradients from.\n        outputs: The key(s) under which to save the gradients.\n        inputs: The tensor(s) to compute gradients with respect to, mutually exclusive with `model`.\n        model: The model instance to compute gradients with respect to, mutually exclusive with `inputs`.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nfinals: Union[str, List[str]],\noutputs: Union[str, List[str]],\ninputs: Union[None, str, List[str]] = None,\nmodel: Union[None, tf.keras.Model, torch.nn.Module] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None):\ninputs = to_list(inputs)\nfinals = to_list(finals)\noutputs = to_list(outputs)\nassert bool(model) != bool(inputs), \"Must provide either one of 'inputs' or 'model'\"\nif model is None:\nassert len(inputs) == len(finals) == len(outputs), \\\n                \"GradientOp requires the same number of inputs, finals, and outputs\"\nelse:\nassert isinstance(model, (tf.keras.Model, torch.nn.Module)), \"Unrecognized model format\"\nassert len(finals) == len(outputs), \"GradientOp requires the same number of finals, and outputs\"\ninputs.extend(finals)\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.model = model\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nself.framework = framework\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nresults = []\nif self.model is None:\ninitials = data[:len(data) // 2]\nfinals = data[len(data) // 2:]\nfor idx, (initial, final) in enumerate(zip(initials, finals)):\nretain_graph = self.retain_graph or not idx == len(finals) - 1\nresults.append(get_gradient(final, initial, tape=state['tape'], retain_graph=retain_graph))\nelse:\nfinals = data\nif self.framework == \"tf\":\ntrainable_params = self.model.trainable_variables\nfor idx, final in enumerate(finals):\ngradient = get_gradient(final, trainable_params, tape=state['tape'])\nresults.append(gradient)\nelif self.framework == \"torch\":\ntrainable_params = [p for p in self.model.parameters() if p.requires_grad]\nfor idx, final in enumerate(finals):\n# get_gradient\nretain_graph = self.retain_graph or not idx == len(finals) - 1\ngradient = get_gradient(final, trainable_params, retain_graph=retain_graph)\nresults.append(gradient)\nelse:\nraise ValueError(f\"Unrecognized framework {self.framework}\")\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/watch.html", "title": "watch", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/watch.html#fastestimator.fastestimator.op.tensorop.gradient.watch.Watch", "title": "<code>Watch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Watch one or more tensors for later gradient computation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>which tensors to watch during future computation.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\watch.py</code> <pre><code>@traceable()\nclass Watch(TensorOp):\n\"\"\"Watch one or more tensors for later gradient computation.\n    Args:\n        inputs: which tensors to watch during future computation.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=inputs, mode=mode, ds_id=ds_id)\nself.in_list, self.out_list = True, True\nself.retain_graph = True\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nfor idx, tensor in enumerate(data):\ndata[idx] = watch(tensor=tensor, tape=state['tape'])\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html", "title": "cross_entropy", "text": ""}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html#fastestimator.fastestimator.op.tensorop.loss.cross_entropy.CrossEntropy", "title": "<code>CrossEntropy</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tuple[str, str], List[str]]</code> <p>A tuple or list like: [, ]. required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss value.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without softmax).</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> <code>form</code> <code>Optional[str]</code> <p>What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will automatically infer the correct form based on tensor shape.</p> <code>None</code> <code>class_weights</code> <code>Optional[Dict[int, float]]</code> <p>Dictionary mapping class indices to a weight for weighting the loss function. Useful when you need to pay more attention to samples from an under-represented class.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>class_weights</code> or it's keys and values are of unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\cross_entropy.py</code> <pre><code>@traceable()\nclass CrossEntropy(LossOp):\n\"\"\"Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss value.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        from_logits: Whether y_pred is logits (without softmax).\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n        form: What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will\n            automatically infer the correct form based on tensor shape.\n        class_weights: Dictionary mapping class indices to a weight for weighting the loss function. Useful when you\n            need to pay more attention to samples from an under-represented class.\n    Raises:\n        AssertionError: If `class_weights` or it's keys and values are of unacceptable data types.\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\nfrom_logits: bool = False,\naverage_loss: bool = True,\nform: Optional[str] = None,\nclass_weights: Optional[Dict[int, float]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id, average_loss=average_loss)\nself.from_logits = from_logits\nself.form = form\nself.cross_entropy_fn = {\n\"binary\": binary_crossentropy,\n\"categorical\": categorical_crossentropy,\n\"sparse\": sparse_categorical_crossentropy\n}\nif class_weights:\nassert isinstance(class_weights, dict), \\\n                \"class_weights should be a dictionary or have None value, got {}\".format(type(class_weights))\nassert all(isinstance(key, int) for key in class_weights.keys()), \\\n                \"Please ensure that the keys of the class_weight dictionary are of type: int\"\nassert all(isinstance(value, float) for value in class_weights.values()), \\\n                \"Please ensure that the values of the class_weight dictionary are of type: float\"\nself.class_weights = class_weights\nself.class_dict = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nif self.class_weights:\nif framework == 'tf':\nkeys_tensor = tf.constant(list(self.class_weights.keys()))\nvals_tensor = tf.constant(list(self.class_weights.values()))\nself.class_dict = tf.lookup.StaticHashTable(\ntf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=1.0)\nelif framework == 'torch':\nself.class_dict = self.class_weights\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nform = self.form\nif form is None:\nif len(y_pred.shape) == 2 and y_pred.shape[-1] &gt; 1:\nif len(y_true.shape) == 2 and y_true.shape[-1] &gt; 1:\nform = \"categorical\"\nelse:\nform = \"sparse\"\nelse:\nform = \"binary\"\nloss = self.cross_entropy_fn[form](y_pred,\ny_true,\nfrom_logits=self.from_logits,\naverage_loss=self.average_loss,\nclass_weights=self.class_dict)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/focal_loss.html", "title": "focal_loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/focal_loss.html#fastestimator.fastestimator.op.tensorop.loss.focal_loss.FocalLoss", "title": "<code>FocalLoss</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate Focal Loss.</p> <p>Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py . Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tuple[str, str], List[str]]</code> <p>A tuple or list like: [, ]. required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss value.</p> required <code>alpha</code> <code>float</code> <p>Weighting factor in range (0,1) to balance     positive vs negative examples or -1 to ignore. Default = 0.25</p> <code>0.25</code> <code>gamma</code> <code>float</code> <p>Exponent of the modulating factor (1 - p_t) to    balance easy vs hard examples.</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>'none' | 'mean' | 'sum'      'none': No reduction will be applied to the output.      'mean': The output will be averaged.      'sum': The output will be summed.</p> <code>'mean'</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without sigmoid).</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize focal loss along samples based on number of positive classes per samples.</p> <code>True</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\focal_loss.py</code> <pre><code>@traceable()\nclass FocalLoss(LossOp):\n\"\"\"Calculate Focal Loss.\n    Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py .\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss value.\n        alpha: Weighting factor in range (0,1) to balance\n                positive vs negative examples or -1 to ignore. Default = 0.25\n        gamma: Exponent of the modulating factor (1 - p_t) to\n               balance easy vs hard examples.\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n        from_logits: Whether y_pred is logits (without sigmoid).\n        normalize: Whether to normalize focal loss along samples based on number of positive classes per samples.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an\n            argument like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\ngamma: float = 2.0,\nalpha: float = 0.25,\nreduction: str = 'mean',\nfrom_logits: bool = False,\nnormalize: bool = True,\nmode: Union[None, str, Iterable[str]] = None,):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.gamma = gamma\nself.alpha = alpha\nself.reduction = reduction\nself.from_logits = from_logits\nself.normalize = normalize\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nreturn focal_loss(y_true,\ny_pred,\ngamma=self.gamma,\nalpha=self.alpha,\nfrom_logits=self.from_logits,\nreduction=self.reduction,\nnormalize=self.normalize)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/hinge.html", "title": "hinge", "text": ""}, {"location": "fastestimator/op/tensorop/loss/hinge.html#fastestimator.fastestimator.op.tensorop.loss.hinge.Hinge", "title": "<code>Hinge</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate the hinge loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tuple[str, str], List[str]]</code> <p>A tuple or list like: [, ]. required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\hinge.py</code> <pre><code>@traceable()\nclass Hinge(LossOp):\n\"\"\"Calculate the hinge loss between two tensors.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nself.average_loss = average_loss\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nloss = hinge(y_true=y_true, y_pred=y_pred)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/l1_loss.html", "title": "l1_loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/l1_loss.html#fastestimator.fastestimator.op.tensorop.loss.l1_loss.L1_Loss", "title": "<code>L1_Loss</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate the L1 loss between two tensors.</p> This LossOp can be used to Implement <p>L1 loss: Is a criterion that calculates Mean Absolute Error between the elements ([, ]). Smooth_L1 loss: Is a criterion that uses squared loss if absolute element wise subtraction ('y_pred - y_true') is less than                 'beta' and vanilla L1 loss otherwise. Huber loss: Is a criterion that uses squared loss if absolute element wise subtraction ('y_pred - y_true') is less than             'beta' and a 'beta' scaled L1 loss otherwise. <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tuple[str, str], List[str]]</code> <p>A tuple or list like: [, ]. required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> <code>loss_type</code> <code>str</code> <p>What type of L1 loss. Can either be 'L1' (L1 Loss), 'Smooth' (Smooth L1 Loss) or 'Huber' (Huber loss). Default:'L1'</p> <code>'L1'</code> <code>beta</code> <code>Union[None, float]</code> <p>A threshold at which to change between L1 and L2 loss. Needs to be a positive number. Default:1.0 . dtype: float16 or float32.</p> <code>1.0</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\l1_loss.py</code> <pre><code>@traceable()\nclass L1_Loss(LossOp):\n\"\"\"Calculate the L1 loss between two tensors.\n    This LossOp can be used to Implement:\n        L1 loss: Is a criterion that calculates Mean Absolute Error between the elements ([&lt;y_pred&gt;, &lt;y_true&gt;]).\n        Smooth_L1 loss: Is a criterion that uses squared loss if absolute element wise subtraction ('y_pred - y_true') is less than\n                        'beta' and vanilla L1 loss otherwise.\n        Huber loss: Is a criterion that uses squared loss if absolute element wise subtraction ('y_pred - y_true') is less than\n                    'beta' and a 'beta' scaled L1 loss otherwise.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n        loss_type: What type of L1 loss. Can either be 'L1' (L1 Loss), 'Smooth' (Smooth L1 Loss) or 'Huber' (Huber loss). Default:'L1'\n        beta: A threshold at which to change between L1 and L2 loss. Needs to be a positive number. Default:1.0 . dtype: float16 or float32.\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True,\nloss_type: str = 'L1',\nbeta: Union[None, float] = 1.0):\nself.average_loss = average_loss\nself.loss_type = loss_type\nself.beta = beta\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nif self.loss_type == 'L1':\nloss = l1_loss(y_true=y_true, y_pred=y_pred)\nelif self.loss_type == 'Smooth':\nloss = smooth_l1_loss(y_true=y_true, y_pred=y_pred, beta=self.beta)\nelif self.loss_type == 'Huber':\nloss = huber(y_true=y_true, y_pred=y_pred, beta=self.beta)\nelse:\nraise ValueError(\n\"Unrecognized Loss type. Can either be None (L1 Loss), 'Smooth' (Smooth L1 Loss) or 'Huber' (Huber loss)\"\n)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/l2_regularization.html", "title": "l2_regularization", "text": ""}, {"location": "fastestimator/op/tensorop/loss/l2_regularization.html#fastestimator.fastestimator.op.tensorop.loss.l2_regularization.L2Regularizaton", "title": "<code>L2Regularizaton</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Calculate L2 Regularization Loss.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>str</code> <p>String key representing input loss.</p> required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss value.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A tensorflow or pytorch model</p> required <code>beta</code> <code>float</code> <p>The multiplicative factor, to weight the l2 regularization loss with the input loss</p> <code>0.01</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\l2_regularization.py</code> <pre><code>@traceable()\nclass L2Regularizaton(TensorOp):\n\"\"\"Calculate L2 Regularization Loss.\n    Args:\n        inputs: String key representing input loss.\n        outputs: String key under which to store the computed loss value.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        model: A tensorflow or pytorch model\n        beta: The multiplicative factor, to weight the l2 regularization loss with the input loss\n    \"\"\"\ndef __init__(self,\ninputs: str,\noutputs: str,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nmode: Union[None, str, Iterable[str]] = None,\nbeta: float = 0.01):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.model = model\nself.beta = beta\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Tensor:\nloss = data\ntotal_loss = l2_regularization(self.model, self.beta) + loss\nreturn total_loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/loss.html", "title": "loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/loss.html#fastestimator.fastestimator.op.tensorop.loss.loss.LossOp", "title": "<code>LossOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Abstract base LossOp class.</p> <p>A base class for loss operations. It can be used directly to perform value pass-through (see the adversarial training showcase for an example of when this is useful).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\loss.py</code> <pre><code>class LossOp(TensorOp):\n\"\"\"Abstract base LossOp class.\n    A base class for loss operations. It can be used directly to perform value pass-through (see the adversarial\n    training showcase for an example of when this is useful).\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.average_loss = average_loss\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[self.true_key_idx]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[self.pred_key_idx]\n@property\ndef true_key_idx(self) -&gt; int:\nreturn 1\n@property\ndef pred_key_idx(self) -&gt; int:\nreturn 0\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html", "title": "mean_squared_error", "text": ""}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html#fastestimator.fastestimator.op.tensorop.loss.mean_squared_error.MeanSquaredError", "title": "<code>MeanSquaredError</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Calculate the mean squared error loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[Tuple[str, str], List[str]]</code> <p>A tuple or list like: [, ]. required <code>outputs</code> <code>str</code> <p>String key under which to store the computed loss.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\mean_squared_error.py</code> <pre><code>@traceable()\nclass MeanSquaredError(LossOp):\n\"\"\"Calculate the mean squared error loss between two tensors.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n    \"\"\"\ndef __init__(self,\ninputs: Union[Tuple[str, str], List[str]],\noutputs: str,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nds_id: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nself.average_loss = average_loss\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nloss = mean_squared_error(y_true=y_true, y_pred=y_pred)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/mix_loss.html", "title": "mix_loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/mix_loss.html#fastestimator.fastestimator.op.tensorop.loss.mix_loss.MixLoss", "title": "<code>MixLoss</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Loss class to compute mix-up and cutmix losses.</p> <p>This class should be used in conjunction with MixUpBatch and CutMixBatch to perform mix-up training, which helps to reduce over-fitting, stabilize GAN training, and harden against adversarial attacks. See https://arxiv.org/abs/1710.09412 for details.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>LossOp</code> <p>A loss object which we use to calculate the underlying loss of MixLoss. This should be an object of type fe.op.tensorop.loss.loss.LossOp.</p> required <code>lam</code> <code>str</code> <p>The key of the lambda value generated by MixUpBatch or CutMixBatch.</p> required <code>average_loss</code> <code>bool</code> <p>Whether the final loss should be averaged or not.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>loss</code> has multiple outputs.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\mix_loss.py</code> <pre><code>class MixLoss(LossOp):\n\"\"\"Loss class to compute mix-up and cutmix losses.\n    This class should be used in conjunction with MixUpBatch and CutMixBatch to perform mix-up training, which helps to\n    reduce over-fitting, stabilize GAN training, and harden against adversarial attacks. See\n    https://arxiv.org/abs/1710.09412 for details.\n    Args:\n        loss: A loss object which we use to calculate the underlying loss of MixLoss. This should be an object of type\n            fe.op.tensorop.loss.loss.LossOp.\n        lam: The key of the lambda value generated by MixUpBatch or CutMixBatch.\n        average_loss: Whether the final loss should be averaged or not.\n    Raises:\n        ValueError: If the provided `loss` has multiple outputs.\n    \"\"\"\ndef __init__(self, loss: LossOp, lam: str, average_loss: bool = True):\nself.loss = loss\nself.loss.average_loss = False\nif len(loss.outputs) != 1:\nraise ValueError(\"MixLoss only supports lossOps which have a single output.\")\nsuper().__init__(inputs=[lam] + loss.inputs,\noutputs=loss.outputs,\nmode=loss.mode,\nds_id=loss.ds_id,\naverage_loss=average_loss)\nself.out_list = False\n@property\ndef pred_key_idx(self) -&gt; int:\nreturn self.loss.pred_key_idx + 1\n@property\ndef true_key_idx(self) -&gt; int:\nreturn self.loss.true_key_idx + 1\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\nlam, *args = data\nloss1 = self.loss.forward(args, state)\nargs[self.loss.true_key_idx] = roll(args[self.loss.true_key_idx], shift=1, axis=0)\nloss2 = self.loss.forward(args, state)\nloss = lam * loss1 + (1.0 - lam) * loss2\nif self.average_loss:\nloss = fe.backend.reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/super_loss.html", "title": "super_loss", "text": ""}, {"location": "fastestimator/op/tensorop/loss/super_loss.html#fastestimator.fastestimator.op.tensorop.loss.super_loss.SuperLoss", "title": "<code>SuperLoss</code>", "text": "<p>         Bases: <code>LossOp</code></p> <p>Loss class to compute a 'super loss' (automatic curriculum learning) based on a regular loss.</p> <p>This class adds automatic curriculum learning on top of any other loss metric. It is especially useful in for noisy datasets. See https://papers.nips.cc/paper/2020/file/2cfa8f9e50e0f510ede9d12338a5f564-Paper.pdf for details.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>LossOp</code> <p>A loss object which we use to calculate the underlying regular loss. This should be an object of type fe.op.tensorop.loss.loss.LossOp.</p> required <code>threshold</code> <code>Union[float, str]</code> <p>Either a constant value corresponding to an average expected loss (for example log(n_classes) for cross-entropy classification), or 'exp' to use an exponential moving average loss.</p> <code>'exp'</code> <code>regularization</code> <code>float</code> <p>The regularization parameter to use for the super loss (must by &gt;0, as regularization approaches infinity the SuperLoss converges to the regular loss value).</p> <code>1.0</code> <code>average_loss</code> <code>bool</code> <p>Whether the final loss should be averaged or not.</p> <code>True</code> <code>output_confidence</code> <code>Optional[str]</code> <p>If not None then the confidence scores for each sample will be written into the specified key. This can be useful for finding difficult or mislabeled data.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>loss</code> has multiple outputs or the <code>regularization</code> / <code>threshold</code> parameters are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\super_loss.py</code> <pre><code>class SuperLoss(LossOp):\n\"\"\"Loss class to compute a 'super loss' (automatic curriculum learning) based on a regular loss.\n    This class adds automatic curriculum learning on top of any other loss metric. It is especially useful in for noisy\n    datasets. See https://papers.nips.cc/paper/2020/file/2cfa8f9e50e0f510ede9d12338a5f564-Paper.pdf for details.\n    Args:\n        loss: A loss object which we use to calculate the underlying regular loss. This should be an object of type\n            fe.op.tensorop.loss.loss.LossOp.\n        threshold: Either a constant value corresponding to an average expected loss (for example log(n_classes) for\n            cross-entropy classification), or 'exp' to use an exponential moving average loss.\n        regularization: The regularization parameter to use for the super loss (must by &gt;0, as regularization approaches\n            infinity the SuperLoss converges to the regular loss value).\n        average_loss: Whether the final loss should be averaged or not.\n        output_confidence: If not None then the confidence scores for each sample will be written into the specified\n            key. This can be useful for finding difficult or mislabeled data.\n    Raises:\n        ValueError: If the provided `loss` has multiple outputs or the `regularization` / `threshold` parameters are\n            invalid.\n    \"\"\"\ndef __init__(self,\nloss: LossOp,\nthreshold: Union[float, str] = 'exp',\nregularization: float = 1.0,\naverage_loss: bool = True,\noutput_confidence: Optional[str] = None):\nif len(loss.outputs) != 1 or loss.out_list:\nraise ValueError(\"SuperLoss only supports lossOps which have a single output.\")\nself.loss = loss\nself.loss.average_loss = False\nsuper().__init__(inputs=loss.inputs,\noutputs=loss.outputs[0] if not output_confidence else (loss.outputs[0], output_confidence),\nmode=loss.mode,\nds_id=loss.ds_id,\naverage_loss=average_loss)\nif not isinstance(threshold, str):\nthreshold = to_number(threshold).item()\nif not isinstance(threshold, float) and threshold != 'exp':\nraise ValueError(f'SuperLoss threshold parameter must be \"exp\" or a float, but got {threshold}')\nself.tau_method = threshold\nif regularization &lt;= 0:\nraise ValueError(f\"SuperLoss regularization parameter must be greater than 0, but got {regularization}\")\nself.lam = regularization\nself.cap = -1.9999998 / e  # Slightly more than -2 / e for numerical stability\nself.initialized = {}\nself.tau = {}\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nself.loss.build(framework, device)\nif framework == 'tf':\nself.initialized = {\n'train': tf.Variable(False, trainable=False),\n'eval': tf.Variable(False, trainable=False),\n'test': tf.Variable(False, trainable=False),\n'infer': tf.Variable(False, trainable=False)\n}\nif self.tau_method == 'exp':\nself.tau = {\n'train': tf.Variable(0.0, trainable=False),\n'eval': tf.Variable(0.0, trainable=False),\n'test': tf.Variable(0.0, trainable=False),\n'infer': tf.Variable(0.0, trainable=False)\n}\nelse:\nself.tau = {\n'train': tf.Variable(self.tau_method, trainable=False),\n'eval': tf.Variable(self.tau_method, trainable=False),\n'test': tf.Variable(self.tau_method, trainable=False),\n'infer': tf.Variable(self.tau_method, trainable=False)\n}\nself.cap = tf.constant(self.cap)\nelif framework == 'torch':\nself.initialized = {\n'train': torch.tensor(False).to(device),\n'eval': torch.tensor(False).to(device),\n'test': torch.tensor(False).to(device),\n'infer': torch.tensor(False).to(device)\n}\nif self.tau_method == 'exp':\nself.tau = {\n'train': torch.tensor(0.0).to(device),\n'eval': torch.tensor(0.0).to(device),\n'test': torch.tensor(0.0).to(device),\n'infer': torch.tensor(0.0).to(device)\n}\nelse:\nself.tau = {\n'train': torch.tensor(self.tau_method).to(device),\n'eval': torch.tensor(self.tau_method).to(device),\n'test': torch.tensor(self.tau_method).to(device),\n'infer': torch.tensor(self.tau_method).to(device)\n}\nself.cap = torch.tensor(self.cap).to(device)\nself.lam = torch.tensor(self.lam).to(device)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\nbase_loss = self.loss.forward(data, state)\ntau = self._accumulate_tau(base_loss, state['mode'], state['warmup'])\nbeta = (base_loss - tau) / self.lam\n# TODO The authors say to remove the gradients. Need to check whether this is necessary (speed or metrics)\nln_sigma = -lambertw(0.5 * maximum(self.cap, beta))\nsuper_loss = (base_loss - tau) * exp(ln_sigma) + self.lam * pow(ln_sigma, 2)\nif self.average_loss:\nsuper_loss = reduce_mean(super_loss)\nif len(self.outputs) == 2:\n# User requested that the confidence score be returned\nreturn [super_loss, exp(ln_sigma)]\nreturn super_loss\ndef _accumulate_tau(self, loss: Tensor, mode: str, warmup: bool) -&gt; Tensor:\n\"\"\"Determine an average loss value based on a particular method chosen during __init__.\n        Right now this only supports constant values or exponential averaging. The original paper also proposed global\n        averaging, but they didn't find much difference between the three methods and global averaging would more\n        complicated memory requirements.\n        Args:\n            loss: The current step loss.\n            mode: The current step mode.\n            warmup: Whether running in warmup mode or not.\n        Returns:\n            Either the static value provided at __init__, or an exponential moving average of the loss over time.\n        \"\"\"\nif self.tau_method == 'exp':\nif _read_variable(self.initialized[mode]):\n_assign(self.tau[mode], self.tau[mode] - 0.1 * (self.tau[mode] - reduce_mean(loss)))\nelse:\n_assign(self.tau[mode], reduce_mean(loss))\nif not warmup:\n_assign(self.initialized[mode], ones_like(self.initialized[mode]))\nreturn self.tau[mode]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/fuse.html", "title": "fuse", "text": ""}, {"location": "fastestimator/op/tensorop/meta/fuse.html#fastestimator.fastestimator.op.tensorop.meta.fuse.Fuse", "title": "<code>Fuse</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Run a sequence of TensorOps as a single Op.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Union[TensorOp, List[TensorOp]]</code> <p>A sequence of TensorOps to run. They must all share the same mode. It also doesn't support scheduled ops at the moment, though the subnet itself may be scheduled.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>ops</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\fuse.py</code> <pre><code>@traceable()\nclass Fuse(TensorOp):\n\"\"\"Run a sequence of TensorOps as a single Op.\n    Args:\n        ops: A sequence of TensorOps to run. They must all share the same mode. It also doesn't support scheduled ops at\n            the moment, though the subnet itself may be scheduled.\n    Raises:\n        ValueError: If `ops` are invalid.\n    \"\"\"\ndef __init__(self, ops: Union[TensorOp, List[TensorOp]]) -&gt; None:\nops = to_list(ops)\nif len(ops) &lt; 1:\nraise ValueError(\"Fuse requires at least one op\")\ninputs = []\noutputs = []\nmode = ops[0].mode\nds_id = ops[0].ds_id\nself.last_retain_idx = 0\nself.models = set()\nself.loss_keys = set()\nfor idx, op in enumerate(ops):\nif op.mode != mode:\nraise ValueError(f\"All Fuse ops must share the same mode, but got {mode} and {op.mode}\")\nif op.ds_id != ds_id:\nraise ValueError(f\"All Fuse ops must share the same ds_id, but got {ds_id} and {op.ds_id}\")\nfor inp in op.inputs:\nif inp not in inputs and inp not in outputs:\ninputs.append(inp)\nfor out in op.outputs:\nif out not in outputs:\noutputs.append(out)\nif op.fe_retain_graph(True) is not None:  # Set all of the internal ops to retain\nself.last_retain_idx = idx  # Keep tabs on the last one since it might be set to False\nself.models |= op.get_fe_models()\nself.loss_keys |= op.get_fe_loss_keys()\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.ops = ops\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nfor op in self.ops:\nop.build(framework, device)\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.models\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.loss_keys\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nreturn self.ops[self.last_retain_idx].fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nBaseNetwork._forward_batch(data, state, self.ops)\nreturn [data[key] for key in self.outputs]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/one_of.html", "title": "one_of", "text": ""}, {"location": "fastestimator/op/tensorop/meta/one_of.html#fastestimator.fastestimator.op.tensorop.meta.one_of.OneOf", "title": "<code>OneOf</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Perform one of several possible TensorOps.</p> <p>Parameters:</p> Name Type Description Default <code>*tensor_ops</code> <code>TensorOp</code> <p>Ops to choose between with a specified (or uniform) probability.</p> <code>()</code> <code>probs</code> <code>Optional[List[float]]</code> <p>List of probabilities, must sum to 1. When None, the probabilities will be equally distributed.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\one_of.py</code> <pre><code>@traceable()\nclass OneOf(TensorOp):\n\"\"\"Perform one of several possible TensorOps.\n    Args:\n        *tensor_ops: Ops to choose between with a specified (or uniform) probability.\n        probs: List of probabilities, must sum to 1. When None, the probabilities will be equally distributed.\n    \"\"\"\ndef __init__(self, *tensor_ops: TensorOp, probs: Optional[List[float]] = None) -&gt; None:\ninputs = tensor_ops[0].inputs\noutputs = tensor_ops[0].outputs\nmode = tensor_ops[0].mode\nds_id = tensor_ops[0].ds_id\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.in_list = tensor_ops[0].in_list\nself.out_list = tensor_ops[0].out_list\nfor op in tensor_ops[1:]:\nassert inputs == op.inputs, \"All ops within a OneOf must share the same inputs\"\nassert self.in_list == op.in_list, \"All ops within OneOf must share the same input configuration\"\nassert outputs == op.outputs, \"All ops within a OneOf must share the same outputs\"\nassert self.out_list == op.out_list, \"All ops within OneOf must share the same output configuration\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nassert ds_id == op.ds_id, \"All ops within a OneOf must share the same ds_id\"\nif probs:\nassert len(tensor_ops) == len(probs), \"The number of probabilities do not match with number of Operators\"\nassert abs(sum(probs) - 1) &lt; 1e-8, \"Probabilities must sum to 1\"\nelse:\nprobs = [1 / len(tensor_ops) for _ in tensor_ops]\nself.ops = tensor_ops\nself.probs = probs\nself.framework = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nassert framework in {\"tf\", \"torch\"}, \"unrecognized framework: {}\".format(framework)\nself.framework = framework\nfor op in self.ops:\nop.build(framework, device)\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn set.union(*[op.get_fe_loss_keys() for op in self.ops])\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn set.union(*[op.get_fe_models() for op in self.ops])\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nresp = None\nfor op in self.ops:\nresp = resp or op.fe_retain_graph(retain)\nreturn resp\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n        Args:\n            data: The information to be passed to one of the wrapped operators.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after application of one of the available numpyOps.\n        \"\"\"\nif self.framework == 'tf':\nidx = cast(tf.random.categorical(tf.math.log([self.probs]), 1), dtype='int32')[0, 0]\nresults = tf.switch_case(idx, [lambda op=op: op.forward(data, state) for op in self.ops])\nelse:\nresults = np.random.choice(self.ops, p=self.probs).forward(data, state)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/one_of.html#fastestimator.fastestimator.op.tensorop.meta.one_of.OneOf.forward", "title": "<code>forward</code>", "text": "<p>Execute a randomly selected op from the list of <code>numpy_ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[Tensor]]</code> <p>The information to be passed to one of the wrapped operators.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>The <code>data</code> after application of one of the available numpyOps.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\one_of.py</code> <pre><code>def forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n    Args:\n        data: The information to be passed to one of the wrapped operators.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after application of one of the available numpyOps.\n    \"\"\"\nif self.framework == 'tf':\nidx = cast(tf.random.categorical(tf.math.log([self.probs]), 1), dtype='int32')[0, 0]\nresults = tf.switch_case(idx, [lambda op=op: op.forward(data, state) for op in self.ops])\nelse:\nresults = np.random.choice(self.ops, p=self.probs).forward(data, state)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/repeat.html", "title": "repeat", "text": ""}, {"location": "fastestimator/op/tensorop/meta/repeat.html#fastestimator.fastestimator.op.tensorop.meta.repeat.Repeat", "title": "<code>Repeat</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Repeat a TensorOp several times in a row.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>TensorOp</code> <p>A TensorOp to be run one or more times in a row.</p> required <code>repeat</code> <code>Union[int, Callable[..., bool]]</code> <p>How many times to repeat the <code>op</code>. This can also be a function return, in which case the function input names will be matched to keys in the data dictionary, and the <code>op</code> will be repeated until the function evaluates to False. The function evaluation will happen at the end of a forward call, so the <code>op</code> will always be evaluated at least once. If a function is provided, any TF ops which are wrapped by Repeat will not have access to the gradient tape, nor to previously deferred model update functions.</p> <code>1</code> <code>max_iter</code> <code>Optional[int]</code> <p>A limit to how many iterations will be run (or None for no limit).</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>repeat</code>, <code>op</code>, or max_iter are invalid.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\repeat.py</code> <pre><code>@traceable()\nclass Repeat(TensorOp):\n\"\"\"Repeat a TensorOp several times in a row.\n    Args:\n        op: A TensorOp to be run one or more times in a row.\n        repeat: How many times to repeat the `op`. This can also be a function return, in which case the function input\n            names will be matched to keys in the data dictionary, and the `op` will be repeated until the function\n            evaluates to False. The function evaluation will happen at the end of a forward call, so the `op` will\n            always be evaluated at least once. If a function is provided, any TF ops which are wrapped by Repeat will\n            not have access to the gradient tape, nor to previously deferred model update functions.\n        max_iter: A limit to how many iterations will be run (or None for no limit).\n    Raises:\n        ValueError: If `repeat`, `op`, or max_iter are invalid.\n    \"\"\"\ndef __init__(self, op: TensorOp, repeat: Union[int, Callable[..., bool]] = 1,\nmax_iter: Optional[int] = None) -&gt; None:\nself.repeat_inputs = []\nextra_reqs = []\nif max_iter is None:\nself.max_iter = max_iter\nelse:\nif max_iter &lt; 1:\nraise ValueError(f\"Repeat requires max_iter to be &gt;=1, but got {max_iter}\")\nself.max_iter = max_iter - 1  # -1 b/c the first invocation happens outside the while loop\nif isinstance(repeat, int):\nif repeat &lt; 1:\nraise ValueError(f\"Repeat requires repeat to be &gt;= 1, but got {repeat}\")\nif max_iter:\nraise ValueError(\"Do not set max_iter when repeat is an integer\")\nelse:\nself.repeat_inputs.extend(inspect.signature(repeat).parameters.keys())\nextra_reqs = list(set(self.repeat_inputs) - set(op.outputs))\nself.repeat = repeat\nsuper().__init__(inputs=op.inputs + extra_reqs, outputs=op.outputs, mode=op.mode, ds_id=op.ds_id)\nself.ops = [op]\nself.retain_graph = None\nself.while_fn = None\n@property\ndef op(self) -&gt; TensorOp:\nreturn self.ops[0]\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nself.op.build(framework, device)\nif framework == 'tf':\nself.while_fn = self._tf_while\nelse:\nself.while_fn = self._torch_while\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.op.get_fe_models()\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.op.get_fe_loss_keys()\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.op.fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {'ops': [elem.__getstate__() if hasattr(elem, '__getstate__') else {} for elem in self.ops]}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n# Set retain to true since might loop over a gradient aware op\nself.op.fe_retain_graph(True)\ndata = {key: elem for key, elem in zip(self.inputs, data)}\nif isinstance(self.repeat, int):\nfor i in range(self.repeat - 1):\n# Perform n-1 rounds with all ops having retain_graph == True\nBaseNetwork._forward_batch(data, state, self.ops)\n# Let retain be whatever it was meant to be for the final sequence\nself.op.fe_retain_graph(self.retain_graph)\n# Final round of ops\nBaseNetwork._forward_batch(data, state, self.ops)\nelse:\nBaseNetwork._forward_batch(data, state, self.ops)\ndata = self.while_fn(data, state)\n# TODO - Find some magic way to invoke this at the right moment\nself.op.fe_retain_graph(self.retain_graph)\nreturn [data[key] for key in self.outputs]\ndef _torch_while(self, data: Dict[str, Tensor], state: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"A helper function to invoke a while loop.\n        Args:\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            A reference to the updated data dictionary.\n        \"\"\"\ni = 0\nwhile self.repeat(*[data[var_name] for var_name in self.repeat_inputs]):\nif self.max_iter and i &gt;= self.max_iter:\nbreak\nBaseNetwork._forward_batch(data, state, self.ops)\ni += 1\nreturn data\ndef _tf_while(self, data: Dict[str, Tensor], state: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"A helper function to invoke a while loop.\n        Args:\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            A reference to the updated data dictionary.\n        \"\"\"\nargs = ([data[var_name] for var_name in self.repeat_inputs], data)\n# Use functools.partial since state may contain objects which cannot be cast to tensors (ex. gradient tape)\nargs = tf.while_loop(self._tf_cond,\nfunctools.partial(self._tf_body, state=state),\nargs,\nmaximum_iterations=self.max_iter)\nreturn args[1]\ndef _tf_cond(self, cnd: List[Tensor], data: Dict[str, Tensor]) -&gt; bool:\n\"\"\"A helper function determine whether to keep invoking the while method.\n        Note that `data` and `state` are unused here, but required since tf.while_loop needs the cond and body to have\n        the same input argument signatures.\n        Args:\n            cnd: A list of arguments to be passed to the condition function.\n            data: A data dictionary to be used during looping.\n        Returns:\n            Whether to continue looping.\n        \"\"\"\nreturn self.repeat(*cnd)\ndef _tf_body(self, cnd: List[Tensor], data: Dict[str, Tensor],\nstate: Dict[str, Any]) -&gt; Tuple[List[Tensor], Dict[str, Tensor]]:\n\"\"\"A helper function to execute the body of a while method.\n        Note that `cnd` is unused here, but required since tf.while_loop needs the cond and body to have the same input\n        argument signatures.\n        Args:\n            cnd: A list of arguments to be passed to the condition function.\n            data: A data dictionary to be used during looping.\n            state: The state variables to be considered during looping.\n        Returns:\n            The updated `cnd` values, along with the modified data and state dictionaries.\n        \"\"\"\nBaseNetwork._forward_batch(data, state, self.ops)\nreturn [data[var_name] for var_name in self.repeat_inputs], data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/sometimes.html", "title": "sometimes", "text": ""}, {"location": "fastestimator/op/tensorop/meta/sometimes.html#fastestimator.fastestimator.op.tensorop.meta.sometimes.Sometimes", "title": "<code>Sometimes</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Perform a NumpyOp with a given probability.</p> <p>Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before invoking the Sometimes.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_op</code> <code>TensorOp</code> <p>The operator to be performed.</p> required <code>prob</code> <code>float</code> <p>The probability of execution, which should be in the range: [0-1).</p> <code>0.5</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\sometimes.py</code> <pre><code>@traceable()\nclass Sometimes(TensorOp):\n\"\"\"Perform a NumpyOp with a given probability.\n    Note that Sometimes should not be used to wrap an op whose output key(s) do not already exist in the data\n    dictionary. This would result in a problem when future ops / traces attempt to reference the output key, but\n    Sometimes declined to generate it. If you want to create a default value for a new key, simply use a LambdaOp before\n    invoking the Sometimes.\n    Args:\n        tensor_op: The operator to be performed.\n        prob: The probability of execution, which should be in the range: [0-1).\n    \"\"\"\ndef __init__(self, tensor_op: TensorOp, prob: float = 0.5) -&gt; None:\n# We're going to try to collect any missing output keys from the data dictionary so that they don't get\n# overridden when Sometimes chooses not to execute.\ninps = set(tensor_op.inputs)\nouts = set(tensor_op.outputs)\nself.extra_inputs = list(outs - inps)  # Used by traceability\nself.inp_idx = len(tensor_op.inputs)\nsuper().__init__(inputs=tensor_op.inputs + self.extra_inputs,\noutputs=tensor_op.outputs,\nmode=tensor_op.mode,\nds_id=tensor_op.ds_id)\n# Note that in_list and out_list will always be true\nself.op = tensor_op\nself.prob = prob\nself.prob_fn = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nself.op.build(framework, device)\nif framework == 'tf':\nself.prob_fn = tfp.distributions.Uniform()\nelif framework == 'torch':\nself.prob_fn = torch.distributions.uniform.Uniform(low=0, high=1)\nelse:\nraise ValueError(\"unrecognized framework: {}\".format(framework))\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn self.op.get_fe_loss_keys()\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn self.op.get_fe_models()\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nreturn self.op.fe_retain_graph(retain)\ndef __getstate__(self) -&gt; Dict[str, Dict[Any, Any]]:\nreturn {'op': self.op.__getstate__() if hasattr(self.op, '__getstate__') else {}}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n        Args:\n            data: The information to be passed to the wrapped operator.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The original `data`, or the `data` after running it through the wrapped operator.\n        \"\"\"\nif self.prob &gt; self.prob_fn.sample():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/meta/sometimes.html#fastestimator.fastestimator.op.tensorop.meta.sometimes.Sometimes.forward", "title": "<code>forward</code>", "text": "<p>Execute the wrapped operator a certain fraction of the time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Tensor]</code> <p>The information to be passed to the wrapped operator.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>The original <code>data</code>, or the <code>data</code> after running it through the wrapped operator.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\meta\\sometimes.py</code> <pre><code>def forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n    Args:\n        data: The information to be passed to the wrapped operator.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The original `data`, or the `data` after running it through the wrapped operator.\n    \"\"\"\nif self.prob &gt; self.prob_fn.sample():\ndata = data[:self.inp_idx]  # Cut off the unnecessary inputs\nif not self.op.in_list:\ndata = data[0]\ndata = self.op.forward(data, state)\nif not self.op.out_list:\ndata = [data]\nelse:\ndata = [data[self.inputs.index(out)] for out in self.outputs]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/model.html", "title": "model", "text": ""}, {"location": "fastestimator/op/tensorop/model/model.html#fastestimator.fastestimator.op.tensorop.model.model.ModelOp", "title": "<code>ModelOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs forward passes of a neural network over batch data to generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model compiled by fe.build.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key of input training data.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store predictions.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Indicates whether the model should have its weights tracked for update.</p> <code>True</code> <code>intermediate_layers</code> <code>Union[None, str, int, List[Union[str, int]]]</code> <p>One or more layers inside of the model from which you would also like to extract output. This can be useful, for example, for visualizing feature extractor embeddings in conjunction with the TensorBoard trace. Layers can be selected by name (str) or index (int). If you are using pytorch, you can look up this information for your model by calling <code>list(model.named_modules())</code>. For TensorFlow you can use <code>model.layers</code>. Tensorflow users should note that if you do not manually assign a name to a model layer, a name will be autogenerated for you (ex. conv2d_2). This autogenerated name will change if you build a new model within the same python session (for example, if you re-run a Jupyter notebook cell, the name could now be conv2d_5). Any <code>intermediate_layers</code> you request will be appended in order to the end of the Op output, so you must provide output key names for them within the <code>outputs</code> argument. Note that layer names may be different between single-gpu and multi-gpu environments, though we attempt to prevent this.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\model.py</code> <pre><code>@traceable()\nclass ModelOp(TensorOp):\n\"\"\"This class performs forward passes of a neural network over batch data to generate predictions.\n    Args:\n        model: A model compiled by fe.build.\n        inputs: String key of input training data.\n        outputs: String key under which to store predictions.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        trainable: Indicates whether the model should have its weights tracked for update.\n        intermediate_layers: One or more layers inside of the model from which you would also like to extract output.\n            This can be useful, for example, for visualizing feature extractor embeddings in conjunction with the\n            TensorBoard trace. Layers can be selected by name (str) or index (int). If you are using pytorch, you can\n            look up this information for your model by calling `list(model.named_modules())`. For TensorFlow you can use\n            `model.layers`. Tensorflow users should note that if you do not manually assign a name to a model layer,\n            a name will be autogenerated for you (ex. conv2d_2). This autogenerated name will change if you build a new\n            model within the same python session (for example, if you re-run a Jupyter notebook cell, the name could now\n            be conv2d_5). Any `intermediate_layers` you request will be appended in order to the end of the Op output,\n            so you must provide output key names for them within the `outputs` argument. Note that layer names may be\n            different between single-gpu and multi-gpu environments, though we attempt to prevent this.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\ntrainable: bool = True,\nintermediate_layers: Union[None, str, int, List[Union[str, int]]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nassert hasattr(model, \"fe_compiled\"), \"must use fe.build to compile the model before use\"\nself.intermediate_outputs = []  # [{device: Tensor}]\nintermediate_layers = to_list(intermediate_layers)\nif intermediate_layers and get_num_devices() &gt; 1:\nprint(\"\\033[93m {}\\033[00m\".format(\n\"FastEstimator-Warn: Layer names / ids may be different between single-gpu and multi-gpu environments\"))\nfor intermediate_layer in intermediate_layers:\nstorage = {}\nif isinstance(model, tf.keras.Model):\nlayers = list(model._flatten_layers(include_self=False, recursive=True))\nif isinstance(intermediate_layer, int):\nintermediate_layer = layers[intermediate_layer]\nelse:\nlayers = {layer.name: layer for layer in layers}\nintermediate_layer = layers[intermediate_layer]\nif not hasattr(intermediate_layer, 'fe_original_call'):\nintermediate_layer.fe_original_call = intermediate_layer.call\nintermediate_layer.call = partial(_capture_call_tf, fe_storage=storage, fe_layer=intermediate_layer)\nelif isinstance(model, torch.nn.Module):\nlayers = model.named_modules()\nif get_num_devices() &gt; 1:\n# Try to automatically adjust parameters for multi-gpu so that user doesn't need to change code\nlayers2 = list(model.named_modules())  # It's a generator, so don't corrupt the other copy\nif isinstance(layers2[0][1], torch.nn.parallel.DataParallel):\nparallel_prefix = \"module.\"\nif isinstance(intermediate_layer, str) and not intermediate_layer.startswith(parallel_prefix):\nintermediate_layer = parallel_prefix + intermediate_layer\nelif isinstance(intermediate_layer, int):\nlayers = layers2[1:]\nif isinstance(intermediate_layer, int):\nintermediate_layer = list(layers)[intermediate_layer][1]\nelse:\nintermediate_layer = dict(layers)[intermediate_layer]\nintermediate_layer.register_forward_hook(partial(_capture_call_torch, fe_storage=storage))\nself.intermediate_outputs.append(storage)\nself.model = model\nself.trainable = trainable\nself.epoch_spec = None\nself.multi_inputs = False\nself.device = ''\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nself.device = device or ''  # TF will just use empty string for device\nif framework == \"torch\" and len(self.inputs) &gt; 1:\nif hasattr(self.model, \"module\"):\n# multi-gpu models have module attribute\nself.multi_inputs = len(inspect.signature(self.model.module.forward).parameters.keys()) &gt; 1\nelse:\nself.multi_inputs = len(inspect.signature(self.model.forward).parameters.keys()) &gt; 1\nelif framework == \"tf\" and \"keras.engine\" not in str(type(self.model)):\nmodel_call_args = {x for x in inspect.signature(self.model.call).parameters.keys()}\nassert len({\"training\", \"mask\"} &amp; model_call_args) == 0, \"Cannot use 'training' nor 'mask' as input args\"\nself.multi_inputs = len(model_call_args) &gt; 1\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn {self.model}\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\ntraining = state['mode'] == \"train\" and self.trainable\nif isinstance(self.model, torch.nn.Module) and self.epoch_spec != state['epoch']:\n# Gather model input specs for the sake of TensorBoard and Traceability\nself.model.fe_input_spec = FeInputSpec(data, self.model)\nself.epoch_spec = state['epoch']\nif self.multi_inputs:\ndata = feed_forward(self.model, *data, training=training)\nelse:\ndata = feed_forward(self.model, data, training=training)\nintermediate_outputs = []\nfor output in self.intermediate_outputs:\nintermediate_outputs.append(_unpack_output(output, self.device))\noutput.clear()  # This will only help with pytorch memory, tf tensors will remain until next forward\nif intermediate_outputs:\ndata = to_list(data) + intermediate_outputs\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/update.html", "title": "update", "text": ""}, {"location": "fastestimator/op/tensorop/model/update.html#fastestimator.fastestimator.op.tensorop.model.update.UpdateOp", "title": "<code>UpdateOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs updates to a model's weights based on the loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>Model instance compiled by fe.build.</p> required <code>loss_name</code> <code>str</code> <p>The input loss key.</p> required <code>gradients</code> <code>Optional[str]</code> <p>An optional key containing model gradients. These will be directly applied to the model weights during an update. If not provided, gradients will be computed based on the specified loss_name, which will automatically handle any desired mixed-precision scaling. This argument shouldn't be used if mixed-precision training is enabled.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'train'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>merge_grad</code> <code>int</code> <p>The gradient accumulation times before model update. Ex: if <code>merge_grad</code> = 3, for every three Op calls only the third one updates the model. The first two calls only accumulate its gradients. This default value is 1 and it will update the model at every step.</p> <code>1</code> <code>defer</code> <code>bool</code> <p>Whether to defer the actual application of the update until the end of the step. This can be necessary in PyTorch when trying to update multiple models which depend on one another (ex. certain GANs). By default, all UpdateOps which appear contiguously as the last ops of a Network will be deferred. We hope that you will never need to worry about this flag, but it's here for you if you need it.</p> <code>False</code> Raise <p>ValueError: When model is mixed-precision and <code>gradients</code> is provided. ValueError: Network framework is not one of \"tf\" or \"torch\". ValueError: <code>merge_grad</code> is larger than 1 in multi-GPU configuration. RuntimeError: If attempting to modify a PyTorch model which relied on gradients within a different PyTorch model     which has in turn already undergone a non-deferred update.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\update.py</code> <pre><code>@traceable()\nclass UpdateOp(TensorOp):\n\"\"\"This class performs updates to a model's weights based on the loss.\n    Args:\n        model: Model instance compiled by fe.build.\n        loss_name: The input loss key.\n        gradients: An optional key containing model gradients. These will be directly applied to the model weights\n            during an update. If not provided, gradients will be computed based on the specified loss_name, which will\n            automatically handle any desired mixed-precision scaling. This argument shouldn't be used if mixed-precision\n            training is enabled.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Op in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        merge_grad: The gradient accumulation times before model update. Ex: if `merge_grad` = 3, for every three Op\n            calls only the third one updates the model. The first two calls only accumulate its gradients. This default\n            value is 1 and it will update the model at every step.\n        defer: Whether to defer the actual application of the update until the end of the step. This can be necessary\n            in PyTorch when trying to update multiple models which depend on one another (ex. certain GANs). By default,\n            all UpdateOps which appear contiguously as the last ops of a Network will be deferred. We hope that you will\n            never need to worry about this flag, but it's here for you if you need it.\n    Raise:\n        ValueError: When model is mixed-precision and `gradients` is provided.\n        ValueError: Network framework is not one of \"tf\" or \"torch\".\n        ValueError: `merge_grad` is larger than 1 in multi-GPU configuration.\n        RuntimeError: If attempting to modify a PyTorch model which relied on gradients within a different PyTorch model\n            which has in turn already undergone a non-deferred update.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nloss_name: str,\ngradients: Optional[str] = None,\nmode: Union[None, str, Iterable[str]] = \"train\",\nds_id: Union[None, str, Iterable[str]] = None,\nmerge_grad: int = 1,\ndefer: bool = False):\nself.extra_loss = isinstance(model, tf.keras.Model) and model.losses\nif gradients is None:\nsuper().__init__(inputs=loss_name, outputs=None, mode=mode, ds_id=ds_id)\nelse:\nif model.mixed_precision:\nraise ValueError(\"Mixed precision training cannot take input gradients, because the gradients need to \"\n\"be computed in this module\")\nif self.extra_loss:\nprint(\"FastEstimator-Warn: Extra model losses are detected and they will be ignored since the gradients\"\n\" are not computed in UpdateOp class.\")\nsuper().__init__(inputs=gradients, outputs=None, mode=mode, ds_id=ds_id)\nif torch.cuda.device_count() &gt; 1 and merge_grad &gt; 1:\nraise ValueError(\"Currently FastEstimator doesn't support merge_grad feature in multi-GPU configuration \"\n\"and thus 'merge_grad' cannot be larger than 1\")\nif not hasattr(model, \"loss_name\"):\nmodel.loss_name = {loss_name}\nelse:\nmodel.loss_name.add(loss_name)\nself.model = model\nself.retain_graph = False\nself.defer = defer\nself.gradients = gradients\nself.loss_name = loss_name\nself.merge_grad = merge_grad\nself.framework = None\ndef build(self, framework: str, device: Optional[torch.device] = None) -&gt; None:\nif framework not in [\"tf\", \"torch\"]:\nraise ValueError(f\"Unrecognized framework {framework}\")\nself.framework = framework\nif self.merge_grad &gt; 1:\nif framework == \"tf\":\nself.step = tf.Variable(0, trainable=False, dtype=tf.int64)\nself.grad_sum = [tf.Variable(tf.zeros_like(x), trainable=False) for x in self.model.trainable_variables]\nelse:  # framework == \"torch\"\nself.step = torch.tensor(0, dtype=torch.int64).to(device)\nself.grad_sum = [torch.zeros_like(x).to(device) for x in self.model.parameters() if x.requires_grad]\ndef get_fe_models(self) -&gt; Set[Model]:\nreturn {self.model}\ndef get_fe_loss_keys(self) -&gt; Set[str]:\nreturn to_set(self.loss_name)\ndef fe_retain_graph(self, retain: Optional[bool] = None) -&gt; Optional[bool]:\nif retain is not None:\nself.retain_graph = retain\nreturn self.retain_graph\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; None:\nif state[\"warmup\"]:\nreturn\nif self.gradients is None:  # data is loss\nloss = self._loss_preprocess(data)\ngradients = self._get_gradient(loss, state[\"tape\"])\nelse:  # data is gradients\ngradients = data\ngradients = self._gradient_postprocess(gradients)\nif self.merge_grad &gt; 1:\nself._merge_grad_update(gradients, deferred=state[\"deferred\"])\nelse:\nupdate_model(model=self.model, gradients=gradients, defer=self.defer, deferred=state[\"deferred\"])\ndef _loss_preprocess(self, loss: Union[Tensor, List[Tensor]]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Loss preprocess for multi-GPU and mixed-precision training.\n        Args:\n            loss: Unprocessed loss.\n        Returns:\n            Processed loss.\n        \"\"\"\nif self.extra_loss:\nloss = loss + tf.reduce_sum(self.model.losses)\nloss = reduce_mean(loss)\nif self.framework == \"tf\":\n# scale up loss for mixed precision training to avoid underflow\nif self.model.mixed_precision:\nloss = self.model.current_optimizer.get_scaled_loss(loss)\n# for multi-gpu training, the gradient will be combined by sum, normalize the loss\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nloss = loss / strategy.num_replicas_in_sync\nelse:  # self.framework == \"torch\"\nif self.model.current_optimizer.scaler is not None:\n# scale up loss for mixed precision training to avoid underflow\nloss = self.model.current_optimizer.scaler.scale(loss)\nreturn loss\ndef _get_gradient(self, loss: Union[Tensor, List[Tensor]],\ntape: Optional[tf.GradientTape] = None) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Get gradient from loss with repect to self.model.\n        Args:\n            loss: Input loss.\n            tape: A TensorFlow GradientTape which was recording when the `loss` was computed (iff using TensorFlow).\n        Returns:\n            Computed gradients.\n        \"\"\"\nif self.framework == \"tf\":\ngradients = get_gradient(loss, self.model.trainable_variables, tape=tape)\nelse:  # self.framework == \"torch\"\ntrainable_params = [p for p in self.model.parameters() if p.requires_grad]\ntry:\ngradients = get_gradient(loss, trainable_params, retain_graph=self.retain_graph)\nexcept RuntimeError as err:\nif err.args and isinstance(err.args[0], str) and err.args[0].startswith(\n'one of the variables needed for gradient computation has been modified by an inplace operation'\n):\nraise RuntimeError(\n\"When computing gradients for '{}', some variables it relied on during the forward pass had\"\n\" been updated. Consider setting defer=True in earlier UpdateOps related to models which \"\n\"interact with this one.\".format(self.model.model_name))\nraise err\nreturn gradients\ndef _gradient_postprocess(self, gradients: Union[Tensor, List[Tensor]]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"Gradient postprocess for multi-GPU and mixed-precision training.\n        Args:\n            gradients: Unprocessed gradients.\n        Returns:\n            Processed gradients.\n        \"\"\"\nif self.framework == \"tf\":\nif self.gradients is not None:  # when user provide gradients\nstrategy = tf.distribute.get_strategy()\n# for multi-gpu training, the gradient will be combined by sum, normalize the gradient\nif isinstance(strategy, tf.distribute.MirroredStrategy):\ngradients = [gs / strategy.num_replicas_in_sync for gs in gradients]\nif self.model.mixed_precision:\n# scale down gradient to balance scale-up loss\ngradients = self.model.current_optimizer.get_unscaled_gradients(gradients)\nreturn gradients\ndef _merge_grad_update(self,\ngradients: Union[Tensor, List[Tensor]],\ndeferred: Optional[Dict[str, List[Callable[[], None]]]] = None) -&gt; None:\n\"\"\"Accumulate gradients and update the model at certain frequency of invocation.\n        Args:\n            gradients: Input gradients.\n            deferred: A dictionary in which model update functions are stored.\n        \"\"\"\n# add current gradient to the cumulative gradient\nfor gs, g in zip(self.grad_sum, gradients):\nself._assign_add(gs, g)\nself._assign_add(self.step, 1)\nif self.step % self.merge_grad == 0:\naverage_grad = [gs / self.merge_grad for gs in self.grad_sum]\nupdate_model(model=self.model, gradients=average_grad, defer=self.defer, deferred=deferred)\nfor gs in self.grad_sum:\nself._assign_add(gs, -gs)  # zero the gradient in place\ndef _assign_add(self, a: Tensor, b: Tensor) -&gt; None:\n\"\"\"In-place addition for both Tensorflow and PyTorch. `a` = `a` + `b`\n        Args:\n            a: A tensor where in-place addition happens.\n            b: Amount to be added.\n        \"\"\"\nif self.framework == \"tf\":\na.assign_add(b)\nelse:  # self.framework == \"torch\"\na += b\n</code></pre>"}, {"location": "fastestimator/schedule/lr_shedule.html", "title": "lr_shedule", "text": ""}, {"location": "fastestimator/schedule/lr_shedule.html#fastestimator.fastestimator.schedule.lr_shedule.ARC", "title": "<code>ARC</code>", "text": "<p>A run-of-the-mill learning rate scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>int</code> <p>invoke frequency in terms of number of epochs.</p> <code>3</code> Source code in <code>fastestimator\\fastestimator\\schedule\\lr_shedule.py</code> <pre><code>@traceable()\nclass ARC:\n\"\"\"A run-of-the-mill learning rate scheduler.\n    Args:\n        frequency: invoke frequency in terms of number of epochs.\n    \"\"\"\ndef __init__(self, frequency: int = 3) -&gt; None:\nself.frequency = frequency\nassert isinstance(self.frequency, int) and self.frequency &gt; 0\nself.model = self._load_arc_model()\nself.lr_multiplier = {0: 1.618, 1: 1.0, 2: 0.618}\nself.train_loss_one_cycle = []\nself.eval_loss_one_cycle = []\nself.all_train_loss = deque([None] * 3, maxlen=3)\nself.all_eval_loss = deque([None] * 3, maxlen=3)\nself.all_train_lr = deque([None] * 3, maxlen=3)\nself.use_eval_loss = True\ndef _load_arc_model(self) -&gt; tf.keras.Model:\narc_weight_path = self._initialize_arc_weight()\nwith tf.device(\"cpu:0\"):  # this ensures ARC model will not occupy gpu memory\nmodel = self.lstm_stacked()\nmodel.load_weights(arc_weight_path)\nreturn model\n@staticmethod\ndef _initialize_arc_weight() -&gt; str:\narc_weight_path = os.path.join(str(Path.home()), \"fastestimator_data\", \"arc_model\", \"arc.h5\")\nif not os.path.exists(arc_weight_path):\nprint(\"FastEstimator - Downloading ARC weights to {}\".format(arc_weight_path))\nos.makedirs(os.path.split(arc_weight_path)[0], exist_ok=True)\nwget.download(\"https://github.com/fastestimator-util/fastestimator-misc/raw/master/resource/arc.h5\",\nout=arc_weight_path)\nreturn arc_weight_path\n@staticmethod\ndef lstm_stacked() -&gt; tf.keras.Model:\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(300, 3)))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2))\nmodel.add(tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling1D(pool_size=2))\nmodel.add(tf.keras.layers.LSTM(64, return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(64))\nmodel.add(tf.keras.layers.Dense(32, activation='relu'))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax'))\nreturn model\ndef accumulate_single_train_loss(self, train_loss: float) -&gt; None:\nself.train_loss_one_cycle.append(train_loss)\ndef accumulate_single_eval_loss(self, eval_loss: float) -&gt; None:\nself.eval_loss_one_cycle.append(eval_loss)\ndef accumulate_all_lrs(self, lr: float) -&gt; None:\nself.all_train_lr.append(lr)\ndef gather_multiple_eval_losses(self) -&gt; None:\nself.all_eval_loss.append(self.eval_loss_one_cycle)\nself.eval_loss_one_cycle = []\ndef gather_multiple_train_losses(self) -&gt; None:\nself.all_train_loss.append(self.train_loss_one_cycle)\nself.train_loss_one_cycle = []\ndef predict_next_multiplier(self) -&gt; float:\ntrain_loss, missing = self._merge_list(self.all_train_loss)\ntrain_loss = self._preprocess_train_loss(train_loss, missing)\nval_loss, missing = self._merge_list(self.all_eval_loss)\nval_loss = self._preprocess_val_loss(val_loss, missing)\ntrain_lr, missing = self._merge_list(self.all_train_lr)\ntrain_lr = self._preprocess_train_lr(train_lr, missing)\nmodel_inputs = np.concatenate((train_loss, val_loss, train_lr), axis=1)\nmodel_inputs = np.expand_dims(model_inputs, axis=0)\nwith tf.device(\"cpu:0\"):\nmodel_pred = self.model(model_inputs, training=False)\naction = np.argmax(model_pred)\nreturn self.lr_multiplier[action]\ndef _preprocess_val_loss(self, val_loss: List[float], missing: int) -&gt; np.ndarray:\nif val_loss:\ntarget_size = (3 - missing) * 100\nval_loss = np.array(val_loss, dtype=\"float32\")\nval_loss = zscore(val_loss)\nval_loss = cv2.resize(val_loss, (1, target_size), interpolation=cv2.INTER_NEAREST)\nif val_loss.size &lt; 300:\nval_loss = np.pad(val_loss, ((300 - val_loss.size, 0), (0, 0)), mode='constant', constant_values=0.0)\nelse:\nval_loss = np.zeros([300, 1], dtype=\"float32\")\nreturn val_loss\ndef _preprocess_train_lr(self, train_lr: List[float], missing: int) -&gt; np.ndarray:\ntarget_size = (3 - missing) * 100\ntrain_lr = np.array(train_lr) / train_lr[0]\ntrain_lr = cv2.resize(train_lr, (1, target_size), interpolation=cv2.INTER_NEAREST)\nif train_lr.size &lt; 300:\ntrain_lr = np.pad(train_lr, ((300 - train_lr.size, 0), (0, 0)), mode='constant', constant_values=1.0)\nreturn train_lr\ndef _preprocess_train_loss(self, train_loss: List[float], missing: int) -&gt; np.ndarray:\ntarget_size = (3 - missing) * 100\ntrain_loss = np.array(train_loss, dtype=\"float32\")\ntrain_loss = cv2.resize(train_loss, (1, target_size))\ntrain_loss = zscore(train_loss)\nif train_loss.size &lt; 300:\ntrain_loss = np.pad(train_loss, ((300 - train_loss.size, 0), (0, 0)), mode='constant', constant_values=0.0)\nreturn train_loss\ndef _merge_list(self, data: List[Union[None, float, List[float]]]) -&gt; Tuple[List[float], int]:\noutput = []\nmissing = 0\nfor item in data:\nif isinstance(item, list):\noutput.extend(item)\nelif item:\noutput.append(item)\nelse:\nmissing += 1\nreturn output, missing\n</code></pre>"}, {"location": "fastestimator/schedule/lr_shedule.html#fastestimator.fastestimator.schedule.lr_shedule.cosine_decay", "title": "<code>cosine_decay</code>", "text": "<p>Learning rate cosine decay function (using half of cosine curve).</p> <p>This method is useful for scheduling learning rates which oscillate over time: <pre><code>s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])\n</code></pre></p> <p>For more information, check out SGDR: https://arxiv.org/pdf/1608.03983.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>The current step or epoch during training starting from 1.</p> required <code>cycle_length</code> <code>int</code> <p>The decay cycle length.</p> required <code>init_lr</code> <code>float</code> <p>Initial learning rate to decay from.</p> required <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <code>start</code> <code>int</code> <p>The step or epoch to start the decay schedule.</p> <code>1</code> <code>cycle_multiplier</code> <code>int</code> <p>The factor by which next cycle length will be multiplied.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>lr</code> <p>learning rate given current step or epoch.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\lr_shedule.py</code> <pre><code>def cosine_decay(time: int,\ncycle_length: int,\ninit_lr: float,\nmin_lr: float = 1e-6,\nstart: int = 1,\ncycle_multiplier: int = 1):\n\"\"\"Learning rate cosine decay function (using half of cosine curve).\n    This method is useful for scheduling learning rates which oscillate over time:\n    ```python\n    s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])\n    ```\n    For more information, check out SGDR: https://arxiv.org/pdf/1608.03983.pdf.\n    Args:\n        time: The current step or epoch during training starting from 1.\n        cycle_length: The decay cycle length.\n        init_lr: Initial learning rate to decay from.\n        min_lr: Minimum learning rate.\n        start: The step or epoch to start the decay schedule.\n        cycle_multiplier: The factor by which next cycle length will be multiplied.\n    Returns:\n        lr: learning rate given current step or epoch.\n    \"\"\"\nif time &lt; start:\nlr = init_lr\nelse:\ntime = time - start + 1\nif cycle_multiplier &gt; 1:\ncurrent_cycle_idx = math.ceil(\nmath.log(time * (cycle_multiplier - 1) / cycle_length + 1) / math.log(cycle_multiplier)) - 1\ncumulative = cycle_length * (cycle_multiplier**current_cycle_idx - 1) / (cycle_multiplier - 1)\nelif cycle_multiplier == 1:\ncurrent_cycle_idx = math.ceil(time / cycle_length) - 1\ncumulative = current_cycle_idx * cycle_length\nelse:\nraise ValueError(\"multiplier must be at least 1\")\ncurrent_cycle_length = cycle_length * cycle_multiplier**current_cycle_idx\ntime_in_cycle = (time - cumulative) / current_cycle_length\nlr = (init_lr - min_lr) / 2 * math.cos(time_in_cycle * math.pi) + (init_lr + min_lr) / 2\nreturn lr\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html", "title": "schedule", "text": ""}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.EpochScheduler", "title": "<code>EpochScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which selects entries based on a specified epoch mapping.</p> <p>This can be useful for making networks grow over time, or to use more challenging data augmentation as training progresses.</p> <pre><code>s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"a\"\ns.get_current_value(epoch=3)  # \"b\"\ns.get_current_value(epoch=4)  # None\ns.get_current_value(epoch=99)  # None\ns.get_current_value(epoch=100)  # \"c\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epoch_dict</code> <code>Dict[int, T]</code> <p>A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key will be used to determine which element to return. None values may be used to cause nothing to happen for a particular epoch.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>epoch_dict</code> is of the wrong type, or contains invalid keys.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass EpochScheduler(Scheduler[T]):\n\"\"\"A scheduler which selects entries based on a specified epoch mapping.\n    This can be useful for making networks grow over time, or to use more challenging data augmentation as training\n    progresses.\n    ```python\n    s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"a\"\n    s.get_current_value(epoch=3)  # \"b\"\n    s.get_current_value(epoch=4)  # None\n    s.get_current_value(epoch=99)  # None\n    s.get_current_value(epoch=100)  # \"c\"\n    ```\n    Args:\n        epoch_dict: A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key\n            will be used to determine which element to return. None values may be used to cause nothing to happen for a\n            particular epoch.\n    Raises:\n        AssertionError: If the `epoch_dict` is of the wrong type, or contains invalid keys.\n    \"\"\"\ndef __init__(self, epoch_dict: Dict[int, T]) -&gt; None:\nassert isinstance(epoch_dict, dict), \"must provide dictionary as epoch_dict\"\nself.epoch_dict = epoch_dict\nself.keys = sorted(self.epoch_dict)\nfor key in self.keys:\nassert isinstance(key, int), \"found non-integer key: {}\".format(key)\nassert key &gt;= 1, \"found non-positive key: {}\".format(key)\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\nif epoch in self.keys:\nvalue = self.epoch_dict[epoch]\nelse:\nlast_key = self._get_last_key(epoch)\nif last_key is None:\nvalue = None\nelse:\nvalue = self.epoch_dict[last_key]\nreturn value\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn list(self.epoch_dict.values())\ndef _get_last_key(self, epoch: int) -&gt; Union[int, None]:\n\"\"\"Find the nearest prior key to the given epoch.\n        Args:\n            epoch: The current target epoch.\n        Returns:\n            The largest epoch number &lt;= the given `epoch` that is in the `epoch_dict`.\n        \"\"\"\nlast_key = None\nfor key in self.keys:\nif key &gt; epoch:\nbreak\nlast_key = key\nreturn last_key\ndef __getstate__(self) -&gt; Dict[str, Dict[int, Dict[Any, Any]]]:\nreturn {\n'epoch_dict': {\nkey: elem if is_restorable(elem)[0] else elem.__getstate__() if hasattr(elem, '__getstate__') else {}\nfor key,\nelem in self.epoch_dict.items()\n}\n}\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.RepeatScheduler", "title": "<code>RepeatScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which repeats a collection of entries one after another every epoch.</p> <p>One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.</p> <pre><code>s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"b\"\ns.get_current_value(epoch=3)  # \"c\"\ns.get_current_value(epoch=4)  # \"a\"\ns.get_current_value(epoch=5)  # \"b\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repeat_list</code> <code>List[Optional[T]]</code> <p>What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>repeat_list</code> is not a List.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass RepeatScheduler(Scheduler[T]):\n\"\"\"A scheduler which repeats a collection of entries one after another every epoch.\n    One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a\n    different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.\n    ```python\n    s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"b\"\n    s.get_current_value(epoch=3)  # \"c\"\n    s.get_current_value(epoch=4)  # \"a\"\n    s.get_current_value(epoch=5)  # \"b\"\n    ```\n    Args:\n        repeat_list: What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing\n        happen for a particular epoch, None values may be used.\n    Raises:\n        AssertionError: If `repeat_list` is not a List.\n    \"\"\"\ndef __init__(self, repeat_list: List[Optional[T]]) -&gt; None:\nassert isinstance(repeat_list, List), \"must provide a list as input of RepeatSchedule\"\nself.repeat_list = repeat_list\nself.cycle_length = len(repeat_list)\nassert self.cycle_length &gt; 1, \"list length must be greater than 1\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n# epoch-1 since the training epoch is 1-indexed rather than 0-indexed.\nreturn self.repeat_list[(epoch - 1) % self.cycle_length]\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn self.repeat_list\ndef __getstate__(self) -&gt; Dict[str, List[Dict[Any, Any]]]:\nreturn {\n'repeat_list': [\nelem if is_restorable(elem)[0] else elem.__getstate__() if hasattr(elem, '__getstate__') else {}\nfor elem in self.repeat_list\n]\n}\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler", "title": "<code>Scheduler</code>", "text": "<p>         Bases: <code>Generic[T]</code></p> <p>A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>@traceable()\nclass Scheduler(Generic[T]):\n\"\"\"A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.\n    \"\"\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n        Args:\n            epoch: The current epoch.\n        Returns:\n            The element from the Scheduler to be used at the given `epoch`. This value might be None.\n        \"\"\"\nraise NotImplementedError\ndef get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n        Returns:\n            A list of all the values stored in the `Scheduler`. This may contain None values.\n        \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_all_values", "title": "<code>get_all_values</code>", "text": "<p>Get a list of all the possible values stored in the <code>Scheduler</code>.</p> <p>Returns:</p> Type Description <code>List[Optional[T]]</code> <p>A list of all the values stored in the <code>Scheduler</code>. This may contain None values.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n    Returns:\n        A list of all the values stored in the `Scheduler`. This may contain None values.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_current_value", "title": "<code>get_current_value</code>", "text": "<p>Fetch whichever of the <code>Scheduler</code>s elements is appropriate based on the current epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch.</p> required <p>Returns:</p> Type Description <code>Optional[T]</code> <p>The element from the Scheduler to be used at the given <code>epoch</code>. This value might be None.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n    Args:\n        epoch: The current epoch.\n    Returns:\n        The element from the Scheduler to be used at the given `epoch`. This value might be None.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_current_items", "title": "<code>get_current_items</code>", "text": "<p>Select items which should be executed for given mode and epoch.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[Union[T, Scheduler[T]]]</code> <p>A list of possible items or Schedulers of items to choose from.</p> required <code>run_modes</code> <code>Optional[Union[str, Iterable[str]]]</code> <p>The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of all modes will be returned.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>The desired execution epoch. If None, items across all epochs will be returned.</p> <code>None</code> <code>ds_id</code> <code>Optional[str]</code> <p>The desired execution dataset id. If None, items across all ds_ids will be returned. An empty string indicates that positive matches should be excluded ('' != 'ds1'), but that negative matches are satisfied ('' == '!ds1').</p> <code>None</code> <p>Returns:</p> Type Description <code>List[T]</code> <p>The items which should be executed.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_items(items: Iterable[Union[T, Scheduler[T]]],\nrun_modes: Optional[Union[str, Iterable[str]]] = None,\nepoch: Optional[int] = None,\nds_id: Optional[str] = None) -&gt; List[T]:\n\"\"\"Select items which should be executed for given mode and epoch.\n    Args:\n        items: A list of possible items or Schedulers of items to choose from.\n        run_modes: The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of\n            all modes will be returned.\n        epoch: The desired execution epoch. If None, items across all epochs will be returned.\n        ds_id: The desired execution dataset id. If None, items across all ds_ids will be returned. An empty string\n            indicates that positive matches should be excluded ('' != 'ds1'), but that negative matches are satisfied\n            ('' == '!ds1').\n    Returns:\n        The items which should be executed.\n    \"\"\"\nselected_items = []\nrun_modes = to_set(run_modes)\nfor item in items:\nif isinstance(item, Scheduler):\nif epoch is None:\nitem = item.get_all_values()\nelse:\nitem = [item.get_current_value(epoch)]\nelse:\nitem = [item]\nfor item_ in item:\n# mode matching\nmode_match = False\nif not run_modes:\nmode_match = True\nif not hasattr(item_, \"mode\"):\nmode_match = True\nelse:\nif not item_.mode:\nmode_match = True\nelif item_.mode.intersection(run_modes):\nmode_match = True\n# ds_id matching\nds_id_match = False\nif ds_id is None:\nds_id_match = True\nif not hasattr(item_, \"ds_id\"):\nds_id_match = True\nelse:\n# If the object has no requirements, then allow it\nif not item_.ds_id:\nds_id_match = True\n# blacklist check (before whitelist due to desired empty string behavior)\n# if any of ds_id starts with \"!\", then they will all start with \"!\"\nelif any([x.startswith(\"!\") for x in item_.ds_id]) and all([ds_id != x[1:] for x in item_.ds_id]):\nds_id_match = True  # Note that empty string will pass this check (unless target is literally \"!\")\n# whitelist check\nelif ds_id in item_.ds_id:\nds_id_match = True  # Note that empty string will fail this check\nif item_ and mode_match and ds_id_match:\nselected_items.append(item_)\nreturn selected_items\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_signature_epochs", "title": "<code>get_signature_epochs</code>", "text": "<p>Find all epochs of changes due to schedulers.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>List of items to scan from.</p> required <code>total_epochs</code> <code>int</code> <p>The maximum epoch number to consider when searching for signature epochs.</p> required <code>mode</code> <code>Optional[str]</code> <p>Current execution mode. If None, all execution modes will be considered.</p> <code>None</code> <code>ds_id</code> <code>Optional[str]</code> <p>Current ds_id. If None, all ds_ids will be considered.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>The epoch numbers of changes.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_signature_epochs(items: List[Any], total_epochs: int, mode: Optional[str] = None, ds_id: Optional[str] = None) \\\n        -&gt; List[int]:\n\"\"\"Find all epochs of changes due to schedulers.\n    Args:\n        items: List of items to scan from.\n        total_epochs: The maximum epoch number to consider when searching for signature epochs.\n        mode: Current execution mode. If None, all execution modes will be considered.\n        ds_id: Current ds_id. If None, all ds_ids will be considered.\n    Returns:\n        The epoch numbers of changes.\n    \"\"\"\nunique_configs = []\nsignature_epochs = []\nfor epoch in range(1, total_epochs + 1):\nepoch_config = get_current_items(items, run_modes=mode, epoch=epoch, ds_id=ds_id)\nif epoch_config not in unique_configs:\nunique_configs.append(epoch_config)\nsignature_epochs.append(epoch)\nreturn signature_epochs\n</code></pre>"}, {"location": "fastestimator/search/golden_section.html", "title": "golden_section", "text": ""}, {"location": "fastestimator/search/golden_section.html#fastestimator.fastestimator.search.golden_section.GoldenSection", "title": "<code>GoldenSection</code>", "text": "<p>         Bases: <code>Search</code></p> <p>A search class that performs the golden-section search on a single variable.</p> <p>Golden-section search is good at finding minimal or maximal values of a unimodal function. Each search step reduces the search range by a constant factor: the inverse of golden ratio. More details are available at: https://en.wikipedia.org/wiki/Golden-section_search.</p> <pre><code>search = GoldenSection(eval_fn=lambda search_idx, n: (n - 3)**2, x_min=0, x_max=6, max_iter=10, best_mode=\"min\")\nsearch.fit()\nprint(search.get_best_parameters()) # {'param': {'n': 3, 'search_idx': 2}, 'result': {'value': 0}}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>eval_fn</code> <code>Callable[[int, Union[int, float]], float]</code> <p>Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will be automatically provided by the search routine. This can help with file saving / logging during the search. The eval_fn should return a dictionary, or else the return would be wrapped inside one. The other argument should be the variable to be searched over.</p> required <code>x_min</code> <code>Union[int, float]</code> <p>Lower limit (inclusive) of the search space.</p> required <code>x_max</code> <code>Union[int, float]</code> <p>Upper limit (inclusive) of the search space.</p> required <code>max_iter</code> <code>int</code> <p>Maximum number of iterations to run. The range at a given iteration i is 0.618**i * (x_max - x_min). Note that the eval_fn will always be evaluated twice before any iterations begin.</p> required <code>best_mode</code> <code>str</code> <p>Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.</p> required <code>optimize_field</code> <code>Optional[str]</code> <p>the key corresponding to the target value when deciding the best. If None and multiple keys exist in result dictionary, the optimization is ambiguous therefore an error will be raised.</p> <code>None</code> <code>integer</code> <code>bool</code> <p>Whether the optimized variable is a discrete integer.</p> <code>True</code> <code>best_mode</code> <code>str</code> <p>Whether maximal or minimal fitness is desired. Must be either 'min' or 'max'.</p> required <code>name</code> <code>str</code> <p>The name of the search instance. This is used for saving and loading purposes.</p> <code>'golden_section_search'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>eval_fn</code>, <code>x_min</code>, <code>x_max</code>, or <code>max_iter</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\search\\golden_section.py</code> <pre><code>class GoldenSection(Search):\n\"\"\"A search class that performs the golden-section search on a single variable.\n    Golden-section search is good at finding minimal or maximal values of a unimodal function. Each search step reduces\n    the search range by a constant factor: the inverse of golden ratio. More details are available at:\n    https://en.wikipedia.org/wiki/Golden-section_search.\n    ```python\n    search = GoldenSection(eval_fn=lambda search_idx, n: (n - 3)**2, x_min=0, x_max=6, max_iter=10, best_mode=\"min\")\n    search.fit()\n    print(search.get_best_parameters()) # {'param': {'n': 3, 'search_idx': 2}, 'result': {'value': 0}}\n    ```\n    Args:\n        eval_fn: Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will\n            be automatically provided by the search routine. This can help with file saving / logging during the search.\n            The eval_fn should return a dictionary, or else the return would be wrapped inside one. The other\n            argument should be the variable to be searched over.\n        x_min: Lower limit (inclusive) of the search space.\n        x_max: Upper limit (inclusive) of the search space.\n        max_iter: Maximum number of iterations to run. The range at a given iteration i is 0.618**i * (x_max - x_min).\n            Note that the eval_fn will always be evaluated twice before any iterations begin.\n        best_mode: Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.\n        optimize_field: the key corresponding to the target value when deciding the best. If None and multiple keys\n            exist in result dictionary, the optimization is ambiguous therefore an error will be raised.\n        integer: Whether the optimized variable is a discrete integer.\n        best_mode: Whether maximal or minimal fitness is desired. Must be either 'min' or 'max'.\n        name: The name of the search instance. This is used for saving and loading purposes.\n    Raises:\n        AssertionError: If `eval_fn`, `x_min`, `x_max`, or `max_iter` are invalid.\n    \"\"\"\ndef __init__(self,\neval_fn: Callable[[int, Union[int, float]], float],\nx_min: Union[int, float],\nx_max: Union[int, float],\nmax_iter: int,\nbest_mode: str,\noptimize_field: Optional[str] = None,\ninteger: bool = True,\nname: str = \"golden_section_search\"):\nsuper().__init__(eval_fn=eval_fn, name=name)\nassert best_mode in [\"max\", \"min\"], \"best_mode must be either 'max' or 'min'\"\nassert x_min &lt; x_max, \"x_min must be smaller than x_max\"\nif integer:\nassert isinstance(x_min, int) and isinstance(x_max, int), \\\n                \"x_min and x_max must be integers when searching in integer mode\"\nargs = set(inspect.signature(eval_fn).parameters.keys()) - {'search_idx'}\nassert len(args) == 1, \"the score function should only contain one argument other than 'search_idx'\"\nassert max_iter &gt; 0, \"max_iter should be greater than 0\"\nself.x_min = x_min\nself.x_max = x_max\nself.max_iter = max_iter\nself.best_mode = best_mode\nself.optimize_field = optimize_field\nself.integer = integer\nself.arg_name = args.pop()\ndef _convert(self, x: Union[float, int]) -&gt; Union[int, float]:\nreturn int(x) if self.integer else x\ndef _is_better(self, a: float, b: float) -&gt; bool:\nreturn a &gt; b if self.best_mode == \"max\" else a &lt; b\ndef _fit(self):\na, b, h = self.x_min, self.x_max, self.x_max - self.x_min\ninvphi, invphi2 = (math.sqrt(5) - 1) / 2, (3 - math.sqrt(5)) / 2\nc, d = self._convert(a + invphi2 * h), self._convert(a + invphi * h)\nyc = self._get_value_from_result(self.evaluate(**{self.arg_name: c}))\nyd = self._get_value_from_result(self.evaluate(**{self.arg_name: d}))\nfor _ in range(self.max_iter):\nif self._is_better(yc, yd):\nb, d, yd = d, c, yc\nh = invphi * h\nc = self._convert(a + invphi2 * h)\nyc = self._get_value_from_result(self.evaluate(**{self.arg_name: c}))\nelse:\na, c, yc = c, d, yd\nh = invphi * h\nd = self._convert(a + invphi * h)\nyd = self._get_value_from_result(self.evaluate(**{self.arg_name: d}))\nbest_results = self.get_best_results()\nprint(\"FastEstimator-Search: Golden Section Search Finished, best parameters: {}, best result: {}\".format(\nbest_results['param'], best_results['result']))\ndef _get_value_from_result(self, result: Dict[str, Any]) -&gt; Union[int, float]:\noptimize_field = self.optimize_field\nif optimize_field is None:\noptimize_field = self._infer_optimize_field(result)\nreturn result[optimize_field]\n</code></pre>"}, {"location": "fastestimator/search/grid_search.html", "title": "grid_search", "text": ""}, {"location": "fastestimator/search/grid_search.html#fastestimator.fastestimator.search.grid_search.GridSearch", "title": "<code>GridSearch</code>", "text": "<p>         Bases: <code>Search</code></p> <p>A class which executes a grid search.</p> <p>Grid search can be used to evaluate or optimize results of one or more hyperparameters.</p> <pre><code>search = GridSearch(eval_fn=lambda search_idx, a, b: a + b, params={\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nsearch.fit()\nprint(search.get_best_results()) # {'param': {'a': 6, 'b': 6, 'search_idx': 10}, 'result': {'value': 12}}\nsearch = GridSearch(eval_fn=lambda search_idx, a, b: {\"sum\": a + b}, params={\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nsearch.fit()\nprint(search.get_best_results()) # {'param': {'a': 6, 'b': 6, 'search_idx': 10}, 'result': {'sum': 12}}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>eval_fn</code> <code>Callable[..., Union[Dict[str, Any], float]]</code> <p>Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will be automatically provided by the search routine. This can help with file saving / logging during the search. The eval_fn should return a dictionary, or else the return would be wrapped inside one.</p> required <code>params</code> <code>Dict[str, List]</code> <p>A dictionary with key names matching the <code>eval_fn</code>'s inputs. Its values should be lists of options.</p> required <code>best_mode</code> <code>Optional[str]</code> <p>Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.</p> <code>None</code> <code>optimize_field</code> <code>Optional[str]</code> <p>the key corresponding to the target value when deciding the best. If None and multiple keys exist in result dictionary, the optimization is ambiguous therefore an error will be raised.</p> <code>None</code> <code>name</code> <code>str</code> <p>The name of the search instance. This is used for saving and loading purposes.</p> <code>'grid_search'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>params</code> is not dictionary, or contains key not used by <code>eval_fn</code></p> Source code in <code>fastestimator\\fastestimator\\search\\grid_search.py</code> <pre><code>class GridSearch(Search):\n\"\"\"A class which executes a grid search.\n    Grid search can be used to evaluate or optimize results of one or more hyperparameters.\n    ```python\n    search = GridSearch(eval_fn=lambda search_idx, a, b: a + b, params={\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    search.fit()\n    print(search.get_best_results()) # {'param': {'a': 6, 'b': 6, 'search_idx': 10}, 'result': {'value': 12}}\n    search = GridSearch(eval_fn=lambda search_idx, a, b: {\"sum\": a + b}, params={\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n    search.fit()\n    print(search.get_best_results()) # {'param': {'a': 6, 'b': 6, 'search_idx': 10}, 'result': {'sum': 12}}\n    ```\n    Args:\n        eval_fn: Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will\n            be automatically provided by the search routine. This can help with file saving / logging during the search.\n            The eval_fn should return a dictionary, or else the return would be wrapped inside one.\n        params: A dictionary with key names matching the `eval_fn`'s inputs. Its values should be lists of options.\n        best_mode: Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.\n        optimize_field: the key corresponding to the target value when deciding the best. If None and multiple keys\n            exist in result dictionary, the optimization is ambiguous therefore an error will be raised.\n        name: The name of the search instance. This is used for saving and loading purposes.\n    Raises:\n        AssertionError: If `params` is not dictionary, or contains key not used by `eval_fn`\n    \"\"\"\ndef __init__(self,\neval_fn: Callable[..., Union[Dict[str, Any], float]],\nparams: Dict[str, List],\nbest_mode: Optional[str] = None,\noptimize_field: Optional[str] = None,\nname: str = \"grid_search\"):\nassert isinstance(params, dict), \"must provide params as a dictionary\"\neval_fn_args, params_args = set(inspect.signature(eval_fn).parameters.keys()), set(params.keys())\nassert eval_fn_args.issuperset(params_args), \"unused param {} in eval_fn\".format(params_args - eval_fn_args)\nsuper().__init__(eval_fn=eval_fn, name=name)\nself.params = params\nself.optimize_field = optimize_field\nself.best_mode = best_mode\ndef _fit(self):\nexperiments = (dict(zip(self.params, x)) for x in itertools.product(*self.params.values()))\nfor exp in experiments:\nself.evaluate(**exp)\n</code></pre>"}, {"location": "fastestimator/search/search.html", "title": "search", "text": ""}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search", "title": "<code>Search</code>", "text": "<p>Base class which other searches inherit from.</p> <p>The base search class takes care of evaluation logging, saving and loading, and is also able to recover from interrupted search runs and cache the search history.</p> <p>Parameters:</p> Name Type Description Default <code>eval_fn</code> <code>Callable[..., Dict]</code> <p>Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will be automatically provided by the search routine. This can help with file saving / logging during the search. The eval_fn should return a dictionary, or else the return would be wrapped inside one.</p> required <code>name</code> <code>str</code> <p>The name of the search instance. This is used for saving and loading purposes.</p> <code>'search'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>best_mode</code> is not 'min' or 'max', or search_idx is not an input argument of <code>eval_fn</code>.</p> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>class Search:\n\"\"\"Base class which other searches inherit from.\n    The base search class takes care of evaluation logging, saving and loading, and is also able to recover from\n    interrupted search runs and cache the search history.\n    Args:\n        eval_fn: Function that evaluates result given parameter. One of its arguments must be 'search_idx' which will\n            be automatically provided by the search routine. This can help with file saving / logging during the search.\n            The eval_fn should return a dictionary, or else the return would be wrapped inside one.\n        name: The name of the search instance. This is used for saving and loading purposes.\n    Raises:\n        AssertionError: If `best_mode` is not 'min' or 'max', or search_idx is not an input argument of `eval_fn`.\n    \"\"\"\ndef __init__(self, eval_fn: Callable[..., Dict], name: str = \"search\"):\nassert \"search_idx\" in inspect.signature(eval_fn).parameters, \\\n            \"eval_fn must take 'search_idx' as one of its input arguments\"\nself.eval_fn = eval_fn\nself.name = name\nself.save_dir = None\nself.best_mode = None\nself.optimize_field = None\nself._initialize_state()\ndef _initialize_state(self):\nself.search_idx = 0\nself.search_summary = []\nself.evaluation_cache = {}\ndef evaluate(self, **kwargs: Any) -&gt; Dict[str, Union[float, int, str]]:\n\"\"\"Evaluate the eval_fn and return the result.\n        Args:\n            kwargs: Any keyword argument(s) to pass to the score function. Should not contain search_idx as this will be\n                populated manually here.\n        Returns:\n            result returned by `eval_fn`.\n        \"\"\"\n# evaluation caching\nhash_value = hash(tuple(sorted(kwargs.items())))\nif hash_value in self.evaluation_cache:\nresult = self.evaluation_cache[hash_value]\nelse:\nself.search_idx += 1\nfe.fe_build_count = 0  # Resetting the build count to refresh the model names\nkwargs[\"search_idx\"] = self.search_idx\nresult = self.eval_fn(**kwargs)\nif not isinstance(result, dict):\nresult = {\"value\": result}\nsummary = {\"param\": kwargs, \"result\": result}\nself.search_summary.append(summary)\nself.evaluation_cache[hash_value] = result\nif self.save_dir is not None:\nself.save(self.save_dir)\nprint(\"FastEstimator-Search: Evaluated {}, result: {}\".format(kwargs, result))\nreturn result\ndef _infer_optimize_field(self, result: Dict[str, Any]) -&gt; str:\n\"\"\"Infer optimize_field based on result, only needed when optimize_field is not provided.\n        Returns:\n            The optimize_field.\n        Raises:\n            Value error if multiple keys exist in the result.\n        \"\"\"\nif len(self.search_summary[0]['result']) == 1:\noptimize_field = list(result.keys())[0]\nelse:\nraise ValueError(\"Multiple keys exist in result dictionary and optimize_field is None.\")\nreturn optimize_field\ndef get_best_results(self,\nbest_mode: Optional[str] = None,\noptimize_field: Optional[str] = None) -&gt; Dict[str, Dict[str, Any]]:\n\"\"\"Get the best result from the current search summary.\n        Args:\n            best_mode: Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.\n            optimize_field: the key corresponding to the target value when deciding the best. If None and multiple keys\n                exist in result dictionary, the optimization is ambiguous therefore an error will be raised.\n        Returns:\n            The best results in the format of {\"param\":parameter, \"result\": result}\n        \"\"\"\noptimize_field = optimize_field or self.optimize_field\nbest_mode = best_mode or self.best_mode\nassert best_mode in [\"max\", \"min\"], \"best_mode must be either 'max' or 'min'\"\nif not self.search_summary:\nraise RuntimeError(\"No search summary yet, so best parameters are not available.\")\nif optimize_field is None:\noptimize_field = self._infer_optimize_field(self.search_summary[0]['result'])\nif best_mode == \"max\":\nbest_results = max(self.search_summary, key=lambda x: x['result'][optimize_field])\nelse:  # min\nbest_results = min(self.search_summary, key=lambda x: x['result'][optimize_field])\nreturn best_results\ndef _get_state(self) -&gt; Dict[str, Any]:\n\"\"\"Get the current state of the search instance, the state is the variables that can be saved or loaded.\n        Returns:\n            The dictionary containing the state variable.\n        \"\"\"\nstate = {\"search_summary\": self.search_summary}\n# Include extra info if it is available for better visualization options later\nif self.name:\nstate[\"name\"] = self.name\nif self.best_mode:\nstate[\"best_mode\"] = self.best_mode\nif self.optimize_field:\nstate[\"optimize_field\"] = self.optimize_field\nreturn state\ndef get_search_summary(self) -&gt; List[Dict[str, Dict[str, Any]]]:\n\"\"\"Get the current search history.\n        Returns:\n            The evaluation history list, with each element being a tuple of parameters and score.\n        \"\"\"\nreturn self.search_summary.copy()\ndef save(self, save_dir: str) -&gt; None:\n\"\"\"Save the state of the instance to a specific directory, it will create `name.json` file in the `save_dir`.\n        Args:\n            save_dir: The folder path to save to.\n        \"\"\"\nfile_path = os.path.join(save_dir, \"{}.json\".format(self.name))\nwith open(file_path, 'w') as fp:\njson.dump(self._get_state(), fp, indent=4)\nprint(\"FastEstimator-Search: Saving the search summary to {}\".format(file_path))\ndef load(self, load_dir: str, not_exist_ok: bool = False) -&gt; None:\n\"\"\"Load the search summary from a given directory. It will look for `name.json` within the `load_dir`.\n        Args:\n            load_dir: The folder path to load the state from.\n            not_exist_ok: whether to ignore when the file does not exist.\n        \"\"\"\nself._initialize_state()\n# load from file\nload_dir = os.path.abspath(os.path.normpath(load_dir))\nfile_path = os.path.join(load_dir, \"{}.json\".format(self.name))\nif os.path.exists(file_path):\nwith open(file_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# restore evaluation cache and search_idx\nfor summary in self.search_summary:\nkwargs = summary['param'].copy()\nsearch_idx = kwargs.pop('search_idx')  # This won't appear in the hash later\nself.search_idx = self.search_idx if self.search_idx &gt; search_idx else search_idx\n# Each python session uses a unique salt for hash, so can't save the hashes to disk for re-use\nself.evaluation_cache[hash(tuple(sorted(kwargs.items())))] = summary['result']\nprint(\"FastEstimator-Search: Loading the search state from {}\".format(file_path))\nelif not not_exist_ok:\nraise ValueError(\"cannot find file to load in {}\".format(file_path))\ndef fit(self, save_dir: str = None) -&gt; None:\n\"\"\"Start the search.\n        Args:\n            save_dir: When `save_dir` is provided, the search results will be backed up to the `save_dir` after each\n                evaluation. It will also attempt to load the search state from `save_dir` if possible. This is useful\n                when the search might experience interruption since it can be restarted using the same command.\n        \"\"\"\nif save_dir is None:\nself._initialize_state()\nelse:\nself.save_dir = save_dir\nos.makedirs(save_dir, exist_ok=True)\nself.load(save_dir, not_exist_ok=True)\nself._fit()\ndef _fit(self) -&gt; None:\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.evaluate", "title": "<code>evaluate</code>", "text": "<p>Evaluate the eval_fn and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>Any keyword argument(s) to pass to the score function. Should not contain search_idx as this will be populated manually here.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Union[float, int, str]]</code> <p>result returned by <code>eval_fn</code>.</p> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def evaluate(self, **kwargs: Any) -&gt; Dict[str, Union[float, int, str]]:\n\"\"\"Evaluate the eval_fn and return the result.\n    Args:\n        kwargs: Any keyword argument(s) to pass to the score function. Should not contain search_idx as this will be\n            populated manually here.\n    Returns:\n        result returned by `eval_fn`.\n    \"\"\"\n# evaluation caching\nhash_value = hash(tuple(sorted(kwargs.items())))\nif hash_value in self.evaluation_cache:\nresult = self.evaluation_cache[hash_value]\nelse:\nself.search_idx += 1\nfe.fe_build_count = 0  # Resetting the build count to refresh the model names\nkwargs[\"search_idx\"] = self.search_idx\nresult = self.eval_fn(**kwargs)\nif not isinstance(result, dict):\nresult = {\"value\": result}\nsummary = {\"param\": kwargs, \"result\": result}\nself.search_summary.append(summary)\nself.evaluation_cache[hash_value] = result\nif self.save_dir is not None:\nself.save(self.save_dir)\nprint(\"FastEstimator-Search: Evaluated {}, result: {}\".format(kwargs, result))\nreturn result\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.fit", "title": "<code>fit</code>", "text": "<p>Start the search.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>When <code>save_dir</code> is provided, the search results will be backed up to the <code>save_dir</code> after each evaluation. It will also attempt to load the search state from <code>save_dir</code> if possible. This is useful when the search might experience interruption since it can be restarted using the same command.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def fit(self, save_dir: str = None) -&gt; None:\n\"\"\"Start the search.\n    Args:\n        save_dir: When `save_dir` is provided, the search results will be backed up to the `save_dir` after each\n            evaluation. It will also attempt to load the search state from `save_dir` if possible. This is useful\n            when the search might experience interruption since it can be restarted using the same command.\n    \"\"\"\nif save_dir is None:\nself._initialize_state()\nelse:\nself.save_dir = save_dir\nos.makedirs(save_dir, exist_ok=True)\nself.load(save_dir, not_exist_ok=True)\nself._fit()\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.get_best_results", "title": "<code>get_best_results</code>", "text": "<p>Get the best result from the current search summary.</p> <p>Parameters:</p> Name Type Description Default <code>best_mode</code> <code>Optional[str]</code> <p>Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.</p> <code>None</code> <code>optimize_field</code> <code>Optional[str]</code> <p>the key corresponding to the target value when deciding the best. If None and multiple keys exist in result dictionary, the optimization is ambiguous therefore an error will be raised.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>The best results in the format of {\"param\":parameter, \"result\": result}</p> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def get_best_results(self,\nbest_mode: Optional[str] = None,\noptimize_field: Optional[str] = None) -&gt; Dict[str, Dict[str, Any]]:\n\"\"\"Get the best result from the current search summary.\n    Args:\n        best_mode: Whether maximal or minimal objective is desired. Must be either 'min' or 'max'.\n        optimize_field: the key corresponding to the target value when deciding the best. If None and multiple keys\n            exist in result dictionary, the optimization is ambiguous therefore an error will be raised.\n    Returns:\n        The best results in the format of {\"param\":parameter, \"result\": result}\n    \"\"\"\noptimize_field = optimize_field or self.optimize_field\nbest_mode = best_mode or self.best_mode\nassert best_mode in [\"max\", \"min\"], \"best_mode must be either 'max' or 'min'\"\nif not self.search_summary:\nraise RuntimeError(\"No search summary yet, so best parameters are not available.\")\nif optimize_field is None:\noptimize_field = self._infer_optimize_field(self.search_summary[0]['result'])\nif best_mode == \"max\":\nbest_results = max(self.search_summary, key=lambda x: x['result'][optimize_field])\nelse:  # min\nbest_results = min(self.search_summary, key=lambda x: x['result'][optimize_field])\nreturn best_results\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.get_search_summary", "title": "<code>get_search_summary</code>", "text": "<p>Get the current search history.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Dict[str, Any]]]</code> <p>The evaluation history list, with each element being a tuple of parameters and score.</p> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def get_search_summary(self) -&gt; List[Dict[str, Dict[str, Any]]]:\n\"\"\"Get the current search history.\n    Returns:\n        The evaluation history list, with each element being a tuple of parameters and score.\n    \"\"\"\nreturn self.search_summary.copy()\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.load", "title": "<code>load</code>", "text": "<p>Load the search summary from a given directory. It will look for <code>name.json</code> within the <code>load_dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The folder path to load the state from.</p> required <code>not_exist_ok</code> <code>bool</code> <p>whether to ignore when the file does not exist.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def load(self, load_dir: str, not_exist_ok: bool = False) -&gt; None:\n\"\"\"Load the search summary from a given directory. It will look for `name.json` within the `load_dir`.\n    Args:\n        load_dir: The folder path to load the state from.\n        not_exist_ok: whether to ignore when the file does not exist.\n    \"\"\"\nself._initialize_state()\n# load from file\nload_dir = os.path.abspath(os.path.normpath(load_dir))\nfile_path = os.path.join(load_dir, \"{}.json\".format(self.name))\nif os.path.exists(file_path):\nwith open(file_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# restore evaluation cache and search_idx\nfor summary in self.search_summary:\nkwargs = summary['param'].copy()\nsearch_idx = kwargs.pop('search_idx')  # This won't appear in the hash later\nself.search_idx = self.search_idx if self.search_idx &gt; search_idx else search_idx\n# Each python session uses a unique salt for hash, so can't save the hashes to disk for re-use\nself.evaluation_cache[hash(tuple(sorted(kwargs.items())))] = summary['result']\nprint(\"FastEstimator-Search: Loading the search state from {}\".format(file_path))\nelif not not_exist_ok:\nraise ValueError(\"cannot find file to load in {}\".format(file_path))\n</code></pre>"}, {"location": "fastestimator/search/search.html#fastestimator.fastestimator.search.search.Search.save", "title": "<code>save</code>", "text": "<p>Save the state of the instance to a specific directory, it will create <code>name.json</code> file in the <code>save_dir</code>.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The folder path to save to.</p> required Source code in <code>fastestimator\\fastestimator\\search\\search.py</code> <pre><code>def save(self, save_dir: str) -&gt; None:\n\"\"\"Save the state of the instance to a specific directory, it will create `name.json` file in the `save_dir`.\n    Args:\n        save_dir: The folder path to save to.\n    \"\"\"\nfile_path = os.path.join(save_dir, \"{}.json\".format(self.name))\nwith open(file_path, 'w') as fp:\njson.dump(self._get_state(), fp, indent=4)\nprint(\"FastEstimator-Search: Saving the search summary to {}\".format(file_path))\n</code></pre>"}, {"location": "fastestimator/search/visualize/cartesian.html", "title": "cartesian", "text": ""}, {"location": "fastestimator/search/visualize/cartesian.html#fastestimator.fastestimator.search.visualize.cartesian.plot_cartesian", "title": "<code>plot_cartesian</code>", "text": "<p>Draw cartesian plot(s) based on search results.</p> <p>Requires exactly 1 param and 1+ results (after accounting for the ignore_keys).</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <code>groups</code> <code>Optional[List[List[str]]]</code> <p>Which result keys should be plotted on the same y-axis, rather than on separate subplots.</p> <code>None</code> <p>Returns:</p> Type Description <code>FigureFE</code> <p>A plotly figure instance.</p> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\cartesian.py</code> <pre><code>def plot_cartesian(search: Union[Search, str],\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None,\ngroups: Optional[List[List[str]]] = None) -&gt; FigureFE:\n\"\"\"Draw cartesian plot(s) based on search results.\n    Requires exactly 1 param and 1+ results (after accounting for the ignore_keys).\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n        groups: Which result keys should be plotted on the same y-axis, rather than on separate subplots.\n    Returns:\n        A plotly figure instance.\n    \"\"\"\nif isinstance(search, str):\nsearch = _load_search_file(search)\nif title is None:\ntitle = search.name\nsearch = SearchData(search=search, ignore_keys=ignore_keys)\n_cartesian_supports_data(search)\n# Figure out how the plots will be grouped together\nif groups is None:\ngroups = [[x] for x in search.results]\nleftovers = {x for x in search.results}\nfor group in groups:\nfor elem in group:\nleftovers.discard(elem)\nif elem not in search.results:\nraise ValueError(f\"The key '{elem}' was specified in 'groups', but was not found in the search \"\nf\"results. Available keys are {search.results}\")\ngroups = groups + [[x] for x in humansorted(leftovers)]\n# Map the metrics into an n x n grid, then remove any extra columns. Final grid will be n x m with m &lt;= n\nn_plots = len(groups)\nn_rows = math.ceil(math.sqrt(n_plots))\nn_cols = math.ceil(n_plots / n_rows)\n# Get basic plot layout\nfig = make_subplots(rows=n_rows, cols=n_cols, shared_xaxes='all')\nfig.update_layout({\n'plot_bgcolor': '#FFF',\n'hovermode': 'closest',\n'margin': {\n't': 50\n},\n'modebar': {\n'add': ['hoverclosest', 'hovercompare'], 'remove': ['select2d', 'lasso2d']\n},\n'legend': {\n'tracegroupgap': 5, 'font': {\n'size': 11\n}\n},\n'title': title,\n'title_x': 0.5\n})\n# Configure x and y labels\nfor idx, group in enumerate(groups, start=1):\nplotly_idx = idx if idx &gt; 1 else \"\"\nx_axis_name = f'xaxis{plotly_idx}'\ny_axis_name = f'yaxis{plotly_idx}'\nfig['layout'][x_axis_name]['title'] = search.params[0]\nfig['layout'][x_axis_name]['showticklabels'] = True\nfig['layout'][x_axis_name]['linecolor'] = \"#BCCCDC\"\nfig['layout'][y_axis_name]['linecolor'] = \"#BCCCDC\"\nif len(group) &gt; 1:\nfig['layout'][y_axis_name]['title'] = ''\nelse:\nfig['layout'][y_axis_name]['title'] = group[0]\nn_results = len(search.results)\ncolors = get_colors(n_colors=n_results)\ncolors = {key: color for key, color in zip(search.results, colors)}\nadd_label = defaultdict(lambda: True)\n# Plot the groups\nfor idx, group in enumerate(groups):\nrow = idx // n_cols\ncol = idx % n_cols\nfor y_key in group:\nfig.add_trace(\nScatter(x=search.data[search.params[0]],\ny=search.data[y_key],\nname=y_key,\nlegendgroup=y_key,\nshowlegend=add_label[y_key],\nmode=\"markers\" if search.ignored_params else \"lines+markers\",\nline={'color': colors[y_key]},\nmarker={'symbol': 'circle'}),\nrow=row + 1,\ncol=col + 1)\nadd_label[y_key] = False\n# If inside a jupyter notebook then force the height based on number of rows\nif in_notebook():\nfig.update_layout(height=280 * n_rows)\nreturn FigureFE.from_figure(fig)\n</code></pre>"}, {"location": "fastestimator/search/visualize/cartesian.html#fastestimator.fastestimator.search.visualize.cartesian.visualize_cartesian", "title": "<code>visualize_cartesian</code>", "text": "<p>Display or save a parallel coordinate plot based on search results.</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <code>groups</code> <code>Optional[List[List[str]]]</code> <p>Which result keys should be plotted on the same y-axis, rather than on separate subplots.</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\cartesian.py</code> <pre><code>def visualize_cartesian(search: Union[Search, str],\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None,\ngroups: Optional[List[List[str]]] = None,\nsave_path: Optional[str] = None,\nverbose: bool = True) -&gt; None:\n\"\"\"Display or save a parallel coordinate plot based on search results.\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n        groups: Which result keys should be plotted on the same y-axis, rather than on separate subplots.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n    \"\"\"\nfig = plot_cartesian(search=search, title=title, ignore_keys=ignore_keys, groups=groups)\nfig.show(save_path=save_path, verbose=verbose, scale=5)\n</code></pre>"}, {"location": "fastestimator/search/visualize/heatmap.html", "title": "heatmap", "text": ""}, {"location": "fastestimator/search/visualize/heatmap.html#fastestimator.fastestimator.search.visualize.heatmap.plot_heatmap", "title": "<code>plot_heatmap</code>", "text": "<p>Draw a colormap plot based on search results.</p> <p>Requires exactly 2 params and 1 result (after accounting for the ignore_keys).</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>FigureFE</code> <p>A plotly figure instance.</p> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\heatmap.py</code> <pre><code>def plot_heatmap(search: Union[Search, str],\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None) -&gt; FigureFE:\n\"\"\"Draw a colormap plot based on search results.\n    Requires exactly 2 params and 1 result (after accounting for the ignore_keys).\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n    Returns:\n        A plotly figure instance.\n    \"\"\"\nif isinstance(search, str):\nsearch = _load_search_file(search)\nif title is None:\ntitle = search.name\nreverse_colors = search.best_mode == 'min'\nsearch = SearchData(search=search, ignore_keys=ignore_keys)\n_heatmap_supports_data(search)\n# Convert all params to be categorical\nx = [search.to_category(key=search.params[0], val=e) for e in search.data[search.params[0]]]\nx_labels = humansorted(set(x))\ny = [search.to_category(key=search.params[1], val=e) for e in search.data[search.params[1]]]\ny_labels = humansorted(set(y))\n# Map the metrics into an n x n grid, then remove any extra columns. Final grid will be n x m with n &lt;= m\nn_plots = len(search.results)\nn_cols = math.ceil(math.sqrt(n_plots))\nn_rows = math.ceil(n_plots / n_cols)\nvertical_gap = 0.15 / n_rows\nhorizontal_gap = 0.2 / n_cols\n# Get basic plot layout\nfig = make_subplots(rows=n_rows,\ncols=n_cols,\nsubplot_titles=search.results,\nshared_xaxes='all',\nshared_yaxes='all',\nvertical_spacing=vertical_gap,\nhorizontal_spacing=horizontal_gap,\nx_title=search.params[0],\ny_title=search.params[1])\nfig.update_layout({'title': title,\n'title_x': 0.5,\n})\n# Fill in the penultimate row x-labels when the last row has empty columns\nfor idx in range((n_plots % n_cols) or n_cols, n_cols):\nplotly_idx = max((n_rows - 2) * n_cols, 0) + idx + 1\nx_axis_name = f'xaxis{plotly_idx}'\nfig['layout'][x_axis_name]['showticklabels'] = True\n# Ensure the categories are in the right order\nfig['layout']['xaxis']['categoryarray'] = x_labels\nfig['layout']['yaxis']['categoryarray'] = y_labels\nplot_height = (1 - (n_rows - 1) * vertical_gap) / n_rows\nplot_width = (1 - (n_cols - 1) * horizontal_gap) / n_cols\n# Plot the groups\nfor idx, plot in enumerate(search.results):\nrow = idx // n_cols\ncol = idx % n_cols\nfig.add_trace(Heatmap(x=x,\ny=y,\nz=search.data[plot],\ncolorscale=\"Viridis\",\nreversescale=reverse_colors,\ncolorbar={'len': plot_height,\n'lenmode': 'fraction',\n'yanchor': 'top',\n'y': 1 - row * (plot_height + vertical_gap),\n'xanchor': 'left',\n'x': col * (plot_width + horizontal_gap) + plot_width},\nname=\"\",\nhovertemplate=search.params[0] + \": %{x}&lt;br&gt;\" + search.params[1] + \": %{y}&lt;br&gt;\" +\nplot + \": %{z}\",\nhoverongaps=False),\nrow=row + 1,\ncol=col + 1)\n# Make sure that the image aspect ratio doesn't get messed up\nx_axis_name = fig.get_subplot(row=row + 1, col=col + 1).xaxis.plotly_name\ny_axis_name = fig.get_subplot(row=row + 1, col=col + 1).yaxis.plotly_name\nfig['layout'][x_axis_name]['scaleanchor'] = 'x'\nfig['layout'][x_axis_name]['scaleratio'] = 1\nfig['layout'][x_axis_name]['constrain'] = 'domain'\nfig['layout'][y_axis_name]['scaleanchor'] = 'x'\nfig['layout'][y_axis_name]['constrain'] = 'domain'\n# If inside a jupyter notebook then force the height based on number of rows\nif in_notebook():\nfig.update_layout(height=500 * max(1.0, len(y_labels)/5.0) * n_rows)\nfig.update_layout(width=500 * max(1.0, len(x_labels)/5.0) * n_cols)\nreturn FigureFE.from_figure(fig)\n</code></pre>"}, {"location": "fastestimator/search/visualize/heatmap.html#fastestimator.fastestimator.search.visualize.heatmap.visualize_heatmap", "title": "<code>visualize_heatmap</code>", "text": "<p>Display or save a parallel coordinate plot based on search results.</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\heatmap.py</code> <pre><code>def visualize_heatmap(search: Union[Search, str],\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None,\nsave_path: Optional[str] = None,\nverbose: bool = True) -&gt; None:\n\"\"\"Display or save a parallel coordinate plot based on search results.\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n    \"\"\"\nfig = plot_heatmap(search=search, title=title, ignore_keys=ignore_keys)\nfig.show(save_path=save_path, verbose=verbose, scale=3)\n</code></pre>"}, {"location": "fastestimator/search/visualize/parallel_coordinate_plot.html", "title": "parallel_coordinate_plot", "text": ""}, {"location": "fastestimator/search/visualize/parallel_coordinate_plot.html#fastestimator.fastestimator.search.visualize.parallel_coordinate_plot.plot_parallel_coordinates", "title": "<code>plot_parallel_coordinates</code>", "text": "<p>Draw a parallel coordinate plot based on search results.</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>color_by</code> <code>Optional[str]</code> <p>Which key to use for line coloring.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>FigureFE</code> <p>A plotly figure instance.</p> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\parallel_coordinate_plot.py</code> <pre><code>def plot_parallel_coordinates(search: Union[Search, str],\ncolor_by: Optional[str] = None,\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None) -&gt; FigureFE:\n\"\"\"Draw a parallel coordinate plot based on search results.\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        color_by: Which key to use for line coloring.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n    Returns:\n        A plotly figure instance.\n    \"\"\"\nif isinstance(search, str):\nsearch = _load_search_file(search)\nif color_by is None:\ncolor_by = search.optimize_field\nif title is None:\ntitle = search.name\nreverse_colors = search.best_mode == 'min'\nsearch = SearchData(search=search, ignore_keys=ignore_keys)\nif not search.data:\nreturn FigureFE()\n# Finalize the result column to color by if none has been inferred yet\nif color_by is None:\ncandidates = set(search.results) - set(search.categorical_maps.keys())\nif candidates:\n# Prefer numeric columns first\ncolor_by = humansorted(candidates)[-1]\nelse:\n# Fall back to categorical columns\ncolor_by = humansorted(search.results)[-1]\n# Currently can't edit line width, but hopefully supported in the future:\n# https://github.com/plotly/plotly.js/issues/2573\nfig = Parcoords(line={'colorscale': 'Viridis',\n'color': search.data[color_by],\n'colorbar': {'title': color_by},\n'showscale': True,\n'reversescale': reverse_colors},\ndimensions=[{'label': x,\n'values': search.data[x],\n'tickvals': list(\nsearch.categorical_maps[x].values()) if x in search.categorical_maps else None,\n'ticktext': list(\nsearch.categorical_maps[x].keys()) if x in search.categorical_maps else None,\n} for x in search.params + search.results],\nlabelfont={'size': 12},\ntickfont={'size': 11},\nrangefont={'size': 12})\nfig = FigureFE(data=fig, layout={'title': title, 'title_x': 0.5})\n# If inside a jupyter notebook then force the height larger\nif in_notebook():\nfig.update_layout(height=500)\nreturn fig\n</code></pre>"}, {"location": "fastestimator/search/visualize/parallel_coordinate_plot.html#fastestimator.fastestimator.search.visualize.parallel_coordinate_plot.visualize_parallel_coordinates", "title": "<code>visualize_parallel_coordinates</code>", "text": "<p>Display or save a parallel coordinate plot based on search results.</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>color_by</code> <code>Optional[str]</code> <p>Which key to use for line coloring.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\parallel_coordinate_plot.py</code> <pre><code>def visualize_parallel_coordinates(search: Union[Search, str],\ncolor_by: Optional[str] = None,\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None,\nsave_path: Optional[str] = None,\nverbose: bool = True) -&gt; None:\n\"\"\"Display or save a parallel coordinate plot based on search results.\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        color_by: Which key to use for line coloring.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n    \"\"\"\nfig = plot_parallel_coordinates(search=search, color_by=color_by, title=title, ignore_keys=ignore_keys)\nfig.show(save_path=save_path, verbose=verbose, scale=2)\n</code></pre>"}, {"location": "fastestimator/search/visualize/vis_util.html", "title": "vis_util", "text": ""}, {"location": "fastestimator/search/visualize/visualize.html", "title": "visualize", "text": ""}, {"location": "fastestimator/search/visualize/visualize.html#fastestimator.fastestimator.search.visualize.visualize.visualize_search", "title": "<code>visualize_search</code>", "text": "<p>Visualize a given search instance, automatically choosing the most appropriate visualization technique to do so.</p> <p>Parameters:</p> Name Type Description Default <code>search</code> <code>Union[Search, str]</code> <p>The search results (in memory or path to disk file) to be visualized.</p> required <code>title</code> <code>Optional[str]</code> <p>The plot title to use.</p> <code>None</code> <code>ignore_keys</code> <code>Union[None, str, Sequence[str]]</code> <p>Which keys in the params/results should be ignored.</p> <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> <code>**kwargs</code> <p>Arguments which can pass through the specific underlying visualizers, like 'groups' for cartesian plotting or 'color_by' for parallel coordinate plots.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\search\\visualize\\visualize.py</code> <pre><code>def visualize_search(search: Union[Search, str],\ntitle: Optional[str] = None,\nignore_keys: Union[None, str, Sequence[str]] = None,\nsave_path: Optional[str] = None,\nverbose: bool = True,\n**kwargs) -&gt; None:\n\"\"\"Visualize a given search instance, automatically choosing the most appropriate visualization technique to do so.\n    Args:\n        search: The search results (in memory or path to disk file) to be visualized.\n        title: The plot title to use.\n        ignore_keys: Which keys in the params/results should be ignored.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n        **kwargs: Arguments which can pass through the specific underlying visualizers, like 'groups' for cartesian\n            plotting or 'color_by' for parallel coordinate plots.\n    \"\"\"\nif isinstance(search, str):\nsearch = _load_search_file(search)\ndata = SearchData(search, ignore_keys=ignore_keys)\nif _cartesian_supports_data(data, throw_on_invalid=False):\nvisualize_cartesian(search=search,\ntitle=title,\nignore_keys=ignore_keys,\nsave_path=save_path,\nverbose=verbose,\ngroups=kwargs.get(\"groups\", None))\nelif _heatmap_supports_data(data, throw_on_invalid=False):\nvisualize_heatmap(search=search, title=title, ignore_keys=ignore_keys, save_path=save_path, verbose=verbose)\nelse:\nvisualize_parallel_coordinates(search=search,\ntitle=title,\nignore_keys=ignore_keys,\nsave_path=save_path,\nverbose=verbose,\ncolor_by=kwargs.get('color_by', None))\n</code></pre>"}, {"location": "fastestimator/summary/history.html", "title": "history", "text": ""}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader", "title": "<code>HistoryReader</code>", "text": "<p>A class to read history information from the database.</p> <p>This class is intentionally not @traceable.</p> <p>This class should be used as as a context manager, for example:</p> <pre><code>with HistoryReader() as reader:\nreader.read_basic()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>class HistoryReader:\n\"\"\"A class to read history information from the database.\n    This class is intentionally not @traceable.\n    This class should be used as as a context manager, for example:\n    ```python\n    with HistoryReader() as reader:\n        reader.read_basic()\n    ```\n    Args:\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndef __init__(self, db_path: Optional[str] = None):\nself.db_path = db_path\nself.db = None  # sql.Connection\nself.response = None  # List[sql.Row]\ndef __enter__(self) -&gt; 'HistoryReader':\nself.db = connect(self.db_path)\nself.db.set_trace_callback(print)  # Show the query in case user wants to adapt it later\nreturn self\ndef __exit__(self, exc_type: Optional[Type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -&gt; None:\nself.db.close()\ndef read_basic(self,\nlimit: int = 10,\ninteractive: bool = False,\ninclude_args: bool = False,\nerrors: bool = False,\ninclude_pk: bool = False,\ninclude_features: bool = False,\ninclude_traces: bool = False,\ninclude_datasets: bool = False,\ninclude_pipeline: bool = False,\ninclude_network: bool = False,\nas_csv: bool = False) -&gt; None:\n\"\"\"Perform a pre-defined (and possibly interactive) set of sql selects against the history database.\n        Outputs will be printed to stdout.\n        Args:\n            limit: The maximum number of responses to look up.\n            interactive: Whether to run this function interactively, prompting the user for additional input along the\n                way. This enables things like error and log retrieval for individual experiments.\n            include_args: Whether to output the arguments used to run each experiment.\n            errors: Whether to filter the output to only include failed experiments, as well as including more\n                information about the specific errors that occurred.\n            include_pk: Whether to include the primary keys (experiment ids) of each history entry.\n            include_features: Whether to include the FE features that were employed by each training.\n            include_traces: Whether to include the traces that were used in each training.\n            include_datasets: Whether to include the dataset (classes) that were used in each training.\n            include_pipeline: Whether to include the pipeline ops that were used in each training.\n            include_network: Whether to include the network (post)processing ops that were used in each training.\n            as_csv: Whether to print the output as a csv rather than in a formatted table.\n        \"\"\"\n# Build the query string\nerror_select = \", errors.exc_type error\" if errors else ''\nerror_join = \"LEFT JOIN errors ON errors.fk = h.pk \" if errors else ''\nerror_where = \" WHERE h.status &lt;&gt; 'Completed' \" if errors else ''\nfeature_select = \", fg.features\" if include_features else ''\nfeature_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(feature, ', ') features FROM features f GROUP BY f.fk\" \\\n                       \") fg ON fg.fk = h.pk \" if include_features else ''\ndataset_select = \", dsg.datasets \" if include_datasets else ''\ndataset_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(dataset || ' (' || mode || ')', ', ') datasets \" \\\n                       \"FROM datasets ds GROUP BY ds.fk\" \\\n                       \") dsg ON dsg.fk = h.pk \" if include_datasets else ''\npipeline_select = \", pg.pipeline_ops\" if include_pipeline else ''\npipeline_join = \"LEFT JOIN (\" \\\n                        \"SELECT fk, GROUP_CONCAT(pipe_op, ', ') pipeline_ops FROM pipeline p GROUP BY p.fk\" \\\n                        \") pg ON pg.fk = h.pk \" if include_pipeline else ''\nnetwork_select = \", ng.network_ops, ppg.postprocessing_ops\" if include_network else ''\nnetwork_join = \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(net_op, ', ') network_ops FROM network n GROUP BY n.fk\" \\\n                       \") ng ON ng.fk = h.pk \" \\\n                       \"LEFT JOIN (\" \\\n                       \"SELECT fk, GROUP_CONCAT(pp_op, ', ') postprocessing_ops FROM postprocess pp GROUP BY pp.fk\" \\\n                       \") ppg ON ppg.fk = h.pk \" if include_network else ''\ntrace_select = \", tg.traces \" if include_traces else ''\ntrace_join = \"LEFT JOIN (\" \\\n                     \"SELECT fk, GROUP_CONCAT(trace, ', ') traces FROM traces t GROUP BY t.fk\" \\\n                     \") tg ON tg.fk = h.pk \" if include_traces else ''\nquery = f\"SELECT h.*{error_select}{feature_select}{dataset_select}{pipeline_select}{network_select}\" \\\n                f\"{trace_select} FROM history h {error_join}{feature_join}{dataset_join}{pipeline_join}{network_join}\" \\\n                f\"{trace_join}{error_where}ORDER BY h.train_start DESC LIMIT (?)\"\n# We have to hide these after-the-fact since later process may require pk behind the scenes\nhide = []\nif not include_pk:\nhide.append('pk')\nif not include_args:\nhide.append('args')\nself.read_sql(query, args=[limit], hide_cols=hide, as_csv=as_csv, interactive=interactive)\ndef read_sql(self,\nquery: str,\nargs: Iterable[Any] = (),\nhide_cols: Iterable[str] = (),\nas_csv: bool = False,\ninteractive: bool = False) -&gt; None:\n\"\"\"Perform a (possibly interactive) sql query against the database.\n        Args:\n            query: The sql query to execute.\n            args: Any parameterized arguments to be inserted into the `query`.\n            hide_cols: Any columns to hide from the printed output.\n            as_csv: Whether to print the output in csv format or in table format.\n            interactive: Whether to run this function interactively, prompting the user for additional input along the\n                way. This enables things like error and log retrieval for individual experiments.\n        \"\"\"\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(query, args)\nself.response = cursor.fetchall()\nnames = [col[0] for col in cursor.description]\n# Build nice output table\ntable = PrettyTable(field_names=names)\nfor row in self.response:\ntable.add_row(row)\nfor col in hide_cols:\nif col in table.field_names:\ntable.del_column(col)\nif interactive:\ntable.add_autoindex()\nif as_csv:\nprint(table.get_csv_string())\nelse:\nprint(table)\nif interactive:\nwhile True:\ninp = input(\"\\033[93m{}\\033[00m\".format(\"Enter --help for available command details. Enter without an \"\n\"argument to re-print the current response. X to exit.\\n\"))\nif inp in ('X', 'x'):\nbreak\nif inp == \"\":\nprint(query)\nprint(table)\ncontinue\nnew_query = self._parse_input(inp)\nif new_query:\nreturn self.read_sql(new_query, hide_cols=hide_cols, as_csv=as_csv, interactive=interactive)\ndef _parse_input(self, inp: str) -&gt; Optional[str]:\n\"\"\"Take cli input and run it through command parsers to execute an appropriate subroutine.\n        Args:\n            inp: The cli input provided by an end user.\n        Returns:\n            The output (if any) of the appropriate sub-command after executing on the given input.\n        \"\"\"\nparser = argparse.ArgumentParser(allow_abbrev=False)\nsubparsers = parser.add_subparsers()\nsubparsers.required = True\nsubparsers.dest = 'cmd'\nself._configure_sql_parser(subparsers)\nself._configure_log_parser(subparsers)\nself._configure_err_parser(subparsers)\nself._configure_vis_parser(subparsers)\ntry:\nargs, unknown = parser.parse_known_args(inp.split())\nexcept SystemExit:\nreturn\nreturn args.func(vars(args), unknown)\ndef _configure_sql_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a sql parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_sql = subparsers.add_parser('sql',\ndescription='Provide a new sql query to be executed',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_sql.add_argument('query', metavar='&lt;Query&gt;', type=str, nargs='+', help=\"ex: sql SELECT * FROM history\")\np_sql.set_defaults(func=self._echo_sql)\ndef _configure_log_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a log parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_log = subparsers.add_parser(\n'log',\ndescription='Retrieve one or more output logs. This command requires '\n'that you currently have experiments selected.',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_log.add_argument('indices',\nmetavar='I',\ntype=int,\nnargs='+',\nhelp=\"Indices of experiments for which to print logs\")\np_log.add_argument(\n'--file',\nmetavar='F',\naction=SaveAction,\ndefault=False,\ndest='file',\nnargs='?',\nhelp='Whether to write the logs to disk. May be accompanied by a directory or filename into which to save \\\n                 the log(s). If none is specified then the ~/fastestimator_data directory will be used.')\np_log.set_defaults(func=self._fetch_logs)\ndef _configure_err_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add an error parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_err = subparsers.add_parser(\n'err',\ndescription='Retrieve one or more error tracebacks. This command requires '\n'that you currently have experiments selected.',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_err.add_argument('indices',\nmetavar='I',\ntype=int,\nnargs='+',\nhelp=\"Indices of experiments for which to print error tracebacks\")\np_err.set_defaults(func=self._fetch_errs)\ndef _configure_vis_parser(self, subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a visualization parser to an existing argparser.\n        Args:\n            subparsers: The parser object to be appended to.\n        \"\"\"\np_vis = subparsers.add_parser(\n'vis',\ndescription='Visualize logs for one or more experiments. This command requires '\n'that you currently have experiments selected.',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\np_vis.add_argument('indices',\nmetavar='idx',\ntype=int,\nnargs='*',\nhelp=\"Indices of experiments for which to print logs\")\ngroup = p_vis.add_mutually_exclusive_group()\ngroup.add_argument('--ignore',\nmetavar='I',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to ignore though they may be present in the log files\")\ngroup.add_argument('--include',\nmetavar='Y',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to include. If provided, any other metrics will be ignored.\")\np_vis.add_argument('--smooth',\nmetavar='&lt;float&gt;',\ntype=float,\nhelp=\"The amount of gaussian smoothing to apply (zero for no smoothing)\",\ndefault=1)\np_vis.add_argument('--pretty_names', help=\"Clean up the metric names for display\", action='store_true')\np_vis.add_argument('-g', '--group', dest='groups', default={}, action=_GroupAction, nargs=\"*\")\nsave_group = p_vis.add_argument_group('output arguments')\nsave_x_group = save_group.add_mutually_exclusive_group(required=False)\nsave_x_group.add_argument(\n'--save',\nnargs='?',\nmetavar='&lt;Save Dir&gt;',\ndest='save',\naction=SaveAction,\ndefault=False,\nhelp=\"Save the output image. May be accompanied by a directory into \\\n                  which the file is saved. If no output directory is specified, the history directory will be used\")\nsave_x_group.add_argument('--display',\ndest='save',\naction='store_false',\nhelp=\"Render the image to the UI (rather than saving it)\",\ndefault=True)\nsave_x_group.set_defaults(save_dir=None)\np_vis.set_defaults(func=self._vis_logs)\n@staticmethod\ndef _echo_sql(args: Dict[str, Any], unknown: List[str]) -&gt; Optional[str]:\n\"\"\"A method to compile parsed user input back into a single sql query.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        Returns:\n            A single string containing the user sql query.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn None\nreturn \" \".join(args['query'])\ndef _fetch_logs(self, args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to collect and return a given set of logs from the database.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn\nsave = args['file']\nsave_path = None\nif save:\nsave_path = args['file_dir']\nif save_path is None:\nsave_path = os.path.join(str(Path.home()), 'fastestimator_data')\nsave = 'dir'\nprint(f\"Writing log(s) to {save_path}\")\nelse:\nsave = 'file'\nprint(f'Writing log to {save_path}')\nlogs = {}\nfor idx in args['indices']:\nselection = self.response[idx - 1]  # Auto index starts at 1\npk = selection['pk']\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(\"SELECT log FROM logs WHERE logs.fk = (?)\", [pk])\nlogs[idx] = cursor.fetchall()\nwith open(save_path, 'w') if save == 'file' else NonContext() as f:\nf = sys.stdout if f is None else f\nfor idx, log in logs.items():\nwith open(os.path.join(save_path, f\"{idx}.txt\"), 'w') if save == 'dir' else NonContext() as f1:\nf1 = f if f1 is None else f1\nif log:\nf1.write(f'\\n@@@@@@@@@@@ Log for Index {idx} @@@@@@@@@@@\\n\\n')\nf1.write(log[0]['log'])\nf1.write('\\n')\nelse:\nf1.write(f\"No logs found for Index {idx}\\n\")\ndef _fetch_errs(self, args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to collect and return a given set of error logs from the database.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn\nfor idx in args['indices']:\nselection = self.response[idx - 1]  # Auto index starts at 1\npk = selection['pk']\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(\"SELECT exc_tb FROM errors WHERE errors.fk = (?)\", [pk])\nerr = cursor.fetchall()\nif err:\nprint(f'@@@@@@@@@@@ Traceback for Index {idx} @@@@@@@@@@@')\nprint(err[0]['exc_tb'])\nelse:\nprint(f\"No error traceback found for Index {idx}\")\ndef _vis_logs(self, args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to collect and visualize a given set of logs from the database.\n        Args:\n            args: The CLI arguments provided by the user.\n            unknown: Any CLI arguments not matching known inputs.\n        \"\"\"\nif len(unknown) &gt; 0:\nprint(\"unrecognized arguments: \", str.join(\", \", unknown))\nreturn\nsave_dir = args['save_dir']\nif args['save'] and save_dir is None:\nsave_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'logs.png')\ngroup_indices = [x for y in args['groups'].values() for x in y]\npks = {idx: self.response[idx - 1]['pk'] for idx in set(args['indices'] + group_indices)}\nif len(pks) == 0:\nreturn\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(\n\"SELECT H.pk, H.experiment, L.log \"\n\"FROM logs L LEFT JOIN history H ON L.fk = H.pk \"\n\"WHERE L.fk IN ({seq})\".format(seq=','.join(['?'] * len(pks))),\nlist(pks.values()))\nlogs = cursor.fetchall()\nlogs = {elem['pk']: elem for elem in logs}\nfailures = 0\nfor idx, pk in pks.items():\nif pk not in logs:\nprint(f\"No logs found for Index {idx}\")\nfailures += 1\nif failures:\nreturn\ngroups = defaultdict(list)  # {group_name: [experiment(s)]}\nfor idx, pk in pks.items():\nlog = logs[pk]\nexperiment = parse_log_iter(\nlog['log'].split('\\n'),\nSummary(str(idx) if log['experiment'] is None else f\"{log['experiment']} ({idx})\"))\nif idx in args['indices']:\ngroups[experiment.name].append(experiment)\nfor group_name, group_indices in args['groups'].items():\nif idx in group_indices:\ngroups[group_name].append(experiment)\nexperiments = [average_summaries(name, exps) for name, exps in groups.items()]\nvisualize_logs(experiments,\nsave_path=save_dir,\nsmooth_factor=args['smooth'],\nshare_legend=args['share_legend'],\npretty_names=args['pretty_names'],\nignore_metrics=args['ignore'],\ninclude_metrics=args['include'])\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader.read_basic", "title": "<code>read_basic</code>", "text": "<p>Perform a pre-defined (and possibly interactive) set of sql selects against the history database.</p> <p>Outputs will be printed to stdout.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of responses to look up.</p> <code>10</code> <code>interactive</code> <code>bool</code> <p>Whether to run this function interactively, prompting the user for additional input along the way. This enables things like error and log retrieval for individual experiments.</p> <code>False</code> <code>include_args</code> <code>bool</code> <p>Whether to output the arguments used to run each experiment.</p> <code>False</code> <code>errors</code> <code>bool</code> <p>Whether to filter the output to only include failed experiments, as well as including more information about the specific errors that occurred.</p> <code>False</code> <code>include_pk</code> <code>bool</code> <p>Whether to include the primary keys (experiment ids) of each history entry.</p> <code>False</code> <code>include_features</code> <code>bool</code> <p>Whether to include the FE features that were employed by each training.</p> <code>False</code> <code>include_traces</code> <code>bool</code> <p>Whether to include the traces that were used in each training.</p> <code>False</code> <code>include_datasets</code> <code>bool</code> <p>Whether to include the dataset (classes) that were used in each training.</p> <code>False</code> <code>include_pipeline</code> <code>bool</code> <p>Whether to include the pipeline ops that were used in each training.</p> <code>False</code> <code>include_network</code> <code>bool</code> <p>Whether to include the network (post)processing ops that were used in each training.</p> <code>False</code> <code>as_csv</code> <code>bool</code> <p>Whether to print the output as a csv rather than in a formatted table.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def read_basic(self,\nlimit: int = 10,\ninteractive: bool = False,\ninclude_args: bool = False,\nerrors: bool = False,\ninclude_pk: bool = False,\ninclude_features: bool = False,\ninclude_traces: bool = False,\ninclude_datasets: bool = False,\ninclude_pipeline: bool = False,\ninclude_network: bool = False,\nas_csv: bool = False) -&gt; None:\n\"\"\"Perform a pre-defined (and possibly interactive) set of sql selects against the history database.\n    Outputs will be printed to stdout.\n    Args:\n        limit: The maximum number of responses to look up.\n        interactive: Whether to run this function interactively, prompting the user for additional input along the\n            way. This enables things like error and log retrieval for individual experiments.\n        include_args: Whether to output the arguments used to run each experiment.\n        errors: Whether to filter the output to only include failed experiments, as well as including more\n            information about the specific errors that occurred.\n        include_pk: Whether to include the primary keys (experiment ids) of each history entry.\n        include_features: Whether to include the FE features that were employed by each training.\n        include_traces: Whether to include the traces that were used in each training.\n        include_datasets: Whether to include the dataset (classes) that were used in each training.\n        include_pipeline: Whether to include the pipeline ops that were used in each training.\n        include_network: Whether to include the network (post)processing ops that were used in each training.\n        as_csv: Whether to print the output as a csv rather than in a formatted table.\n    \"\"\"\n# Build the query string\nerror_select = \", errors.exc_type error\" if errors else ''\nerror_join = \"LEFT JOIN errors ON errors.fk = h.pk \" if errors else ''\nerror_where = \" WHERE h.status &lt;&gt; 'Completed' \" if errors else ''\nfeature_select = \", fg.features\" if include_features else ''\nfeature_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(feature, ', ') features FROM features f GROUP BY f.fk\" \\\n                   \") fg ON fg.fk = h.pk \" if include_features else ''\ndataset_select = \", dsg.datasets \" if include_datasets else ''\ndataset_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(dataset || ' (' || mode || ')', ', ') datasets \" \\\n                   \"FROM datasets ds GROUP BY ds.fk\" \\\n                   \") dsg ON dsg.fk = h.pk \" if include_datasets else ''\npipeline_select = \", pg.pipeline_ops\" if include_pipeline else ''\npipeline_join = \"LEFT JOIN (\" \\\n                    \"SELECT fk, GROUP_CONCAT(pipe_op, ', ') pipeline_ops FROM pipeline p GROUP BY p.fk\" \\\n                    \") pg ON pg.fk = h.pk \" if include_pipeline else ''\nnetwork_select = \", ng.network_ops, ppg.postprocessing_ops\" if include_network else ''\nnetwork_join = \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(net_op, ', ') network_ops FROM network n GROUP BY n.fk\" \\\n                   \") ng ON ng.fk = h.pk \" \\\n                   \"LEFT JOIN (\" \\\n                   \"SELECT fk, GROUP_CONCAT(pp_op, ', ') postprocessing_ops FROM postprocess pp GROUP BY pp.fk\" \\\n                   \") ppg ON ppg.fk = h.pk \" if include_network else ''\ntrace_select = \", tg.traces \" if include_traces else ''\ntrace_join = \"LEFT JOIN (\" \\\n                 \"SELECT fk, GROUP_CONCAT(trace, ', ') traces FROM traces t GROUP BY t.fk\" \\\n                 \") tg ON tg.fk = h.pk \" if include_traces else ''\nquery = f\"SELECT h.*{error_select}{feature_select}{dataset_select}{pipeline_select}{network_select}\" \\\n            f\"{trace_select} FROM history h {error_join}{feature_join}{dataset_join}{pipeline_join}{network_join}\" \\\n            f\"{trace_join}{error_where}ORDER BY h.train_start DESC LIMIT (?)\"\n# We have to hide these after-the-fact since later process may require pk behind the scenes\nhide = []\nif not include_pk:\nhide.append('pk')\nif not include_args:\nhide.append('args')\nself.read_sql(query, args=[limit], hide_cols=hide, as_csv=as_csv, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryReader.read_sql", "title": "<code>read_sql</code>", "text": "<p>Perform a (possibly interactive) sql query against the database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The sql query to execute.</p> required <code>args</code> <code>Iterable[Any]</code> <p>Any parameterized arguments to be inserted into the <code>query</code>.</p> <code>()</code> <code>hide_cols</code> <code>Iterable[str]</code> <p>Any columns to hide from the printed output.</p> <code>()</code> <code>as_csv</code> <code>bool</code> <p>Whether to print the output in csv format or in table format.</p> <code>False</code> <code>interactive</code> <code>bool</code> <p>Whether to run this function interactively, prompting the user for additional input along the way. This enables things like error and log retrieval for individual experiments.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def read_sql(self,\nquery: str,\nargs: Iterable[Any] = (),\nhide_cols: Iterable[str] = (),\nas_csv: bool = False,\ninteractive: bool = False) -&gt; None:\n\"\"\"Perform a (possibly interactive) sql query against the database.\n    Args:\n        query: The sql query to execute.\n        args: Any parameterized arguments to be inserted into the `query`.\n        hide_cols: Any columns to hide from the printed output.\n        as_csv: Whether to print the output in csv format or in table format.\n        interactive: Whether to run this function interactively, prompting the user for additional input along the\n            way. This enables things like error and log retrieval for individual experiments.\n    \"\"\"\nwith closing(self.db.cursor()) as cursor:\ncursor.execute(query, args)\nself.response = cursor.fetchall()\nnames = [col[0] for col in cursor.description]\n# Build nice output table\ntable = PrettyTable(field_names=names)\nfor row in self.response:\ntable.add_row(row)\nfor col in hide_cols:\nif col in table.field_names:\ntable.del_column(col)\nif interactive:\ntable.add_autoindex()\nif as_csv:\nprint(table.get_csv_string())\nelse:\nprint(table)\nif interactive:\nwhile True:\ninp = input(\"\\033[93m{}\\033[00m\".format(\"Enter --help for available command details. Enter without an \"\n\"argument to re-print the current response. X to exit.\\n\"))\nif inp in ('X', 'x'):\nbreak\nif inp == \"\":\nprint(query)\nprint(table)\ncontinue\nnew_query = self._parse_input(inp)\nif new_query:\nreturn self.read_sql(new_query, hide_cols=hide_cols, as_csv=as_csv, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.HistoryRecorder", "title": "<code>HistoryRecorder</code>", "text": "<p>A class to record what you're doing.</p> <p>This class is intentionally not @traceable.</p> <p>It will capture output logs, exceptions, and general information about the training / environment. This class should be used as a context manager.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>System</code> <p>The system object corresponding to the current training.</p> required <code>est_path</code> <code>str</code> <p>The path to the file responsible for creating the current estimator (this is for bookkeeping, it can technically be any string).</p> required <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>class HistoryRecorder:\n\"\"\"A class to record what you're doing.\n    This class is intentionally not @traceable.\n    It will capture output logs, exceptions, and general information about the training / environment. This class should\n    be used as a context manager.\n    Args:\n        system: The system object corresponding to the current training.\n        est_path: The path to the file responsible for creating the current estimator (this is for bookkeeping, it can\n            technically be any string).\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndef __init__(self, system: System, est_path: str, db_path: Optional[str] = None):\n# Prepare db adapters\nsql.register_adapter(bool, int)\nsql.register_converter(\"BOOL\", lambda v: bool(int(v)))\nsql.register_adapter(list, str)\nsql.register_converter(\"LIST[STR]\", lambda v: parse_string_to_python(v))\n# Prepare variables\nself.filename = os.path.basename(est_path)\nself.db_path = db_path if db_path else os.path.join(str(Path.home()), 'fastestimator_data', 'history.db')\nself.system = system\nself.db = None\nself.ident = (multiprocessing.current_process().pid, threading.get_ident())\nself.pk = None\nself.stdout = None\ndef __enter__(self) -&gt; None:\nself.db = connect(self.db_path)\nself.ident = (multiprocessing.current_process().pid, threading.get_ident())\nself.pk = self.system.exp_id  # This might be changed later by RestoreWizard. See the _check_for_restart method\n# Check whether an entry for this pk already exists, for example if a user ran .fit() and is now running .test()\nwith closing(self.db.cursor()) as cursor:\nexists = cursor.execute(\"SELECT pk FROM history WHERE pk = (?)\", [self.pk])\nexists = exists.fetchall()\nif not exists:\nself.db.execute(\n_MAKE_HIST_ENTRY,\n{\n'pk': self.pk,\n'fname': self.filename,\n'status': 'Launched',\n'exp': self.system.summary.name,\n'args': sys.argv[1:],\n'version': sys.modules['fastestimator'].__version__,\n'start': datetime.now(),\n'gpus': torch.cuda.device_count(),\n'cpus': cpu_count(),\n'workers': self.system.pipeline.num_process\n})\nself.db.executemany(_MAKE_FEAT_ENTRY, self._get_features_in_use())\nself.db.executemany(_MAKE_DS_ENTRY, self._get_datasets_in_use())\nself.db.executemany(_MAKE_PIPE_ENTRY, self._get_items_in_use(self.system.pipeline.ops))\nself.db.executemany(_MAKE_NET_ENTRY, self._get_items_in_use(self.system.network.ops))\nself.db.executemany(_MAKE_PP_ENTRY, self._get_items_in_use(self.system.network.postprocessing))\nself.db.executemany(_MAKE_TRACE_ENTRY, self._get_items_in_use(self.system.traces))\nself.db.execute(_MAKE_LOG_ENTRY, {'log': '', 'fk': self.pk})\nself.db.commit()\n# Take over the output logging\nself.stdout = sys.stdout\nsys.stdout = self\ndef __exit__(self, exc_type: Optional[Type], exc_val: Optional[Exception], exc_tb: Optional[Any]) -&gt; None:\ntry:\nself.flush()\nsys.stdout = self.stdout\nself._check_for_restart()\n# In test mode only overwrite the train_end time if it hasn't already been set\nquery = \"UPDATE history set train_end = (?), status = (?) WHERE pk = (?)\" \\\n                if self.system.mode in ('train', 'eval') else \\\n                \"UPDATE history set train_end = IFNULL(train_end, (?)), status = (?) WHERE pk = (?)\"\nself.db.execute(\nquery,\n[\ndatetime.now(),\n\"Completed\" if exc_type is None else \"Aborted\" if exc_type == KeyboardInterrupt else \"Failed\",\nself.pk\n])\nif exc_type is not None:\nargs = {\n'type': exc_type.__name__,\n'tb': \"\\n\".join(traceback.format_exception(exc_type, exc_val, exc_tb)),\n'fk': self.pk\n}\nself.db.execute(_MAKE_ERR_ENTRY_P1, args)\nself.db.execute(_MAKE_ERR_ENTRY_P2, args)\nself.db.commit()\nself._apply_limits()\nexcept (sql.OperationalError, sql.DatabaseError) as err:\n# This could happen if user has multiple trainings running simultaneously, the database becomes corrupted,\n# and then a new training is launched which detects the corrupted database of the old training and re-names\n# the old database before the old job completes.\nprint(f\"FastEstimator-Warn: FastEstimator history tracking failed to capture the final status of the \"\nf\"experiment: {err}\")\nself.db.close()\ndef _check_for_restart(self) -&gt; None:\n\"\"\"Determine whether a training has been restarted via RestoreWizard. If so, update the history accordingly.\n        If RestoreWizard has been invoked, then the system exp_id will have changed since self.pk was initialized. This\n        method will do related bookkeeping, and then swap self.pk for the restored id.\n        \"\"\"\nif self.pk == self.system.exp_id:\nreturn\n# RestoreWizard reset the system, we are continuing an old training rather than starting a new one\n# First make sure the old entry is still available to edit\nwith closing(self.db.cursor()) as cursor:\nexists = cursor.execute(\"SELECT pk FROM history WHERE pk = (?)\", [self.system.exp_id])\nexists = exists.fetchall()\nif exists:\n# If we still have the original entry, we will delete our new one and update the old instead\nself.db.execute(\"DELETE FROM history WHERE pk = (?)\", [self.pk])\nelse:\n# The old record doesn't exist, so we will use the new record instead\nself.db.execute(\"UPDATE history SET pk = (?) WHERE pk = (?)\", [self.system.exp_id, self.pk])\nself.pk = self.system.exp_id\nself.db.execute(\"UPDATE history SET n_restarts = n_restarts + 1 WHERE pk = (?)\", [self.pk])\nself.db.commit()\ndef _get_features_in_use(self) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which interesting FE features are being used by the current training.\n        Returns:\n            A list of entries which can be written into the 'features' db table.\n        \"\"\"\nfeatures = []\nif sys.modules['fastestimator'].fe_deterministic_seed is not None:\nfeatures.append({'feature': 'Deterministic', 'fk': self.pk})\nif any([len(mode_dict) &gt; 1 for mode_dict in self.system.pipeline.data.values()]):\nfeatures.append({'feature': 'MultiDataset', 'fk': self.pk})\nif mixed_precision.global_policy().compute_dtype == 'float16':\nfeatures.append({'feature': 'MixedPrecision', 'fk': self.pk})\nreturn features\ndef _get_datasets_in_use(self) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which datasets are being used by the current training.\n        Returns:\n            A list of entries which can be written into the 'datasets' db table.\n        \"\"\"\ndatasets = []\nfor mode, group in self.system.pipeline.data.items():\nfor _, ds in group.items():\ndatasets.append({'mode': mode, 'dataset': type(ds).__name__, 'fk': self.pk})\nreturn datasets\ndef _get_items_in_use(self, items: List[Any]) -&gt; List[Dict[str, str]]:\n\"\"\"Determine which objects are being used by the current training.\n        Args:\n            items: A list of Schedulers, Ops, and/or traces which are being used by the system.\n        Returns:\n            The elements from `items` converted into database-ready entries.\n        \"\"\"\nops = []\nfor op in items:\nop_list = [op]\nif isinstance(op, Scheduler):\nop_list = list(filter(lambda x: x is not None, op.get_all_values()))\nop_list.append(op)  # Put scheduler in too so that usage can be tracked too\nops.extend([{'op': type(elem).__name__, 'fk': self.pk} for elem in op_list])\nreturn ops\ndef _apply_limits(self) -&gt; None:\n\"\"\"Remove old history and/or log entries if they exceed the limits defined in the settings table.\n        \"\"\"\nself.db.execute(\"DELETE FROM history WHERE train_start &lt;= (\"\n\"SELECT train_start FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (\"\n\"SELECT n_keep FROM settings WHERE pk = 0))\")\nself.db.execute(\"DELETE FROM logs WHERE fk IN (\"\n\"SELECT pk FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (\"\n\"SELECT n_keep_logs FROM settings WHERE pk = 0))\")\nself.db.commit()  # Have to commit before vacuuming\nif sum(int(digit) for digit in str(abs(self.pk))) % 10 == 0:\n# 10% of time do a vacuum (expensive). We don't use random.randint here due to deterministic training. Also,\n# don't use pk directly because last digit is not uniformly distributed.\nself.db.execute(\"PRAGMA VACUUM;\")\nself.db.commit()\nelse:\n# Otherwise do a less costly optimize\nself.db.execute(\"PRAGMA optimize;\")\nself.db.commit()\nself.db.close()\ndef write(self, output: str) -&gt; None:\nself.stdout.write(output)\nif multiprocessing.current_process().pid == self.ident[0] and threading.get_ident() == self.ident[1]:\n# Flush can also get invoked by pipeline multi-processing, but db should only be accessed by main thread.\n# This can happen, for example, when pipeline prints a warning that a certain key is unused and will be\n# dropped.\ntry:\nself._check_for_restart()  # Check here instead of waiting for __exit__ in case system powers off later\nself.db.execute('UPDATE logs SET log = log || (?) WHERE fk = (?)', [output, self.pk])\nself.db.commit()\nexcept (sql.OperationalError, sql.DatabaseError) as err:\nself.ident = (-2, -2)  # No threads should match this identity in the future\nprint(f\"\\nFastEstimator-Warn: There was a problem writing to the FastEstimator history database. Log \"\nf\"capture will be disabled for the rest of this experiment. Error: {err}\")\ndef flush(self) -&gt; None:\nself.stdout.flush()\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.connect", "title": "<code>connect</code>", "text": "<p>Open a connection to a sqlite database, creating one if it does not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>Optional[str]</code> <p>The path to the database file. Or None to default to ~/fastestimator_data/history.db</p> <code>None</code> <p>Returns:</p> Type Description <code>sql.Connection</code> <p>An open connection to the database, with schema instantiated and foreign keys enabled.</p> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def connect(db_path: Optional[str] = None) -&gt; sql.Connection:\n\"\"\"Open a connection to a sqlite database, creating one if it does not already exist.\n    Args:\n        db_path: The path to the database file. Or None to default to ~/fastestimator_data/history.db\n    Returns:\n        An open connection to the database, with schema instantiated and foreign keys enabled.\n    \"\"\"\nif db_path is None:\ndb_path = os.path.join(str(Path.home()), 'fastestimator_data', 'history.db')\nif db_path != ':memory:':  # This is a reserved keyword to create an in-memory database\nos.makedirs(os.path.dirname(db_path), exist_ok=True)  # Make sure folders exist before creating disk file\nconnection = sql.connect(db_path, detect_types=sql.PARSE_DECLTYPES | sql.PARSE_COLNAMES)\nif db_path != \":memory:\":\n# Check to ensure the database isn't corrupted\ncur = connection.execute(\"PRAGMA integrity_check\")\nresponse = cur.fetchall()[0]\nif response != ('ok',):\nconnection.close()\ncorrupt_path = os.path.join(os.path.dirname(db_path),\nf\"corrupt_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_\"\nf\"{os.path.basename(db_path)}\")\nos.renames(db_path, corrupt_path)\nprint(f\"FastEstimator-Warn: The FastEstimator history database has been corrupted. It has been moved to \"\nf\"{corrupt_path} and a new one has been automatically created. The integrity check output is: \"\nf\"{response}\")\nconnection = sql.connect(db_path, detect_types=sql.PARSE_DECLTYPES | sql.PARSE_COLNAMES)\nconnection.execute(\"PRAGMA foreign_keys = 1\")  # Enable FK constraints\nconnection.row_factory = sql.Row  # Get nice query return objects\n# Build the schema if it doesn't exist\nconnection.execute(_MAKE_HIST_TABLE)\nconnection.execute(_MAKE_FEAT_TABLE)\nconnection.execute(_MAKE_DS_TABLE)\nconnection.execute(_MAKE_PIPELINE_TABLE)\nconnection.execute(_MAKE_NETWORK_TABLE)\nconnection.execute(_MAKE_POST_PROCESS_TABLE)\nconnection.execute(_MAKE_TRACE_TABLE)\nconnection.execute(_MAKE_ERR_TABLE)\nconnection.execute(_MAKE_LOG_TABLE)\nconnection.execute(_MAKE_SETTINGS_TABLE)\nconnection.execute(_MAKE_SETTINGS_ENTRY)\nconnection.commit()\nreturn connection\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.delete", "title": "<code>delete</code>", "text": "<p>Remove history entries from a database.</p> <p>This will also remove associated data such as logs due to foreign key constraints.</p> <p>Parameters:</p> Name Type Description Default <code>n_keep</code> <code>int</code> <p>How many history entries to keep.</p> <code>20</code> <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def delete(n_keep: int = 20, db_path: Optional[str] = None) -&gt; None:\n\"\"\"Remove history entries from a database.\n    This will also remove associated data such as logs due to foreign key constraints.\n    Args:\n        n_keep: How many history entries to keep.\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndb = connect(db_path)\ndb.execute(\n\"DELETE FROM history WHERE train_start &lt;= (\"\n\"SELECT train_start FROM history ORDER BY train_start DESC LIMIT 1 OFFSET (?))\", [n_keep])\ndb.commit()  # Can't vacuum while there are uncommitted changes\ndb.execute(\"VACUUM\")  # Free the memory\ndb.commit()\ndb.close()\n</code></pre>"}, {"location": "fastestimator/summary/history.html#fastestimator.fastestimator.summary.history.update_settings", "title": "<code>update_settings</code>", "text": "<p>Update the history database settings.</p> <p>Updated settings will be enforced the next time a training or delete operation is called.</p> <p>Parameters:</p> Name Type Description Default <code>n_keep</code> <code>Optional[int]</code> <p>How many history entries should be retained.</p> <code>None</code> <code>n_keep_logs</code> <code>Optional[int]</code> <p>How many logs should be retained. This value should be &lt;= <code>n_keep</code>.</p> <code>None</code> <code>db_path</code> <code>Optional[str]</code> <p>The path to the database, or None to use the default location.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\history.py</code> <pre><code>def update_settings(n_keep: Optional[int] = None, n_keep_logs: Optional[int] = None,\ndb_path: Optional[str] = None) -&gt; None:\n\"\"\"Update the history database settings.\n    Updated settings will be enforced the next time a training or delete operation is called.\n    Args:\n        n_keep: How many history entries should be retained.\n        n_keep_logs: How many logs should be retained. This value should be &lt;= `n_keep`.\n        db_path: The path to the database, or None to use the default location.\n    \"\"\"\ndb = connect(db_path)\n# Ensure limits are non-negative\nif n_keep:\nn_keep = max(n_keep, 0)\nif n_keep_logs:\nn_keep_logs = max(n_keep_logs, 0)\n# Perform the update\nif n_keep is not None and n_keep_logs is not None:\ndb.execute(\"UPDATE settings SET n_keep = :keep, n_keep_logs = MIN(:keep, :logs) WHERE pk = 0\", {\n'keep': n_keep, 'logs': n_keep_logs\n})\nelif n_keep is not None:\ndb.execute(\"UPDATE settings SET n_keep = :keep, n_keep_logs = MIN(n_keep_logs, :keep) WHERE pk = 0\",\n{'keep': n_keep})\nelif n_keep_logs is not None:\ndb.execute(\"UPDATE settings SET n_keep_logs = MIN(n_keep, (?)) WHERE pk = 0\", [n_keep_logs])\ndb.commit()\nwith closing(db.cursor()) as cursor:\ncursor.execute(\"SELECT * FROM settings\")\nresponse = from_db_cursor(cursor)\n# Hide implementation details from end user\nresponse.del_column('pk')\nresponse.del_column('schema_version')\nprint(response)\ndb.close()\n</code></pre>"}, {"location": "fastestimator/summary/summary.html", "title": "summary", "text": ""}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary", "title": "<code>Summary</code>", "text": "<p>A summary object that records training history.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the experiment. If None then experiment results will be ignored.</p> required <code>system_config</code> <code>Optional[List[FeSummaryTable]]</code> <p>A description of the initialization parameters defining the estimator associated with this experiment.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>class Summary:\n\"\"\"A summary object that records training history.\n    This class is intentionally not @traceable.\n    Args:\n        name: Name of the experiment. If None then experiment results will be ignored.\n        system_config: A description of the initialization parameters defining the estimator associated with this\n            experiment.\n    \"\"\"\ndef __init__(self, name: Optional[str], system_config: Optional[List['FeSummaryTable']] = None) -&gt; None:\nself.name = name\nself.system_config = system_config\nself.history = defaultdict(lambda: defaultdict(dict))  # {mode: {key: {step: value}}}\ndef merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n        Args:\n            other: Other `summary` object to be merged.\n        \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\ndef __bool__(self) -&gt; bool:\n\"\"\"Whether training history should be recorded.\n        Returns:\n            True iff this `Summary` has a non-None name.\n        \"\"\"\nreturn bool(self.name)\ndef __getstate__(self) -&gt; Dict[str, Any]:\n\"\"\"Get a representation of the state of this object.\n        This method is invoked by pickle.\n        Returns:\n            The information to be recorded by a pickle summary of this object.\n        \"\"\"\nstate = self.__dict__.copy()\ndel state['system_config']\nstate['history'] = dict(state['history'])\nreturn state\ndef __setstate__(self, state: Dict[str, Any]) -&gt; None:\n\"\"\"Set this objects internal state from a dictionary of variables.\n        This method is invoked by pickle.\n        Args:\n            state: The saved state to be used by this object.\n        \"\"\"\nhistory = defaultdict(lambda: defaultdict(dict))\nhistory.update(state.get('history', {}))\nstate['history'] = history\nself.__dict__.update(state)\n</code></pre>"}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary.merge", "title": "<code>merge</code>", "text": "<p>Merge another <code>Summary</code> into this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Summary</code> <p>Other <code>summary</code> object to be merged.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>def merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n    Args:\n        other: Other `summary` object to be merged.\n    \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\n</code></pre>"}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.ValWithError", "title": "<code>ValWithError</code>", "text": "<p>         Bases: <code>NamedTuple</code></p> <p>A class to record values with error bars (for special visualization in the logger).</p> Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>class ValWithError(NamedTuple):\n\"\"\"A class to record values with error bars (for special visualization in the logger).\n    \"\"\"\ny_min: float\ny: float\ny_max: float\ndef __str__(self):\nreturn f\"({self.y_min}, {self.y}, {self.y_max})\"\n</code></pre>"}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.average_summaries", "title": "<code>average_summaries</code>", "text": "<p>Average multiple summaries together, storing their metric means +- stdevs.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name for the new summary.</p> required <code>summaries</code> <code>List[Summary]</code> <p>A list of summaries to be averaged.</p> required <p>Returns:</p> Type Description <code>Summary</code> <p>A single summary object reporting mean+-stddev for each metric. If a particular value has only 1 datapoint, it</p> <code>Summary</code> <p>will not be averaged.</p> Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>def average_summaries(name: str, summaries: List[Summary]) -&gt; Summary:\n\"\"\"Average multiple summaries together, storing their metric means +- stdevs.\n    Args:\n        name: The name for the new summary.\n        summaries: A list of summaries to be averaged.\n    Returns:\n        A single summary object reporting mean+-stddev for each metric. If a particular value has only 1 datapoint, it\n        will not be averaged.\n    \"\"\"\nif len(summaries) == 0:\nreturn Summary(name=name)\nif len(summaries) == 1:\nsummaries[0].name = name\nreturn summaries[0]\nconsolidated = Summary(name=name)\n# Find all of the modes, keys, and steps over the various summaries\nmodes = {mode for summary in summaries for mode in summary.history.keys()}\nkeys = {key for summary in summaries for key_pairs in summary.history.values() for key in key_pairs.keys()}\nsteps = {\nstep\nfor summary in summaries for key_pairs in summary.history.values() for val_pair in key_pairs.values()\nfor step in val_pair.keys()\n}\n# Average everything\nfor mode in modes:\nfor key in keys:\nif key == 'epoch':\n# doesn't make sense to average the epoch over different summaries\n# TODO - if all summaries have same epoch dict then preserve it\ncontinue\nfor step in steps:\nvals = []\nfor summary in summaries:\nhistory = summary.history\nif mode in history and key in history[mode] and step in history[mode][key]:\nval = history[mode][key][step]\nif isinstance(val, str):\n# Can't plot strings over time...\nval = [float(s) for s in re.findall(r'(\\d+\\.\\d+|\\.?\\d+)', val)]\nif len(val) == 1:\n# We got an unambiguous number\nval = val[0]\nelse:\nval = None\nelif isinstance(val, ValWithError):\nval = val.y\nelif not isinstance(val, (int, float)):\nval = None\nif val is not None:\nvals.append(val)\nif mode == 'test':\n# We will consolidate these later\nval = vals\nelse:\nval = _reduce_list(vals)\nif val is None:\ncontinue\nconsolidated.history[mode][key][step] = val\nif mode == 'test':\n# Due to early stopping, the test mode might be invoked at different steps/epochs. These values will be\n# merged and assigned to the largest available step.\nfor key, step_val in consolidated.history[mode].items():\nvals = []\nfor step, val in step_val.items():\nif isinstance(val, list):\nvals.extend(val)\nelse:\nvals.append(val)\nstep = max(step_val.keys())\nval = _reduce_list(vals)\nconsolidated.history[mode][key].clear()\nif val is not None:\nconsolidated.history[mode][key][step] = val\nreturn consolidated\n</code></pre>"}, {"location": "fastestimator/summary/system.html", "title": "system", "text": ""}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System", "title": "<code>System</code>", "text": "<p>A class which tracks state information while the fe.Estimator is running.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>BaseNetwork</code> <p>The network instance being used by the current fe.Estimator.</p> required <code>pipeline</code> <code>Pipeline</code> <p>The pipeline instance being used by the current fe.Estimator.</p> required <code>traces</code> <code>List[Union[Trace, Scheduler[Trace]]]</code> <p>The traces provided to the current fe.Estimator.</p> required <code>mode</code> <code>Optional[str]</code> <p>The current execution mode (or None for warmup).</p> <code>None</code> <code>num_devices</code> <code>int</code> <p>How many GPUs are available for training.</p> <code>torch.cuda.device_count()</code> <code>log_steps</code> <code>Optional[int]</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>None</code> <code>total_epochs</code> <code>int</code> <p>How many epochs training is expected to run for.</p> <code>0</code> <code>train_steps_per_epoch</code> <code>Optional[int]</code> <p>Whether training iterations will be cut short or extended to complete N steps (or use None if they will run to completion)</p> <code>None</code> <code>eval_steps_per_epoch</code> <code>Optional[int]</code> <p>Whether evaluation iterations will be cut short or extended to complete N steps (or use None if they will run to completion)</p> <code>None</code> <code>eval_log_steps</code> <code>Sequence[int]</code> <p>The list of steps on which evaluation progress logs need to be printed.</p> <code>()</code> <code>system_config</code> <code>Optional[List[FeSummaryTable]]</code> <p>A description of the initialization parameters defining the associated estimator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>mode</code> <p>What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.</p> <code>ds_id</code> <p>The current dataset id, Empty string if there is only one dataset in each mode.</p> <code>exp_id</code> <code>int</code> <p>A unique identifier for current training experiment.</p> <code>global_step</code> <code>Optional[int]</code> <p>How many training steps have elapsed.</p> <code>num_devices</code> <p>How many GPUs are available for training.</p> <code>log_steps</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>total_epochs</code> <p>How many epochs training is expected to run for.</p> <code>epoch_idx</code> <code>Optional[int]</code> <p>The current epoch index for the training (starting from 1).</p> <code>batch_idx</code> <p>The current batch index within an epoch (starting from 1).</p> <code>stop_training</code> <p>A flag to signal that training should abort.</p> <code>network</code> <p>A reference to the network being used.</p> <code>pipeline</code> <p>A reference to the pipeline being used.</p> <code>traces</code> <p>The traces being used.</p> <code>train_steps_per_epoch</code> <p>Training will be cut short or extended to complete N steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>eval_steps_per_epoch</code> <p>Evaluation will be cut short or extended to complete N steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>eval_log_steps</code> <p>The list of steps on which evaluation progress logs need to be printed.</p> <code>summary</code> <p>An object to write experiment results to.</p> <code>experiment_time</code> <p>A timestamp indicating when this model was trained.</p> <code>custom_graphs</code> <p>A place to store extra graphs which are too complicated for the primary history.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>class System:\n\"\"\"A class which tracks state information while the fe.Estimator is running.\n    This class is intentionally not @traceable.\n    Args:\n        network: The network instance being used by the current fe.Estimator.\n        pipeline: The pipeline instance being used by the current fe.Estimator.\n        traces: The traces provided to the current fe.Estimator.\n        mode: The current execution mode (or None for warmup).\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        train_steps_per_epoch: Whether training iterations will be cut short or extended to complete N steps (or use None if they will run\n            to completion)\n        eval_steps_per_epoch: Whether evaluation iterations will be cut short or extended to complete N steps (or use None if they will run\n            to completion)\n        eval_log_steps: The list of steps on which evaluation progress logs need to be printed.\n        system_config: A description of the initialization parameters defining the associated estimator.\n    Attributes:\n        mode: What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.\n        ds_id: The current dataset id, Empty string if there is only one dataset in each mode.\n        exp_id: A unique identifier for current training experiment.\n        global_step: How many training steps have elapsed.\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        epoch_idx: The current epoch index for the training (starting from 1).\n        batch_idx: The current batch index within an epoch (starting from 1).\n        stop_training: A flag to signal that training should abort.\n        network: A reference to the network being used.\n        pipeline: A reference to the pipeline being used.\n        traces: The traces being used.\n        train_steps_per_epoch: Training will be cut short or extended to complete N steps even if loader is not yet\n            exhausted. If None, all data will be used.\n        eval_steps_per_epoch: Evaluation will be cut short or extended to complete N steps even if loader is not yet\n            exhausted. If None, all data will be used.\n        eval_log_steps: The list of steps on which evaluation progress logs need to be printed.\n        summary: An object to write experiment results to.\n        experiment_time: A timestamp indicating when this model was trained.\n        custom_graphs: A place to store extra graphs which are too complicated for the primary history.\n    \"\"\"\nmode: Optional[str]\nds_id: str\nexp_id: int\nglobal_step: Optional[int]\nnum_devices: int\nlog_steps: Optional[int]\ntotal_epochs: int\nepoch_idx: Optional[int]\nbatch_idx: Optional[int]\nstop_training: bool\nnetwork: BaseNetwork\npipeline: Pipeline\ntraces: List[Union['Trace', Scheduler['Trace']]]\ntrain_steps_per_epoch: Optional[int]\neval_steps_per_epoch: Optional[int]\neval_log_steps: Sequence[int]\nsummary: Summary\nexperiment_time: str\ncustom_graphs: Dict[str, List[Summary]]\ndef __init__(self,\nnetwork: BaseNetwork,\npipeline: Pipeline,\ntraces: List[Union['Trace', Scheduler['Trace']]],\nmode: Optional[str] = None,\nds_id: str = '',\nnum_devices: int = torch.cuda.device_count(),\nlog_steps: Optional[int] = None,\ntotal_epochs: int = 0,\ntrain_steps_per_epoch: Optional[int] = None,\neval_steps_per_epoch: Optional[int] = None,\neval_log_steps: Sequence[int] = (),\nsystem_config: Optional[List[FeSummaryTable]] = None) -&gt; None:\nself.network = network\nself.pipeline = pipeline\nself.eval_log_steps = eval_log_steps\nself.traces = traces\nself.mode = mode\nself.ds_id = ds_id\nself.num_devices = num_devices\nself.log_steps = log_steps\nself.total_epochs = total_epochs\nself.batch_idx = None\nself.train_steps_per_epoch = train_steps_per_epoch\nself.eval_steps_per_epoch = eval_steps_per_epoch\nself.stop_training = False\nself.summary = Summary(None, system_config)\nself.experiment_time = \"\"\nself.custom_graphs = {}\nself._initialize_state()\n@property\ndef steps_per_epoch(self) -&gt; Optional[int]:\nif self.mode == 'train':\nreturn self.train_steps_per_epoch\nelif self.mode == 'eval':\nreturn self.eval_steps_per_epoch\nelse:\nreturn None\ndef _initialize_state(self) -&gt; None:\n\"\"\"Initialize the training state.\n        \"\"\"\nself.global_step = None\nself.epoch_idx = 0\n# Get a 64 bit random id related to current time\nself.exp_id = int.from_bytes(uuid.uuid1().bytes, byteorder='big', signed=True) &gt;&gt; 64\ndef update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n        \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\ndef update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n        \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\ndef reset(self, summary_name: Optional[str] = None, system_config: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n        Args:\n            summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n            system_config: A description of the initialization parameters defining the associated estimator.\n        \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself.ds_id = ''\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name, system_config)\nself.custom_graphs = {}\ndef reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n        Args:\n            summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n        \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nself.ds_id = ''\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\nfor graph_set in self.custom_graphs.values():\nfor graph in graph_set:\ngraph.history.pop('test', None)\ndef write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n        Args:\n            key: The key to write into the summary object.\n            value: The value to write into the summary object.\n        \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\ndef add_graph(self, graph_name: str, graph: Union[Summary, List[Summary]]) -&gt; None:\n\"\"\"Write custom summary graphs into the System.\n        This can be useful for things like the LabelTracker trace to interact with Traceability reports.\n        Args:\n            graph_name: The name of the graph (so that you can override it later if desired).\n            graph: The custom summary to be tracked.\n        \"\"\"\nif isinstance(graph, Summary):\nself.custom_graphs[graph_name] = [graph]\nelse:\nself.custom_graphs[graph_name] = list(graph)\ndef save_state(self, save_dir: str) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            save_dir: The directory into which to save the state\n        \"\"\"\nos.makedirs(save_dir, exist_ok=True)\n# Start with the high-level info. We could use pickle for this but having it human readable is nice.\nstate = {key: value for key, value in self.__dict__.items() if is_restorable(value)[0]}\nwith open(os.path.join(save_dir, 'system.json'), 'w') as fp:\njson.dump(state, fp, indent=4)\n# Save all of the models / optimizer states\nfor model in self.network.models:\nsave_model(model, save_dir=save_dir, save_optimizer=True)\n# Save everything else\nobjects = {\n'summary': self.summary,\n'custom_graphs': self.custom_graphs,\n'traces': [trace.__getstate__() if hasattr(trace, '__getstate__') else {} for trace in self.traces],\n'tops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.ops],\n'pops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.postprocessing],\n'nops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.pipeline.ops],\n'ds': {\nmode: {key: value.__getstate__()\nfor key, value in ds.items() if hasattr(value, '__getstate__')}\nfor mode,\nds in self.pipeline.data.items()\n}\n}\nwith open(os.path.join(save_dir, 'objects.pkl'), 'wb') as file:\n# We need to use a custom pickler here to handle MirroredStrategy, which will show up inside of tf\n# MirroredVariables in multi-gpu systems.\np = pickle.Pickler(file)\np.dispatch_table = copyreg.dispatch_table.copy()\np.dispatch_table[MirroredStrategy] = pickle_mirroredstrategy\np.dump(objects)\ndef load_state(self, load_dir: str) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            load_dir: The directory from which to reload the state.\n        Raises:\n            FileNotFoundError: If necessary files can not be found.\n        \"\"\"\n# Reload the high-level system information\nsystem_path = os.path.join(load_dir, 'system.json')\nif not os.path.exists(system_path):\nraise FileNotFoundError(f\"Could not find system summary file at {system_path}\")\nwith open(system_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# Reload the models\nfor model in self.network.models:\nself._load_model(model, load_dir)\n# Reload everything else\nobjects_path = os.path.join(load_dir, 'objects.pkl')\nif not os.path.exists(objects_path):\nraise FileNotFoundError(f\"Could not find the objects summary file at {objects_path}\")\nwith open(objects_path, 'rb') as file:\nobjects = pickle.load(file)\nself.summary.__dict__.update(objects['summary'].__dict__)\nself.custom_graphs = objects['custom_graphs']\nself._load_list(objects, 'traces', self.traces)\nself._load_list(objects, 'tops', self.network.ops)\nself._load_list(objects, 'pops', self.network.postprocessing)\nself._load_list(objects, 'nops', self.pipeline.ops)\nself._load_dict(objects, 'ds', self.pipeline.data)\n@staticmethod\ndef _load_model(model: Model, base_path: str) -&gt; None:\n\"\"\"Load model and optimizer weights from disk.\n        Args:\n            model: The model to be loaded.\n            base_path: The folder where the model should be located.\n        Raises:\n            ValueError: If the model is of an unknown type.\n            FileNotFoundError: If the model weights or optimizer state is missing.\n        \"\"\"\nif isinstance(model, tf.keras.Model):\nmodel_ext, optimizer_ext = 'h5', 'pkl'\nelif isinstance(model, torch.nn.Module):\nmodel_ext, optimizer_ext = 'pt', 'pt'\nelse:\nraise ValueError(f\"Unknown model type: {type(model)}\")\nweights_path = os.path.join(base_path, f\"{model.model_name}.{model_ext}\")\nif not os.path.exists(weights_path):\nraise FileNotFoundError(f\"Cannot find model weights file at {weights_path}\")\noptimizer_path = os.path.join(base_path, f\"{model.model_name}_opt.{optimizer_ext}\")\nif not os.path.exists(optimizer_path):\nraise FileNotFoundError(f\"Cannot find model optimizer file at {optimizer_path}\")\nload_model(model, weights_path=weights_path, load_optimizer=True)\n@staticmethod\ndef _load_list(states: Dict[str, Any], state_key: str, in_memory_objects: List[Any]) -&gt; None:\n\"\"\"Load a list of pickled states from the disk.\n        Args:\n            states: The states to be restored.\n            state_key: Which state to select from the dictionary.\n            in_memory_objects: The existing in memory objects to be updated.\n        Raises:\n            ValueError: If the number of saved states does not match the number of in-memory objects.\n        \"\"\"\nstates = states[state_key]\nif not isinstance(states, list):\nraise ValueError(f\"Expected {state_key} to contain a list, but found a {type(states)}\")\nif len(states) != len(in_memory_objects):\nraise ValueError(\"Expected saved {} to contain {} objects, but found {} instead\".format(\nstate_key, len(in_memory_objects), len(states)))\nfor obj, state in zip(in_memory_objects, states):\nif hasattr(obj, '__setstate__'):\nobj.__setstate__(state)\nelif hasattr(obj, '__dict__'):\nobj.__dict__.update(state)\nelse:\n# Might be a None or something else that can't be updated\npass\n@staticmethod\ndef _load_dict(states: Dict[str, Any], state_key: str, in_memory_objects: Dict[Any, Any]) -&gt; None:\n\"\"\"Load a dictionary of pickled states from the disk.\n        Args:\n            states: The states to be restored.\n            state_key: Which state to select from the dictionary.\n            in_memory_objects: The existing in memory objects to be updated.\n        Raises:\n            ValueError: If the configuration of saved states does not match the number of in-memory objects.\n            FileNotFoundError: If the desired state file cannot be found.\n        \"\"\"\nstates = states[state_key]\nif not isinstance(states, dict):\nraise ValueError(f\"Expected {state_key} to contain a dict, but found a {type(states)}\")\n# Note that not being a subset is different from being a superset\nif not states.keys() &lt;= in_memory_objects.keys():\nraise ValueError(\"Saved {} contained unexpected keys: {}\".format(state_key,\nstates.keys() - in_memory_objects.keys()))\nfor key, state in states.items():\nobj = in_memory_objects[key]\nif hasattr(obj, '__setstate__'):\nobj.__setstate__(state)\nelif hasattr(obj, '__dict__'):\nobj.__dict__.update(state)\nelif isinstance(obj, dict):\nSystem._load_dict(states, key, obj)\nelse:\n# Might be a None or something else that can't be updated\npass\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.add_graph", "title": "<code>add_graph</code>", "text": "<p>Write custom summary graphs into the System.</p> <p>This can be useful for things like the LabelTracker trace to interact with Traceability reports.</p> <p>Parameters:</p> Name Type Description Default <code>graph_name</code> <code>str</code> <p>The name of the graph (so that you can override it later if desired).</p> required <code>graph</code> <code>Union[Summary, List[Summary]]</code> <p>The custom summary to be tracked.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def add_graph(self, graph_name: str, graph: Union[Summary, List[Summary]]) -&gt; None:\n\"\"\"Write custom summary graphs into the System.\n    This can be useful for things like the LabelTracker trace to interact with Traceability reports.\n    Args:\n        graph_name: The name of the graph (so that you can override it later if desired).\n        graph: The custom summary to be tracked.\n    \"\"\"\nif isinstance(graph, Summary):\nself.custom_graphs[graph_name] = [graph]\nelse:\nself.custom_graphs[graph_name] = list(graph)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.load_state", "title": "<code>load_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>load_dir</code> <code>str</code> <p>The directory from which to reload the state.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If necessary files can not be found.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def load_state(self, load_dir: str) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        load_dir: The directory from which to reload the state.\n    Raises:\n        FileNotFoundError: If necessary files can not be found.\n    \"\"\"\n# Reload the high-level system information\nsystem_path = os.path.join(load_dir, 'system.json')\nif not os.path.exists(system_path):\nraise FileNotFoundError(f\"Could not find system summary file at {system_path}\")\nwith open(system_path, 'r') as fp:\nstate = json.load(fp)\nself.__dict__.update(state)\n# Reload the models\nfor model in self.network.models:\nself._load_model(model, load_dir)\n# Reload everything else\nobjects_path = os.path.join(load_dir, 'objects.pkl')\nif not os.path.exists(objects_path):\nraise FileNotFoundError(f\"Could not find the objects summary file at {objects_path}\")\nwith open(objects_path, 'rb') as file:\nobjects = pickle.load(file)\nself.summary.__dict__.update(objects['summary'].__dict__)\nself.custom_graphs = objects['custom_graphs']\nself._load_list(objects, 'traces', self.traces)\nself._load_list(objects, 'tops', self.network.ops)\nself._load_list(objects, 'pops', self.network.postprocessing)\nself._load_list(objects, 'nops', self.pipeline.ops)\nself._load_dict(objects, 'ds', self.pipeline.data)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset", "title": "<code>reset</code>", "text": "<p>Reset the current <code>System</code> for a new round of training, including a new <code>Summary</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. The <code>Summary</code> object will store information iff name is not None.</p> <code>None</code> <code>system_config</code> <code>Optional[str]</code> <p>A description of the initialization parameters defining the associated estimator.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset(self, summary_name: Optional[str] = None, system_config: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n    Args:\n        summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n        system_config: A description of the initialization parameters defining the associated estimator.\n    \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself.ds_id = ''\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name, system_config)\nself.custom_graphs = {}\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset_for_test", "title": "<code>reset_for_test</code>", "text": "<p>Partially reset the current <code>System</code> object for a new round of testing.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. If not provided, the system will re-use the previous summary name.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n    Args:\n        summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n    \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nself.ds_id = ''\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\nfor graph_set in self.custom_graphs.values():\nfor graph in graph_set:\ngraph.history.pop('test', None)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.save_state", "title": "<code>save_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str</code> <p>The directory into which to save the state</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def save_state(self, save_dir: str) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        save_dir: The directory into which to save the state\n    \"\"\"\nos.makedirs(save_dir, exist_ok=True)\n# Start with the high-level info. We could use pickle for this but having it human readable is nice.\nstate = {key: value for key, value in self.__dict__.items() if is_restorable(value)[0]}\nwith open(os.path.join(save_dir, 'system.json'), 'w') as fp:\njson.dump(state, fp, indent=4)\n# Save all of the models / optimizer states\nfor model in self.network.models:\nsave_model(model, save_dir=save_dir, save_optimizer=True)\n# Save everything else\nobjects = {\n'summary': self.summary,\n'custom_graphs': self.custom_graphs,\n'traces': [trace.__getstate__() if hasattr(trace, '__getstate__') else {} for trace in self.traces],\n'tops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.ops],\n'pops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.network.postprocessing],\n'nops': [op.__getstate__() if hasattr(op, '__getstate__') else {} for op in self.pipeline.ops],\n'ds': {\nmode: {key: value.__getstate__()\nfor key, value in ds.items() if hasattr(value, '__getstate__')}\nfor mode,\nds in self.pipeline.data.items()\n}\n}\nwith open(os.path.join(save_dir, 'objects.pkl'), 'wb') as file:\n# We need to use a custom pickler here to handle MirroredStrategy, which will show up inside of tf\n# MirroredVariables in multi-gpu systems.\np = pickle.Pickler(file)\np.dispatch_table = copyreg.dispatch_table.copy()\np.dispatch_table[MirroredStrategy] = pickle_mirroredstrategy\np.dump(objects)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_batch_idx", "title": "<code>update_batch_idx</code>", "text": "<p>Increment the current <code>batch_idx</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n    \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_global_step", "title": "<code>update_global_step</code>", "text": "<p>Increment the current <code>global_step</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n    \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.write_summary", "title": "<code>write_summary</code>", "text": "<p>Write an entry into the <code>Summary</code> object (iff the experiment was named).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to write into the summary object.</p> required <code>value</code> <code>Any</code> <p>The value to write into the summary object.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n    Args:\n        key: The key to write into the summary object.\n        value: The value to write into the summary object.\n    \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.pickle_mirroredstrategy", "title": "<code>pickle_mirroredstrategy</code>", "text": "<p>A custom reduce function to use when Pickle encounters a tf MirroredStrategy.</p> <p>This relies on the fact that the tf strategy will already be set before the System.load_state method gets called.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>MirroredStrategy</code> <p>The MirroredStrategy instance.</p> required <p>Returns:</p> Type Description <code>Tuple[Callable, Tuple]</code> <p>The mechanism to construct a new instance of the MirroredStrategy. See Python docs on the reduce method.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def pickle_mirroredstrategy(obj: MirroredStrategy) -&gt; Tuple[Callable, Tuple]:\n\"\"\"A custom reduce function to use when Pickle encounters a tf MirroredStrategy.\n    This relies on the fact that the tf strategy will already be set before the System.load_state method gets called.\n    Args:\n        obj: The MirroredStrategy instance.\n    Returns:\n        The mechanism to construct a new instance of the MirroredStrategy. See Python docs on the __reduce__ method.\n    \"\"\"\nreturn tf.distribute.get_strategy, ()\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html", "title": "log_parse", "text": ""}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_dir", "title": "<code>parse_log_dir</code>", "text": "<p>A function which will gather all log files within a given folder and pass them along for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>The path to a directory containing log files.</p> required <code>log_extension</code> <code>str</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>recursive_search</code> <code>bool</code> <p>Whether to recursively search sub-directories for log files.</p> <code>False</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of metric keys (None whitelists all keys).</p> <code>None</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>group_by</code> <code>Optional[str]</code> <p>Combine multiple log files by a regex to visualize their mean+-stddev. For example, to group together files like [a_1.txt, a_2.txt] vs [b_1.txt, b_2.txt] you can use: r'(.*)_[\\d]+.txt'.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_dir(dir_path: str,\nlog_extension: str = '.txt',\nrecursive_search: bool = False,\nsmooth_factor: float = 0,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\ninclude_metrics: Optional[Set[str]] = None,\npretty_names: bool = False,\ngroup_by: Optional[str] = None) -&gt; None:\n\"\"\"A function which will gather all log files within a given folder and pass them along for visualization.\n    Args:\n        dir_path: The path to a directory containing log files.\n        log_extension: The extension of the log files.\n        recursive_search: Whether to recursively search sub-directories for log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        include_metrics: A whitelist of metric keys (None whitelists all keys).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        group_by: Combine multiple log files by a regex to visualize their mean+-stddev. For example, to group together\n            files like [a_1.txt, a_2.txt] vs [b_1.txt, b_2.txt] you can use: r'(.*)_[\\d]+\\.txt'.\n    \"\"\"\nfile_paths = list_files(root_dir=dir_path, file_extension=log_extension, recursive_search=recursive_search)\nparse_log_files(file_paths,\nlog_extension,\nsmooth_factor,\nsave,\nsave_path,\nignore_metrics,\ninclude_metrics,\npretty_names,\ngroup_by)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_file", "title": "<code>parse_log_file</code>", "text": "<p>A function which will parse log files into a dictionary of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to a log file.</p> required <code>file_extension</code> <code>str</code> <p>The extension of the log file.</p> required <p>Returns:</p> Type Description <code>Summary</code> <p>An experiment summarizing the given log file.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_file(file_path: str, file_extension: str) -&gt; Summary:\n\"\"\"A function which will parse log files into a dictionary of metrics.\n    Args:\n        file_path: The path to a log file.\n        file_extension: The extension of the log file.\n    Returns:\n        An experiment summarizing the given log file.\n    \"\"\"\n# TODO: need to handle multi-line output like confusion matrix\nexperiment = Summary(strip_suffix(os.path.split(file_path)[1].strip(), file_extension))\nwith open(file_path) as file:\nparse_log_iter(source=file, sync=experiment)\nreturn experiment\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_files", "title": "<code>parse_log_files</code>", "text": "<p>Parse one or more log files for graphing.</p> <p>This function which will iterate through the given log file paths, parse them to extract metrics, remove any metrics which are blacklisted, and then pass the necessary information on the graphing function.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>A list of paths to various log files.</p> required <code>log_extension</code> <code>Optional[str]</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of metric keys (None whitelists all keys).</p> <code>None</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>group_by</code> <code>Optional[str]</code> <p>Combine multiple log files by a regex to visualize their mean+-stddev. For example, to group together files like [a_1.txt, a_2.txt] vs [b_1.txt, b_2.txt] you can use: r'(.*)_[\\d]+.txt'.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no log files are provided.</p> <code>ValueError</code> <p>If a log file does not match the <code>group_by</code> regex pattern.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_files(file_paths: List[str],\nlog_extension: Optional[str] = '.txt',\nsmooth_factor: float = 0,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\ninclude_metrics: Optional[Set[str]] = None,\npretty_names: bool = False,\ngroup_by: Optional[str] = None) -&gt; None:\n\"\"\"Parse one or more log files for graphing.\n    This function which will iterate through the given log file paths, parse them to extract metrics, remove any\n    metrics which are blacklisted, and then pass the necessary information on the graphing function.\n    Args:\n        file_paths: A list of paths to various log files.\n        log_extension: The extension of the log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        include_metrics: A whitelist of metric keys (None whitelists all keys).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        group_by: Combine multiple log files by a regex to visualize their mean+-stddev. For example, to group together\n            files like [a_1.txt, a_2.txt] vs [b_1.txt, b_2.txt] you can use: r'(.*)_[\\d]+\\.txt'.\n    Raises:\n        AssertionError: If no log files are provided.\n        ValueError: If a log file does not match the `group_by` regex pattern.\n    \"\"\"\nif file_paths is None or len(file_paths) &lt; 1:\nraise AssertionError(\"must provide at least one log file\")\nif save and save_path is None:\nsave_path = os.path.join(os.path.dirname(file_paths[0]), 'parse_logs.html')\ngroups = defaultdict(list)  # {group_name: [experiment(s)]}\nfor path in file_paths:\nexperiment = parse_log_file(path, log_extension)\ntry:\nkey = (re.findall(group_by, os.path.split(path)[1]))[0] if group_by else experiment.name\nexcept IndexError:\nraise ValueError(f\"The log {os.path.split(path)[1]} did not match the given regex pattern: {group_by}\")\ngroups[key].append(experiment)\nexperiments = [average_summaries(name, exps) for name, exps in groups.items()]\nvisualize_logs(experiments,\nsave_path=save_path,\nsmooth_factor=smooth_factor,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics,\ninclude_metrics=include_metrics)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_iter", "title": "<code>parse_log_iter</code>", "text": "<p>A function which will parse lines into a dictionary of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Iterable[str]</code> <p>A collection of lines to parse.</p> required <code>sync</code> <code>Summary</code> <p>The summary to append into.</p> required <p>Returns:</p> Type Description <code>Summary</code> <p>The updated summary object.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_iter(source: Iterable[str], sync: Summary) -&gt; Summary:\n\"\"\"A function which will parse lines into a dictionary of metrics.\n    Args:\n        source: A collection of lines to parse.\n        sync: The summary to append into.\n    Returns:\n        The updated summary object.\n    \"\"\"\nlast_step = 0\nlast_epoch = 0\nfor line in source:\nmode = None\nif line.startswith(\"FastEstimator-Train: step\") or line.startswith(\"FastEstimator-Finish\"):\nmode = \"train\"\nelif line.startswith(\"FastEstimator-Eval: step\"):\nmode = \"eval\"\nelif line.startswith(\"FastEstimator-Test: step\"):\nmode = \"test\"\nif mode is None:\ncontinue\nnum = r\"([-]?[0-9]+[.]?[0-9]*(e[-]?[0-9]+[.]?[0-9]*)?)\"\nparsed_line = re.findall(r\"([^:;]+):[\\s]*(\" + num + r\"|None|\\(\" + num + \", \" + num + \", \" + num + r\"\\));\", line)\nstep = parsed_line[0]\nassert step[0].strip() == \"step\", \\\n            \"Log file (%s) seems to be missing step information, or step is not listed first\" % sync.name\nstep = step[1]\nadjust_epoch = False\nif step == 'None':\n# This might happen if someone runs the test mode from the cli\nstep = last_step\n# If the test mode was just guessing its epoch, use the prior epoch instead\nadjust_epoch = mode == 'test'\nelse:\nstep = int(step)\nlast_step = step\nfor metric in parsed_line[1:]:\nif metric[4]:\nval = ValWithError(float(metric[4]), float(metric[6]), float(metric[8]))\nelse:\nval = metric[1]\nif val == 'None':\ncontinue\nval = float(val)\nkey = metric[0].strip()\nif key == 'epoch':\nif adjust_epoch:\nval = last_epoch\nelse:\nlast_epoch = val\nsync.history[mode][key].update({step: val})\nreturn sync\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html", "title": "log_plot", "text": ""}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.plot_logs", "title": "<code>plot_logs</code>", "text": "<p>A function which will plot experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any keys to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of keys to include during plotting. If None then all will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>FigureFE</code> <p>The handle of the pyplot figure.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def plot_logs(experiments: List[Summary],\nsmooth_factor: float = 0,\nignore_metrics: Optional[Set[str]] = None,\npretty_names: bool = False,\ninclude_metrics: Optional[Set[str]] = None) -&gt; FigureFE:\n\"\"\"A function which will plot experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any keys to ignore during plotting.\n        include_metrics: A whitelist of keys to include during plotting. If None then all will be included.\n    Returns:\n        The handle of the pyplot figure.\n    \"\"\"\n# Sort to keep same colors between multiple runs of visualization\nexperiments = humansorted(to_list(experiments), lambda exp: exp.name)\nn_experiments = len(experiments)\nif n_experiments == 0:\nreturn FigureFE.from_figure(make_subplots())\nignore_keys = ignore_metrics or set()\nignore_keys = to_set(ignore_keys)\nignore_keys |= {'epoch'}\ninclude_keys = to_set(include_metrics)\n# TODO: epoch should be indicated on the axis (top x axis?). Problem - different epochs per experiment.\n# TODO: figure out how ignore_metrics should interact with mode\n# TODO: when ds_id switches during training, prevent old id from connecting with new one (break every epoch?)\nds_ids = set()\nmetric_histories = defaultdict(_MetricGroup)  # metric: MetricGroup\nfor idx, experiment in enumerate(experiments):\nhistory = experiment.history\n# Since python dicts remember insertion order, sort the history so that train mode is always plotted on bottom\nfor mode, metrics in sorted(history.items(),\nkey=lambda x: 0 if x[0] == 'train' else 1 if x[0] == 'eval' else 2 if x[0] == 'test'\nelse 3 if x[0] == 'infer' else 4):\nfor metric, step_val in metrics.items():\nbase_metric, ds_id, *_ = f'{metric}|'.split('|')  # Plot acc|ds1 and acc|ds2 on same acc graph\nif len(step_val) == 0:\ncontinue  # Ignore empty metrics\nif metric in ignore_keys or base_metric in ignore_keys:\ncontinue\n# Here we intentionally check against metric and not base_metric. If user wants to display per-ds they\n#  can specify that in their include list: --include mcc 'mcc|usps'\nif include_keys and metric not in include_keys:\ncontinue\nmetric_histories[base_metric].add(idx, mode, ds_id, step_val)\nds_ids.add(ds_id)\nmetric_list = list(sorted(metric_histories.keys()))\nif len(metric_list) == 0:\nreturn FigureFE.from_figure(make_subplots())\nds_ids = humansorted(ds_ids)  # Sort them to have consistent ordering (and thus symbols) between plot runs\nn_plots = len(metric_list)\nif len(ds_ids) &gt; 9:  # 9 b/c None is included\nprint(\"FastEstimator-Warn: Plotting more than 8 different datasets isn't well supported. Symbols will be \"\n\"reused.\")\n# Non-Shared legends aren't supported yet. If they get supported then maybe can have that feature here too.\n#  https://github.com/plotly/plotly.js/issues/5099\n#  https://github.com/plotly/plotly.js/issues/5098\n# map the metrics into an n x n grid, then remove any extra columns. Final grid will be n x m with m &lt;= n\nn_rows = math.ceil(math.sqrt(n_plots))\nn_cols = math.ceil(n_plots / n_rows)\nmetric_grid_location = {}\nnd1_metrics = []\nidx = 0\nfor metric in metric_list:\nif metric_histories[metric].ndim() == 1:\n# Delay placement of the 1D plots until the end\nnd1_metrics.append(metric)\nelse:\nmetric_grid_location[metric] = (idx // n_cols, idx % n_cols)\nidx += 1\nfor metric in nd1_metrics:\nmetric_grid_location[metric] = (idx // n_cols, idx % n_cols)\nidx += 1\ntitles = [k for k, v in sorted(list(metric_grid_location.items()), key=lambda e: e[1][0] * n_cols + e[1][1])]\nif pretty_names:\ntitles = [prettify_metric_name(title) for title in titles]\nfig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=titles, shared_xaxes='all')\nfig.update_layout({'plot_bgcolor': '#FFF',\n'hovermode': 'closest',\n'margin': {'t': 50},\n'modebar': {'add': ['hoverclosest', 'hovercompare'],\n'remove': ['select2d', 'lasso2d']},\n'legend': {'tracegroupgap': 5,\n'font': {'size': 11}}})\n# Set x-labels\nfor idx, metric in enumerate(titles, start=1):\nplotly_idx = idx if idx &gt; 1 else \"\"\nx_axis_name = f'xaxis{plotly_idx}'\ny_axis_name = f'yaxis{plotly_idx}'\nif metric_histories[metric].ndim() &gt; 1:\nfig['layout'][x_axis_name]['title'] = 'Steps'\nfig['layout'][x_axis_name]['showticklabels'] = True\nfig['layout'][x_axis_name]['linecolor'] = \"#BCCCDC\"\nfig['layout'][y_axis_name]['linecolor'] = \"#BCCCDC\"\nelse:\n# Put blank data onto the axis to instantiate the domain\nrow, col = metric_grid_location[metric][0], metric_grid_location[metric][1]\nfig.add_annotation(text='', showarrow=False, row=row + 1, col=col + 1)\n# Hide the axis stuff\nfig['layout'][x_axis_name]['showgrid'] = False\nfig['layout'][x_axis_name]['zeroline'] = False\nfig['layout'][x_axis_name]['visible'] = False\nfig['layout'][y_axis_name]['showgrid'] = False\nfig['layout'][y_axis_name]['zeroline'] = False\nfig['layout'][y_axis_name]['visible'] = False\n# If there is only 1 experiment, we will use alternate colors based on mode\ncolor_offset = defaultdict(lambda: 0)\nn_colors = n_experiments\nif n_experiments == 1:\nn_colors = 4\ncolor_offset['eval'] = 1\ncolor_offset['test'] = 2\ncolor_offset['infer'] = 3\ncolors = get_colors(n_colors=n_colors)\nalpha_colors = get_colors(n_colors=n_colors, alpha=0.3)\n# exp_id : {mode: {ds_id: {type: True}}}\nadd_label = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: True))))\n# {row: {col: (x, y)}}\nax_text = defaultdict(lambda: defaultdict(lambda: (0.0, 0.9)))  # Where to put the text on a given axis\n# Set up ds_id markers. The empty ds_id will have no extra marker. After that there are 4 configurations of 3-arm\n# marker, followed by 'x', '+', '*', and pound. After that it will just repeat the symbol set.\nds_id_markers = [None, 37, 38, 39, 40, 34, 33, 35, 36]  # https://plotly.com/python/marker-style/\nds_id_markers = {k: v for k, v in zip(ds_ids, cycle(ds_id_markers))}\n# Plotly doesn't support z-order, so delay insertion until all the plots are figured out:\n# https://github.com/plotly/plotly.py/issues/2345\nz_order = defaultdict(list)  # {order: [(plotly element, row, col), ...]}\n# Figure out the legend ordering\nlegend_order = []\nfor exp_idx, experiment in enumerate(experiments):\nfor metric, group in metric_histories.items():\nfor mode in group.modes(exp_idx):\nfor ds_id in group.ds_ids(exp_idx, mode):\nds_title = f\"{ds_id} \" if ds_id else ''\ntitle = f\"{experiment.name} ({ds_title}{mode})\" if n_experiments &gt; 1 else f\"{ds_title}{mode}\"\nlegend_order.append(title)\nlegend_order.sort()\nlegend_order = {legend: order for order, legend in enumerate(legend_order)}\n# Actually do the plotting\nfor exp_idx, experiment in enumerate(experiments):\nfor metric, group in metric_histories.items():\nrow, col = metric_grid_location[metric][0], metric_grid_location[metric][1]\nif group.ndim() == 1:\n# Single value\nfor mode in group.modes(exp_idx):\nfor ds_id in group.ds_ids(exp_idx, mode):\nds_title = f\"{ds_id} \" if ds_id else ''\nprefix = f\"{experiment.name} ({ds_title}{mode})\" if n_experiments &gt; 1 else f\"{ds_title}{mode}\"\nplotly_idx = row * n_cols + col + 1 if row * n_cols + col + 1 &gt; 1 else ''\nfig.add_annotation(text=f\"{prefix}: {group.get_val(exp_idx, mode, ds_id)}\",\nfont={'color': colors[exp_idx + color_offset[mode]]},\nshowarrow=False,\nxref=f'x{plotly_idx} domain',\nxanchor='left',\nx=ax_text[row][col][0],\nyref=f'y{plotly_idx} domain',\nyanchor='top',\ny=ax_text[row][col][1],\nexclude_empty_subplots=False)\nax_text[row][col] = (ax_text[row][col][0], ax_text[row][col][1] - 0.1)\nif ax_text[row][col][1] &lt; 0:\nax_text[row][col] = (ax_text[row][col][0] + 0.5, 0.9)\nelif group.ndim() == 2:\nfor mode, dsv in group[exp_idx].items():\ncolor = colors[exp_idx + color_offset[mode]]\nfor ds_id, data in dsv.items():\nds_title = f\"{ds_id} \" if ds_id else ''\ntitle = f\"{experiment.name} ({ds_title}{mode})\" if n_experiments &gt; 1 else f\"{ds_title}{mode}\"\nif data.shape[0] &lt; 2:\nx = data[0][0]\ny = data[0][1]\ny_min = None\ny_max = None\nif isinstance(y, ValWithError):\ny_min = y.y_min\ny_max = y.y_max\ny = y.y\nmarker_style = 'circle' if mode == 'train' else 'diamond' if mode == 'eval' \\\n                                else 'square' if mode == 'test' else 'hexagram'\nlimit_data = [(y_max, y_min)] if y_max is not None and y_min is not None else None\ntip_text = \"%{x}: (%{customdata[1]:.3f}, %{y:.3f}, %{customdata[0]:.3f})\" if \\\n                                limit_data is not None else \"%{x}: %{y:.3f}\"\nerror_y = None if limit_data is None else {'type': 'data',\n'symmetric': False,\n'array': [y_max - y],\n'arrayminus': [y - y_min]}\nz_order[2].append((go.Scatter(x=[x],\ny=[y],\nname=title,\nlegendgroup=title,\ncustomdata=limit_data,\nhovertemplate=tip_text,\nmode='markers',\nmarker={'color': color,\n'size': 12,\n'symbol': _symbol_mash(marker_style,\nds_id_markers[ds_id]),\n'line': {'width': 1.5,\n'color': 'White'}},\nerror_y=error_y,\nshowlegend=add_label[exp_idx][mode][ds_id]['patch'],\nlegendrank=legend_order[title]),\nrow,\ncol))\nadd_label[exp_idx][mode][ds_id]['patch'] = False\nelse:\n# We can draw a line\ny = data[:, 1]\ny_min = None\ny_max = None\nif isinstance(y[0], ValWithError):\ny = np.stack(y)\ny_min = y[:, 0]\ny_max = y[:, 2]\ny = y[:, 1]\nif smooth_factor != 0:\ny_min = gaussian_filter1d(y_min, sigma=smooth_factor)\ny_max = gaussian_filter1d(y_max, sigma=smooth_factor)\n# TODO - for smoothed lines, plot original data in background but greyed out\nif smooth_factor != 0:\ny = gaussian_filter1d(y, sigma=smooth_factor)\nx = data[:, 0]\nlinestyle = 'solid' if mode == 'train' else 'dash' if mode == 'eval' else 'dot' if \\\n                                mode == 'test' else 'dashdot'\nlimit_data = [(mx, mn) for mx, mn in zip(y_max, y_min)] if y_max is not None and y_min is \\\n                                                                                       not None else None\ntip_text = \"%{x}: (%{customdata[1]:.3f}, %{y:.3f}, %{customdata[0]:.3f})\" if \\\n                                limit_data is not None else \"%{x}: %{y:.3f}\"\nz_order[1].append((go.Scatter(x=x,\ny=y,\nname=title,\nlegendgroup=title,\nmode=\"lines+markers\" if ds_id_markers[ds_id] else 'lines',\nmarker={'color': color,\n'size': 8,\n'line': {'width': 2,\n'color': 'DarkSlateGrey'},\n'maxdisplayed': 10,\n'symbol': ds_id_markers[ds_id]},\nline={'dash': linestyle,\n'color': color},\ncustomdata=limit_data,\nhovertemplate=tip_text,\nshowlegend=add_label[exp_idx][mode][ds_id]['line'],\nlegendrank=legend_order[title]),\nrow,\ncol))\nadd_label[exp_idx][mode][ds_id]['line'] = False\nif limit_data is not None:\nz_order[0].append((go.Scatter(x=x,\ny=y_max,\nmode='lines',\nline={'width': 0},\nlegendgroup=title,\nshowlegend=False,\nhoverinfo='skip'),\nrow,\ncol))\nz_order[0].append((go.Scatter(x=x,\ny=y_min,\nmode='lines',\nline={'width': 0},\nfillcolor=alpha_colors[exp_idx + color_offset[mode]],\nfill='tonexty',\nlegendgroup=title,\nshowlegend=False,\nhoverinfo='skip'),\nrow,\ncol))\nelse:\n# Some kind of image or matrix. Not implemented yet.\npass\nfor z in sorted(list(z_order.keys())):\nplts = z_order[z]\nfor plt, row, col in plts:\nfig.add_trace(plt, row=row + 1, col=col + 1)\n# If inside a jupyter notebook then force the height based on number of rows\nif in_notebook():\nfig.update_layout(height=280 * n_rows)\nreturn FigureFE.from_figure(fig)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.visualize_logs", "title": "<code>visualize_logs</code>", "text": "<p>A function which will save or display experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>save_path</code> <code>str</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of metric keys (None whitelists all keys).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def visualize_logs(experiments: List[Summary],\nsave_path: str = None,\nsmooth_factor: float = 0,\npretty_names: bool = False,\nignore_metrics: Optional[Set[str]] = None,\ninclude_metrics: Optional[Set[str]] = None,\nverbose: bool = True):\n\"\"\"A function which will save or display experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any metrics to ignore during plotting.\n        include_metrics: A whitelist of metric keys (None whitelists all keys).\n        verbose: Whether to print out the save location.\n    \"\"\"\nfig = plot_logs(experiments,\nsmooth_factor=smooth_factor,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics,\ninclude_metrics=include_metrics)\nfig.show(save_path=save_path, verbose=verbose, scale=5)\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html", "title": "nightly_util", "text": ""}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_apphub_source_dir_path", "title": "<code>get_apphub_source_dir_path</code>", "text": "<p>Get the absolute path to the apphub folder containing the files to be tested by the <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the corresponding apphub directory.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_apphub_source_dir_path(working_file: str) -&gt; str:\n\"\"\"Get the absolute path to the apphub folder containing the files to be tested by the `working_file`.\n    Args:\n        working_file: The absolute path to a test file.\n    Returns:\n        The absolute path to the corresponding apphub directory.\n    \"\"\"\napphub_path = get_uncle_path(\"apphub\", working_file)\nrelative_dir_path = get_relative_path(\"apphub_scripts\", working_file)\nsource_dir_path = os.path.join(apphub_path, relative_dir_path)\nreturn source_dir_path\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_relative_path", "title": "<code>get_relative_path</code>", "text": "<p>Convert an absolute path into a relative path within the parent folder.</p> <p>Parameters:</p> Name Type Description Default <code>parent_dir</code> <code>str</code> <p>A parent folder</p> required <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The relative path to the test file within the parent_dir folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> is not located within the parent_dir folder.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_relative_path(parent_dir: str, working_file: str) -&gt; str:\n\"\"\"Convert an absolute path into a relative path within the parent folder.\n    Args:\n        parent_dir: A parent folder\n        working_file: The absolute path to a test file.\n    Returns:\n        The relative path to the test file within the parent_dir folder.\n    Raises:\n        OSError: If the `working_file` is not located within the parent_dir folder.\n    \"\"\"\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nsplit = current_dir.split(\"{}/\".format(parent_dir))\nif len(split) == 1:\nraise OSError(\"This file need to be put inside {} directory\".format(parent_dir))\nreturn split[-1]\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_uncle_path", "title": "<code>get_uncle_path</code>", "text": "<p>Find the path to the uncle folder of <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uncle_dir</code> <code>str</code> <p>A target uncle folder</p> required <code>working_file</code> <code>str</code> <p>A file within the same FastEstimator repository as apphub examples.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The root path to the apphub folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> does not correspond to any of the uncle paths.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_uncle_path(uncle_dir: str, working_file: str) -&gt; str:\n\"\"\"Find the path to the uncle folder of `working_file`.\n    Args:\n        uncle_dir: A target uncle folder\n        working_file: A file within the same FastEstimator repository as apphub examples.\n    Returns:\n        The root path to the apphub folder.\n    Raises:\n        OSError: If the `working_file` does not correspond to any of the uncle paths.\n    \"\"\"\nuncle_path = None\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nwhile current_dir != \"/\":\ncurrent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\nif uncle_dir in os.listdir(current_dir):\nuncle_path = os.path.abspath(os.path.join(current_dir, uncle_dir))\nbreak\nif uncle_path is None:\nraise OSError(\"Could not find the {} directory\".format(uncle_dir))\nreturn uncle_path\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html", "title": "unittest_util", "text": ""}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.OneLayerTorchModel", "title": "<code>OneLayerTorchModel</code>", "text": "<p>         Bases: <code>torch.nn.Module</code></p> <p>Torch Model with one dense layer without activation function. * Model input shape: (3,) * Model output: (1,) * dense layer weight: [1.0, 2.0, 3.0]</p> <p>How to feed_forward this model <pre><code>model = OneLayerTorchModel()\nx = torch.tensor([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\nb = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n</code></pre></p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>class OneLayerTorchModel(torch.nn.Module):\n\"\"\"Torch Model with one dense layer without activation function.\n    * Model input shape: (3,)\n    * Model output: (1,)\n    * dense layer weight: [1.0, 2.0, 3.0]\n    How to feed_forward this model\n    ```python\n    model = OneLayerTorchModel()\n    x = torch.tensor([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\n    b = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n    ```\n    \"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__()\nself.fc1 = torch.nn.Linear(3, 1, bias=False)\nself.fc1.weight.data = torch.tensor([[1, 2, 3]], dtype=torch.float32)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = self.fc1(x)\nreturn x\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.TraceRun", "title": "<code>TraceRun</code>", "text": "<p>Class to simulate the trace calling protocol.</p> <p>This serve for testing purpose without using estimator class.</p> <p>Parameters:</p> Name Type Description Default <code>trace</code> <code>Trace</code> <p>Target trace to run.</p> required <code>batch</code> <code>Dict[str, Any]</code> <p>Batch data from pipepline.</p> required <code>prediction</code> <code>Dict[str, Any]</code> <p>Batch data from network.</p> required Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>class TraceRun:\n\"\"\"Class to simulate the trace calling protocol.\n    This serve for testing purpose without using estimator class.\n    Args:\n        trace: Target trace to run.\n        batch: Batch data from pipepline.\n        prediction: Batch data from network.\n    \"\"\"\ndef __init__(self, trace: Trace, batch: Dict[str, Any], prediction: Dict[str, Any]):\nself.trace = trace\nself.batch = batch\nself.prediction = prediction\nself.data_on_begin = None\nself.data_on_end = None\nself.data_on_epoch_begin = None\nself.data_on_epoch_end = None\nself.data_on_batch_begin = None\nself.data_on_batch_end = None\ndef run_trace(self) -&gt; None:\nsystem = sample_system_object()\nself.trace.system = system\nself.data_on_begin = Data()\nself.trace.on_begin(self.data_on_begin)\nself.data_on_epoch_begin = Data()\nself.trace.on_epoch_begin(self.data_on_epoch_begin)\nself.data_on_batch_begin = Data(self.batch)\nself.trace.on_batch_begin(self.data_on_batch_begin)\nself.data_on_batch_end = Data(ChainMap(self.prediction, self.batch))\nself.trace.on_batch_end(self.data_on_batch_end)\nself.data_on_epoch_end = Data()\nself.trace.on_epoch_end(self.data_on_epoch_end)\nself.data_on_end = Data()\nself.trace.on_end(self.data_on_end)\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.check_img_similar", "title": "<code>check_img_similar</code>", "text": "<p>Check whether img1 and img2 array are similar based on pixel to pixel comparision</p> <p>Parameters:</p> Name Type Description Default <code>img1</code> <code>np.ndarray</code> <p>Image 1</p> required <code>img2</code> <code>np.ndarray</code> <p>Image 2</p> required <code>ptol</code> <code>int</code> <p>Pixel value tolerance</p> <code>3</code> <code>ntol</code> <code>float</code> <p>Number of pixel difference tolerace rate</p> <code>0.01</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean of whether the images are similar</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def check_img_similar(img1: np.ndarray, img2: np.ndarray, ptol: int = 3, ntol: float = 0.01) -&gt; bool:\n\"\"\"Check whether img1 and img2 array are similar based on pixel to pixel comparision\n    Args:\n        img1: Image 1\n        img2: Image 2\n        ptol: Pixel value tolerance\n        ntol: Number of pixel difference tolerace rate\n    Returns:\n        Boolean of whether the images are similar\n    \"\"\"\nif img1.shape == img2.shape:\ndiff = np.abs(img1.astype(np.float32) - img2.astype(np.float32))\nn_pixel_diff = diff[diff &gt; ptol].size\nif n_pixel_diff &lt; img1.size * ntol:\nreturn True\nelse:\nreturn False\nreturn False\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.fig_to_rgb_array", "title": "<code>fig_to_rgb_array</code>", "text": "<p>Convert image in plt.Figure to numpy array</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Input figure object</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image array</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def fig_to_rgb_array(fig: Figure) -&gt; np.ndarray:\n\"\"\"Convert image in plt.Figure to numpy array\n    Args:\n        fig: Input figure object\n    Returns:\n        Image array\n    \"\"\"\np = fig.to_image(format='png')\ndecoded = cv2.imdecode(np.frombuffer(p, np.uint8), cv2.IMREAD_COLOR)\ndecoded = cv2.cvtColor(decoded, cv2.COLOR_BGR2RGB)\nreturn decoded\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.img_to_rgb_array", "title": "<code>img_to_rgb_array</code>", "text": "<p>Read png file to numpy array (RGB)</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Image path</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Image numpy array</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def img_to_rgb_array(path: str) -&gt; np.ndarray:\n\"\"\"Read png file to numpy array (RGB)\n    Args:\n        path: Image path\n    Returns:\n        Image numpy array\n    \"\"\"\nreturn np.asarray(Image.open(path).convert('RGB'))\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.is_equal", "title": "<code>is_equal</code>", "text": "<p>Check whether input objects are equal. The object type can be nested iterable (list, tuple, set, dict) and with elements such as int, float, np.ndarray, tf.Tensor, tf.Varaible, torch.Tensor</p> <p>Parameters:</p> Name Type Description Default <code>obj1</code> <code>Any</code> <p>Input object 1</p> required <code>obj2</code> <code>Any</code> <p>Input object 2</p> required <code>assert_type</code> <code>bool</code> <p>Whether to assert the same data type</p> <code>True</code> <code>assert_dtype</code> <code>bool</code> <p>Whether to assert the same dtype in case of nd.array, tf.Tensor, torch.Tensor</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean of whether those two object are equal</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def is_equal(obj1: Any, obj2: Any, assert_type: bool = True, assert_dtype: bool = False) -&gt; bool:\n\"\"\"Check whether input objects are equal. The object type can be nested iterable (list, tuple, set, dict) and\n    with elements such as int, float, np.ndarray, tf.Tensor, tf.Varaible, torch.Tensor\n    Args:\n        obj1: Input object 1\n        obj2: Input object 2\n        assert_type: Whether to assert the same data type\n        assert_dtype: Whether to assert the same dtype in case of nd.array, tf.Tensor, torch.Tensor\n    Returns:\n        Boolean of whether those two object are equal\n    \"\"\"\nif assert_type and type(obj1) != type(obj2):\nreturn False\nif type(obj1) in [list, set, tuple]:\nif len(obj1) != len(obj2):\nreturn False\nfor iter1, iter2 in zip(obj1, obj2):\nif not is_equal(iter1, iter2):\nreturn False\nreturn True\nelif type(obj1) == dict:\nif len(obj1) != len(obj2):\nreturn False\nif obj1.keys() != obj2.keys():\nreturn False\nfor value1, value2 in zip(obj1.values(), obj2.values()):\nif not is_equal(value1, value2):\nreturn False\nreturn True\nelif type(obj1) == np.ndarray:\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nreturn np.array_equal(obj1, obj2)\nelif tf.is_tensor(obj1):\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nobj1 = obj1.numpy()\nobj2 = obj2.numpy()\nreturn np.array_equal(obj1, obj2)\nelif isinstance(obj1, torch.Tensor):\nif assert_dtype and obj1.dtype != obj2.dtype:\nreturn False\nreturn torch.equal(obj1, obj2)\nelse:\nreturn obj1 == obj2\n</code></pre>"}, {"location": "fastestimator/test/unittest_util.html#fastestimator.fastestimator.test.unittest_util.one_layer_tf_model", "title": "<code>one_layer_tf_model</code>", "text": "<p>Tensorflow Model with one dense layer without activation function. * Model input shape: (3,) * Model output: (1,) * dense layer weight: [1.0, 2.0, 3.0]</p> <p>How to feed_forward this model <pre><code>model = one_layer_tf_model()\nx = tf.constant([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\nb = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n</code></pre></p> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>tf.keras.Model: The model</p> Source code in <code>fastestimator\\fastestimator\\test\\unittest_util.py</code> <pre><code>def one_layer_tf_model() -&gt; tf.keras.Model:\n\"\"\"Tensorflow Model with one dense layer without activation function.\n    * Model input shape: (3,)\n    * Model output: (1,)\n    * dense layer weight: [1.0, 2.0, 3.0]\n    How to feed_forward this model\n    ```python\n    model = one_layer_tf_model()\n    x = tf.constant([[1.0, 1.0, 1.0], [1.0, -1.0, -0.5]])\n    b = fe.backend.feed_forward(model, x) # [[6.0], [-2.5]]\n    ```\n    Returns:\n        tf.keras.Model: The model\n    \"\"\"\ninp = tf.keras.layers.Input([3])\nx = tf.keras.layers.Dense(units=1, use_bias=False)(inp)\nmodel = tf.keras.models.Model(inputs=inp, outputs=x)\nmodel.layers[1].set_weights([np.array([[1.0], [2.0], [3.0]])])\nreturn model\n</code></pre>"}, {"location": "fastestimator/trace/trace.html", "title": "trace", "text": ""}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.EvalEssential", "title": "<code>EvalEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during evaluation.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Any keys which should be collected over the course of an eval epoch.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass EvalEssential(Trace):\n\"\"\"A trace to collect important information during evaluation.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Any keys which should be collected over the course of an eval epoch.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(mode=\"eval\", inputs=monitor_names, outputs=[\"steps/sec\"])\nself.step_start = time.perf_counter()\nself.eval_results = defaultdict(lambda: defaultdict(list))\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.eval_results = defaultdict(lambda: defaultdict(list))\nself.eval_step = 0\nself.elapsed_step = 0\nself.step_start = time.perf_counter()\ndef on_batch_begin(self, data: Data) -&gt; None:\nself.eval_step += 1\ndef on_batch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nself.eval_results[key][self.system.ds_id].append(data[key])\nif self.system.mode == \"eval\" and self.eval_step in self.system.eval_log_steps:\nif self.eval_step &gt; 1:\nelapsed_time = time.perf_counter() - self.step_start\nelapsed_step = self.eval_step - self.elapsed_step\ndata.write_with_log(\"steps/sec\", round(elapsed_step / elapsed_time, 2))\nself.elapsed_step = self.eval_step\nself.step_start = time.perf_counter()\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key, ds_vals in self.eval_results.items():\nfor ds_id, vals in ds_vals.items():\nif ds_id != '':\nd = DSData(ds_id, data)\nd.write_with_log(key, np.mean(np.array(vals), axis=0))\ndata.write_with_log(key, np.mean(np.array([e for x in ds_vals.values() for e in x]), axis=0))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Logger", "title": "<code>Logger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace that prints log messages.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass Logger(Trace):\n\"\"\"A Trace that prints log messages.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    \"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__(inputs=\"*\")\ndef on_begin(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nstart_step = 1 if not self.system.global_step else self.system.global_step\nself._print_message(\"FastEstimator-Start: step: {}; \".format(start_step), data)\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.system.mode == 'eval':\nself.eval_step = 0\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.system.log_steps and (self.system.global_step % self.system.log_steps\n== 0 or self.system.global_step == 1):\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data)\nif self.system.mode == \"eval\":\nself.eval_step += 1\nif self.eval_step in self.system.eval_log_steps:\nself._print_message(\"Eval Progress: {}/{}; \".format(self.eval_step, self.system.eval_log_steps[-1]),\ndata)\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\":\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == 'eval':\nself._print_message(\"FastEstimator-Eval: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Test: step: {}; \".format(self.system.global_step), data, True)\ndef on_end(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Finish: step: {}; \".format(self.system.global_step), data)\ndef _print_message(self, header: str, data: Data, log_epoch: bool = False) -&gt; None:\n\"\"\"Print a log message to the screen, and record the `data` into the `system` summary.\n        Args:\n            header: The prefix for the log message.\n            data: A collection of data to be recorded.\n            log_epoch: Whether epoch information should be included in the log message.\n        \"\"\"\nlog_message = header\nif log_epoch:\nlog_message += \"epoch: {}; \".format(self.system.epoch_idx)\nself.system.write_summary('epoch', self.system.epoch_idx)\ndeferred = []\nfor key, val in humansorted(data.read_logs().items(), key=lambda x: x[0]):\nif isinstance(val, ValWithError):\nlog_message += \"{}: {}; \".format(key, str(val))\nelse:\nval = to_number(val)\nif val.size &gt; 1:\ndeferred.append(\"\\n{}:\\n{};\".format(key, np.array2string(val, separator=',')))\nelse:\nlog_message += \"{}: {}; \".format(key, str(val))\nself.system.write_summary(key, val)\nlog_message = log_message.strip()\nfor elem in deferred:\nlog_message += elem\nprint(log_message)\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.PerDSTrace", "title": "<code>PerDSTrace</code>", "text": "<p>         Bases: <code>Trace</code></p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>class PerDSTrace(Trace):\ndef on_ds_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each dataset.\n        Args:\n            data: A dictionary through which traces can communicate with each other. Output here will not be logged.\n        \"\"\"\npass\ndef on_ds_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each dataset.\n        Args:\n            data: A dictionary through which traces can communicate with each other. Output here will be accumulated\n                across all available datasets and then logged during on_epoch_end.\n        \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.PerDSTrace.on_ds_begin", "title": "<code>on_ds_begin</code>", "text": "<p>Runs at the beginning of each dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other. Output here will not be logged.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_ds_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each dataset.\n    Args:\n        data: A dictionary through which traces can communicate with each other. Output here will not be logged.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.PerDSTrace.on_ds_end", "title": "<code>on_ds_end</code>", "text": "<p>Runs at the beginning of each dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other. Output here will be accumulated across all available datasets and then logged during on_epoch_end.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_ds_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each dataset.\n    Args:\n        data: A dictionary through which traces can communicate with each other. Output here will be accumulated\n            across all available datasets and then logged during on_epoch_end.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.TestEssential", "title": "<code>TestEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during evaluation.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Any keys which should be collected over the course of an test epoch.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass TestEssential(Trace):\n\"\"\"A trace to collect important information during evaluation.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Any keys which should be collected over the course of an test epoch.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(mode=\"test\", inputs=monitor_names)\nself.test_results = defaultdict(lambda: defaultdict(list))\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.test_results = defaultdict(lambda: defaultdict(list))\ndef on_batch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nself.test_results[key][self.system.ds_id].append(data[key])\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key, ds_vals in self.test_results.items():\nfor ds_id, vals in ds_vals.items():\nif ds_id != '':\nd = DSData(ds_id, data)\nd.write_with_log(key, np.mean(np.array(vals), axis=0))\ndata.write_with_log(key, np.mean(np.array([e for x in ds_vals.values() for e in x]), axis=0))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace", "title": "<code>Trace</code>", "text": "<p>Trace controls the training loop. Users can use the <code>Trace</code> base class to customize their own functionality.</p> <p>Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are also given a pointer to the current <code>System</code> instance which allows access to more information as well as giving the ability to modify or even cancel training. The order of function invocations is as follows:</p> <pre><code>        Training:                                       Testing:\n\n    on_begin                                            on_begin\n        |                                                   |\n    on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n        |                          |                        |                         |\n    on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n        |                       |  |                        |                      |  |\n    on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n        |                          ^                        |                         |\n    on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n        |                          |                        |\n    on_epoch_begin (eval)          |                    on_end\n        |                          ^\n    on_batch_begin (eval) &lt;----&lt;   |\n        |                      |   |\n    on_batch_end (eval) &gt;-----^    |\n        |                          |\n    on_epoch_end (eval) &gt;----------^\n        |\n    on_end\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to read from the state dictionary as inputs.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to write into the system buffer.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass Trace:\n\"\"\"Trace controls the training loop. Users can use the `Trace` base class to customize their own functionality.\n    Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are\n    also given a pointer to the current `System` instance which allows access to more information as well as giving the\n    ability to modify or even cancel training. The order of function invocations is as follows:\n    ``` plot\n            Training:                                       Testing:\n        on_begin                                            on_begin\n            |                                                   |\n        on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n            |                          |                        |                         |\n        on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n            |                       |  |                        |                      |  |\n        on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n            |                          ^                        |                         |\n        on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n            |                          |                        |\n        on_epoch_begin (eval)          |                    on_end\n            |                          ^\n        on_batch_begin (eval) &lt;----&lt;   |\n            |                      |   |\n        on_batch_end (eval) &gt;-----^    |\n            |                          |\n        on_epoch_end (eval) &gt;----------^\n            |\n        on_end\n    ```\n    Args:\n        inputs: A set of keys that this trace intends to read from the state dictionary as inputs.\n        outputs: A set of keys that this trace intends to write into the system buffer.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\nsystem: System\ninputs: List[str]\noutputs: List[str]\nmode: Set[str]\nds_id: Set[str]\n# You can put keys in here to have them automatically added to EvalEssential without the user having to manually add\n# them to the Estimator monitor_names. See BestModelSaver for an example.\nfe_monitor_names: Set[str]\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = check_io_names(to_list(inputs))\nself.outputs = check_io_names(to_list(outputs))\nself.mode = parse_modes(to_set(mode))\nself.ds_id = check_ds_id(to_set(ds_id))\nself.fe_monitor_names = set()  # The use-case here is rare enough that we don't want to add this to the init sig\ndef on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n        Args:\n            data: The current batch and prediction data, as well as any information written by prior `Traces`.\n        \"\"\"\npass\ndef on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef get_outputs(self, ds_ids: Union[None, str, List[str]]) -&gt; List[str]:\n\"\"\"What outputs will be generated by this trace.\n        You can ignore this unless you are designing a new trace that has special interactions between its outputs and\n        particular dataset ids.\n        Args:\n            ds_ids: The ds_ids under which this trace will execute.\n        Returns:\n            The outputs of this trace.\n        \"\"\"\nreturn list(self.outputs)\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.get_outputs", "title": "<code>get_outputs</code>", "text": "<p>What outputs will be generated by this trace.</p> <p>You can ignore this unless you are designing a new trace that has special interactions between its outputs and particular dataset ids.</p> <p>Parameters:</p> Name Type Description Default <code>ds_ids</code> <code>Union[None, str, List[str]]</code> <p>The ds_ids under which this trace will execute.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>The outputs of this trace.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def get_outputs(self, ds_ids: Union[None, str, List[str]]) -&gt; List[str]:\n\"\"\"What outputs will be generated by this trace.\n    You can ignore this unless you are designing a new trace that has special interactions between its outputs and\n    particular dataset ids.\n    Args:\n        ds_ids: The ds_ids under which this trace will execute.\n    Returns:\n        The outputs of this trace.\n    \"\"\"\nreturn list(self.outputs)\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Runs at the beginning of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_end", "title": "<code>on_batch_end</code>", "text": "<p>Runs at the end of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>The current batch and prediction data, as well as any information written by prior <code>Traces</code>.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n    Args:\n        data: The current batch and prediction data, as well as any information written by prior `Traces`.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_begin", "title": "<code>on_begin</code>", "text": "<p>Runs once at the beginning of training or testing.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_end", "title": "<code>on_end</code>", "text": "<p>Runs once at the end training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Runs at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_end", "title": "<code>on_epoch_end</code>", "text": "<p>Runs at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.TrainEssential", "title": "<code>TrainEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during training.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Which keys from the data dictionary to monitor during training.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>@traceable()\nclass TrainEssential(Trace):\n\"\"\"A trace to collect important information during training.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Which keys from the data dictionary to monitor during training.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(inputs=monitor_names, mode=\"train\", outputs=[\"steps/sec\", \"epoch_time\", \"total_time\"])\nself.elapse_times = []\nself.train_start = None\nself.epoch_start = None\nself.step_start = None\ndef on_begin(self, data: Data) -&gt; None:\nself.train_start = time.perf_counter()\ndata.write_with_log(\"num_device\", self.system.num_devices)\ndata.write_with_log(\"logging_interval\", self.system.log_steps)\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.epoch_start = time.perf_counter()\nself.step_start = time.perf_counter()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.log_steps and (self.system.global_step % self.system.log_steps == 0\nor self.system.global_step == 1):\nif self.system.ds_id != '':\ndata = DSData(self.system.ds_id, data)\nfor key in self.inputs:\nif key in data:\ndata.write_with_log(key, data[key])\nif self.system.global_step &gt; 1:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"steps/sec\", round(self.system.log_steps / np.sum(self.elapse_times), 2))\nself.elapse_times = []\nself.step_start = time.perf_counter()\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"epoch_time\", \"{} sec\".format(round(time.perf_counter() - self.epoch_start, 2)))\ndef on_end(self, data: Data) -&gt; None:\nself.system.mode = 'train'  # Set mode to 'train' for better log visualization\ndata.write_with_log(\"total_time\", \"{} sec\".format(round(time.perf_counter() - self.train_start, 2)))\nfor model in self.system.network.models:\nif hasattr(model, \"current_optimizer\"):\ndata.write_with_log(model.model_name + \"_lr\", get_lr(model))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.sort_traces", "title": "<code>sort_traces</code>", "text": "<p>Sort traces to attempt to resolve any dependency issues.</p> <p>This is essentially a topological sort, but it doesn't seem worthwhile to convert the data into a graph representation in order to get the slightly better asymptotic runtime complexity.</p> <p>Parameters:</p> Name Type Description Default <code>traces</code> <code>List[Trace]</code> <p>A list of traces (not inside schedulers) to be sorted.</p> required <code>ds_ids</code> <code>List[str]</code> <p>The ds_ids currently available to the traces.</p> required <code>available_outputs</code> <code>Union[None, str, Set[str]]</code> <p>What output keys are already available for the traces to use. If None are provided, the sorting algorithm will assume that any keys not generated by traces are being provided by the system. This results in a less rigorous sorting.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Trace]</code> <p>The sorted list of <code>traces</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If Traces have circular dependencies or require input keys which are not available.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def sort_traces(traces: List[Trace], ds_ids: List[str], available_outputs: Union[None, str,\nSet[str]] = None) -&gt; List[Trace]:\n\"\"\"Sort traces to attempt to resolve any dependency issues.\n    This is essentially a topological sort, but it doesn't seem worthwhile to convert the data into a graph\n    representation in order to get the slightly better asymptotic runtime complexity.\n    Args:\n        traces: A list of traces (not inside schedulers) to be sorted.\n        ds_ids: The ds_ids currently available to the traces.\n        available_outputs: What output keys are already available for the traces to use. If None are provided, the\n            sorting algorithm will assume that any keys not generated by traces are being provided by the system.\n            This results in a less rigorous sorting.\n    Returns:\n        The sorted list of `traces`.\n    Raises:\n        AssertionError: If Traces have circular dependencies or require input keys which are not available.\n    \"\"\"\nsorted_traces = []\ntrace_outputs = {output for trace in traces for output in trace.get_outputs(ds_ids=ds_ids)}\nif available_outputs is None:\n# Assume that anything not generated by a Trace is provided by the system\navailable_outputs = {inp for trace in traces for inp in trace.inputs} - trace_outputs\nweak_sort = True\nelse:\navailable_outputs = to_set(available_outputs)\nweak_sort = False\nend_traces = deque()\nintermediate_traces = deque()\nintermediate_outputs = set()\ntrace_deque = deque(traces)\nwhile trace_deque:\ntrace = trace_deque.popleft()\nins = set(trace.inputs)\nouts = set(trace.get_outputs(ds_ids=ds_ids))\nif not ins or isinstance(trace, (TrainEssential, EvalEssential, TestEssential)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelif \"*\" in ins:\nif outs:\nend_traces.appendleft(trace)\nelse:\nend_traces.append(trace)\nelif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelse:\nintermediate_traces.append(trace)\nintermediate_outputs |= outs\nalready_seen = set()\nwhile intermediate_traces:\ntrace = intermediate_traces.popleft()\nins = set(trace.inputs)\nouts = set(trace.get_outputs(ds_ids=ds_ids))\nalready_seen.add(trace)\nif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nalready_seen.clear()\nelif ins &lt;= (available_outputs | intermediate_outputs):\nintermediate_traces.append(trace)\nelse:\nraise AssertionError(\"The {} trace has unsatisfiable inputs: {}\".format(\ntype(trace).__name__, \", \".join(ins - (available_outputs | intermediate_outputs))))\nif intermediate_traces and len(already_seen) == len(intermediate_traces):\nraise AssertionError(\"Dependency cycle detected amongst traces: {}\".format(\", \".join(\n[type(tr).__name__ for tr in already_seen])))\nsorted_traces.extend(list(end_traces))\nreturn sorted_traces\n</code></pre>"}, {"location": "fastestimator/trace/adapt/early_stopping.html", "title": "early_stopping", "text": ""}, {"location": "fastestimator/trace/adapt/early_stopping.html#fastestimator.fastestimator.trace.adapt.early_stopping.EarlyStopping", "title": "<code>EarlyStopping</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Stop training when a monitored quantity has stopped improving.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>str</code> <p>Quantity to be monitored.</p> <code>'loss'</code> <code>min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement.</p> <code>0.0</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped.</p> <code>0</code> <code>compare</code> <code>str</code> <p>One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in <code>max</code> mode it will stop when the quantity monitored has stopped increasing.</p> <code>'min'</code> <code>baseline</code> <code>Optional[float]</code> <p>Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.</p> <code>None</code> <code>mode</code> <code>str</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>compare</code> is an invalid value or more than one <code>monitor</code> is provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\early_stopping.py</code> <pre><code>@traceable()\nclass EarlyStopping(Trace):\n\"\"\"Stop training when a monitored quantity has stopped improving.\n    Args:\n        monitor: Quantity to be monitored.\n        min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an\n            absolute change of less than min_delta will count as no improvement.\n        patience: Number of epochs with no improvement after which training will be stopped.\n        compare: One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored\n            has stopped decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing.\n        baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't\n            show improvement over the baseline.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Raises:\n        ValueError: If `compare` is an invalid value or more than one `monitor` is provided.\n    \"\"\"\ndef __init__(self,\nmonitor: str = \"loss\",\nmin_delta: float = 0.0,\npatience: int = 0,\ncompare: str = 'min',\nbaseline: Optional[float] = None,\nmode: str = 'eval') -&gt; None:\nsuper().__init__(inputs=monitor, mode=mode)\nif len(self.inputs) != 1:\nraise ValueError(\"EarlyStopping supports only one monitor key\")\nif compare not in ['min', 'max']:\nraise ValueError(\"compare_mode can only be `min` or `max`\")\nself.monitored_key = monitor\nself.fe_monitor_names.add(monitor)\nself.min_delta = abs(min_delta)\nself.wait = 0\nself.best = 0\nself.patience = patience\nself.baseline = baseline\nif compare == 'min':\nself.monitor_op = np.less\nself.min_delta *= -1\nelse:\nself.monitor_op = np.greater\ndef on_begin(self, data: Data) -&gt; None:\nself.wait = 0\nif self.baseline is not None:\nself.best = self.baseline\nelse:\nself.best = np.Inf if self.monitor_op == np.less else -np.Inf\ndef on_epoch_end(self, data: Data) -&gt; None:\ncurrent = data[self.monitored_key]\nif current is None:\nreturn\nif self.monitor_op(current - self.min_delta, self.best):\nself.best = current\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nself.system.stop_training = True\nprint(\"FastEstimator-EarlyStopping: '{}' triggered an early stop. Its best value was {} at epoch {}\".\nformat(self.monitored_key, self.best, self.system.epoch_idx - self.wait))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/lr_scheduler.html", "title": "lr_scheduler", "text": ""}, {"location": "fastestimator/trace/adapt/lr_scheduler.html#fastestimator.fastestimator.trace.adapt.lr_scheduler.LRScheduler", "title": "<code>LRScheduler</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Learning rate scheduler trace that changes the learning rate while training.</p> <p>This class requires an input function which takes either 'epoch' or 'step' as input: <pre><code>s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on step\ns = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>lr_fn</code> <code>Union[str, Callable[[int], float]]</code> <p>A lr scheduling function that takes either 'epoch' or 'step' as input, or the string 'arc'.</p> required <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>lr_fn</code> is not configured properly.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\lr_scheduler.py</code> <pre><code>@traceable()\nclass LRScheduler(Trace):\n\"\"\"Learning rate scheduler trace that changes the learning rate while training.\n    This class requires an input function which takes either 'epoch' or 'step' as input:\n    ```python\n    s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on step\n    s = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n    ```\n    Args:\n        model: A model instance compiled with fe.build.\n        lr_fn: A lr scheduling function that takes either 'epoch' or 'step' as input, or the string 'arc'.\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    Raises:\n        AssertionError: If the `lr_fn` is not configured properly.\n    \"\"\"\nsystem: System\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nlr_fn: Union[str, Callable[[int], float]],\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.model = model\nself.lr_fn = ARC() if lr_fn == \"arc\" else lr_fn\nassert hasattr(self.lr_fn, \"__call__\") or isinstance(self.lr_fn, ARC), \"lr_fn must be either a function or ARC\"\nif isinstance(self.lr_fn, ARC):\nself.schedule_mode = \"epoch\"\nelse:\narg = list(inspect.signature(lr_fn).parameters.keys())\nassert len(arg) == 1 and arg[0] in {\"step\", \"epoch\"}, \"the lr_fn input arg must be either 'step' or 'epoch'\"\nself.schedule_mode = arg[0]\nsuper().__init__(outputs=self.model.model_name + \"_lr\", ds_id=ds_id)\ndef on_begin(self, data: Data) -&gt; None:\nif isinstance(self.lr_fn, ARC):\nassert len(self.model.loss_name) == 1, \"arc can only work with single model loss\"\nself.lr_fn.use_eval_loss = \"eval\" in self.system.pipeline.data\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.schedule_mode == \"epoch\":\nif isinstance(self.lr_fn, ARC):\nif self.system.epoch_idx &gt; 1 and (self.system.epoch_idx % self.lr_fn.frequency == 1\nor self.lr_fn.frequency == 1):\nmultiplier = self.lr_fn.predict_next_multiplier()\nnew_lr = np.float32(get_lr(model=self.model) * multiplier)\nset_lr(self.model, new_lr)\nprint(\"FastEstimator-ARC: Multiplying LR by {}\".format(multiplier))\nelse:\nnew_lr = np.float32(self.lr_fn(self.system.epoch_idx))\nset_lr(self.model, new_lr)\ndef on_batch_begin(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.schedule_mode == \"step\":\nnew_lr = np.float32(self.lr_fn(self.system.global_step))\nset_lr(self.model, new_lr)\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and isinstance(self.lr_fn, ARC):\nself.lr_fn.accumulate_single_train_loss(data[min(self.model.loss_name)].numpy())\nif self.system.mode == \"train\" and self.system.log_steps and (self.system.global_step % self.system.log_steps\n== 0 or self.system.global_step == 1):\ncurrent_lr = np.float32(get_lr(self.model))\ndata.write_with_log(self.outputs[0], current_lr)\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"eval\" and isinstance(self.lr_fn, ARC):\nself.lr_fn.accumulate_single_eval_loss(data[min(self.model.loss_name)])\nif self.system.epoch_idx % self.lr_fn.frequency == 0:\nself.lr_fn.gather_multiple_eval_losses()\nif self.system.mode == \"train\" and isinstance(self.lr_fn,\nARC) and self.system.epoch_idx % self.lr_fn.frequency == 0:\nself.lr_fn.accumulate_all_lrs(get_lr(model=self.model))\nself.lr_fn.gather_multiple_train_losses()\n</code></pre>"}, {"location": "fastestimator/trace/adapt/pbm_calibrator.html", "title": "pbm_calibrator", "text": ""}, {"location": "fastestimator/trace/adapt/pbm_calibrator.html#fastestimator.fastestimator.trace.adapt.pbm_calibrator.PBMCalibrator", "title": "<code>PBMCalibrator</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to generate a PlattBinnerMarginalCalibrator given a set of predictions.</p> <p>Unlike many common calibration error correction algorithms, this one has actual theoretical bounds on the quality of its output: https://arxiv.org/pdf/1909.10155v1.pdf. This trace is commonly used together with the Calibrate NumpyOp for postprocessing. This trace will collect data from whichever <code>mode</code> it is set to run on in order to perform empirical probability calibration. The calibrated predictions will be output on epoch end. The trained calibration function will also be saved if <code>save_path</code> is provided.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>Optional[str]</code> <p>What to call the output from this trace. If None, the default will be '_calibrated'. <code>None</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the calibrator generated by this Trace. If None, then no saving will be performed.</p> <code>None</code> <code>save_if_key</code> <code>Optional[str]</code> <p>Name of a key to control whether to save the calibrator. For example \"since_best_acc\". If provided, then the calibrator will only be saved when the save_if_key value is zero.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'save_if_key' is provided but no 'save_path' is given.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\pbm_calibrator.py</code> <pre><code>@traceable()\nclass PBMCalibrator(Trace):\n\"\"\"A trace to generate a PlattBinnerMarginalCalibrator given a set of predictions.\n    Unlike many common calibration error correction algorithms, this one has actual theoretical bounds on the quality\n    of its output: https://arxiv.org/pdf/1909.10155v1.pdf. This trace is commonly used together with the Calibrate\n    NumpyOp for postprocessing. This trace will collect data from whichever `mode` it is set to run on in order to\n    perform empirical probability calibration. The calibrated predictions will be output on epoch end. The trained\n    calibration function will also be saved if `save_path` is provided.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: What to call the output from this trace. If None, the default will be '&lt;pred_key&gt;_calibrated'.\n        save_path: Where to save the calibrator generated by this Trace. If None, then no saving will be performed.\n        save_if_key: Name of a key to control whether to save the calibrator. For example \"since_best_acc\". If provided,\n            then the calibrator will only be saved when the save_if_key value is zero.\n    Raises:\n        ValueError: If 'save_if_key' is provided but no 'save_path' is given.\n    \"\"\"\nsystem: System\ndef __init__(self,\ntrue_key: str,\npred_key: str,\noutput_name: Optional[str] = None,\nsave_path: Optional[str] = None,\nsave_if_key: Optional[str] = None,\nmode: Union[str, Set[str]] = \"eval\",\nds_id: Union[None, str, Iterable[str]] = None) -&gt; None:\nif output_name is None:\noutput_name = f\"{pred_key}_calibrated\"\nif save_if_key is not None and save_path is None:\nraise ValueError(\"If 'save_if_key' is provided, then a 'save_path' must also be provided.\")\nsuper().__init__(inputs=[true_key, pred_key] + to_list(save_if_key),\noutputs=output_name,\nmode=mode,\nds_id=ds_id)\nself.y_true = []\nself.y_pred = []\nif save_path is not None:\nsave_path = os.path.abspath(os.path.normpath(save_path))\nself.save_path = save_path\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\n@property\ndef save_key(self) -&gt; Optional[str]:\nif len(self.inputs) == 3:\nreturn self.inputs[2]\nreturn None\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nassert y_pred.shape[0] == y_true.shape[0]\nself.y_true.extend(y_true)\nself.y_pred.extend(y_pred)\ndef on_epoch_end(self, data: Data) -&gt; None:\nself.y_true = np.squeeze(np.stack(self.y_true))\nself.y_pred = np.stack(self.y_pred)\ncalibrator = cal.PlattBinnerMarginalCalibrator(num_calibration=len(self.y_true), num_bins=10)\ncalibrator.train_calibration(probs=self.y_pred, labels=self.y_true)\nif self.save_path:\nif not self.save_key or (self.save_key and to_number(data[self.save_key]) == 0):\nwith open(self.save_path, 'wb') as f:\ndill.dump(calibrator.calibrate, file=f)\nprint(f\"FastEstimator-PBMCalibrator: Calibrator written to {self.save_path}\")\ndata.write_without_log(self.outputs[0], calibrator.calibrate(self.y_pred))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html", "title": "reduce_lr_on_plateau", "text": ""}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html#fastestimator.fastestimator.trace.adapt.reduce_lr_on_plateau.ReduceLROnPlateau", "title": "<code>ReduceLROnPlateau</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Reduce learning rate based on evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>metric</code> <code>Optional[str]</code> <p>The metric name to be monitored. If None, the model's validation loss will be used as the metric.</p> <code>None</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait before reducing LR again.</p> <code>10</code> <code>factor</code> <code>float</code> <p>Reduce factor for the learning rate.</p> <code>0.1</code> <code>best_mode</code> <code>str</code> <p>Higher is better (\"max\") or lower is better (\"min\").</p> <code>'min'</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the loss cannot be inferred from the <code>model</code> and a <code>metric</code> was not provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\reduce_lr_on_plateau.py</code> <pre><code>@traceable()\nclass ReduceLROnPlateau(Trace):\n\"\"\"Reduce learning rate based on evaluation results.\n    Args:\n        model: A model instance compiled with fe.build.\n        metric: The metric name to be monitored. If None, the model's validation loss will be used as the metric.\n        patience: Number of epochs to wait before reducing LR again.\n        factor: Reduce factor for the learning rate.\n        best_mode: Higher is better (\"max\") or lower is better (\"min\").\n        min_lr: Minimum learning rate.\n    Raises:\n        AssertionError: If the loss cannot be inferred from the `model` and a `metric` was not provided.\n    \"\"\"\nsystem: System\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nmetric: Optional[str] = None,\npatience: int = 10,\nfactor: float = 0.1,\nbest_mode: str = \"min\",\nmin_lr: float = 1e-6) -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \\\n                \"ReduceLROnPlateau cannot infer model loss name. Provide a metric or use the model in an UpdateOp.\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\", inputs=metric, outputs=model.model_name + \"_lr\")\nself.fe_monitor_names.add(metric)\nself.model = model\nself.patience = patience\nself.factor = factor\nself.best_mode = best_mode\nself.min_lr = min_lr\nself.wait = 0\nif self.best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"best_mode must be either 'min' or 'max'\")\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.inputs[0]], self.best):\nself.best = data[self.inputs[0]]\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nnew_lr = max(self.min_lr, np.float32(self.factor * get_lr(self.model)))\nset_lr(self.model, new_lr)\nself.wait = 0\ndata.write_with_log(self.outputs[0], new_lr)\nprint(\"FastEstimator-ReduceLROnPlateau: learning rate reduced to {}\".format(new_lr))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/terminate_on_nan.html", "title": "terminate_on_nan", "text": ""}, {"location": "fastestimator/trace/adapt/terminate_on_nan.html#fastestimator.fastestimator.trace.adapt.terminate_on_nan.TerminateOnNaN", "title": "<code>TerminateOnNaN</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>End Training if a NaN value is detected.</p> <p>By default (monitor_names=None) it will monitor all loss values at the end of each batch. If one or more inputs are specified, it will only monitor those values. Inputs may be loss keys and/or the keys corresponding to the outputs of other traces (ex. accuracy).</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Union[None, str, Iterable[str]]</code> <p>key(s) to monitor for NaN values. If None, all loss values will be monitored. \"*\" will monitor all trace output keys and losses.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\terminate_on_nan.py</code> <pre><code>@traceable()\nclass TerminateOnNaN(Trace):\n\"\"\"End Training if a NaN value is detected.\n    By default (monitor_names=None) it will monitor all loss values at the end of each batch. If one or more inputs are\n    specified, it will only monitor those values. Inputs may be loss keys and/or the keys corresponding to the outputs\n    of other traces (ex. accuracy).\n    Args:\n        monitor_names: key(s) to monitor for NaN values. If None, all loss values will be monitored. \"*\" will monitor\n            all trace output keys and losses.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(\nself,\nmonitor_names: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Set[str]] = None,\nds_id: Union[None, str, Iterable[str]] = None,\n) -&gt; None:\nsuper().__init__(inputs=monitor_names, mode=mode, ds_id=ds_id)\nself.monitor_keys = {}\nself.in_list = True\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif not self.inputs:\nself.monitor_keys = self.system.network.get_loss_keys()\nelif \"*\" in self.inputs:\nself.monitor_keys = self.system.network.get_loss_keys()\nfor trace in get_current_items(self.system.traces, run_modes=self.system.mode, epoch=self.system.epoch_idx):\nself.monitor_keys.update(trace.outputs)\nelse:\nself.monitor_keys = self.inputs\ndef on_batch_end(self, data: Data) -&gt; None:\nfor key in self.monitor_keys:\nif key in data:\nif check_nan(data[key]):\nself.system.stop_training = True\nprint(\"FastEstimator-TerminateOnNaN: NaN Detected in: {}\".format(key))\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.monitor_keys:\nif key in data:\nif check_nan(data[key]):\nself.system.stop_training = True\nprint(\"FastEstimator-TerminateOnNaN: NaN Detected in: {}\".format(key))\n</code></pre>"}, {"location": "fastestimator/trace/io/best_model_saver.html", "title": "best_model_saver", "text": ""}, {"location": "fastestimator/trace/io/best_model_saver.html#fastestimator.fastestimator.trace.io.best_model_saver.BestModelSaver", "title": "<code>BestModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save the weights of best model based on a given evaluation metric.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the model.</p> required <code>metric</code> <code>Optional[str]</code> <p>Eval metric name to monitor. If None, the model's loss will be used.</p> <code>None</code> <code>save_best_mode</code> <code>str</code> <p>Can be 'min' or 'max'.</p> <code>'min'</code> <code>load_best_final</code> <code>bool</code> <p>Whether to automatically reload the best model (if available) after training.</p> <code>False</code> <code>save_architecture</code> <code>bool</code> <p>Whether to save the full model architecture in addition to the model weights. This option is only available for TensorFlow models at present, and will generate a folder containing several files. The model can then be re-instantiated even without access to the original code by calling: tf.keras.models.load_model(). <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a <code>metric</code> is not provided and it cannot be inferred from the <code>model</code>.</p> <code>ValueError</code> <p>If <code>save_best_mode</code> is an unacceptable string, or <code>save_architecture</code> is used with a PyTorch model.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\best_model_saver.py</code> <pre><code>@traceable()\nclass BestModelSaver(Trace):\n\"\"\"Save the weights of best model based on a given evaluation metric.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the model.\n        metric: Eval metric name to monitor. If None, the model's loss will be used.\n        save_best_mode: Can be 'min' or 'max'.\n        load_best_final: Whether to automatically reload the best model (if available) after training.\n        save_architecture: Whether to save the full model architecture in addition to the model weights. This option is\n            only available for TensorFlow models at present, and will generate a folder containing several files. The\n            model can then be re-instantiated even without access to the original code by calling:\n            tf.keras.models.load_model(&lt;path to model folder&gt;).\n    Raises:\n        AssertionError: If a `metric` is not provided and it cannot be inferred from the `model`.\n        ValueError: If `save_best_mode` is an unacceptable string, or `save_architecture` is used with a PyTorch model.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmetric: Optional[str] = None,\nsave_best_mode: str = \"min\",\nload_best_final: bool = False,\nsave_architecture: bool = False) -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \\\n                \"BestModelSaver cannot infer model loss name. Provide a metric or use the model in an UpdateOp.\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\",\ninputs=metric,\noutputs=[\"since_best_{}\".format(metric), \"{}_{}\".format(save_best_mode, metric)])\nself.fe_monitor_names.add(metric)\nself.model = model\nself.model_name = \"{}_best_{}\".format(self.model.model_name, self.metric)\nself.save_dir = save_dir\nself.save_best_mode = save_best_mode\nself.load_best_final = load_best_final\nself.save_architecture = save_architecture\nif save_architecture and isinstance(model, torch.nn.Module):\nraise ValueError(\"Sorry, architecture saving is not currently enabled for PyTorch\")\nself.model_path = None\nself.since_best = 0\nif self.save_best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.save_best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"save_best_mode must be either 'min' or 'max'\")\n@property\ndef metric(self) -&gt; str:\nreturn self.inputs[0]\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.metric], self.best):\nself.best = data[self.metric]\nself.since_best = 0\nif self.save_dir:\nself.model_path = save_model(model=self.model,\nsave_dir=self.save_dir,\nmodel_name=self.model_name,\nsave_architecture=self.save_architecture)\nprint(\"FastEstimator-BestModelSaver: Saved model to {}\".format(self.model_path))\nelse:\nself.since_best += 1\ndata.write_with_log(self.outputs[0], self.since_best)\ndata.write_with_log(self.outputs[1], self.best)\ndef on_end(self, data: Data) -&gt; None:\nif self.load_best_final and self.model_path:\nprint(\"FastEstimator-BestModelSaver: Restoring model from {}\".format(self.model_path))\nload_model(self.model, self.model_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/csv_logger.html", "title": "csv_logger", "text": ""}, {"location": "fastestimator/trace/io/csv_logger.html#fastestimator.fastestimator.trace.io.csv_logger.CSVLogger", "title": "<code>CSVLogger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Log monitored quantities in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>monitor_names</code> <code>Optional[Union[List[str], str]]</code> <p>List of keys to monitor. If None then all metrics will be recorded.</p> <code>None</code> <code>instance_id_key</code> <code>Optional[str]</code> <p>A key corresponding to data instance ids. If provided, the CSV logger will record per-instance metric information into a second csv file.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\csv_logger.py</code> <pre><code>@traceable()\nclass CSVLogger(Trace):\n\"\"\"Log monitored quantities in a CSV file.\n    Args:\n        filename: Output filename.\n        monitor_names: List of keys to monitor. If None then all metrics will be recorded.\n        instance_id_key: A key corresponding to data instance ids. If provided, the CSV logger will record per-instance\n            metric information into a second csv file.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfilename: str,\nmonitor_names: Optional[Union[List[str], str]] = None,\ninstance_id_key: Optional[str] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.instance_id_key = instance_id_key\nmonitor_names = to_list(monitor_names)\ninstance_id_key = to_list(instance_id_key)\ninputs = monitor_names if monitor_names else [\"*\"]\ninputs.extend(instance_id_key)\nsuper().__init__(inputs=inputs, mode=mode)\nself.filename = filename\nself.df_agg = None  # DataFrame for aggregate metrics\nself.df_ins = None  # DataFrame for instance metrics\ndef on_begin(self, data: Data) -&gt; None:\nbase_keys = [\"instance_id\", \"mode\", \"step\", \"epoch\"] if self.instance_id_key else [\"mode\", \"step\", \"epoch\"]\nself.df_agg = pd.DataFrame(columns=base_keys)\nself.df_ins = pd.DataFrame(columns=base_keys)\ndef on_epoch_end(self, data: Data) -&gt; None:\ntmpdic = {\"mode\": self.system.mode, \"step\": self.system.global_step, \"epoch\": self.system.epoch_idx}\nif \"*\" in self.inputs:\nfor key, value in data.read_logs().items():\ntmpdic[key] = self._parse_val(value)\nif key not in self.df_agg.columns:\nself.df_agg[key] = ''\nelse:\nfor key in self.inputs:\nif key == self.instance_id_key:\ncontinue\ntmpdic[key] = self._parse_val(data[key])\nif key not in self.df_agg.columns:\nself.df_agg[key] = ''\nfor col in self.df_agg.columns:\nif col not in tmpdic.keys():\ntmpdic[col] = ''\nself.df_agg = self.df_agg.append(tmpdic, ignore_index=True)\nself._save()  # Write on epoch end so that people can see results sooner if debugging\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.instance_id_key:\nins_data = data.read_per_instance_logs()\nif ins_data:\nkeys = list((ins_data.keys() if \"*\" in self.inputs else set(self.inputs)) - {self.instance_id_key})\nids = data[self.instance_id_key]\nvals = [ins_data.get(key, data.get(key, '')) for key in keys]\nfor key in keys:\nif key not in self.df_ins.columns:\nself.df_ins[key] = ''\nrows = []\nfor sample in zip(ids, *vals):\nrow = {\"instance_id\": self._parse_val(sample[0]),\n\"mode\": self.system.mode,\n\"step\": self.system.global_step,\n\"epoch\": self.system.epoch_idx,\n**{key: self._parse_val(val) for key, val in zip(keys, sample[1:])}}\nfor col in self.df_ins.columns:\nif col not in row.keys():\nrow[col] = ''\nrows.append(row)\nself.df_ins = self.df_ins.append(rows, ignore_index=True)\nif self.system.mode == \"train\" and self.system.log_steps and (self.system.global_step % self.system.log_steps\n== 0 or self.system.global_step == 1):\ntmpdic = {\"mode\": self.system.mode, \"step\": self.system.global_step, \"epoch\": self.system.epoch_idx}\nif \"*\" in self.inputs:\nfor key, value in data.read_logs().items():\ntmpdic[key] = self._parse_val(value)\nif key not in self.df_agg.columns:\nself.df_agg[key] = ''\nelse:\nfor key in self.inputs:\nif key == self.instance_id_key:\ncontinue\ntmpdic[key] = self._parse_val(data[key])\nif key not in self.df_agg.columns:\nself.df_agg[key] = ''\nfor col in self.df_agg.columns:\nif col not in tmpdic.keys():\ntmpdic[col] = ''\nself.df_agg = self.df_agg.append(tmpdic, ignore_index=True)\ndef _save(self) -&gt; None:\n\"\"\"Write the current state to disk.\n        \"\"\"\nstack = [self.df_ins, self.df_agg]\nif self.system.mode == \"test\":\nif os.path.exists(self.filename):\ndf1 = pd.read_csv(self.filename)\nstack.insert(0, df1)\nstack = pd.concat(stack, axis=0, ignore_index=True)\nstack.to_csv(self.filename, index=False)\n@staticmethod\ndef _parse_val(val: Any) -&gt; str:\n\"\"\"Convert values into string representations.\n        Args:\n            val: A value to be printed.\n        Returns:\n            A formatted version of `val` appropriate for a csv file.\n        \"\"\"\nif isinstance(val, str):\nreturn val\nif isinstance(val, ValWithError):\nreturn str(val).replace(',', ';')\nval = to_number(val)\nif val.size &gt; 1:\nreturn np.array2string(val, separator=';')\nif val.dtype.kind in {'U', 'S'}:  # Unicode or String\n# remove the b'' from strings stored in tensors\nreturn str(val, 'utf-8')\nreturn str(val)\n</code></pre>"}, {"location": "fastestimator/trace/io/image_saver.html", "title": "image_saver", "text": ""}, {"location": "fastestimator/trace/io/image_saver.html#fastestimator.fastestimator.trace.io.image_saver.ImageSaver", "title": "<code>ImageSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that saves images to the disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be saved.</p> required <code>save_dir</code> <code>str</code> <p>The directory into which to write the images.</p> <code>os.getcwd()</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_saver.py</code> <pre><code>@traceable()\nclass ImageSaver(Trace):\n\"\"\"A trace that saves images to the disk.\n    Args:\n        inputs: Key(s) of images to be saved.\n        save_dir: The directory into which to write the images.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nsave_dir: str = os.getcwd(),\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\")) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nself.save_dir = save_dir\ndef on_epoch_end(self, data: Data) -&gt; None:\nself._save_images(data)\ndef on_end(self, data: Data) -&gt; None:\nself._save_images(data)\ndef _save_images(self, data: Data):\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nim_path = os.path.join(self.save_dir,\n\"{}_{}_epoch_{}.png\".format(key, self.system.mode, self.system.epoch_idx))\nif isinstance(imgs, Display):\nimgs.show(save_path=im_path, verbose=False)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\nelif isinstance(imgs, Summary):\nvisualize_logs([imgs], save_path=im_path, verbose=False)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\nelif isinstance(imgs, (list, tuple)) and all([isinstance(img, Summary) for img in imgs]):\nvisualize_logs(imgs, save_path=im_path, verbose=False)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\nelse:\nfor idx, img in enumerate(imgs):\nf = ImageDisplay(image=img, title=key)\nim_path = os.path.join(\nself.save_dir,\n\"{}_{}_epoch_{}_elem_{}.png\".format(key, self.system.mode, self.system.epoch_idx, idx))\nf.show(save_path=im_path, verbose=False)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\n</code></pre>"}, {"location": "fastestimator/trace/io/image_viewer.html", "title": "image_viewer", "text": ""}, {"location": "fastestimator/trace/io/image_viewer.html#fastestimator.fastestimator.trace.io.image_viewer.ImageViewer", "title": "<code>ImageViewer</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that interrupts your training in order to display images on the screen.</p> <p>This class is useful primarily for Jupyter Notebook, or for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be displayed.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>interactive</code> <code>bool</code> <p>Whether the image should be interactive. This is False by default to reduce jupyter file size.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_viewer.py</code> <pre><code>@traceable()\nclass ImageViewer(Trace):\n\"\"\"A trace that interrupts your training in order to display images on the screen.\n    This class is useful primarily for Jupyter Notebook, or for debugging purposes.\n    Args:\n        inputs: Key(s) of images to be displayed.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        interactive: Whether the image should be interactive. This is False by default to reduce jupyter file size.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\ninteractive: bool = False,\n) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nself.interactive = interactive\ndef on_epoch_end(self, data: Data) -&gt; None:\nself._display_images(data)\ndef on_end(self, data: Data) -&gt; None:\nself._display_images(data)\ndef _display_images(self, data: Data) -&gt; None:\n\"\"\"A method to render images to the screen.\n        Args:\n            data: Data possibly containing images to render.\n        \"\"\"\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nif isinstance(imgs, Display):\nimgs.show(interactive=self.interactive)\nelif isinstance(imgs, Summary):\nvisualize_logs([imgs])\nelif isinstance(imgs, (list, tuple)) and all([isinstance(img, Summary) for img in imgs]):\nvisualize_logs(imgs)\nelse:\nfor idx, img in enumerate(imgs):\nfig = ImageDisplay(image=img, title=\"{}_{}\".format(key, idx))\nfig.show(interactive=self.interactive)\n</code></pre>"}, {"location": "fastestimator/trace/io/model_saver.html", "title": "model_saver", "text": ""}, {"location": "fastestimator/trace/io/model_saver.html#fastestimator.fastestimator.trace.io.model_saver.ModelSaver", "title": "<code>ModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save model weights based on epoch frequency during training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the <code>model</code>.</p> required <code>frequency</code> <code>int</code> <p>Model saving frequency in epoch(s).</p> <code>1</code> <code>max_to_keep</code> <code>Optional[int]</code> <p>Maximum number of latest saved files to keep. If 0 or None, all models will be saved.</p> <code>None</code> <code>save_architecture</code> <code>bool</code> <p>Whether to save the full model architecture in addition to the model weights. This option is only available for TensorFlow models at present, and will generate a folder containing several files. The model can then be re-instantiated even without access to the original code by calling: tf.keras.models.load_model(). <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>max_to_keep</code> is negative, or if save_architecture is used with a PyTorch model.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\model_saver.py</code> <pre><code>@traceable()\nclass ModelSaver(Trace):\n\"\"\"Save model weights based on epoch frequency during training.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the `model`.\n        frequency: Model saving frequency in epoch(s).\n        max_to_keep: Maximum number of latest saved files to keep. If 0 or None, all models will be saved.\n        save_architecture: Whether to save the full model architecture in addition to the model weights. This option is\n            only available for TensorFlow models at present, and will generate a folder containing several files. The\n            model can then be re-instantiated even without access to the original code by calling:\n            tf.keras.models.load_model(&lt;path to model folder&gt;).\n    Raises:\n        ValueError: If `max_to_keep` is negative, or if save_architecture is used with a PyTorch model.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nfrequency: int = 1,\nmax_to_keep: Optional[int] = None,\nsave_architecture: bool = False) -&gt; None:\nsuper().__init__(mode=\"train\")\nself.model = model\nself.save_dir = save_dir\nself.frequency = frequency\nself.save_architecture = save_architecture\nif save_architecture and isinstance(model, torch.nn.Module):\nraise ValueError(\"Sorry, architecture saving is not currently enabled for PyTorch\")\nif max_to_keep is not None and max_to_keep &lt; 0:\nraise ValueError(f\"max_to_keep should be a non-negative integer, but got {max_to_keep}\")\nself.file_queue = deque([None] * (max_to_keep or 0), maxlen=max_to_keep or 0)\ndef on_epoch_end(self, data: Data) -&gt; None:\n# No model will be saved when save_dir is None, which makes smoke test easier.\nif self.save_dir and self.system.epoch_idx % self.frequency == 0:\nmodel_name = \"{}_epoch_{}\".format(self.model.model_name, self.system.epoch_idx)\nmodel_path = save_model(model=self.model,\nsave_dir=self.save_dir,\nmodel_name=model_name,\nsave_architecture=self.save_architecture)\nprint(\"FastEstimator-ModelSaver: Saved model to {}\".format(model_path))\nrm_path = self.file_queue[self.file_queue.maxlen - 1] if self.file_queue.maxlen else None\nif rm_path:\nos.remove(rm_path)\nif self.save_architecture:\nshutil.rmtree(os.path.splitext(rm_path)[0])\nprint(\"FastEstimator-ModelSaver: Removed model {} due to file number exceeding max_to_keep\".format(\nrm_path))\nself.file_queue.appendleft(model_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/restore_wizard.html", "title": "restore_wizard", "text": ""}, {"location": "fastestimator/trace/io/restore_wizard.html#fastestimator.fastestimator.trace.io.restore_wizard.RestoreWizard", "title": "<code>RestoreWizard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that can backup and load your entire training status.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory to save and load the training status.</p> required <code>frequency</code> <code>int</code> <p>Saving frequency in epoch(s).</p> <code>1</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\restore_wizard.py</code> <pre><code>@traceable()\nclass RestoreWizard(Trace):\n\"\"\"A trace that can backup and load your entire training status.\n    Args:\n        directory: Directory to save and load the training status.\n        frequency: Saving frequency in epoch(s).\n    \"\"\"\ndef __init__(self, directory: str, frequency: int = 1) -&gt; None:\nsuper().__init__(inputs=\"*\", mode=\"train\")  # inputs to cause this trace to sort to the end of the list\nself.directory = os.path.abspath(os.path.normpath(directory))\nself.frequency = frequency\n# For robust saving, we need to create 2 different directories and have a key file to switch between them\nself.dirs = [os.path.join(self.directory, 'A'), os.path.join(self.directory, 'B')]\nself.key_path = os.path.join(self.directory, 'key.txt')\nself.dir_idx = 0\ndef on_begin(self, data: Data) -&gt; None:\nif fe.fe_deterministic_seed is not None:\nraise RuntimeError(\"You cannot use RestoreWizard while in deterministic training mode since a restored\" +\n\" training can't guarantee that all prngs will be reset to exactly the same position\")\nif not self.should_restore():\nself._cleanup(self.dirs)  # Remove any partially completed checkpoints\nprint(\"FastEstimator-RestoreWizard: Backing up to {}\".format(self.directory))\nelse:\nself._load_key()\ndirectory = self.dirs[self.dir_idx]\nself.system.load_state(directory)\ndata.write_with_log(\"epoch\", self.system.epoch_idx)\nprint(\"FastEstimator-RestoreWizard: Restoring from {}, resume training\".format(directory))\nself.dir_idx = int(not self.dir_idx)  # Flip the idx so that next save goes to other dir\nself._cleanup(self.dirs[self.dir_idx])  # Clean out the other dir in case it had a partial save\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.epoch_idx % self.frequency == 0:\ndirectory = self.dirs[self.dir_idx]\nself.system.save_state(directory)\nself._write_key()\n# Everything after this is free to die without causing problems with restore\nself.dir_idx = int(not self.dir_idx)\nself._cleanup(self.dirs[self.dir_idx])\nprint(\"FastEstimator-RestoreWizard: Saved milestones to {}\".format(directory))\ndef should_restore(self) -&gt; bool:\n\"\"\"Whether a restore will be performed.\n        Returns:\n            True iff the wizard will perform a restore.\n        \"\"\"\nreturn os.path.exists(self.directory) and os.path.exists(self.key_path)\ndef _load_key(self) -&gt; None:\n\"\"\"Set the dir_idx based on the key last saved by the restore wizard.\n        Raises:\n            ValueError: If the key file has been modified.\n        \"\"\"\nwith open(self.key_path, 'r') as key_file:\nkey = key_file.readline()\nif key not in ('A', 'B'):\nraise ValueError(\"RestoreWizard encountered an invalid key file at {}. Either delete it to restart, or undo\"\n\" whatever manual changes were made to the file.\".format(self.key_path))\nself.dir_idx = 0 if key == 'A' else 1\ndef _write_key(self) -&gt; None:\n\"\"\"Generate a new key file and then atomically replace the old key file.\n        \"\"\"\nsub_dir = self.dirs[self.dir_idx]\nnew_key_path = os.path.join(sub_dir, 'key.txt')\nwith open(new_key_path, 'w') as new_key_file:\nnew_key_file.write(\"B\" if self.dir_idx else \"A\")\nos.replace(new_key_path, self.key_path)  # This operation is atomic per POSIX requirements\n@staticmethod\ndef _cleanup(paths: Union[str, List[str]]) -&gt; None:\n\"\"\"Delete stale directories if they exist.\n        Args:\n            paths: Which directories to delete.\n        \"\"\"\npaths = to_list(paths)\nfor path in paths:\nif os.path.exists(path):\nshutil.rmtree(path)\n</code></pre>"}, {"location": "fastestimator/trace/io/restore_wizard.html#fastestimator.fastestimator.trace.io.restore_wizard.RestoreWizard.should_restore", "title": "<code>should_restore</code>", "text": "<p>Whether a restore will be performed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True iff the wizard will perform a restore.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\restore_wizard.py</code> <pre><code>def should_restore(self) -&gt; bool:\n\"\"\"Whether a restore will be performed.\n    Returns:\n        True iff the wizard will perform a restore.\n    \"\"\"\nreturn os.path.exists(self.directory) and os.path.exists(self.key_path)\n</code></pre>"}, {"location": "fastestimator/trace/io/tensorboard.html", "title": "tensorboard", "text": ""}, {"location": "fastestimator/trace/io/tensorboard.html#fastestimator.fastestimator.trace.io.tensorboard.TensorBoard", "title": "<code>TensorBoard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Output data for use in TensorBoard.</p> <p>Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the --reload_multifile=true flag until their multi-writer use case is finished: https://github.com/tensorflow/tensorboard/issues/1063</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Path of the directory where the log files to be parsed by TensorBoard should be saved.</p> <code>'logs'</code> <code>update_freq</code> <code>Union[None, int, str]</code> <p>'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000, the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace mostly useless.</p> <code>100</code> <code>write_graph</code> <code>bool</code> <p>Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.</p> <code>True</code> <code>write_images</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard images.</p> <code>None</code> <code>weight_histogram_freq</code> <code>Union[None, int, str]</code> <p>Frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. Same argument format as <code>update_freq</code>.</p> <code>None</code> <code>paint_weights</code> <code>bool</code> <p>If True the system will attempt to visualize model weights as an image.</p> <code>False</code> <code>write_embeddings</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard embeddings.</p> <code>None</code> <code>embedding_labels</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to label information for the <code>write_embeddings</code>.</p> <code>None</code> <code>embedding_images</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to raw images to be associated with the <code>write_embeddings</code>.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\tensorboard.py</code> <pre><code>@traceable()\nclass TensorBoard(Trace):\n\"\"\"Output data for use in TensorBoard.\n    Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the\n    --reload_multifile=true flag until their multi-writer use case is finished:\n    https://github.com/tensorflow/tensorboard/issues/1063\n    Args:\n        log_dir: Path of the directory where the log files to be parsed by TensorBoard should be saved.\n        update_freq: 'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and\n            metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000,\n            the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings\n            like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to\n            TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace\n            mostly useless.\n        write_graph: Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph\n            is set to True.\n        write_images: If a string or list of strings is provided, the corresponding keys will be written to TensorBoard\n            images.\n        weight_histogram_freq: Frequency (in epochs) at which to compute activation and weight histograms for the layers\n            of the model. Same argument format as `update_freq`.\n        paint_weights: If True the system will attempt to visualize model weights as an image.\n        write_embeddings: If a string or list of strings is provided, the corresponding keys will be written to\n            TensorBoard embeddings.\n        embedding_labels: Keys corresponding to label information for the `write_embeddings`.\n        embedding_images: Keys corresponding to raw images to be associated with the `write_embeddings`.\n    \"\"\"\nFreq = namedtuple('Freq', ['is_step', 'freq'])\nwriter: _BaseWriter\n# TODO - support for per-instance tracking\ndef __init__(self,\nlog_dir: str = 'logs',\nupdate_freq: Union[None, int, str] = 100,\nwrite_graph: bool = True,\nwrite_images: Union[None, str, List[str]] = None,\nweight_histogram_freq: Union[None, int, str] = None,\npaint_weights: bool = False,\nembedding_freq: Union[None, int, str] = 'epoch',\nwrite_embeddings: Union[None, str, List[str]] = None,\nembedding_labels: Union[None, str, List[str]] = None,\nembedding_images: Union[None, str, List[str]] = None) -&gt; None:\nsuper().__init__(inputs=[\"*\"] + to_list(write_images) + to_list(write_embeddings) + to_list(embedding_labels) +\nto_list(embedding_images))\nself.root_log_dir = log_dir\nself.update_freq = self._parse_freq(update_freq)\nself.write_graph = write_graph\nself.painted_graphs = set()\nself.write_images = to_set(write_images)\nself.histogram_freq = self._parse_freq(weight_histogram_freq)\nif paint_weights and self.histogram_freq.freq == 0:\nself.histogram_freq.is_step = False\nself.histogram_freq.freq = 1\nself.paint_weights = paint_weights\nif write_embeddings is None and embedding_labels is None and embedding_images is None:\n# Speed up if-check short-circuiting later\nembedding_freq = None\nself.embedding_freq = self._parse_freq(embedding_freq)\nwrite_embeddings = to_list(write_embeddings)\nembedding_labels = to_list(embedding_labels)\nif embedding_labels:\nassert len(embedding_labels) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_labels keys, but recieved {len(embedding_labels)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_labels = [None for _ in range(len(write_embeddings))]\nembedding_images = to_list(embedding_images)\nif embedding_images:\nassert len(embedding_images) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_images keys, but recieved {len(embedding_images)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_images = [None for _ in range(len(write_embeddings))]\nself.write_embeddings = [(feature, label, img_label) for feature,\nlabel,\nimg_label in\nzip(write_embeddings, embedding_labels, embedding_images)]\nself.collected_embeddings = defaultdict(list)\ndef _parse_freq(self, freq: Union[None, str, int]) -&gt; Freq:\n\"\"\"A helper function to convert string based frequency inputs into epochs or steps\n        Args:\n            freq: One of either None, \"step\", \"epoch\", \"#s\", \"#e\", or #, where # is an integer.\n        Returns:\n            A `Freq` object recording whether the trace should run on an epoch basis or a step basis, as well as the\n            frequency with which it should run.\n        \"\"\"\nif freq is None:\nreturn self.Freq(False, 0)\nif isinstance(freq, int):\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(True, freq)\nif isinstance(freq, str):\nif freq in {'step', 's'}:\nreturn self.Freq(True, 1)\nif freq in {'epoch', 'e'}:\nreturn self.Freq(False, 1)\nparts = re.match(r\"^([0-9]+)([se])$\", freq)\nif parts is None:\nraise ValueError(f\"Tensorboard frequency argument must be formatted like &lt;int&gt;&lt;s|e&gt; but got {freq}\")\nfreq = int(parts[1])\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(parts[2] == 's', freq)\nelse:\nraise ValueError(f\"Unrecognized type passed as Tensorboard frequency: {type(freq)}\")\ndef on_begin(self, data: Data) -&gt; None:\nprint(\"FastEstimator-Tensorboard: writing logs to {}\".format(\nos.path.abspath(os.path.join(self.root_log_dir, self.system.experiment_time))))\nself.writer = _TfWriter(self.root_log_dir, self.system.experiment_time, self.system.network) if isinstance(\nself.system.network, TFNetwork) else _TorchWriter(\nself.root_log_dir, self.system.experiment_time, self.system.network)\nif self.write_graph and self.system.global_step == 1:\nself.painted_graphs = set()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.write_graph and self.system.network.epoch_models.symmetric_difference(self.painted_graphs):\nself.writer.write_epoch_models(mode=self.system.mode, epoch=self.system.epoch_idx)\nself.painted_graphs = self.system.network.epoch_models\n# Collect embeddings if present in batch but viewing per epoch. Don't aggregate during training though\nif self.system.mode != 'train' and self.embedding_freq.freq and not self.embedding_freq.is_step and self.system.epoch_idx % self.embedding_freq.freq == 0:\nfor elem in self.write_embeddings:\nname, lbl, img = elem\nif name in data:\nself.collected_embeddings[name].append((data.get(name), data.get(lbl), data.get(img)))\n# Handle embeddings if viewing per step\nif self.embedding_freq.freq and self.embedding_freq.is_step and self.system.global_step % self.embedding_freq.freq == 0:\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\nif self.system.mode != 'train':\nreturn\nif self.histogram_freq.freq and self.histogram_freq.is_step and \\\n                self.system.global_step % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\nif self.update_freq.freq and self.update_freq.is_step and self.system.global_step % self.update_freq.freq == 0:\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == 'train' and self.histogram_freq.freq and not self.histogram_freq.is_step and \\\n                self.system.epoch_idx % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\n# Write out any embeddings which were aggregated over batches\nfor name, val_list in self.collected_embeddings.items():\nembeddings = None if any(x[0] is None for x in val_list) else concat([x[0] for x in val_list])\nlabels = None if any(x[1] is None for x in val_list) else concat([x[1] for x in val_list])\nimgs = None if any(x[2] is None for x in val_list) else concat([x[2] for x in val_list])\nself.writer.write_embeddings(mode=self.system.mode,\nstep=self.system.global_step,\nembeddings=[(name, embeddings, labels, imgs)])\nself.collected_embeddings.clear()\n# Get any embeddings which were generated externally on epoch end\nif self.embedding_freq.freq and (self.embedding_freq.is_step\nor self.system.epoch_idx % self.embedding_freq.freq == 0):\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\nif self.update_freq.freq and (self.update_freq.is_step or self.system.epoch_idx % self.update_freq.freq == 0):\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\ndef on_end(self, data: Data) -&gt; None:\nself.writer.close()\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html", "title": "test_report", "text": ""}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestCase", "title": "<code>TestCase</code>", "text": "<p>This class defines the test case that the TestReport trace will take to perform auto-testing.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A test description.</p> required <code>criteria</code> <code>Callable[..., Union[bool, np.ndarray]]</code> <p>A function to perform the test. For an aggregate test, <code>criteria</code> needs to return True when the test passes and False when it fails. For a per-instance test, <code>criteria</code> needs to return a boolean np.ndarray, where entries show corresponding test results (True if the test of that data instance passes; False if it fails).</p> required <code>aggregate</code> <code>bool</code> <p>If True, this test is aggregate type and its <code>criteria</code> function will be examined at epoch_end. If False, this test is per-instance type and its <code>criteria</code> function will be examined at batch_end.</p> <code>True</code> <code>fail_threshold</code> <code>int</code> <p>Threshold of failure instance number to judge the per-instance test as failed or passed. If the failure number is above this value, then the test fails; otherwise it passes. It can only be set when <code>aggregate</code> is equal to False.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If user set <code>fail_threshold</code> for an aggregate test.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@traceable()\nclass TestCase:\n\"\"\"This class defines the test case that the TestReport trace will take to perform auto-testing.\n    Args:\n        description: A test description.\n        criteria: A function to perform the test. For an aggregate test, `criteria` needs to return True when the test\n            passes and False when it fails. For a per-instance test, `criteria` needs to return a boolean np.ndarray,\n            where entries show corresponding test results (True if the test of that data instance passes; False if it\n            fails).\n        aggregate: If True, this test is aggregate type and its `criteria` function will be examined at epoch_end. If\n            False, this test is per-instance type and its `criteria` function will be examined at batch_end.\n        fail_threshold: Threshold of failure instance number to judge the per-instance test as failed or passed. If\n            the failure number is above this value, then the test fails; otherwise it passes. It can only be set when\n            `aggregate` is equal to False.\n    Raises:\n        ValueError: If user set `fail_threshold` for an aggregate test.\n    \"\"\"\ndef __init__(self,\ndescription: str,\ncriteria: Callable[..., Union[bool, np.ndarray]],\naggregate: bool = True,\nfail_threshold: int = 0) -&gt; None:\nself.description = description\nself.criteria = criteria\nself.criteria_inputs = inspect.signature(criteria).parameters.keys()\nself.aggregate = aggregate\nif self.aggregate:\nif fail_threshold:\nraise ValueError(\"fail_threshold cannot be set in a aggregate test\")\nelse:\nself.fail_threshold = fail_threshold\nself.result = None\nself.input_val = None\nself.fail_id = []\nself.init_result()\ndef init_result(self) -&gt; None:\n\"\"\"Reset the test result.\n        \"\"\"\nif self.aggregate:\nself.result = None\nself.input_val = None\nelse:\nself.result = []\nself.fail_id = []\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestCase.init_result", "title": "<code>init_result</code>", "text": "<p>Reset the test result.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>def init_result(self) -&gt; None:\n\"\"\"Reset the test result.\n    \"\"\"\nif self.aggregate:\nself.result = None\nself.input_val = None\nelse:\nself.result = []\nself.fail_id = []\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport", "title": "<code>TestReport</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Automate testing and report generation.</p> <p>This trace will evaluate all its <code>test_cases</code> during test mode and generate a PDF report and a JSON test result.</p> <p>Parameters:</p> Name Type Description Default <code>test_cases</code> <code>Union[TestCase, List[TestCase]]</code> <p>The test(s) to be run.</p> required <code>save_path</code> <code>str</code> <p>Where to save the outputs.</p> required <code>test_title</code> <code>Optional[str]</code> <p>The title of the test, or None to use the experiment name.</p> <code>None</code> <code>data_id</code> <code>str</code> <p>Data instance ID key. If provided, then per-instances test will include failing instance IDs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@traceable()\nclass TestReport(Trace):\n\"\"\"Automate testing and report generation.\n    This trace will evaluate all its `test_cases` during test mode and generate a PDF report and a JSON test result.\n    Args:\n        test_cases: The test(s) to be run.\n        save_path: Where to save the outputs.\n        test_title: The title of the test, or None to use the experiment name.\n        data_id: Data instance ID key. If provided, then per-instances test will include failing instance IDs.\n    \"\"\"\ndef __init__(self,\ntest_cases: Union[TestCase, List[TestCase]],\nsave_path: str,\ntest_title: Optional[str] = None,\ndata_id: str = None) -&gt; None:\nself.check_pdf_dependency()\nself.test_title = test_title\nself.report_name = None\nself.instance_cases = []\nself.aggregate_cases = []\nself.data_id = data_id\nall_inputs = to_set(self.data_id)\nfor case in to_list(test_cases):\nall_inputs.update(case.criteria_inputs)\nif case.aggregate:\nself.aggregate_cases.append(case)\nelse:\nself.instance_cases.append(case)\npath = os.path.normpath(save_path)\npath = os.path.abspath(path)\nroot_dir = os.path.dirname(path)\nreport = os.path.basename(path) or 'report'\nreport = report.split('.')[0]\nself.save_dir = os.path.join(root_dir, report)\nself.resource_dir = os.path.join(self.save_dir, \"resources\")\nos.makedirs(self.save_dir, exist_ok=True)\nos.makedirs(self.resource_dir, exist_ok=True)\nself.json_summary = {}\n# PDF document related\nself.doc = None\nself.test_id = None\nsuper().__init__(inputs=all_inputs, mode=\"test\")\ndef on_begin(self, data: Data) -&gt; None:\nself._sanitize_report_name()\nself._initialize_json_summary()\nfor case in self.instance_cases + self.aggregate_cases:\ncase.init_result()\ndef on_batch_end(self, data: Data) -&gt; None:\nfor case in self.instance_cases:\nresult = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\nif not isinstance(result, np.ndarray):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of per-instance test needs to be ndarray with dtype bool.\")\nelif result.dtype != np.dtype(\"bool\"):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of per-instance test needs to be ndarray with dtype bool.\")\nresult = result.reshape(-1)\ncase.result.append(result)\nif self.data_id:\ndata_id = to_number(data[self.data_id]).reshape((-1, ))\nif data_id.size != result.size:\nraise ValueError(f\"In test with description '{case.description}': \"\n\"Array size of criteria return doesn't match ID array size. Size of criteria\"\n\"return should be equal to the batch_size such that each entry represents the test\"\n\"result of its corresponding data instance.\")\ncase.fail_id.append(data_id[result == False])\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor case in self.aggregate_cases:\nresult = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\nif not isinstance(result, (bool, np.bool_)):\nraise TypeError(f\"In test with description '{case.description}': \"\n\"Criteria return of aggregate-case test needs to be a bool.\")\ncase.result = case.criteria(*[data[var_name] for var_name in case.criteria_inputs])\ncase.input_val = {var_name: self._to_serializable(data[var_name]) for var_name in case.criteria_inputs}\ndef on_end(self, data: Data) -&gt; None:\nfor case in self.instance_cases:\ncase_dict = {\"test_type\": \"per-instance\", \"description\": case.description}\nresult = np.hstack(case.result)\nfail_num = np.sum(result == False)\ncase_dict[\"passed\"] = self._to_serializable(fail_num &lt;= case.fail_threshold)\ncase_dict[\"fail_threshold\"] = case.fail_threshold\ncase_dict[\"fail_number\"] = self._to_serializable(fail_num)\nif self.data_id:\nfail_id = np.hstack(case.fail_id)\ncase_dict[\"fail_id\"] = self._to_serializable(fail_id)\nself.json_summary[\"tests\"].append(case_dict)\nfor case in self.aggregate_cases:\ncase_dict = {\n\"test_type\": \"aggregate\",\n\"description\": case.description,\n\"passed\": self._to_serializable(case.result),\n\"inputs\": case.input_val\n}\nself.json_summary[\"tests\"].append(case_dict)\nself.json_summary[\"execution_time(s)\"] = time() - self.json_summary[\"execution_time(s)\"]\nself._dump_json()\nself._init_document()\nself._write_body_content()\nself._dump_pdf()\ndef _initialize_json_summary(self) -&gt; None:\n\"\"\"Initialize json summary.\n        \"\"\"\nself.json_summary = {\n\"title\": self.test_title, \"timestamp\": str(datetime.now()), \"execution_time(s)\": time(), \"tests\": []\n}\ndef _sanitize_report_name(self) -&gt; None:\n\"\"\"Sanitize report name and make it class attribute.\n        Raises:\n            RuntimeError: If a test title was not provided and the user did not set an experiment name.\n        \"\"\"\nexp_name = self.system.summary.name or self.test_title\nif not exp_name:\nraise RuntimeError(\"TestReport requires an experiment name to be provided in estimator.fit(), or a title\")\n# Convert the experiment name to a report name (useful for saving multiple experiments into same directory)\nreport_name = \"\".join('_' if c == ' ' else c for c in exp_name\nif c.isalnum() or c in (' ', '_')).rstrip(\"_\").lower()\nself.report_name = re.sub('_{2,}', '_', report_name) + \"_TestReport\"\nif self.test_title is None:\nself.test_title = exp_name\ndef _init_document(self) -&gt; None:\n\"\"\"Initialize latex document.\n        \"\"\"\nself.doc = self._init_document_geometry()\nself.doc.packages.append(Package(name='placeins', options=['section']))\nself.doc.packages.append(Package(name='float'))\nself.doc.packages.append(Package(name='hyperref', options='hidelinks'))\nself.doc.preamble.append(NoEscape(r'\\aboverulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\belowrulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\renewcommand{\\arraystretch}{1.2}'))\n# new column type for tabularx\nself.doc.preamble.append(NoEscape(r'\\newcolumntype{Y}{&gt;{\\centering\\arraybackslash}X}'))\nself._write_title()\nself._write_toc()\ndef _write_title(self) -&gt; None:\n\"\"\"Write the title content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.preamble.append(Command('title', self.json_summary[\"title\"]))\nself.doc.preamble.append(Command('author', f\"FastEstimator {fe.__version__}\"))\nself.doc.preamble.append(Command('date', NoEscape(r'\\today')))\nself.doc.append(NoEscape(r'\\maketitle'))\ndef _write_toc(self) -&gt; None:\n\"\"\"Write the table of contents. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.append(NoEscape(r'\\tableofcontents'))\nself.doc.append(NoEscape(r'\\newpage'))\ndef _write_body_content(self) -&gt; None:\n\"\"\"Write the main content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself._document_test_result()\ndef _document_test_result(self) -&gt; None:\n\"\"\"Document test results including test summary, passed tests, and failed tests.\n        \"\"\"\nself.test_id = 1\ninstance_pass_tests, aggregate_pass_tests, instance_fail_tests, aggregate_fail_tests = [], [], [], []\nfor test in self.json_summary[\"tests\"]:\nif test[\"test_type\"] == \"per-instance\" and test[\"passed\"]:\ninstance_pass_tests.append(test)\nelif test[\"test_type\"] == \"per-instance\" and not test[\"passed\"]:\ninstance_fail_tests.append(test)\nelif test[\"test_type\"] == \"aggregate\" and test[\"passed\"]:\naggregate_pass_tests.append(test)\nelif test[\"test_type\"] == \"aggregate\" and not test[\"passed\"]:\naggregate_fail_tests.append(test)\nwith self.doc.create(Section(\"Test Summary\")):\nwith self.doc.create(Itemize()) as itemize:\nitemize.add_item(\nescape_latex(\"Execution time: {:.2f} seconds\".format(self.json_summary['execution_time(s)'])))\nwith self.doc.create(Table(position='H')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\nself._document_summary_table(pass_num=len(instance_pass_tests) + len(aggregate_pass_tests),\nfail_num=len(instance_fail_tests) + len(aggregate_fail_tests))\nif instance_fail_tests or aggregate_fail_tests:\nwith self.doc.create(Section(\"Failed Tests\")):\nif len(aggregate_fail_tests) &gt; 0:\nwith self.doc.create(Subsection(\"Failed Aggregate Tests\")):\nself._document_aggregate_table(tests=aggregate_fail_tests)\nif len(instance_fail_tests) &gt; 0:\nwith self.doc.create(Subsection(\"Failed Per-Instance Tests\")):\nself._document_instance_table(tests=instance_fail_tests, with_id=bool(self.data_id))\nif instance_pass_tests or aggregate_pass_tests:\nwith self.doc.create(Section(\"Passed Tests\")):\nif aggregate_pass_tests:\nwith self.doc.create(Subsection(\"Passed Aggregate Tests\")):\nself._document_aggregate_table(tests=aggregate_pass_tests)\nif instance_pass_tests:\nwith self.doc.create(Subsection(\"Passed Per-Instance Tests\")):\nself._document_instance_table(tests=instance_pass_tests, with_id=bool(self.data_id))\nself.doc.append(NoEscape(r'\\newpage'))  # For QMS report\ndef _document_summary_table(self, pass_num: int, fail_num: int) -&gt; None:\n\"\"\"Document a summary table.\n        Args:\n            pass_num: Total number of passed tests.\n            fail_num: Total number of failed tests.\n        \"\"\"\nwith self.doc.create(Tabularx('|Y|Y|Y|', booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\ntabular.add_row((\"Total Tests\", \"Total Passed \", \"Total Failed\"), strict=False)\ntabular.add_hline()\ntabular.add_row((pass_num + fail_num, pass_num, fail_num), strict=False)\ndef _document_instance_table(self, tests: List[Dict[str, Any]], with_id: bool):\n\"\"\"Document a result table of per-instance tests.\n        Args:\n            tests: List of corresponding test dictionary to make a table.\n            with_id: Whether the test information includes data ID.\n        \"\"\"\nif with_id:\ntable_spec = '|c|p{5cm}|c|c|p{5cm}|'\ncolumn_num = 5\nelse:\ntable_spec = '|c|p{10cm}|c|c|'\ncolumn_num = 4\nwith self.doc.create(LongTable(table_spec, pos=['h!'], booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\nrow_cells = [\nMultiColumn(size=1, align='|c|', data=\"Test ID\"),\nMultiColumn(size=1, align='c|', data=\"Test Description\"),\nMultiColumn(size=1, align='c|', data=\"Pass Threshold\"),\nMultiColumn(size=1, align='c|', data=\"Failure Count\")\n]\nif with_id:\nrow_cells.append(MultiColumn(size=1, align='c|', data=\"Failure Data Instance ID\"))\ntabular.add_row(row_cells)\n# add table header and footer\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(column_num, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\nfor idx, test in enumerate(tests):\nif idx &gt; 0:\ntabular.add_hline()\ndes_data = [WrapText(data=x, threshold=27) for x in test[\"description\"].split(\" \")]\nrow_cells = [\nself.test_id,\nIterJoin(data=des_data, token=\" \"),\nNoEscape(r'$\\le $' + str(test[\"fail_threshold\"])),\ntest[\"fail_number\"]\n]\nif with_id:\nid_data = [WrapText(data=x, threshold=27) for x in test[\"fail_id\"]]\nrow_cells.append(IterJoin(data=id_data, token=\", \"))\ntabular.add_row(row_cells)\nself.test_id += 1\ndef _document_aggregate_table(self, tests: List[Dict[str, Any]]) -&gt; None:\n\"\"\"Document a result table of aggregate tests.\n        Args:\n            tests: List of corresponding test dictionary to make a table.\n        \"\"\"\nwith self.doc.create(LongTable('|c|p{8cm}|p{7.3cm}|', booktabs=True)) as tabular:\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\n# add table heading\ntabular.add_row((MultiColumn(size=1, align='|c|', data=\"Test ID\"),\nMultiColumn(size=1, align='c|', data=\"Test Description\"),\nMultiColumn(size=1, align='c|', data=\"Input Value\")))\n# add table header and footer\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(3, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\nfor idx, test in enumerate(tests):\nif idx &gt; 0:\ntabular.add_hline()\ninp_data = [f\"{arg}={self.sanitize_value(value)}\" for arg, value in test[\"inputs\"].items()]\ninp_data = [WrapText(data=x, threshold=27) for x in inp_data]\ndes_data = [WrapText(data=x, threshold=27) for x in test[\"description\"].split(\" \")]\nrow_cells = [\nself.test_id,\nIterJoin(data=des_data, token=\" \"),\nIterJoin(data=inp_data, token=escape_latex(\", \\n\")),\n]\ntabular.add_row(row_cells)\nself.test_id += 1\ndef _dump_pdf(self) -&gt; None:\n\"\"\"Dump PDF summary report.\n        \"\"\"\nif shutil.which(\"latexmk\") is None and shutil.which(\"pdflatex\") is None:\n# No LaTeX Compiler is available\nself.doc.generate_tex(os.path.join(self.save_dir, self.report_name))\nsuffix = '.tex'\nelse:\n# Force a double-compile since some compilers will struggle with TOC generation\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False, clean=False)\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False)\nsuffix = '.pdf'\nprint(\"FastEstimator-TestReport: Report written to {}{}\".format(os.path.join(self.save_dir, self.report_name),\nsuffix))\ndef _dump_json(self) -&gt; None:\n\"\"\"Dump JSON file.\n        \"\"\"\njson_path = os.path.join(self.resource_dir, self.report_name + \".json\")\nwith open(json_path, 'w') as fp:\njson.dump(self.json_summary, fp, indent=4)\n@staticmethod\ndef _to_serializable(obj: Any) -&gt; Union[float, int, list]:\n\"\"\"Convert to JSON serializable type.\n        Args:\n            obj: Any object that needs to be converted.\n        Return:\n            JSON serializable object that essentially is equivalent to input obj.\n        \"\"\"\nif isinstance(obj, np.ndarray):\nif obj.size &gt; 0:\nshape = obj.shape\nobj = obj.reshape((-1, ))\nobj = np.vectorize(TestReport._element_to_serializable)(obj)\nobj = obj.reshape(shape)\nobj = obj.tolist()\nelse:\nobj = TestReport._element_to_serializable(obj)\nreturn obj\n@staticmethod\ndef _element_to_serializable(obj: Any) -&gt; Any:\n\"\"\"Convert to JSON serializable type.\n        This function can handle any object type except ndarray.\n        Args:\n            obj: Any object except ndarray that needs to be converted.\n        Return:\n            JSON serializable object that essentially is equivalent to input obj.\n        \"\"\"\nif isinstance(obj, bytes):\nobj = obj.decode('utf-8')\nelif isinstance(obj, np.generic):\nobj = obj.item()\nreturn obj\n@staticmethod\ndef check_pdf_dependency() -&gt; None:\n\"\"\"Check dependency of PDF-generating packages.\n        Raises:\n            OSError: Some required package has not been installed.\n        \"\"\"\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n                'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\n@staticmethod\ndef sanitize_value(value: Union[int, float]) -&gt; str:\n\"\"\"Sanitize input value for a better report display.\n        Args:\n            value: Value to be sanitized.\n        Returns:\n            Sanitized string of `value`.\n        \"\"\"\nif 1000 &gt; value &gt;= 0.001:\nreturn f\"{value:.3f}\"\nelse:\nreturn f\"{value:.3e}\"\n@staticmethod\ndef _init_document_geometry() -&gt; Document:\n\"\"\"Init geometry setting of the document.\n        Return:\n            Initialized Document object.\n        \"\"\"\nreturn Document(geometry_options=['lmargin=2cm', 'rmargin=2cm', 'bmargin=2cm'])\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport.check_pdf_dependency", "title": "<code>check_pdf_dependency</code>  <code>staticmethod</code>", "text": "<p>Check dependency of PDF-generating packages.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>Some required package has not been installed.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@staticmethod\ndef check_pdf_dependency() -&gt; None:\n\"\"\"Check dependency of PDF-generating packages.\n    Raises:\n        OSError: Some required package has not been installed.\n    \"\"\"\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n            'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\n</code></pre>"}, {"location": "fastestimator/trace/io/test_report.html#fastestimator.fastestimator.trace.io.test_report.TestReport.sanitize_value", "title": "<code>sanitize_value</code>  <code>staticmethod</code>", "text": "<p>Sanitize input value for a better report display.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[int, float]</code> <p>Value to be sanitized.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sanitized string of <code>value</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\test_report.py</code> <pre><code>@staticmethod\ndef sanitize_value(value: Union[int, float]) -&gt; str:\n\"\"\"Sanitize input value for a better report display.\n    Args:\n        value: Value to be sanitized.\n    Returns:\n        Sanitized string of `value`.\n    \"\"\"\nif 1000 &gt; value &gt;= 0.001:\nreturn f\"{value:.3f}\"\nelse:\nreturn f\"{value:.3e}\"\n</code></pre>"}, {"location": "fastestimator/trace/io/traceability.html", "title": "traceability", "text": ""}, {"location": "fastestimator/trace/io/traceability.html#fastestimator.fastestimator.trace.io.traceability.Traceability", "title": "<code>Traceability</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Automatically generate summary reports of the training.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Where to save the output files. Note that this will generate a new folder with the given name, into which the report and corresponding graphics assets will be written.</p> required <code>extra_objects</code> <code>Any</code> <p>Any extra objects which are not part of the Estimator, but which you want to capture in the summary report. One example could be an extra pipeline which performs pre-processing.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If graphviz is not installed.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\traceability.py</code> <pre><code>@traceable()\nclass Traceability(Trace):\n\"\"\"Automatically generate summary reports of the training.\n    Args:\n        save_path: Where to save the output files. Note that this will generate a new folder with the given name, into\n            which the report and corresponding graphics assets will be written.\n        extra_objects: Any extra objects which are not part of the Estimator, but which you want to capture in the\n            summary report. One example could be an extra pipeline which performs pre-processing.\n    Raises:\n        OSError: If graphviz is not installed.\n    \"\"\"\ndef __init__(self, save_path: str, extra_objects: Any = None):\n# Verify that graphviz is available on this machine\ntry:\npydot.Dot.create(pydot.Dot())\nexcept OSError:\nraise OSError(\n\"Traceability requires that graphviz be installed. See www.graphviz.org/download for more information.\")\n# Verify that the system locale is functioning correctly\ntry:\nlocale.getlocale()\nexcept ValueError:\nraise OSError(\"Your system locale is not configured correctly. On mac this can be resolved by adding \\\n                'export LC_ALL=en_US.UTF-8' and 'export LANG=en_US.UTF-8' to your ~/.bash_profile\")\nsuper().__init__(inputs=\"*\", mode=\"!infer\")  # Claim wildcard inputs to get this trace sorted last\n# Report assets will get saved into a folder for portability\npath = os.path.normpath(save_path)\npath = os.path.abspath(path)\nroot_dir = os.path.dirname(path)\nreport = os.path.basename(path) or 'report'\nreport = report.split('.')[0]\nself.save_dir = os.path.join(root_dir, report)\nself.resource_dir = os.path.join(self.save_dir, 'resources')\nself.report_name = None  # This will be set later by the experiment name\nos.makedirs(self.save_dir, exist_ok=True)\nos.makedirs(self.resource_dir, exist_ok=True)\n# Other member variables\nself.config_tables = []\n# Extra objects will automatically get included in the report since this Trace is @traceable, so we don't need\n# to do anything with them. Referencing here to stop IDEs from flagging the argument as unused and removing it.\nto_list(extra_objects)\nself.doc = Document()\nself.log_splicer = None\ndef on_begin(self, data: Data) -&gt; None:\nexp_name = self.system.summary.name\nif not exp_name:\nraise RuntimeError(\"Traceability reports require an experiment name to be provided in estimator.fit()\")\n# Convert the experiment name to a report name (useful for saving multiple experiments into same directory)\nreport_name = \"\".join('_' if c == ' ' else c for c in exp_name\nif c.isalnum() or c in (' ', '_')).rstrip().lower()\nreport_name = re.sub('_{2,}', '_', report_name)\nself.report_name = report_name or 'report'\n# Send experiment logs into a file\nlog_path = os.path.join(self.resource_dir, f\"{report_name}.txt\")\nif self.system.mode != 'test':\n# See if there's a RestoreWizard\nrestore = False\nfor trace in self.system.traces:\nif isinstance(trace, RestoreWizard):\nrestore = trace.should_restore()\nif not restore:\n# If not running in test mode, we need to remove any old log file since it would get appended to\nwith contextlib.suppress(FileNotFoundError):\nos.remove(log_path)\nself.log_splicer = LogSplicer(log_path)\nself.log_splicer.__enter__()\n# Get the initialization summary information for the experiment\nself.config_tables = self.system.summary.system_config\nmodels = self.system.network.models\nn_floats = len(self.config_tables) + len(models)\nself.doc = self._init_document_geometry()\n# Keep tables/figures in their sections\nself.doc.packages.append(Package(name='placeins', options=['section']))\nself.doc.preamble.append(NoEscape(r'\\usetikzlibrary{positioning}'))\n# Fix an issue with too many tables for LaTeX to render\nself.doc.preamble.append(NoEscape(r'\\maxdeadcycles=' + str(2 * n_floats + 10) + ''))\nself.doc.preamble.append(NoEscape(r'\\extrafloats{' + str(n_floats + 10) + '}'))\n# Manipulate booktab tables so that their horizontal lines don't break\nself.doc.preamble.append(NoEscape(r'\\aboverulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\belowrulesep=0ex'))\nself.doc.preamble.append(NoEscape(r'\\renewcommand{\\arraystretch}{1.2}'))\nself._write_title()\nself._write_toc()\ndef on_end(self, data: Data) -&gt; None:\nself._write_body_content()\n# Need to move the tikz dependency after the xcolor package\nself.doc.dumps_packages()\npackages = self.doc.packages\ntikz = Package(name='tikz')\npackages.discard(tikz)\npackages.add(tikz)\nif shutil.which(\"latexmk\") is None and shutil.which(\"pdflatex\") is None:\n# No LaTeX Compiler is available\nself.doc.generate_tex(os.path.join(self.save_dir, self.report_name))\nsuffix = '.tex'\nelse:\n# Force a double-compile since some compilers will struggle with TOC generation\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False, clean=False)\nself.doc.generate_pdf(os.path.join(self.save_dir, self.report_name), clean_tex=False)\nsuffix = '.pdf'\nprint(\"FastEstimator-Traceability: Report written to {}{}\".format(os.path.join(self.save_dir, self.report_name),\nsuffix))\nself.log_splicer.__exit__()\ndef _write_title(self) -&gt; None:\n\"\"\"Write the title content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.preamble.append(Command('title', self.system.summary.name))\nself.doc.preamble.append(Command('author', f\"FastEstimator {fe.__version__}\"))\nself.doc.preamble.append(Command('date', NoEscape(r'\\today')))\nself.doc.append(NoEscape(r'\\maketitle'))\ndef _write_toc(self) -&gt; None:\n\"\"\"Write the table of contents. Override if you want to build on top of base traceability report.\n        \"\"\"\nself.doc.append(NoEscape(r'\\tableofcontents'))\nself.doc.append(NoEscape(r'\\newpage'))\ndef _write_body_content(self) -&gt; None:\n\"\"\"Write the main content of the file. Override if you want to build on top of base traceability report.\n        \"\"\"\nself._document_training_graphs()\nself.doc.append(NoEscape(r'\\newpage'))\nself._document_fe_graph()\nself.doc.append(NoEscape(r'\\newpage'))\nself._document_init_params()\nself._document_models()\nself._document_sys_config()\nself.doc.append(NoEscape(r'\\newpage'))\ndef _document_training_graphs(self) -&gt; None:\n\"\"\"Add training graphs to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"Training Graphs\")):\nlog_path = os.path.join(self.resource_dir, f'{self.report_name}_logs.png')\nvisualize_logs(experiments=[self.system.summary],\nsave_path=log_path,\nverbose=False,\nignore_metrics={'num_device', 'logging_interval'})\nwith self.doc.create(Figure(position='h!')) as plot:\nplot.add_image(os.path.relpath(log_path, start=self.save_dir),\nwidth=NoEscape(r'1.0\\textwidth,height=0.95\\textheight,keepaspectratio'))\nfor idx, graph in enumerate(self.system.custom_graphs.values()):\ngraph_path = os.path.join(self.resource_dir, f'{self.report_name}_custom_graph_{idx}.png')\nvisualize_logs(experiments=graph, save_path=graph_path, verbose=False)\nwith self.doc.create(Figure(position='h!')) as plot:\nplot.add_image(os.path.relpath(graph_path, start=self.save_dir),\nwidth=NoEscape(r'1.0\\textwidth,height=0.95\\textheight,keepaspectratio'))\ndef _document_fe_graph(self) -&gt; None:\n\"\"\"Add FE execution graphs into the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"FastEstimator Architecture\")):\nfor mode in self.system.pipeline.data.keys():\nscheduled_items = self.system.pipeline.get_scheduled_items(\nmode) + self.system.network.get_scheduled_items(mode) + self.system.traces\nsignature_epochs = get_signature_epochs(scheduled_items, total_epochs=self.system.epoch_idx, mode=mode)\nepochs_with_data = self.system.pipeline.get_epochs_with_data(total_epochs=self.system.epoch_idx,\nmode=mode)\nif set(signature_epochs) &amp; epochs_with_data:\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(mode.capitalize())):\nfor epoch in signature_epochs:\nif epoch not in epochs_with_data:\ncontinue\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(\nSubsubsection(f\"Epoch {epoch}\",\nlabel=Label(Marker(name=f\"{mode}{epoch}\", prefix=\"ssubsec\")))):\nds_ids = self.system.pipeline.get_ds_ids(epoch=epoch, mode=mode)\nfor ds_id in ds_ids:\nwith NonContext() if ds_id == '' else self.doc.create(\nParagraph(f\"Dataset {ds_id}\",\nlabel=Label(Marker(name=f\"{mode}{epoch}{ds_id}\",\nprefix=\"para\")))):\ndiagram = self._draw_diagram(mode, epoch, ds_id)\nltx = d2t.dot2tex(diagram.to_string(), figonly=True)\nargs = Arguments(**{'max width': r'\\textwidth, max height=0.9\\textheight'})\nargs.escape = False\nwith self.doc.create(Center()):\nwith self.doc.create(AdjustBox(arguments=args)) as box:\nbox.append(NoEscape(ltx))\ndef _document_init_params(self) -&gt; None:\n\"\"\"Add initialization parameters to the traceability document.\n        \"\"\"\nfrom fastestimator.estimator import Estimator  # Avoid circular import\nwith self.doc.create(Section(\"Parameters\")):\nmodel_ids = {\nFEID(id(model))\nfor model in self.system.network.models if isinstance(model, (tf.keras.Model, torch.nn.Module))\n}\n# Locate the datasets in order to provide extra details about them later in the summary\ndatasets = {}\nfor mode in ['train', 'eval', 'test']:\nobjs = to_list(self.system.pipeline.data.get(mode, None))\nidx = 0\nwhile idx &lt; len(objs):\nobj = objs[idx]\nif obj:\nfeid = FEID(id(obj))\nif feid not in datasets:\ndatasets[feid] = ({mode}, obj)\nelse:\ndatasets[feid][0].add(mode)\nif isinstance(obj, Scheduler):\nobjs.extend(obj.get_all_values())\nidx += 1\n# Parse the config tables\nstart = 0\nstart = self._loop_tables(start,\nclasses=(Estimator, BaseNetwork, Pipeline),\nname=\"Base Classes\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=Scheduler,\nname=\"Schedulers\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start, classes=Trace, name=\"Traces\", model_ids=model_ids, datasets=datasets)\nstart = self._loop_tables(start, classes=Op, name=\"Operators\", model_ids=model_ids, datasets=datasets)\nstart = self._loop_tables(start,\nclasses=(Dataset, tf.data.Dataset),\nname=\"Datasets\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=(tf.keras.Model, torch.nn.Module),\nname=\"Models\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=types.FunctionType,\nname=\"Functions\",\nmodel_ids=model_ids,\ndatasets=datasets)\nstart = self._loop_tables(start,\nclasses=(np.ndarray, tf.Tensor, tf.Variable, torch.Tensor),\nname=\"Tensors\",\nmodel_ids=model_ids,\ndatasets=datasets)\nself._loop_tables(start, classes=Any, name=\"Miscellaneous\", model_ids=model_ids, datasets=datasets)\ndef _loop_tables(self,\nstart: int,\nclasses: Union[type, Tuple[type, ...]],\nname: str,\nmodel_ids: Set[FEID],\ndatasets: Dict[FEID, Tuple[Set[str], Any]]) -&gt; int:\n\"\"\"Iterate through tables grouping them into subsections.\n        Args:\n            start: What index to start searching from.\n            classes: What classes are acceptable for this subsection.\n            name: What to call this subsection.\n            model_ids: The ids of any known models.\n            datasets: A mapping like {ID: ({modes}, dataset)}. Useful for augmenting the displayed information.\n        Returns:\n            The new start index after traversing as many spaces as possible along the list of tables.\n        \"\"\"\nstop = start\nwhile stop &lt; len(self.config_tables):\nif classes == Any or issubclass(self.config_tables[stop].type, classes):\nstop += 1\nelse:\nbreak\nif stop &gt; start:\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(name)):\nself._write_tables(self.config_tables[start:stop], model_ids, datasets)\nreturn stop\ndef _write_tables(self,\ntables: List[FeSummaryTable],\nmodel_ids: Set[FEID],\ndatasets: Dict[FEID, Tuple[Set[str], Any]]) -&gt; None:\n\"\"\"Insert a LaTeX representation of a list of tables into the current doc.\n        Args:\n            tables: The tables to write into the doc.\n            model_ids: The ids of any known models.\n            datasets: A mapping like {ID: ({modes}, dataset)}. Useful for augmenting the displayed information.\n        \"\"\"\nfor tbl in tables:\nname_override = None\ntoc_ref = None\nextra_rows = None\nif tbl.fe_id in model_ids:\n# Link to a later detailed model description\nname_override = Hyperref(Marker(name=str(tbl.name), prefix=\"subsec\"),\ntext=NoEscape(r'\\textcolor{blue}{') + bold(tbl.name) + NoEscape('}'))\nif tbl.fe_id in datasets:\nmodes, dataset = datasets[tbl.fe_id]\ntitle = \", \".join([s.capitalize() for s in modes])\nname_override = bold(f'{tbl.name} ({title})')\n# Enhance the dataset summary\nif isinstance(dataset, FEDataset):\nextra_rows = list(dataset.summary().__getstate__().items())\nfor idx, (key, val) in enumerate(extra_rows):\nkey = f\"{prettify_metric_name(key)}:\"\nif isinstance(val, dict) and val:\nif isinstance(list(val.values())[0], (int, float, str, bool, type(None))):\nval = jsonpickle.dumps(val, unpicklable=False)\nelse:\nsubtable = Tabularx('l|X', width_argument=NoEscape(r'\\linewidth'))\nfor k, v in val.items():\nif hasattr(v, '__getstate__'):\nv = jsonpickle.dumps(v, unpicklable=False)\nsubtable.add_row((k, v))\n# To nest TabularX, have to wrap it in brackets\nsubtable = ContainerList(data=[NoEscape(\"{\"), subtable, NoEscape(\"}\")])\nval = subtable\nextra_rows[idx] = (key, val)\ntbl.render_table(self.doc, name_override=name_override, toc_ref=toc_ref, extra_rows=extra_rows)\ndef _document_models(self) -&gt; None:\n\"\"\"Add model summaries to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"Models\")):\nfor model in humansorted(self.system.network.models, key=lambda m: m.model_name):\nif not isinstance(model, (tf.keras.Model, torch.nn.Module)):\ncontinue\nself.doc.append(NoEscape(r'\\FloatBarrier'))\nwith self.doc.create(Subsection(f\"{model.model_name.capitalize()}\")):\nif isinstance(model, tf.keras.Model):\n# Text Summary\nsummary = []\nmodel.summary(line_length=92, print_fn=lambda x: summary.append(x))\nsummary = \"\\n\".join(summary)\nself.doc.append(Verbatim(summary))\nwith self.doc.create(Center()):\nself.doc.append(HrefFEID(FEID(id(model)), model.model_name))\n# Visual Summary\n# noinspection PyBroadException\ntry:\nfile_path = os.path.join(self.resource_dir,\n\"{}_{}.pdf\".format(self.report_name, model.model_name))\ndot = tf.keras.utils.model_to_dot(model, show_shapes=True, expand_nested=True)\n# LaTeX \\maxdim is around 575cm (226 inches), so the image must have max dimension less than\n# 226 inches. However, the 'size' parameter doesn't account for the whole node height, so\n# set the limit lower (100 inches) to leave some wiggle room.\ndot.set('size', '100')\ndot.write(file_path, format='pdf')\nexcept Exception:\nfile_path = None\nprint(\nf\"FastEstimator-Warn: Model {model.model_name} could not be visualized by Traceability\")\nelif isinstance(model, torch.nn.Module):\nif hasattr(model, 'fe_input_spec'):\n# Text Summary\n# noinspection PyUnresolvedReferences\ninputs = model.fe_input_spec.get_dummy_input()\nself.doc.append(\nVerbatim(\npms.summary(model.module if self.system.num_devices &gt; 1 else model,\ninputs,\nprint_summary=False)))\nwith self.doc.create(Center()):\nself.doc.append(HrefFEID(FEID(id(model)), model.model_name))\n# Visual Summary\n# Import has to be done while matplotlib is using the Agg backend\nold_backend = matplotlib.get_backend() or 'Agg'\nmatplotlib.use('Agg')\n# noinspection PyBroadException\ntry:\n# Fake the IPython import when user isn't running from Jupyter\nsys.modules.setdefault('IPython', MagicMock())\nsys.modules.setdefault('IPython.display', MagicMock())\nimport hiddenlayer as hl\nwith Suppressor():\ngraph = hl.build_graph(model.module if self.system.num_devices &gt; 1 else model,\ninputs)\ngraph = graph.build_dot()\ngraph.attr(rankdir='TB')  # Switch it to Top-to-Bottom instead of Left-to-Right\n# LaTeX \\maxdim is around 575cm (226 inches), so the image must have max dimension less\n# than 226 inches. However, the 'size' parameter doesn't account for the whole node\n# height, so set the limit lower (100 inches) to leave some wiggle room.\ngraph.attr(size=\"100,100\")\ngraph.attr(margin='0')\nfile_path = graph.render(filename=\"{}_{}\".format(self.report_name, model.model_name),\ndirectory=self.resource_dir,\nformat='pdf',\ncleanup=True)\nexcept Exception:\nfile_path = None\nprint(\"FastEstimator-Warn: Model {} could not be visualized by Traceability\".format(\nmodel.model_name))\nfinally:\nmatplotlib.use(old_backend)\nelse:\nfile_path = None\nself.doc.append(\"This model was not used by the Network during training.\")\nif file_path:\nwith self.doc.create(Figure(position='ht!')) as fig:\nfig.append(Label(Marker(name=str(FEID(id(model))), prefix=\"model\")))\nfig.add_image(os.path.relpath(file_path, start=self.save_dir),\nwidth=NoEscape(r'1.0\\textwidth,height=0.95\\textheight,keepaspectratio'))\nfig.add_caption(NoEscape(HrefFEID(FEID(id(model)), model.model_name).dumps()))\ndef _document_sys_config(self) -&gt; None:\n\"\"\"Add a system config summary to the traceability document.\n        \"\"\"\nwith self.doc.create(Section(\"System Configuration\")):\nwith self.doc.create(Itemize()) as itemize:\nitemize.add_item(escape_latex(f\"FastEstimator {fe.__version__}\"))\nitemize.add_item(escape_latex(f\"Python {platform.python_version()}\"))\nitemize.add_item(escape_latex(f\"OS: {sys.platform}\"))\nitemize.add_item(f\"Number of GPUs: {torch.cuda.device_count()}\")\nif fe.fe_deterministic_seed is not None:\nitemize.add_item(escape_latex(f\"Deterministic Seed: {fe.fe_deterministic_seed}\"))\nwith self.doc.create(LongTable('|lr|', pos=['h!'], booktabs=True)) as tabular:\ntabular.add_row((bold(\"Module\"), bold(\"Version\")))\ntabular.add_hline()\ntabular.end_table_header()\ntabular.add_hline()\ntabular.add_row((MultiColumn(2, align='r', data='Continued on Next Page'), ))\ntabular.add_hline()\ntabular.end_table_footer()\ntabular.end_table_last_footer()\ncolor = True\nfor name, module in humansorted(sys.modules.items(), key=lambda x: x[0]):\nif \".\" in name:\ncontinue  # Skip sub-packages\nif name.startswith(\"_\"):\ncontinue  # Skip private packages\nif isinstance(module, Base):\ncontinue  # Skip fake packages we mocked\nif hasattr(module, '__version__'):\ntabular.add_row((escape_latex(name), escape_latex(str(module.__version__))),\ncolor='black!5' if color else 'white')\ncolor = not color\nelif hasattr(module, 'VERSION'):\ntabular.add_row((escape_latex(name), escape_latex(str(module.VERSION))),\ncolor='black!5' if color else 'white')\ncolor = not color\ndef _draw_diagram(self, mode: str, epoch: int, ds_id: str) -&gt; pydot.Dot:\n\"\"\"Draw a summary diagram of the FastEstimator Ops / Traces.\n        Args:\n            mode: The execution mode to summarize ('train', 'eval', 'test', or 'infer').\n            epoch: The epoch to summarize.\n            ds_id: The ds_id to summarize.\n        Returns:\n            A pydot digraph representing the execution flow.\n        \"\"\"\nds = self.system.pipeline.data[mode][ds_id]\nif isinstance(ds, Scheduler):\nds = ds.get_current_value(epoch)\npipe_ops = get_current_items(self.system.pipeline.ops, run_modes=mode, epoch=epoch, ds_id=ds_id) if isinstance(\nds, Dataset) else []\nnet_ops = get_current_items(self.system.network.ops, run_modes=mode, epoch=epoch, ds_id=ds_id)\nnet_post = get_current_items(self.system.network.postprocessing, run_modes=mode, epoch=epoch, ds_id=ds_id)\ntraces = sort_traces(get_current_items(self.system.traces, run_modes=mode, epoch=epoch, ds_id=ds_id),\nds_ids=self.system.pipeline.get_ds_ids(epoch=epoch, mode=mode))\ndiagram = pydot.Dot(compound='true')  # Compound lets you draw edges which terminate at sub-graphs\ndiagram.set('rankdir', 'TB')\ndiagram.set('dpi', 300)\ndiagram.set_node_defaults(shape='box')\n# Make the dataset the first of the pipeline ops\npipe_ops.insert(0, ds)\nlabel_last_seen = DefaultKeyDict(lambda k: str(id(ds)))  # Where was this key last generated\nbatch_size = \"\"\nif isinstance(ds, Dataset):\nif hasattr(ds, \"fe_batch\") and ds.fe_batch:\nbatch_size = ds.fe_batch\nelse:\nbatch_size = self.system.pipeline.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\nif isinstance(batch_size, dict):\nbatch_size = batch_size[mode]\nif batch_size is not None:\nbatch_size = f\" (Batch Size: {batch_size})\"\nself._draw_subgraph(diagram, diagram, label_last_seen, f'Pipeline{batch_size}', pipe_ops, ds_id)\nself._draw_subgraph(diagram, diagram, label_last_seen, 'Network', net_ops + net_post, ds_id)\nself._draw_subgraph(diagram, diagram, label_last_seen, 'Traces', traces, ds_id)\nreturn diagram\ndef _draw_subgraph(self,\nprogenitor: pydot.Dot,\ndiagram: Union[pydot.Dot, pydot.Cluster],\nlabel_last_seen: DefaultKeyDict[str, str],\nsubgraph_name: str,\nsubgraph_ops: List[Union[Op, Trace, Any]],\nds_id: Optional[str]) -&gt; None:\n\"\"\"Draw a subgraph of ops into an existing `diagram`.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            diagram: The diagram into which to add new Nodes.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n            subgraph_name: The name to be associated with this subgraph.\n            subgraph_ops: The ops to be wrapped in this subgraph.\n            ds_id: The ds_id to be associated with this subgraph.\n        \"\"\"\nsubgraph = pydot.Cluster(style='dashed', graph_name=subgraph_name, color='black')\nsubgraph.set('label', subgraph_name)\nsubgraph.set('labeljust', 'l')\nfor idx, op in enumerate(subgraph_ops):\nnode_id = str(id(op))\nself._add_node(progenitor, subgraph, op, label_last_seen, ds_id)\nif isinstance(op, Trace) and idx &gt; 0:\n# Invisibly connect traces in order so that they aren't all just squashed horizontally into the image\nprogenitor.add_edge(pydot.Edge(src=str(id(subgraph_ops[idx - 1])), dst=node_id, style='invis'))\ndiagram.add_subgraph(subgraph)\ndef _add_node(self,\nprogenitor: pydot.Dot,\ndiagram: Union[pydot.Dot, pydot.Cluster],\nop: Union[Op, Trace, Any],\nlabel_last_seen: DefaultKeyDict[str, str],\nds_id: Optional[str],\nedges: bool = True) -&gt; None:\n\"\"\"Draw a node onto a diagram based on a given op.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            diagram: The diagram to be appended to.\n            op: The op (or trace) to be visualized.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n            ds_id: The ds_id under which the node is currently running.\n            edges: Whether to write Edges to/from this Node.\n        \"\"\"\nnode_id = str(id(op))\nif isinstance(op, (Sometimes, SometimesT)) and op.op:\nwrapper = pydot.Cluster(style='dotted', color='red', graph_name=str(id(op)))\nwrapper.set('label', f'Sometimes ({op.prob}):')\nwrapper.set('labeljust', 'l')\nedge_srcs = defaultdict(lambda: [])\nif op.extra_inputs:\nfor inp in op.extra_inputs:\nif inp == '*':\ncontinue\nedge_srcs[label_last_seen[inp]].append(inp)\nself._add_node(progenitor, wrapper, op.op, label_last_seen, ds_id)\ndiagram.add_subgraph(wrapper)\ndst_id = self._get_all_nodes(wrapper)[0].get_name()\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(\npydot.Edge(src=src, dst=dst_id, lhead=wrapper.get_name(), label=f\" {', '.join(labels)} \"))\nelif isinstance(op, (OneOf, OneOfT)) and op.ops:\nwrapper = pydot.Cluster(style='dotted', color='darkorchid4', graph_name=str(id(op)))\nwrapper.set('label', 'One Of:')\nwrapper.set('labeljust', 'l')\nself._add_node(progenitor, wrapper, op.ops[0], label_last_seen, ds_id, edges=True)\nfor sub_op in op.ops[1:]:\nself._add_node(progenitor, wrapper, sub_op, label_last_seen, ds_id, edges=False)\ndiagram.add_subgraph(wrapper)\nelif isinstance(op, (Fuse, FuseT)) and op.ops:\nself._draw_subgraph(progenitor, diagram, label_last_seen, 'Fuse:', op.ops, ds_id)\nelif isinstance(op, (Repeat, RepeatT)) and op.op:\nwrapper = pydot.Cluster(style='dotted', color='darkgreen', graph_name=str(id(op)))\nwrapper.set('label', f'Repeat:')\nwrapper.set('labeljust', 'l')\nwrapper.add_node(\npydot.Node(node_id,\nlabel=f'{op.repeat if isinstance(op.repeat, int) else \"?\"}',\nshape='doublecircle',\nwidth=0.1))\n# dot2tex doesn't seem to handle edge color conversion correctly, so have to set hex color\nprogenitor.add_edge(pydot.Edge(src=node_id + \":ne\", dst=node_id + \":w\", color='#006300'))\nself._add_node(progenitor, wrapper, op.op, label_last_seen, ds_id)\n# Add repeat edges\nedge_srcs = defaultdict(lambda: [])\nfor out in op.outputs:\nif out in op.inputs and out not in op.repeat_inputs:\nedge_srcs[label_last_seen[out]].append(out)\nfor inp in op.repeat_inputs:\nedge_srcs[label_last_seen[inp]].append(inp)\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(pydot.Edge(src=src, dst=node_id, constraint=False, label=f\" {', '.join(labels)} \"))\ndiagram.add_subgraph(wrapper)\nelse:\nif isinstance(op, ModelOp):\nlabel = f\"{op.__class__.__name__} ({FEID(id(op))}): {op.model.model_name}\"\nmodel_ref = Hyperref(Marker(name=str(op.model.model_name), prefix='subsec'),\ntext=NoEscape(r'\\textcolor{blue}{') + bold(op.model.model_name) +\nNoEscape('}')).dumps()\ntexlbl = f\"{HrefFEID(FEID(id(op)), name=op.__class__.__name__).dumps()}: {model_ref}\"\nelif isinstance(op, Batch):\nlabel = f\"{op.__class__.__name__} ({FEID(id(op))})\"\ntexlbl = HrefFEID(FEID(id(op)), name=op.__class__.__name__, color='purple').dumps()\nif op.batch_size is not None:\ndiagram.set_label(f\"Pipeline (Batch Size: {op.batch_size})\")\nlabel_last_seen.factory = functools.partial(self._delayed_edge,\nprogenitor=progenitor,\nold_source=label_last_seen.factory(''),\nnew_source=str(id(op)))\nelse:\nlabel = f\"{op.__class__.__name__} ({FEID(id(op))})\"\ntexlbl = HrefFEID(FEID(id(op)), name=op.__class__.__name__).dumps()\ndiagram.add_node(pydot.Node(node_id, label=label, texlbl=texlbl))\nif isinstance(op, (Op, Trace)) and edges:\n# Need the instance check since subgraph_ops might contain a tf dataset or torch data loader\nself._add_edge(progenitor, op, label_last_seen, ds_id)\n@staticmethod\ndef _delayed_edge(key: str, progenitor: pydot.Dot, old_source: str, new_source: str) -&gt; str:\n\"\"\"Draw a specific edge between two nodes, modifying the old label if applicable.\n        Args:\n            key: The key associated with the edge.\n            progenitor: The parent cluster.\n            old_source: The edge source.\n            new_source: The edge sync.\n        Returns:\n            The `new_source`.\n        \"\"\"\nedge = progenitor.get_edge(old_source, new_source)\nif edge:\nedge = edge[0]\nlabel = f\"{edge.get_label()}, {key}\"\nedge.set_label(label)\nelse:\nprogenitor.add_edge(pydot.Edge(src=old_source, dst=new_source, label=f\" {key}\"))\nreturn new_source\ndef _add_edge(self,\nprogenitor: pydot.Dot,\nop: Union[Trace, Op],\nlabel_last_seen: Dict[str, str],\nds_id: Optional[str]):\n\"\"\"Draw edges into a given Node.\n        Args:\n            progenitor: The very top level diagram onto which Edges should be written.\n            op: The op (or trace) to be visualized.\n            label_last_seen: A mapping of {data_dict_key: node_id} indicating the last node which generated the key.\n            ds_id: The ds_id under which the node is currently running.\n        \"\"\"\nnode_id = str(id(op))\nedge_srcs = defaultdict(lambda: [])\nglobal_ds_ids = {key for vals in self.system.pipeline.data.values() for key in vals.keys() if key is not None}\nfor inp in label_last_seen.keys() if isinstance(op, Batch) else op.inputs:\nif inp == '*':\ncontinue\n_, candidate_id, *_ = f\"{inp}|\".split('|')\nif candidate_id in global_ds_ids and candidate_id != ds_id:\ncontinue  # Skip inputs which will be provided in other ds_id plots\nedge_srcs[label_last_seen[inp]].append(inp)\nfor src, labels in edge_srcs.items():\nprogenitor.add_edge(pydot.Edge(src=src, dst=node_id, label=f\" {', '.join(labels)} \"))\noutputs = op.get_outputs(ds_ids=ds_id) if isinstance(op, Trace) else op.outputs\nfor out in label_last_seen.keys() if isinstance(op, Batch) else outputs:\nlabel_last_seen[out] = node_id\n@staticmethod\ndef _get_all_nodes(diagram: Union[pydot.Dot, pydot.Cluster]) -&gt; List[pydot.Node]:\n\"\"\"Recursively search through a `diagram` looking for Nodes.\n        Args:\n            diagram: The diagram to be inspected.\n        Returns:\n            All of the Nodes available within this diagram and its child diagrams.\n        \"\"\"\nnodes = diagram.get_nodes()\nfor subgraph in diagram.get_subgraphs():\nnodes.extend(Traceability._get_all_nodes(subgraph))\nreturn nodes\n@staticmethod\ndef _init_document_geometry() -&gt; Document:\n\"\"\"Init geometry setting of the document.\n        Return:\n            Initialized Document object.\n        \"\"\"\nreturn Document(geometry_options=['lmargin=2cm', 'rmargin=2cm', 'bmargin=2cm'])\n</code></pre>"}, {"location": "fastestimator/trace/meta/_per_ds.html", "title": "_per_ds", "text": ""}, {"location": "fastestimator/trace/meta/_per_ds.html#fastestimator.fastestimator.trace.meta._per_ds.per_ds", "title": "<code>per_ds</code>", "text": "<p>A class annotation which will convert regular traces into dataset-sensitive traces.</p> <p>Parameters:</p> Name Type Description Default <code>clz</code> <code>type(Trace)</code> <p>The base class to be converted.</p> required <p>Returns:</p> Type Description <p>A dataset aware version of the class. Note that if the annotated class instance has a 'per_ds' member variable</p> <p>which is set to False, or has outputs containing the '|' character, then a normal (non-ds-aware) instance will</p> <p>be returned instead.</p> Source code in <code>fastestimator\\fastestimator\\trace\\meta\\_per_ds.py</code> <pre><code>def per_ds(clz: type(Trace)):\n\"\"\"A class annotation which will convert regular traces into dataset-sensitive traces.\n    Args:\n        clz: The base class to be converted.\n    Returns:\n        A dataset aware version of the class. Note that if the annotated class instance has a 'per_ds' member variable\n        which is set to False, or has outputs containing the '|' character, then a normal (non-ds-aware) instance will\n        be returned instead.\n    \"\"\"\nclass PerDS(clz, PerDSTrace):\n@functools.wraps(clz.__new__)\ndef __new__(cls, *args, **kwargs):\n# We will dynamically determine whether to return a base object or a PerDS variant\n# If any of the outputs already use the | character then we cannot make this a PerDS variant\nbase_obj = clz.__new__(clz)\nbase_obj.__init__(*args, **kwargs)\nfor output in base_obj.outputs:\nif '|' in output:\nreturn base_obj\n# If the user set per_ds to False in the constructor then we will not make this a PerDS variant\nif hasattr(base_obj, 'per_ds') and base_obj.per_ds is False:\nreturn base_obj\n# Otherwise we are good to go with the PerDS variant\nreturn super().__new__(cls)\n@functools.wraps(clz.__init__)\ndef __init__(self, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.fe_per_ds_trace = clz.__new__(clz)\nself.fe_per_ds_trace.__init__(*args, **kwargs)\ndef get_outputs(self, ds_ids: Union[None, str, List[str]]) -&gt; List[str]:\nds_ids = to_list(ds_ids)\noutputs = list(self.outputs)\nfor output in self.outputs:\nfor ds_id in ds_ids:\noutputs.append(f\"{output}|{ds_id}\")\nreturn outputs\ndef on_begin(self, data: Data) -&gt; None:\nsuper().on_begin(data)\nself.fe_per_ds_trace.on_begin(data)\ndef on_ds_begin(self, data: Data) -&gt; None:\nif self.system.ds_id != '':\nself.fe_per_ds_trace.on_epoch_begin(DSData(self.system.ds_id, data))\ndef on_batch_begin(self, data: Data) -&gt; None:\nsuper().on_batch_begin(data)\nif self.system.ds_id != '':\nself.fe_per_ds_trace.on_batch_begin(DSData(self.system.ds_id, data))\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.ds_id != '':\nself.fe_per_ds_trace.on_batch_end(DSData(self.system.ds_id, data))\n# Block the main process from writing per-instance info since we already have the more detailed key\ndata.per_instance_enabled = False\nsuper().on_batch_end(data)\ndata.per_instance_enabled = True\ndef on_ds_end(self, data: Data) -&gt; None:\nif self.system.ds_id != '':\nself.fe_per_ds_trace.on_epoch_end(DSData(self.system.ds_id, data))\ndef on_end(self, data: Data) -&gt; None:\nsuper().on_end(data)\nself.fe_per_ds_trace.on_end(data)\nPerDS.__name__ = clz.__name__\nPerDS.__qualname__ = clz.__qualname__\nPerDS.__module__ = clz.__module__\nPerDS.__doc__ = clz.__doc__  # We want to preserve the docstring of the original class\nreturn PerDS\n</code></pre>"}, {"location": "fastestimator/trace/metric/accuracy.html", "title": "accuracy", "text": ""}, {"location": "fastestimator/trace/metric/accuracy.html#fastestimator.fastestimator.trace.metric.accuracy.Accuracy", "title": "<code>Accuracy</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the accuracy for a given set of predictions.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'accuracy'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\accuracy.py</code> <pre><code>@per_ds\n@traceable()\nclass Accuracy(Trace):\n\"\"\"A trace which computes the accuracy for a given set of predictions.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        output_name: What to call the output from this trace (for example in the logger output).\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\nfrom_logits: bool = False,\noutput_name: str = \"accuracy\",\nper_ds: bool = True) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name, ds_id=ds_id)\nself.from_logits = from_logits\nself.total = 0\nself.correct = 0\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.total = 0\nself.correct = 0\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:  # binaray classification (pred shape is [batch, 1])\nif self.from_logits:\ny_pred = 1 / (1 + np.exp(-y_pred))\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.correct += np.sum(y_pred.ravel() == y_true.ravel())\nself.total += len(y_pred.ravel())\ndata.write_per_instance_log(self.outputs[0], np.array(y_pred.ravel() == y_true.ravel(), dtype=np.int8))\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.correct / self.total)\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html", "title": "bleu_score", "text": ""}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore", "title": "<code>BleuScore</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate the Bleu score for a nlp task and report it back to the logger.</p> <p>Calculate BLEU score (Bilingual Evaluation Understudy) from Papineni, Kishore, Salim Roukos, Todd Ward, and    Wei-Jing Zhu. 2002.\"BLEU: a method for automatic evaluation of machine translation.\"In Proceedings of ACL.    https://www.aclweb.org/anthology/P02-1040.pdf</p> <p>The BLEU metric scores a translation on a scale of 0 to 1, in an attempt to measure the adequacy and fluency of    the Machine Translation output. The closer to 1 the test sentences score, the more overlap there is with their    human reference translations and thus, the better the system is deemed to be. The MT output would score 1 only    if it is identical to the reference human translation. But even two competent human translations of the exact    same material may only score in the 0.6 or 0.7 range as they are likely to use different vocabulary and phrasing.    We should be wary of very high BLEU scores (in excess of 0.7) as it is probably measuring improperly or overfitting.</p> <p>The default BLEU calculates a score for up to 4-grams using uniform weights (this is called BLEU-4). To evaluate    your translations with lower order ngrams, use customized \"n_gram\". E.g. when accounting for up to 2-grams    with uniform weights (this is called BLEU-2) use n_gram=2.</p> <p>If there is no ngrams overlap for any order of n-grams, BLEU returns the value 0. This is because the precision    for the order of n-grams withoutoverlap is 0, and the geometric mean in the final BLEU score computation multiplies    the 0 with the precision of other n-grams. This results in 0. Shorter translations may have inflated precision values due to having    smaller denominators; therefore, we give them proportionally smaller smoothed counts. Instead of scaling to 1/(2^k),    Chen and Cherry suggests dividing by 1/ln(len(T)), where T is the length of the translation.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>Name of the key to store back to the state.</p> <code>'bleu_score'</code> <code>n_gram</code> <code>int</code> <p>Number of grams used to calculate bleu score.</p> <code>4</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>@per_ds\n@traceable()\nclass BleuScore(Trace):\n\"\"\"Calculate the Bleu score for a nlp task and report it back to the logger.\n       Calculate BLEU score (Bilingual Evaluation Understudy) from Papineni, Kishore, Salim Roukos, Todd Ward, and\n       Wei-Jing Zhu. 2002.\"BLEU: a method for automatic evaluation of machine translation.\"In Proceedings of ACL.\n       https://www.aclweb.org/anthology/P02-1040.pdf\n       The BLEU metric scores a translation on a scale of 0 to 1, in an attempt to measure the adequacy and fluency of\n       the Machine Translation output. The closer to 1 the test sentences score, the more overlap there is with their\n       human reference translations and thus, the better the system is deemed to be. The MT output would score 1 only\n       if it is identical to the reference human translation. But even two competent human translations of the exact\n       same material may only score in the 0.6 or 0.7 range as they are likely to use different vocabulary and phrasing.\n       We should be wary of very high BLEU scores (in excess of 0.7) as it is probably measuring improperly or overfitting.\n       The default BLEU calculates a score for up to 4-grams using uniform weights (this is called BLEU-4). To evaluate\n       your translations with lower order ngrams, use customized \"n_gram\". E.g. when accounting for up to 2-grams\n       with uniform weights (this is called BLEU-2) use n_gram=2.\n       If there is no ngrams overlap for any order of n-grams, BLEU returns the value 0. This is because the precision\n       for the order of n-grams withoutoverlap is 0, and the geometric mean in the final BLEU score computation multiplies\n       the 0 with the precision of other n-grams. This results in 0. Shorter translations may have inflated precision values due to having\n       smaller denominators; therefore, we give them proportionally smaller smoothed counts. Instead of scaling to 1/(2^k),\n       Chen and Cherry suggests dividing by 1/ln(len(T)), where T is the length of the translation.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: Name of the key to store back to the state.\n        n_gram: Number of grams used to calculate bleu score.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = 'bleu_score',\nn_gram: int = 4,\nper_ds: bool = True) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nself.n_gram = n_gram\nself.weights = self.get_output_weights()\nself.per_ds = per_ds\nself.smoothing_function = SmoothingFunction().method4\nself.no_of_correct_predicted = Counter()\nself.no_of_total_predicted = Counter()\nself.total_hypotheses_length = 0\nself.total_references_length = 0\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef get_output_weights(self) -&gt; Tuple[float, ...]:\n\"\"\"\n            Generate weights tuple based on n_gram.\n            Returns:\n                Tuple of n_gram weights\n            Raises:\n                ValueError: When n_gram provided is less than or equal to 0..\n        \"\"\"\nif self.n_gram &gt; 0:\nreturn (1 / self.n_gram, ) * self.n_gram\nelse:\nraise ValueError(\"N Gram should be a positive integer.\")\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.no_of_correct_predicted = Counter()\nself.no_of_total_predicted = Counter()\nself.total_hypotheses_length = 0\nself.total_references_length = 0\ndef get_brevity_penalty(self) -&gt; float:\n\"\"\"\n            Calculate the brevity penalty of the corpus.\n            Returns:\n                Brevity penalty for corpus.\n        \"\"\"\nreturn brevity_penalty(self.total_references_length, self.total_hypotheses_length)\ndef get_smoothened_modified_precision(self) -&gt; List[float]:\n\"\"\"\n            Calculate the smoothened modified precision.\n            Returns:\n                List of smoothened modified precision of n_grams.\n        \"\"\"\n# Collects the various precision values for the different ngram orders.\np_n = [\nFraction(self.no_of_correct_predicted[i], self.no_of_total_predicted[i], _normalize=False)\nfor i in range(1, self.n_gram + 1)\n]\n# Smoothen the modified precision.\nreturn self.smoothing_function(p_n, [], [], hyp_len=self.total_hypotheses_length)\ndef get_corpus_bleu_score(self) -&gt; float:\n\"\"\"\n            Calculate the bleu score using corpus level brevity penalty and geometric average precision.\n            Returns:\n                Corpus level bleu score.\n        \"\"\"\n# Calculate corpus-level brevity penalty.\nbp = self.get_brevity_penalty()\n# Returns 0 if there's no matching 1-gram\nif self.no_of_correct_predicted[1] == 0:\nreturn 0\nn_gram_precision = self.get_smoothened_modified_precision()\ngeometric_average_precision = math.exp(\nmath.fsum((w_i * math.log(p_i) for w_i, p_i in zip(self.weights, n_gram_precision) if p_i &gt; 0)))\nbleu_score = bp * geometric_average_precision\nreturn bleu_score\ndef batch_precision_parameters(self, references: List[np.ndarray], hypotheses: List[np.ndarray]) -&gt; List[float]:\n\"\"\"\n            Calculate modified precision per n_gram for input references and hypotheses combinations.\n            Args:\n                references: Ground truth sentences.\n                hypotheses: Predicted sentences.\n            Returns:\n                List of sentence level bleu scores\n        \"\"\"\nassert len(references) == len(hypotheses), (\n\"The number of hypotheses and their reference(s) should be the same \")\nsentence_level_scores = []\n# Iterate through each hypothesis and their corresponding references.\nfor reference, hypothesis in zip(references, hypotheses):\n# For each order of ngram, calculate the correct predicted words and\n# total predicted words for the corpus-level modified precision.\nreference = get_formated_reference(reference)\nhypothesis = get_formated_list(hypothesis)\nfor i in range(1, self.n_gram + 1):\np_i = modified_precision(reference, hypothesis, i)\nself.no_of_correct_predicted[i] += p_i.numerator\nself.no_of_total_predicted[i] += p_i.denominator\nsentence_level_scores.append(sentence_bleu(reference, hypothesis, self.weights, self.smoothing_function))\n# Calculate the hypothesis length and the closest reference length.\n# Adds them to the corpus-level hypothesis and reference counts.\nhyp_len = len(hypothesis)\nself.total_hypotheses_length += hyp_len\nref_lens = (len(ref) for ref in reference)\nself.total_references_length += min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\nreturn sentence_level_scores\ndef on_batch_end(self, data: Data) -&gt; None:\ny_pred, y_true = to_number(data['pred']), to_number(data['target_real'])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 2:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 2:\ny_pred = np.argmax(y_pred, axis=-1)\nsentence_level_scores = self.batch_precision_parameters(y_true, y_pred)\ndata.write_per_instance_log(self.outputs[0], sentence_level_scores)\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], round(self.get_corpus_bleu_score(), 5))\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore.batch_precision_parameters", "title": "<code>batch_precision_parameters</code>", "text": "<p>Calculate modified precision per n_gram for input references and hypotheses combinations.</p> <p>Parameters:</p> Name Type Description Default <code>references</code> <code>List[np.ndarray]</code> <p>Ground truth sentences.</p> required <code>hypotheses</code> <code>List[np.ndarray]</code> <p>Predicted sentences.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List of sentence level bleu scores</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def batch_precision_parameters(self, references: List[np.ndarray], hypotheses: List[np.ndarray]) -&gt; List[float]:\n\"\"\"\n        Calculate modified precision per n_gram for input references and hypotheses combinations.\n        Args:\n            references: Ground truth sentences.\n            hypotheses: Predicted sentences.\n        Returns:\n            List of sentence level bleu scores\n    \"\"\"\nassert len(references) == len(hypotheses), (\n\"The number of hypotheses and their reference(s) should be the same \")\nsentence_level_scores = []\n# Iterate through each hypothesis and their corresponding references.\nfor reference, hypothesis in zip(references, hypotheses):\n# For each order of ngram, calculate the correct predicted words and\n# total predicted words for the corpus-level modified precision.\nreference = get_formated_reference(reference)\nhypothesis = get_formated_list(hypothesis)\nfor i in range(1, self.n_gram + 1):\np_i = modified_precision(reference, hypothesis, i)\nself.no_of_correct_predicted[i] += p_i.numerator\nself.no_of_total_predicted[i] += p_i.denominator\nsentence_level_scores.append(sentence_bleu(reference, hypothesis, self.weights, self.smoothing_function))\n# Calculate the hypothesis length and the closest reference length.\n# Adds them to the corpus-level hypothesis and reference counts.\nhyp_len = len(hypothesis)\nself.total_hypotheses_length += hyp_len\nref_lens = (len(ref) for ref in reference)\nself.total_references_length += min(ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len))\nreturn sentence_level_scores\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore.get_brevity_penalty", "title": "<code>get_brevity_penalty</code>", "text": "<p>Calculate the brevity penalty of the corpus.</p> <p>Returns:</p> Type Description <code>float</code> <p>Brevity penalty for corpus.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_brevity_penalty(self) -&gt; float:\n\"\"\"\n        Calculate the brevity penalty of the corpus.\n        Returns:\n            Brevity penalty for corpus.\n    \"\"\"\nreturn brevity_penalty(self.total_references_length, self.total_hypotheses_length)\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore.get_corpus_bleu_score", "title": "<code>get_corpus_bleu_score</code>", "text": "<p>Calculate the bleu score using corpus level brevity penalty and geometric average precision.</p> <p>Returns:</p> Type Description <code>float</code> <p>Corpus level bleu score.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_corpus_bleu_score(self) -&gt; float:\n\"\"\"\n        Calculate the bleu score using corpus level brevity penalty and geometric average precision.\n        Returns:\n            Corpus level bleu score.\n    \"\"\"\n# Calculate corpus-level brevity penalty.\nbp = self.get_brevity_penalty()\n# Returns 0 if there's no matching 1-gram\nif self.no_of_correct_predicted[1] == 0:\nreturn 0\nn_gram_precision = self.get_smoothened_modified_precision()\ngeometric_average_precision = math.exp(\nmath.fsum((w_i * math.log(p_i) for w_i, p_i in zip(self.weights, n_gram_precision) if p_i &gt; 0)))\nbleu_score = bp * geometric_average_precision\nreturn bleu_score\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore.get_output_weights", "title": "<code>get_output_weights</code>", "text": "<p>Generate weights tuple based on n_gram.</p> <p>Returns:</p> Type Description <code>Tuple[float, ...]</code> <p>Tuple of n_gram weights</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When n_gram provided is less than or equal to 0..</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_output_weights(self) -&gt; Tuple[float, ...]:\n\"\"\"\n        Generate weights tuple based on n_gram.\n        Returns:\n            Tuple of n_gram weights\n        Raises:\n            ValueError: When n_gram provided is less than or equal to 0..\n    \"\"\"\nif self.n_gram &gt; 0:\nreturn (1 / self.n_gram, ) * self.n_gram\nelse:\nraise ValueError(\"N Gram should be a positive integer.\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.BleuScore.get_smoothened_modified_precision", "title": "<code>get_smoothened_modified_precision</code>", "text": "<p>Calculate the smoothened modified precision.</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>List of smoothened modified precision of n_grams.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_smoothened_modified_precision(self) -&gt; List[float]:\n\"\"\"\n        Calculate the smoothened modified precision.\n        Returns:\n            List of smoothened modified precision of n_grams.\n    \"\"\"\n# Collects the various precision values for the different ngram orders.\np_n = [\nFraction(self.no_of_correct_predicted[i], self.no_of_total_predicted[i], _normalize=False)\nfor i in range(1, self.n_gram + 1)\n]\n# Smoothen the modified precision.\nreturn self.smoothing_function(p_n, [], [], hyp_len=self.total_hypotheses_length)\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.get_formated_list", "title": "<code>get_formated_list</code>", "text": "<p>Filter the padding(elements with 0 value) and typecast the elements of list to str.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Formated list.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_formated_list(input_data: np.ndarray) -&gt; List[str]:\n\"\"\"\n        Filter the padding(elements with 0 value) and typecast the elements of list to str.\n        Returns:\n            Formated list.\n    \"\"\"\nreturn [str(i) for i in input_data if i != 0]\n</code></pre>"}, {"location": "fastestimator/trace/metric/bleu_score.html#fastestimator.fastestimator.trace.metric.bleu_score.get_formated_reference", "title": "<code>get_formated_reference</code>", "text": "<p>Encapsulate formated list in another list.</p> <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List encapsulated formated list.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\bleu_score.py</code> <pre><code>def get_formated_reference(input_data: np.ndarray) -&gt; List[List[str]]:\n\"\"\"\n        Encapsulate formated list in another list.\n        Returns:\n            List encapsulated formated list.\n    \"\"\"\nreturn [get_formated_list(input_data)]\n</code></pre>"}, {"location": "fastestimator/trace/metric/calibration_error.html", "title": "calibration_error", "text": ""}, {"location": "fastestimator/trace/metric/calibration_error.html#fastestimator.fastestimator.trace.metric.calibration_error.CalibrationError", "title": "<code>CalibrationError</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the calibration error for a given set of predictions.</p> <p>Unlike many common calibration error estimation algorithms, this one has actual theoretical bounds on the quality of its output: https://arxiv.org/pdf/1909.10155v1.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'calibration_error'</code> <code>method</code> <code>str</code> <p>Either 'marginal' or 'top-label'. 'marginal' calibration averages the calibration error over each class, whereas 'top-label' computes the error based on only the most confident predictions.</p> <code>'marginal'</code> <code>confidence_interval</code> <code>Optional[int]</code> <p>The calibration error confidence interval to be reported (estimated empirically). Should be in the range (0, 100), or else None to omit this extra calculation.</p> <code>None</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\calibration_error.py</code> <pre><code>@per_ds\n@traceable()\nclass CalibrationError(Trace):\n\"\"\"A trace which computes the calibration error for a given set of predictions.\n    Unlike many common calibration error estimation algorithms, this one has actual theoretical bounds on the quality\n    of its output: https://arxiv.org/pdf/1909.10155v1.pdf.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: What to call the output from this trace (for example in the logger output).\n        method: Either 'marginal' or 'top-label'. 'marginal' calibration averages the calibration error over each class,\n            whereas 'top-label' computes the error based on only the most confident predictions.\n        confidence_interval: The calibration error confidence interval to be reported (estimated empirically). Should be\n            in the range (0, 100), or else None to omit this extra calculation.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"calibration_error\",\nmethod: str = \"marginal\",\nconfidence_interval: Optional[int] = None,\nper_ds: bool = True):\nself.y_true = []\nself.y_pred = []\nassert method in ('marginal', 'top-label'), \\\n            f\"CalibrationError 'method' must be either 'marginal' or 'top-label', but got {method}.\"\nself.method = method\nif confidence_interval is not None:\nassert 0 &lt; confidence_interval &lt; 100, \\\n                f\"CalibrationError 'confidence_interval' must be between 0 and 100, but got {confidence_interval}.\"\nconfidence_interval = 1.0 - confidence_interval / 100.0\nself.confidence_interval = confidence_interval\nsuper().__init__(inputs=[true_key, pred_key], outputs=output_name, mode=mode, ds_id=ds_id)\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nassert y_pred.shape[0] == y_true.shape[0]\nself.y_true.extend(y_true)\nself.y_pred.extend(y_pred)\ndef on_epoch_end(self, data: Data) -&gt; None:\nself.y_true = np.squeeze(np.stack(self.y_true))\nself.y_pred = np.stack(self.y_pred)\nmid = round(cal.get_calibration_error(probs=self.y_pred, labels=self.y_true, mode=self.method), 4)\nlow = None\nhigh = None\nif self.confidence_interval is not None:\nlow, _, high = cal.get_calibration_error_uncertainties(probs=self.y_pred, labels=self.y_true,\nmode=self.method,\nalpha=self.confidence_interval)\nlow = round(low, 4)\nhigh = round(high, 4)\ndata.write_with_log(self.outputs[0], ValWithError(low, mid, high) if low is not None else mid)\n</code></pre>"}, {"location": "fastestimator/trace/metric/confusion_matrix.html", "title": "confusion_matrix", "text": ""}, {"location": "fastestimator/trace/metric/confusion_matrix.html#fastestimator.fastestimator.trace.metric.confusion_matrix.ConfusionMatrix", "title": "<code>ConfusionMatrix</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes the confusion matrix between y_true (rows) and y_predicted (columns).</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes of the confusion matrix.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'confusion_matrix'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments that pass to sklearn.metrics.confusion_matrix()</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_pred\", \"y_true\", \"labels\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\confusion_matrix.py</code> <pre><code>@per_ds\n@traceable()\nclass ConfusionMatrix(Trace):\n\"\"\"Computes the confusion matrix between y_true (rows) and y_predicted (columns).\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        num_classes: Total number of classes of the confusion matrix.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: Name of the key to store to the state.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n        **kwargs: Additional keyword arguments that pass to sklearn.metrics.confusion_matrix()\n    Raises:\n        ValueError: One of [\"y_pred\", \"y_true\", \"labels\"] argument exists in `kwargs`.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nnum_classes: int,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"confusion_matrix\",\nper_ds: bool = True,\n**kwargs) -&gt; None:\nConfusionMatrix.check_kwargs(kwargs)\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nself.num_classes = num_classes\nself.matrix = None\nself.kwargs = kwargs\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.matrix = None\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nbatch_confusion = confusion_matrix(y_true, y_pred, labels=list(range(0, self.num_classes)), **self.kwargs)\nif self.matrix is None:\nself.matrix = batch_confusion\nelse:\nself.matrix += batch_confusion\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.matrix)\n@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n        Args:\n            kwargs: Keywork arguments to be examined.\n        Raises:\n            ValueError: One of [\"y_pred\", \"y_true\", \"labels\"] argument exists in `kwargs`.\n        \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"labels\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.confusion_matrix()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/confusion_matrix.html#fastestimator.fastestimator.trace.metric.confusion_matrix.ConfusionMatrix.check_kwargs", "title": "<code>check_kwargs</code>  <code>staticmethod</code>", "text": "<p>Check if <code>kwargs</code> has any blacklist argument and raise an error if it does.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keywork arguments to be examined.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_pred\", \"y_true\", \"labels\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\confusion_matrix.py</code> <pre><code>@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n    Args:\n        kwargs: Keywork arguments to be examined.\n    Raises:\n        ValueError: One of [\"y_pred\", \"y_true\", \"labels\"] argument exists in `kwargs`.\n    \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"labels\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.confusion_matrix()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/dice.html", "title": "dice", "text": ""}, {"location": "fastestimator/trace/metric/dice.html#fastestimator.fastestimator.trace.metric.dice.Dice", "title": "<code>Dice</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Dice score for binary classification between y_true and y_predicted.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>The key of the ground truth mask.</p> required <code>pred_key</code> <code>str</code> <p>The key of the prediction values.</p> required <code>threshold</code> <code>float</code> <p>The threshold for binarizing the prediction.</p> <code>0.5</code> <code>channel_average</code> <code>bool</code> <p>Whether the average channel-wise dice loss.</p> <code>False</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'Dice'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\dice.py</code> <pre><code>@per_ds\n@traceable()\nclass Dice(Trace):\n\"\"\"Dice score for binary classification between y_true and y_predicted.\n    Args:\n        true_key: The key of the ground truth mask.\n        pred_key: The key of the prediction values.\n        threshold: The threshold for binarizing the prediction.\n        channel_average: Whether the average channel-wise dice loss.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: What to call the output from this trace (for example in the logger output).\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nthreshold: float = 0.5,\nchannel_average: bool = False,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"Dice\",\nper_ds: bool = True) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key),\nmode=mode, outputs=output_name, ds_id=ds_id)\nself.threshold = threshold\nself.smooth = 1e-8\nself.channel_average = channel_average\nself.dice = []\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.dice = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(\ndata[self.true_key]), to_number(data[self.pred_key])\ny_pred = np.where(y_pred &gt; self.threshold, 1.0,\n0.0).astype(y_pred.dtype)\ndice = dice_score(y_pred=y_pred, y_true=y_true,\nchannel_average=self.channel_average, epsilon=self.smooth)\ndata.write_per_instance_log(self.outputs[0], dice)\nself.dice.extend(list(dice))\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], np.mean(self.dice))\n</code></pre>"}, {"location": "fastestimator/trace/metric/f1_score.html", "title": "f1_score", "text": ""}, {"location": "fastestimator/trace/metric/f1_score.html#fastestimator.fastestimator.trace.metric.f1_score.F1Score", "title": "<code>F1Score</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate the F1 score for a classification task and report it back to the logger.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>Name of the key to store back to the state.</p> <code>'f1_score'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments that pass to sklearn.metrics.f1_score()</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_pred\", \"y_true\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\f1_score.py</code> <pre><code>@per_ds\n@traceable()\nclass F1Score(Trace):\n\"\"\"Calculate the F1 score for a classification task and report it back to the logger.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: Name of the key to store back to the state.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n        **kwargs: Additional keyword arguments that pass to sklearn.metrics.f1_score()\n    Raises:\n        ValueError: One of [\"y_pred\", \"y_true\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"f1_score\",\nper_ds: bool = True,\n**kwargs) -&gt; None:\nF1Score.check_kwargs(kwargs)\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\nself.kwargs = kwargs\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = f1_score(self.y_true, self.y_pred, average='binary', **self.kwargs)\nelse:\nscore = f1_score(self.y_true, self.y_pred, average=None, **self.kwargs)\ndata.write_with_log(self.outputs[0], score)\n@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n        Args:\n            kwargs: Keywork arguments to be examined.\n        Raises:\n            ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n        \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.f1_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/f1_score.html#fastestimator.fastestimator.trace.metric.f1_score.F1Score.check_kwargs", "title": "<code>check_kwargs</code>  <code>staticmethod</code>", "text": "<p>Check if <code>kwargs</code> has any blacklist argument and raise an error if it does.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keywork arguments to be examined.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\f1_score.py</code> <pre><code>@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n    Args:\n        kwargs: Keywork arguments to be examined.\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.f1_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/mcc.html", "title": "mcc", "text": ""}, {"location": "fastestimator/trace/metric/mcc.html#fastestimator.fastestimator.trace.metric.mcc.MCC", "title": "<code>MCC</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the Matthews Correlation Coefficient for a given set of predictions.</p> <p>This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,  a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies anti-correlation (you should invert your classifier predictions in order to do better).</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'mcc'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments that pass to sklearn.metrics.matthews_corrcoef()</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mcc.py</code> <pre><code>@per_ds\n@traceable()\nclass MCC(Trace):\n\"\"\"A trace which computes the Matthews Correlation Coefficient for a given set of predictions.\n    This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does\n    not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,\n     a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies\n    anti-correlation (you should invert your classifier predictions in order to do better).\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: What to call the output from this trace (for example in the logger output).\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n        **kwargs: Additional keyword arguments that pass to sklearn.metrics.matthews_corrcoef()\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\"] argument exists in `kwargs`.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"mcc\",\nper_ds: bool = True,\n**kwargs) -&gt; None:\nMCC.check_kwargs(kwargs)\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name, ds_id=ds_id)\nself.kwargs = kwargs\nself.y_true = []\nself.y_pred = []\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_true.extend(y_true)\nself.y_pred.extend(y_pred)\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], matthews_corrcoef(y_true=self.y_true, y_pred=self.y_pred, **self.kwargs))\n@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n        Args:\n            kwargs: Keywork arguments to be examined.\n        Raises:\n            ValueError: One of [\"y_true\", \"y_pred\"] argument exists in `kwargs`.\n        \"\"\"\nblacklist = [\"y_true\", \"y_pred\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.matthews_corrcoef()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/mcc.html#fastestimator.fastestimator.trace.metric.mcc.MCC.check_kwargs", "title": "<code>check_kwargs</code>  <code>staticmethod</code>", "text": "<p>Check if <code>kwargs</code> has any blacklist argument and raise an error if it does.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keywork arguments to be examined.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mcc.py</code> <pre><code>@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n    Args:\n        kwargs: Keywork arguments to be examined.\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\"] argument exists in `kwargs`.\n    \"\"\"\nblacklist = [\"y_true\", \"y_pred\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.matthews_corrcoef()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html", "title": "mean_average_precision", "text": "<p>COCO Mean average precisin (mAP) implementation.</p>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision", "title": "<code>MeanAveragePrecision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate COCO mean average precision.</p> <p>The value of 'y_pred' has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select is either 0 or 1. The value of 'bbox' has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label].</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> <code>'bbox'</code> <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> <code>'pred'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>num_classes</code> <code>int</code> <p>Maximum <code>int</code> value for your class label. In COCO dataset we only used 80 classes, but the maxium value of the class label is <code>90</code>. In this case <code>num_classes</code> should be <code>90</code>.</p> required <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <p>What to call the outputs from this trace (for example in the logger output).</p> <code>('mAP', 'AP50', 'AP75')</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <p>Returns:</p> Type Description <p>Mean Average Precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>@per_ds\n@traceable()\nclass MeanAveragePrecision(Trace):\n\"\"\"Calculate COCO mean average precision.\n    The value of 'y_pred' has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select\n    is either 0 or 1.\n    The value of 'bbox' has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label].\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        num_classes: Maximum `int` value for your class label. In COCO dataset we only used 80 classes, but the maxium\n            value of the class label is `90`. In this case `num_classes` should be `90`.\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: What to call the outputs from this trace (for example in the logger output).\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n    Returns:\n        Mean Average Precision.\n    \"\"\"\ndef __init__(self,\nnum_classes: int,\ntrue_key='bbox',\npred_key: str = 'pred',\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name=(\"mAP\", \"AP50\", \"AP75\"),\nper_ds: bool = True) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nassert len(self.outputs) == 3, 'MeanAvgPrecision trace adds 3 fields mAP AP50 AP75 to state dict'\nself.iou_thres = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05).astype(np.int) + 1, endpoint=True)\nself.recall_thres = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01).astype(np.int) + 1, endpoint=True)\nself.categories = range(num_classes)\nself.max_detection = 100\nself.image_ids = []\nself.per_ds = per_ds\n# eval\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0  # reset per epoch\n# reset per batch\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\nself.counter = 0  # REMOVE\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef _get_id_in_epoch(self, idx_in_batch: int) -&gt; int:\n\"\"\"Get unique image id in epoch.\n        Id starts from 1.\n        Args:\n            idx_in_batch: Image id within a batch.\n        Returns:\n            Global unique id within one epoch.\n        \"\"\"\n# for this batch\nnum_unique_id_previous = len(np.unique(self.ids_unique))\nself.ids_unique.append(idx_in_batch)\nnum_unique_id = len(np.unique(self.ids_unique))\nif num_unique_id &gt; num_unique_id_previous:\n# for epoch\nself.ids_in_epoch += 1\nself.ids_batch_to_epoch[idx_in_batch] = self.ids_in_epoch\nreturn self.ids_in_epoch\ndef on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\ndef on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n@staticmethod\ndef _reshape_gt(gt_array: np.ndarray) -&gt; np.ndarray:\n\"\"\"Reshape ground truth and add local image id within batch.\n        The input ground truth array has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label] for each\n        bounding box.\n        For output we drop all padded bounding boxes (all zeros), and flatten the batch dimension. The output shape is\n        (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n        Args:\n            gt_array: Ground truth with shape (batch_size, num_bbox, 5).\n        Returns:\n            Ground truth with shape (batch_size * num_bbox, 6).\n        \"\"\"\nlocal_ids = np.repeat(range(gt_array.shape[0]), gt_array.shape[1], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\ngt_with_id = np.concatenate([local_ids, gt_array.reshape(-1, 5)], axis=1)\nkeep = ~np.all(gt_with_id[:, 1:] == 0, axis=1)  # remove rows of all 0 bounding boxes\nreturn gt_with_id[keep]\n@staticmethod\ndef _reshape_pred(pred: List[np.ndarray]) -&gt; np.ndarray:\n\"\"\"Reshape predicted bounding boxes and add local image id within batch.\n        The input pred array has shape [batch, num_box, 7] where 7 is [x1, y1, w, h, label, label_score, select], select\n        is either 0 or 1.\n        For output we flatten the batch dimension. The output shape is (total_num_bbox_in_batch, 7). The 7 is\n        [id_in_batch, x1, y1, w, h, label, score].\n        Args:\n            pred: List of predected bounding boxes for each image. Each element in the list has shape (num_bbox, 6).\n        Returns:\n            Predected bounding boxes with shape (total_num_bbox_in_batch, 7).\n        \"\"\"\npred_with_id = []\nfor id_batch in range(pred.shape[0]):\npred_single = pred[id_batch]\nlocal_ids = np.repeat([id_batch], pred_single.shape[0], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\npred_single = np.concatenate([local_ids, pred_single], axis=1)\npred_with_id.append(pred_single[pred_single[:, -1] &gt; 0, :-1])\npred_with_id = np.concatenate(pred_with_id, axis=0)\nreturn pred_with_id\ndef on_batch_end(self, data: Data):\n# begin of reading det and gt\npred = to_number(data[self.pred_key])  # pred is [batch, nms_max_outputs, 7]\npred = self._reshape_pred(pred)\ngt = to_number(data[self.true_key])  # gt is np.array (batch, box, 5), box dimension is padded\ngt = self._reshape_gt(gt)\nground_truth_bb = []\nfor gt_item in gt:\nidx_in_batch, x1, y1, w, h, label = gt_item\nlabel = int(label)\nid_epoch = self._get_id_in_epoch(idx_in_batch)\nself.batch_image_ids.append(id_epoch)\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label}\nground_truth_bb.append(tmp_dict)\npredicted_bb = []\nfor pred_item in pred:\nidx_in_batch, x1, y1, w, h, label, score = pred_item\nlabel = int(label)\nid_epoch = self.ids_batch_to_epoch[idx_in_batch]\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label, 'score': score}\npredicted_bb.append(tmp_dict)\nfor dict_elem in ground_truth_bb:\nself.gt[dict_elem['idx'], dict_elem['label']].append(dict_elem)\nfor dict_elem in predicted_bb:\nself.det[dict_elem['idx'], dict_elem['label']].append(dict_elem)\n# end of reading det and gt\n# compute iou matrix, matrix index is (img_id, cat_id), each element in matrix has shape (num_det, num_gt)\nself.ious = {(img_id, cat_id): self.compute_iou(self.det[img_id, cat_id], self.gt[img_id, cat_id])\nfor img_id in self.batch_image_ids for cat_id in self.categories}\nfor cat_id in self.categories:\nfor img_id in self.batch_image_ids:\nself.evalimgs[(cat_id, img_id)] = self.evaluate_img(cat_id, img_id)\ndef on_epoch_end(self, data: Data):\nself.accumulate()\nmean_ap = self.summarize()\nap50 = self.summarize(iou=0.5)\nap75 = self.summarize(iou=0.75)\ndata[self.outputs[0]] = mean_ap\ndata[self.outputs[1]] = ap50\ndata[self.outputs[2]] = ap75\ndef evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n        Args:\n            cat_id:\n            img_id:\n        Returns:\n        \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\ndef accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept IndexError:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\ndef summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n        Args:\n            iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n                is the mean average precision.\n        Returns:\n            Average precision.\n        \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\ndef compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n        We leverage `maskUtils.iou`.\n        Args:\n            det: Detection array.\n            gt: Ground truth array.\n        Returns:\n            Intersection of union array.\n        \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.accumulate", "title": "<code>accumulate</code>", "text": "<p>Generate precision-recall curve.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept IndexError:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.compute_iou", "title": "<code>compute_iou</code>", "text": "<p>Compute intersection over union.</p> <p>We leverage <code>maskUtils.iou</code>.</p> <p>Parameters:</p> Name Type Description Default <code>det</code> <code>np.ndarray</code> <p>Detection array.</p> required <code>gt</code> <code>np.ndarray</code> <p>Ground truth array.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Intersection of union array.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n    We leverage `maskUtils.iou`.\n    Args:\n        det: Detection array.\n        gt: Ground truth array.\n    Returns:\n        Intersection of union array.\n    \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.evaluate_img", "title": "<code>evaluate_img</code>", "text": "<p>Find gt matches for det given one image and one category.</p> <p>Parameters:</p> Name Type Description Default <code>cat_id</code> <code>int</code> required <code>img_id</code> <code>int</code> required Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n    Args:\n        cat_id:\n        img_id:\n    Returns:\n    \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.summarize", "title": "<code>summarize</code>", "text": "<p>Compute average precision given one intersection union threshold.</p> <p>Parameters:</p> Name Type Description Default <code>iou</code> <code>float</code> <p>Intersection over union threshold. If this value is <code>None</code>, then average all iou thresholds. The result is the mean average precision.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Average precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n    Args:\n        iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n            is the mean average precision.\n    Returns:\n        Average precision.\n    \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\n</code></pre>"}, {"location": "fastestimator/trace/metric/precision.html", "title": "precision", "text": ""}, {"location": "fastestimator/trace/metric/precision.html#fastestimator.fastestimator.trace.metric.precision.Precision", "title": "<code>Precision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes precision for a classification task and reports it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'precision'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments that pass to sklearn.metrics.precision_score()</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\precision.py</code> <pre><code>@per_ds\n@traceable()\nclass Precision(Trace):\n\"\"\"Computes precision for a classification task and reports it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: Name of the key to store to the state.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n        **kwargs: Additional keyword arguments that pass to sklearn.metrics.precision_score()\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"precision\",\nper_ds: bool = True,\n**kwargs) -&gt; None:\nPrecision.check_kwargs(kwargs)\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\nself.kwargs = kwargs\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = precision_score(self.y_true, self.y_pred, average='binary', **self.kwargs)\nelse:\nscore = precision_score(self.y_true, self.y_pred, average=None, **self.kwargs)\ndata.write_with_log(self.outputs[0], score)\n@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n        Args:\n            kwargs: Keywork arguments to be examined.\n        Raises:\n            ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n        \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.precision_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/precision.html#fastestimator.fastestimator.trace.metric.precision.Precision.check_kwargs", "title": "<code>check_kwargs</code>  <code>staticmethod</code>", "text": "<p>Check if <code>kwargs</code> has any blacklist argument and raise an error if it does.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keywork arguments to be examined.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\precision.py</code> <pre><code>@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n    Args:\n        kwargs: Keywork arguments to be examined.\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.precision_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/recall.html", "title": "recall", "text": ""}, {"location": "fastestimator/trace/metric/recall.html#fastestimator.fastestimator.trace.metric.recall.Recall", "title": "<code>Recall</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Compute recall for a classification task and report it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'recall'</code> <code>per_ds</code> <code>bool</code> <p>Whether to automatically compute this metric individually for every ds_id it runs on, in addition to computing an aggregate across all ds_ids on which it runs. This is automatically False if <code>output_name</code> contains a \"|\" character.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments that pass to sklearn.metrics.recall_score()</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\recall.py</code> <pre><code>@per_ds\n@traceable()\nclass Recall(Trace):\n\"\"\"Compute recall for a classification task and report it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        output_name: Name of the key to store to the state.\n        per_ds: Whether to automatically compute this metric individually for every ds_id it runs on, in addition to\n            computing an aggregate across all ds_ids on which it runs. This is automatically False if `output_name`\n            contains a \"|\" character.\n        **kwargs: Additional keyword arguments that pass to sklearn.metrics.recall_score()\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\noutput_name: str = \"recall\",\nper_ds: bool = True,\n**kwargs) -&gt; None:\nRecall.check_kwargs(kwargs)\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode, ds_id=ds_id)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\nself.kwargs = kwargs\nself.per_ds = per_ds\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1 and y_pred.ndim &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = recall_score(self.y_true, self.y_pred, average='binary', **self.kwargs)\nelse:\nscore = recall_score(self.y_true, self.y_pred, average=None, **self.kwargs)\ndata.write_with_log(self.outputs[0], score)\n@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n        Args:\n            kwargs: Keywork arguments to be examined.\n        Raises:\n            ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n        \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.recall_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/metric/recall.html#fastestimator.fastestimator.trace.metric.recall.Recall.check_kwargs", "title": "<code>check_kwargs</code>  <code>staticmethod</code>", "text": "<p>Check if <code>kwargs</code> has any blacklist argument and raise an error if it does.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Dict[str, Any]</code> <p>Keywork arguments to be examined.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>One of [\"y_true\", \"y_pred\", \"average\"] argument exists in <code>kwargs</code>.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\recall.py</code> <pre><code>@staticmethod\ndef check_kwargs(kwargs: Dict[str, Any]) -&gt; None:\n\"\"\"Check if `kwargs` has any blacklist argument and raise an error if it does.\n    Args:\n        kwargs: Keywork arguments to be examined.\n    Raises:\n        ValueError: One of [\"y_true\", \"y_pred\", \"average\"] argument exists in `kwargs`.\n    \"\"\"\nblacklist = [\"y_true\", \"y_pred\", \"average\"]\nillegal_kwarg = [x for x in blacklist if x in kwargs]\nif illegal_kwarg:\nraise ValueError(\nf\"Arguments {illegal_kwarg} cannot exist in kwargs, since FastEstimator will later directly use them in\"\n\" sklearn.metrics.recall_score()\")\n</code></pre>"}, {"location": "fastestimator/trace/xai/eigen_cam.html", "title": "eigen_cam", "text": ""}, {"location": "fastestimator/trace/xai/eigen_cam.html#fastestimator.fastestimator.trace.xai.eigen_cam.EigenCAM", "title": "<code>EigenCAM</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which draws EigenCAM heatmaps on top of images.</p> <p>These are useful for visualizing the outputs of the feature extractor component of a model. They are relatively insensitive to adversarial attacks, so don't use them to try and detect those. See https://arxiv.org/abs/2008.00299 for more details.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>str</code> <p>The key corresponding to images onto which to draw the CAM outputs.</p> required <code>activations</code> <code>str</code> <p>The key corresponding to outputs from a convolution layer from which to draw the CAM outputs. You can easily extract these from any model by using the 'intermediate_layers' variable in a ModelOp.</p> required <code>n_components</code> <code>Union[int, float]</code> <p>How many principal components to visualize. If you pass a float between 0 and 1 it will instead visualize however many components are required in order to capture the corresponding percentage of the variance in the image.</p> <code>3</code> <code>n_samples</code> <code>Optional[int]</code> <p>How many images in total to display every epoch (or None to display all available images).</p> <code>5</code> <code>downsize</code> <code>Optional[int]</code> <p>Whether to downsize the inputs before svd decomposition in order to speed up processing. If provided, the inputs will be proportionally downscaled such that their longest axis length is equal to the <code>downsize</code> parameter. 64 seems like a good value to try if you are having performance problems.</p> <code>None</code> <code>labels</code> <code>Optional[str]</code> <p>The key corresponding to the true labels of the images to be visualized.</p> <code>None</code> <code>preds</code> <code>Optional[str]</code> <p>The key corresponding to the model prediction for each image.</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>{class_string: model_output_value}.</p> <code>None</code> <code>outputs</code> <code>str</code> <p>The key into which to write the eigencam images.</p> <code>'eigencam'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!train'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\eigen_cam.py</code> <pre><code>@traceable()\nclass EigenCAM(Trace):\n\"\"\"A trace which draws EigenCAM heatmaps on top of images.\n    These are useful for visualizing the outputs of the feature extractor component of a model. They are relatively\n    insensitive to adversarial attacks, so don't use them to try and detect those. See https://arxiv.org/abs/2008.00299\n    for more details.\n    Args:\n        images: The key corresponding to images onto which to draw the CAM outputs.\n        activations: The key corresponding to outputs from a convolution layer from which to draw the CAM outputs. You\n            can easily extract these from any model by using the 'intermediate_layers' variable in a ModelOp.\n        n_components: How many principal components to visualize. If you pass a float between 0 and 1 it will instead\n            visualize however many components are required in order to capture the corresponding percentage of the\n            variance in the image.\n        n_samples: How many images in total to display every epoch (or None to display all available images).\n        downsize: Whether to downsize the inputs before svd decomposition in order to speed up processing. If provided,\n            the inputs will be proportionally downscaled such that their longest axis length is equal to the `downsize`\n            parameter. 64 seems like a good value to try if you are having performance problems.\n        labels: The key corresponding to the true labels of the images to be visualized.\n        preds: The key corresponding to the model prediction for each image.\n        label_mapping: {class_string: model_output_value}.\n        outputs: The key into which to write the eigencam images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nimages: str,\nactivations: str,\nn_components: Union[int, float] = 3,\nn_samples: Optional[int] = 5,\ndownsize: Optional[int] = None,\nlabels: Optional[str] = None,\npreds: Optional[str] = None,\nlabel_mapping: Optional[Dict[str, Any]] = None,\noutputs: str = \"eigencam\",\nmode: Union[None, str, Iterable[str]] = \"!train\",\nds_id: Union[None, str, Iterable[str]] = None):\nself.image_key = images\nself.activation_key = activations\nself.true_label_key = labels\nself.pred_label_key = preds\ninputs = [x for x in (images, activations, labels, preds) if x is not None]\nself.n_components = n_components\nself.n_samples = n_samples\n# TODO - handle non-hashable labels\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nself.downsize = downsize\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.images = []\nself.activations = []\nself.labels = []\nself.preds = []\nself.n_found = 0\ndef _reset(self) -&gt; None:\n\"\"\"Clear memory for next epoch.\n        \"\"\"\nself.images = []\nself.activations = []\nself.labels = []\nself.preds = []\nself.n_found = 0\ndef _project_2d(self, activations: np.ndarray) -&gt; Tuple[int, List[List[np.ndarray]]]:\n\"\"\"Project 2D convolution activations maps into 2D principal component maps.\n        Args:\n            activations: A tensor of shape (batch, channels, height, width) to be transformed.\n        Returns:\n            (max N_components, Principal component projections of the `activations` (batch x components x image)).\n        \"\"\"\nprojections = []\nfor activation in activations:\nif self.downsize:\nlong_axis = 1 if activation.shape[1] &gt; activation.shape[2] else 2\nlong_len = activation.shape[long_axis]\nif long_len &gt; self.downsize:\nscale = self.downsize / long_len\nsmall_activations = []\nfor i in range(activation.shape[0]):\nsmall_activations.append(\ncv2.resize(src=activation[i, ...],\ndsize=(int(activation.shape[1]*scale), int(activation.shape[2]*scale)),\ninterpolation=cv2.INTER_AREA))\nactivation = np.array(small_activations)\nflat = activation.reshape(activation.shape[0], -1).transpose()\nflat = flat - flat.mean(axis=0)\nU, S, VT = np.linalg.svd(flat, full_matrices=True)\ncomponents = []\nn_components = self.n_components\nif isinstance(n_components, float):\neig_vals = np.square(S)\npct_explained = np.cumsum(eig_vals) / np.cumsum(eig_vals)[-1]\nn_components = 1 + np.searchsorted(pct_explained, self.n_components)\nfor i in range(n_components):\ncomponent_i = flat @ VT[i, :]\ncomponent_i = component_i.reshape(activation.shape[1:])\ncomponents.append(np.maximum(component_i, 0))\nprojections.append(components)\nreturn max([len(x) for x in projections]), projections\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.n_samples is None or self.n_found &lt; self.n_samples:\nself.images.append(data[self.image_key])\nself.activations.append(data[self.activation_key])\nif self.true_label_key:\nself.labels.append(data[self.true_label_key])\nif self.pred_label_key:\nself.preds.append(data[self.pred_label_key])\nself.n_found += len(data[self.image_key])\ndef on_epoch_end(self, data: Data) -&gt; None:\n# Keep only the user-specified number of samples\nimages = concat(self.images)[:self.n_samples or self.n_found]\n_, height, width = get_image_dims(images)\nactivations = to_number(concat(self.activations)[:self.n_samples or self.n_found])\nif tf.is_tensor(images):\nactivations = np.moveaxis(activations, source=-1, destination=1)  # Activations should be channel first\ncolumns = []\nlabels = None if not self.labels else concat(self.labels)[:self.n_samples or self.n_found]\nif labels is not None:\nif len(labels.shape) &gt; 1:\nlabels = argmax(labels, axis=-1)\nif self.label_mapping:\nlabels = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(labels))])\ncolumns.append(BatchDisplay(text=labels, title=self.true_label_key))\npreds = None if not self.preds else concat(self.preds)[:self.n_samples or self.n_found]\nif preds is not None:\nif len(preds.shape) &gt; 1:\npreds = argmax(preds, axis=-1)\nif self.label_mapping:\npreds = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(preds))])\ncolumns.append(BatchDisplay(text=preds, title=self.pred_label_key))\ncolumns.append(BatchDisplay(image=images, title=self.image_key))\n# Clear memory\nself._reset()\n# Make the image\nn_components, batch_component_image = self._project_2d(activations)\ncomponents = []  # component x image (batch x image)\nfor component_idx in range(n_components):\nbatch = []\nfor base_image, component_image in zip(images, batch_component_image):\nif len(component_image) &gt; component_idx:\nmask = component_image[component_idx]\nmask = cv2.resize(mask, (width, height))\nmask = mask - np.min(mask)\nmask = mask / np.max(mask)\nmask = cv2.cvtColor(cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)\nmask = np.float32(mask) / 255\n# switch to channel first for pytorch\nif isinstance(base_image, torch.Tensor):\nmask = np.moveaxis(mask, source=-1, destination=1)\nnew_image = base_image + mask\nnew_image = new_image / reduce_max(new_image)\nelse:\n# There's no component for this image, so display an empty image here\nnew_image = np.ones_like(base_image)\nbatch.append(new_image)\ncomponents.append(np.array(batch, dtype=np.float32))\nfor idx, elem in enumerate(components):\ncolumns.append(BatchDisplay(image=elem, title=f\"Component {idx}\"))\nresult = GridDisplay(columns=columns)\ndata.write_without_log(self.outputs[0], result)\n</code></pre>"}, {"location": "fastestimator/trace/xai/grad_cam.html", "title": "grad_cam", "text": ""}, {"location": "fastestimator/trace/xai/grad_cam.html#fastestimator.fastestimator.trace.xai.grad_cam.GradCAM", "title": "<code>GradCAM</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which draws GradCAM heatmaps on top of images.</p> <p>These are useful for visualizing supports for a model's classification. See https://arxiv.org/pdf/1610.02391.pdf for more details.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>str</code> <p>The key corresponding to images onto which to draw the CAM outputs.</p> required <code>grads</code> <code>str</code> <p>The key corresponding to gradients of the model output with respect to a convolution layer of the model. You can easily extract these from any model by using the 'intermediate_layers' variable in a ModelOp, along with the GradientOp. Make sure to select a particular component of y_pred when computing gradients rather than using the entire vector. See our GradCAM XAI tutorial for an example.</p> required <code>n_components</code> <code>int</code> <p>How many principal components to visualize.</p> <code>3</code> <code>n_samples</code> <code>Optional[int]</code> <p>How many images in total to display every epoch (or None to display all available images).</p> <code>5</code> <code>labels</code> <code>Optional[str]</code> <p>The key corresponding to the true labels of the images to be visualized.</p> <code>None</code> <code>preds</code> <code>Optional[str]</code> <p>The key corresponding to the model prediction for each image.</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>{class_string: model_output_value}.</p> <code>None</code> <code>outputs</code> <code>str</code> <p>The key into which to write the eigencam images.</p> <code>'gradcam'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!train'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\grad_cam.py</code> <pre><code>@traceable()\nclass GradCAM(Trace):\n\"\"\"A trace which draws GradCAM heatmaps on top of images.\n    These are useful for visualizing supports for a model's classification. See https://arxiv.org/pdf/1610.02391.pdf\n    for more details.\n    Args:\n        images: The key corresponding to images onto which to draw the CAM outputs.\n        grads: The key corresponding to gradients of the model output with respect to a convolution layer of the model.\n            You can easily extract these from any model by using the 'intermediate_layers' variable in a ModelOp, along\n            with the GradientOp. Make sure to select a particular component of y_pred when computing gradients rather\n            than using the entire vector. See our GradCAM XAI tutorial for an example.\n        n_components: How many principal components to visualize.\n        n_samples: How many images in total to display every epoch (or None to display all available images).\n        labels: The key corresponding to the true labels of the images to be visualized.\n        preds: The key corresponding to the model prediction for each image.\n        label_mapping: {class_string: model_output_value}.\n        outputs: The key into which to write the eigencam images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n    \"\"\"\ndef __init__(self,\nimages: str,\ngrads: str,\nn_components: int = 3,\nn_samples: Optional[int] = 5,\nlabels: Optional[str] = None,\npreds: Optional[str] = None,\nlabel_mapping: Optional[Dict[str, Any]] = None,\noutputs: str = \"gradcam\",\nmode: Union[None, str, Iterable[str]] = \"!train\",\nds_id: Union[None, str, Iterable[str]] = None):\nself.image_key = images\nself.grad_key = grads\nself.true_label_key = labels\nself.pred_label_key = preds\ninputs = [x for x in (images, grads, labels, preds) if x is not None]\nself.n_components = n_components\nself.n_samples = n_samples\n# TODO - handle non-hashable labels\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode, ds_id=ds_id)\nself.images = []\nself.grads = []\nself.labels = []\nself.preds = []\nself.n_found = 0\ndef _reset(self) -&gt; None:\n\"\"\"Clear memory for next epoch.\n        \"\"\"\nself.images = []\nself.grads = []\nself.labels = []\nself.preds = []\nself.n_found = 0\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.n_samples is None or self.n_found &lt; self.n_samples:\nself.images.append(data[self.image_key])\nself.grads.append(data[self.grad_key])\nif self.true_label_key:\nself.labels.append(data[self.true_label_key])\nif self.pred_label_key:\nself.preds.append(data[self.pred_label_key])\nself.n_found += len(data[self.image_key])\ndef on_epoch_end(self, data: Data) -&gt; None:\n# Keep only the user-specified number of samples\nimages = concat(self.images)[:self.n_samples or self.n_found]\n_, height, width = get_image_dims(images)\ngrads = to_number(concat(self.grads)[:self.n_samples or self.n_found])\nif tf.is_tensor(images):\ngrads = np.moveaxis(grads, source=-1, destination=1)  # grads should be channel first\ncolumns = []\nlabels = None if not self.labels else concat(self.labels)[:self.n_samples or self.n_found]\nif labels is not None:\nif len(labels.shape) &gt; 1:\nlabels = argmax(labels, axis=-1)\nif self.label_mapping:\nlabels = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(labels))])\ncolumns.append(BatchDisplay(text=labels, title=self.true_label_key))\npreds = None if not self.preds else concat(self.preds)[:self.n_samples or self.n_found]\nif preds is not None:\nif len(preds.shape) &gt; 1:\npreds = argmax(preds, axis=-1)\nif self.label_mapping:\npreds = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(preds))])\ncolumns.append(BatchDisplay(text=preds, title=self.pred_label_key))\ncolumns.append(BatchDisplay(image=images, title=self.image_key))\n# Clear memory\nself._reset()\n# Make the image\n# TODO: In future maybe allow multiple different grads to have side-by-side comparisons of classes\ncomponents = [np.mean(grads, axis=1)]\ncomponents = [np.maximum(component, 0) for component in components]\nmasks = []\nfor component_batch in components:\nimg_batch = []\nfor img in component_batch:\nimg = cv2.resize(img, (width, height))\nimg = img - np.min(img)\nimg = img / np.max(img)\nimg = cv2.cvtColor(cv2.applyColorMap(np.uint8(255 * img), cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)\nimg = np.float32(img) / 255\nimg_batch.append(img)\nimg_batch = np.array(img_batch, dtype=np.float32)\n# Switch to channel first for pytorch\nif isinstance(images, torch.Tensor):\nimg_batch = np.moveaxis(img_batch, source=-1, destination=1)\nmasks.append(img_batch)\ncomponents = [images + mask for mask in masks]  # This seems to work even if the image is 1 channel instead of 3\ncomponents = [image / reduce_max(image) for image in components]\nfor elem in components:\ncolumns.append(BatchDisplay(image=elem, title=self.grad_key))\nresult = GridDisplay(columns=columns)\ndata.write_without_log(self.outputs[0], result)\n</code></pre>"}, {"location": "fastestimator/trace/xai/instance_tracker.html", "title": "instance_tracker", "text": ""}, {"location": "fastestimator/trace/xai/instance_tracker.html#fastestimator.fastestimator.trace.xai.instance_tracker.InstanceTracker", "title": "<code>InstanceTracker</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace to track metrics by instances, for example per-instance loss over time during training.</p> <p>Use this in conjunction with ImageViewer or ImageSaver to see the graph at training end. This also automatically integrates with Traceability reports.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>str</code> <p>A key containing data indices to track over time.</p> required <code>metric</code> <code>str</code> <p>The key of the metric by which to score data.</p> required <code>n_max_to_keep</code> <code>int</code> <p>At the end of training, the n samples with the highest metric score will be plotted.</p> <code>5</code> <code>n_min_to_keep</code> <code>int</code> <p>At the end of training, the n samples with the lowest metric score will be plotted.</p> <code>5</code> <code>list_to_keep</code> <code>Optional[Iterable[Any]]</code> <p>A list of particular indices to pay attention to. This can be used in addition to <code>n_max_to_keep</code> and/or <code>n_min_to_keep</code>, or set those to zero to only track specific indices.</p> <code>None</code> <code>epoch_frequency</code> <code>int</code> <p>How frequently to collect data. Increase this value to reduce ram consumption.</p> <code>1</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>outputs</code> <code>Optional[str]</code> <p>The name of the output which will be generated by this trace at the end of training. If None then it will default to \"by\". <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>n_max_to_keep</code> or <code>n_min_to_keep</code> are invalid.</p> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\instance_tracker.py</code> <pre><code>@traceable()\nclass InstanceTracker(Trace):\n\"\"\"A Trace to track metrics by instances, for example per-instance loss over time during training.\n    Use this in conjunction with ImageViewer or ImageSaver to see the graph at training end. This also automatically\n    integrates with Traceability reports.\n    Args:\n        index: A key containing data indices to track over time.\n        metric: The key of the metric by which to score data.\n        n_max_to_keep: At the end of training, the n samples with the highest metric score will be plotted.\n        n_min_to_keep: At the end of training, the n samples with the lowest metric score will be plotted.\n        list_to_keep: A list of particular indices to pay attention to. This can be used in addition to `n_max_to_keep`\n            and/or `n_min_to_keep`, or set those to zero to only track specific indices.\n        epoch_frequency: How frequently to collect data. Increase this value to reduce ram consumption.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        outputs: The name of the output which will be generated by this trace at the end of training. If None then it\n            will default to \"&lt;metric&gt;_by_&lt;index&gt;\".\n    Raises:\n        ValueError: If `n_max_to_keep` or `n_min_to_keep` are invalid.\n    \"\"\"\ndef __init__(self,\nindex: str,\nmetric: str,\nn_max_to_keep: int = 5,\nn_min_to_keep: int = 5,\nlist_to_keep: Optional[Iterable[Any]] = None,\nepoch_frequency: int = 1,\nmode: Union[None, str, Iterable[str]] = \"eval\",\nds_id: Union[None, str, Iterable[str]] = None,\noutputs: Optional[str] = None):\n# TODO - highlight 'interesting' samples (sudden changes in relative ordering?)\nsuper().__init__(inputs=[index, metric], outputs=outputs or f\"{metric}_by_{index}\", mode=mode, ds_id=ds_id)\nself.points = []\nif n_max_to_keep &lt; 0:\nraise ValueError(f\"n_max_to_keep must be non-negative, but got {n_max_to_keep}\")\nself.n_max_to_keep = n_max_to_keep\nif n_min_to_keep &lt; 0:\nraise ValueError(f\"n_min_to_keep must be non-negative, but got {n_min_to_keep}\")\nself.n_min_to_keep = n_min_to_keep\nself.idx_to_keep = to_set(list_to_keep)\n# Ideally the step and metric would be separated to save space, but a given idx may not appear each epoch\nself.index_history = defaultdict(lambda: defaultdict(list))  # {mode: {idx: [(step, metric)]}}\nself.epoch_frequency = epoch_frequency\n@property\ndef index_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef metric_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.epoch_idx % self.epoch_frequency == 0:\nself.points.append((to_number(data[self.index_key]), to_number(data[self.metric_key])))\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.epoch_idx % self.epoch_frequency == 0:\nidx_scores = {}\nfor batch in self.points:\nfor idx, metric in ((batch[0][i], batch[1][i]) for i in range(len(batch[0]))):\nidx_scores[idx.item()] = metric.item()\nfor idx, metric in idx_scores.items():\nif self.idx_to_keep and self.n_min_to_keep == 0 and self.n_max_to_keep == 0:\n# We can only skip recording if max_to_keep and min_to_keep are 0 since otherwise we don't know\n# which histories will need to be thrown out later.\nif idx not in self.idx_to_keep:\n# Skip labels which the user does not want to inspect\ncontinue\nself.index_history[self.system.mode][idx].append((self.system.global_step, metric))\nself.points = []\ndef on_end(self, data: Data) -&gt; None:\nindex_summaries = DefaultKeyDict(default=lambda x: Summary(name=x))\nfor mode in self.mode:\nfinal_scores = sorted([(idx, elem[-1][1]) for idx, elem in self.index_history[mode].items()],\nkey=lambda x: x[1])\nmax_idx_list = {elem[0] for elem in final_scores[-1:-self.n_max_to_keep - 1:-1]}\nmin_idx_list = {elem[0] for elem in final_scores[:self.n_min_to_keep]}\ntarget_idx_list = Set.union(min_idx_list, max_idx_list, self.idx_to_keep)\nfor idx in target_idx_list:\nfor step, score in self.index_history[mode][idx]:\nindex_summaries[idx].history[mode][self.metric_key][step] = score\nself.system.add_graph(self.outputs[0], list(index_summaries.values()))  # So traceability can draw it\ndata.write_without_log(self.outputs[0], list(index_summaries.values()))\ndef __getstate__(self) -&gt; Dict[str, Any]:\n\"\"\"Get a representation of the state of this object.\n        This method is invoked by pickle.\n        Returns:\n            The information to be recorded by a pickle summary of this object.\n        \"\"\"\nstate = self.__dict__.copy()\nstate['index_history'] = dict(state['index_history'])\nreturn state\ndef __setstate__(self, state: Dict[str, Any]) -&gt; None:\n\"\"\"Set this objects internal state from a dictionary of variables.\n        This method is invoked by pickle.\n        Args:\n            state: The saved state to be used by this object.\n        \"\"\"\nindex_history = defaultdict(lambda: defaultdict(list))\nindex_history.update(state.get('index_history', {}))\nstate['index_history'] = index_history\nself.__dict__.update(state)\n</code></pre>"}, {"location": "fastestimator/trace/xai/label_tracker.html", "title": "label_tracker", "text": ""}, {"location": "fastestimator/trace/xai/label_tracker.html#fastestimator.fastestimator.trace.xai.label_tracker.LabelTracker", "title": "<code>LabelTracker</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace to track metrics grouped by labels, for example per-class loss over time during training.</p> <p>Use this in conjunction with ImageViewer or ImageSaver to see the graph at training end. This also automatically integrates with Traceability reports.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>The key of the labels by which to group data.</p> required <code>metric</code> <code>str</code> <p>The key of the metric by which to score data.</p> required <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A mapping of {DisplayName: LabelValue} to use when generating the graph. This can also be used to limit which label values are graphed, since any label values not included here will not be graphed. A None value will monitor all label values.</p> <code>None</code> <code>bounds</code> <code>Union[None, str, Iterable[Union[str, None]]]</code> <p>What error bounds should be graphed around the mean value. Options include None, 'std' for standard deviation, and 'range' to plot (min_value, mean, max_value). Multiple values can be specified, ex.  ['std', 'range'] to generate multiple graphs.</p> <code>'std'</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>outputs</code> <code>Optional[str]</code> <p>The name of the output which will be generated by this trace at the end of training. If None then it will default to \"by\". <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>bounds</code> is not one of the allowed options.</p> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\label_tracker.py</code> <pre><code>@traceable()\nclass LabelTracker(Trace):\n\"\"\"A Trace to track metrics grouped by labels, for example per-class loss over time during training.\n    Use this in conjunction with ImageViewer or ImageSaver to see the graph at training end. This also automatically\n    integrates with Traceability reports.\n    Args:\n        label: The key of the labels by which to group data.\n        metric: The key of the metric by which to score data.\n        label_mapping: A mapping of {DisplayName: LabelValue} to use when generating the graph. This can also be used to\n            limit which label values are graphed, since any label values not included here will not be graphed. A None\n            value will monitor all label values.\n        bounds: What error bounds should be graphed around the mean value. Options include None, 'std' for standard\n            deviation, and 'range' to plot (min_value, mean, max_value). Multiple values can be specified, ex.\n             ['std', 'range'] to generate multiple graphs.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        outputs: The name of the output which will be generated by this trace at the end of training. If None then it\n            will default to \"&lt;metric&gt;_by_&lt;label&gt;\".\n    Raises:\n        ValueError: If `bounds` is not one of the allowed options.\n    \"\"\"\ndef __init__(self,\nlabel: str,\nmetric: str,\nlabel_mapping: Optional[Dict[str, Any]] = None,\nbounds: Union[None, str, Iterable[Union[str, None]]] = \"std\",\nmode: Union[None, str, Iterable[str]] = \"eval\",\nds_id: Union[None, str, Iterable[str]] = None,\noutputs: Optional[str] = None):\nsuper().__init__(inputs=[label, metric], outputs=outputs or f\"{metric}_by_{label}\", mode=mode, ds_id=ds_id)\nself.points = []\nself.label_summaries = DefaultKeyDict(default=lambda x: Summary(name=x))\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nbounds = to_set(bounds)\nif not bounds:\nbounds.add(None)\nfor option in bounds:\nif option not in (None, \"std\", \"range\"):\nraise ValueError(f\"'interval' must be either None, 'std', or 'range', but got '{bounds}'.\")\nself.bounds = bounds\n@property\ndef label_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef metric_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_batch_end(self, data: Data) -&gt; None:\nself.points.append((to_number(data[self.label_key]), to_number(data[self.metric_key])))\ndef on_epoch_end(self, data: Data) -&gt; None:\nlabel_scores = defaultdict(list)\nfor batch in self.points:\nfor label, metric in ((batch[0][i], batch[1][i]) for i in range(len(batch[0]))):\nlabel_scores[label.item()].append(metric.item())\nfor label, metric in label_scores.items():\nif self.label_mapping:\nif label in self.label_mapping:\nlabel = self.label_mapping[label]\nelse:\n# Skip labels which the user does not want to inspect\ncontinue\nif 'std' in self.bounds:\nmean, std = stats.mean(metric), stats.stdev(metric) if len(metric) &gt; 1 else 0.0\nval = ValWithError(mean - std, mean, mean + std)\nkey = f\"{self.metric_key} ($\\\\mu \\\\pm \\\\sigma$)\"\n# {label: {mode: {key: {step: value}}}}\nself.label_summaries[label].history[self.system.mode][key][self.system.global_step] = val\nif 'range' in self.bounds:\nval = ValWithError(min(metric), stats.mean(metric), max(metric))\nkey = f\"{self.metric_key} ($min, \\\\mu, max$)\"\nself.label_summaries[label].history[self.system.mode][key][self.system.global_step] = val\nif None in self.bounds:\nval = stats.mean(metric)\nkey = self.metric_key\nself.label_summaries[label].history[self.system.mode][key][self.system.global_step] = val\nself.points = []\ndef on_end(self, data: Data) -&gt; None:\nself.system.add_graph(self.outputs[0], list(self.label_summaries.values()))  # So traceability can draw it\ndata.write_without_log(self.outputs[0], list(self.label_summaries.values()))\ndef __getstate__(self) -&gt; Dict[str, Any]:\n\"\"\"Get a representation of the state of this object.\n        This method is invoked by pickle.\n        Returns:\n            The information to be recorded by a pickle summary of this object.\n        \"\"\"\nstate = self.__dict__.copy()\nstate['label_summaries'] = dict(state['label_summaries'])\nreturn state\ndef __setstate__(self, state: Dict[str, Any]) -&gt; None:\n\"\"\"Set this objects internal state from a dictionary of variables.\n        This method is invoked by pickle.\n        Args:\n            state: The saved state to be used by this object.\n        \"\"\"\nlabel_summaries = DefaultKeyDict(default=lambda x: Summary(name=x))\nlabel_summaries.update(state.get('label_summaries', {}))\nstate['label_summaries'] = label_summaries\nself.__dict__.update(state)\n</code></pre>"}, {"location": "fastestimator/trace/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/trace/xai/saliency.html#fastestimator.fastestimator.trace.xai.saliency.Saliency", "title": "<code>Saliency</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace which computes saliency maps for a given model throughout training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>A model compiled with fe.build to be analyzed.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the input values for the model.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the output values from a model.</p> required <code>class_key</code> <code>Optional[str]</code> <p>The key of the true labels corresponding to the model inputs (not required).</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>{class_string: model_output_value}.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The name of the output which will be generated by this trace.</p> <code>'saliency'</code> <code>samples</code> <code>Union[None, int, Dict[str, Any]]</code> <p>How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>ds_id</code> <code>Union[None, str, Iterable[str]]</code> <p>What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all ds_ids except for a particular one, you can pass an argument like \"!ds1\".</p> <code>None</code> <code>smoothing</code> <code>int</code> <p>How many rounds of smoothing should be applied to the saliency mask (0 to disable).</p> <code>25</code> <code>integrating</code> <code>Union[int, Tuple[int, int]]</code> <p>How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was provided by the smoothing variable (useful if you want to compare techniques / save on computation time).</p> <code>(100, 6)</code> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\saliency.py</code> <pre><code>@traceable()\nclass Saliency(Trace):\n\"\"\"A Trace which computes saliency maps for a given model throughout training.\n    Args:\n        model: A model compiled with fe.build to be analyzed.\n        model_inputs: Keys for the input values for the model.\n        model_outputs: Keys for the output values from a model.\n        class_key: The key of the true labels corresponding to the model inputs (not required).\n        label_mapping: {class_string: model_output_value}.\n        outputs: The name of the output which will be generated by this trace.\n        samples: How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        ds_id: What dataset id(s) to execute this Trace in. To execute regardless of ds_id, pass None. To execute in all\n            ds_ids except for a particular one, you can pass an argument like \"!ds1\".\n        smoothing: How many rounds of smoothing should be applied to the saliency mask (0 to disable).\n        integrating: How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may\n            be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was\n            provided by the smoothing variable (useful if you want to compare techniques / save on computation time).\n    \"\"\"\nsamples: Dict[str, Union[None, int, Dict[str, Any]]]  # {mode: val}\nn_found: Dict[str, int]  # {mode: val}\nn_required: Dict[str, int]  # {mode: val}\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\nclass_key: Optional[str] = None,\nlabel_mapping: Optional[Dict[str, Any]] = None,\noutputs: Union[str, List[str]] = \"saliency\",\nsamples: Union[None, int, Dict[str, Any]] = None,\nmode: Union[None, str, Iterable[str]] = (\"eval\", \"test\"),\nds_id: Union[None, str, Iterable[str]] = None,\nsmoothing: int = 25,\nintegrating: Union[int, Tuple[int, int]] = (100, 6)) -&gt; None:\n# Model outputs are required due to inability to statically determine the number of outputs from a pytorch model\nself.class_key = class_key\nself.model_outputs = to_list(model_outputs)\nsuper().__init__(inputs=to_list(self.class_key) + to_list(model_inputs), outputs=outputs, mode=mode,\nds_id=ds_id)\nself.smoothing = smoothing\nself.integrating = integrating\nself.samples = {}\nself.n_found = {}\nself.n_required = {}\n# TODO - handle non-hashable labels\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nfor mode in mode or (\"train\", \"eval\", \"test\"):\nself.samples[mode] = samples\nif isinstance(samples, int):\nself.samples[mode] = None\nself.n_found[mode] = 0\nself.n_required[mode] = samples\nelse:\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nif self.samples[mode] is None:\nself.samples[mode] = defaultdict(list)\nself.salnet = SaliencyNet(model=model, model_inputs=model_inputs, model_outputs=model_outputs, outputs=outputs)\ndef on_batch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif not self.samples[mode] or self.n_found[mode] &lt; self.n_required[mode]:\nn_samples = 0\nfor key in self.inputs:\nself.samples[mode][key].append(data[key])\nn_samples = len(data[key])\nself.n_found[mode] += n_samples\ndef on_epoch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif self.n_found[mode] &gt; 0:\nif self.n_required[mode] &gt; 0:\n# We are keeping a user-specified number of samples\nself.samples[mode] = {\nkey: concat(val)[:self.n_required[mode]]\nfor key, val in self.samples[mode].items()\n}\nelse:\n# We are keeping one batch of data\nself.samples[mode] = {key: val[0] for key, val in self.samples[mode].items()}\n# even if you haven't found n_required samples, you're at end of epoch so no point trying to collect more\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nmasks = self.salnet.get_masks(self.samples[mode])\nsmoothed, integrated, smint = {}, {}, {}\nif self.smoothing:\nsmoothed = self.salnet.get_smoothed_masks(self.samples[mode], nsamples=self.smoothing)\nif self.integrating:\nif isinstance(self.integrating, Tuple):\nn_integration, n_smoothing = self.integrating\nelse:\nn_integration = self.integrating\nn_smoothing = self.smoothing\nintegrated = self.salnet.get_integrated_masks(self.samples[mode], nsamples=n_integration)\nif n_smoothing:\nsmint = self.salnet.get_smoothed_masks(self.samples[mode],\nnsamples=n_smoothing,\nnintegration=n_integration)\n# Arrange the outputs\ncolumns = []\nif self.class_key:\nclasses = self.samples[mode][self.class_key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\ncolumns.append(BatchDisplay(text=classes, title=self.class_key))\nfor key in self.model_outputs:\nclasses = masks[key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\ncolumns.append(BatchDisplay(text=classes, title=key))\nsal = smint or integrated or smoothed or masks\nfor key, val in self.samples[mode].items():\nif key is not self.class_key:\ncolumns.append(BatchDisplay(image=val, title=key))\n# Create a linear combination of the original image, the saliency mask, and the product of the two in\n# order to highlight regions of importance\nmin_val = reduce_min(val)\ndiff = reduce_max(val) - min_val\nfor outkey in self.outputs:\ncolumns.append(BatchDisplay(image=(0.3 * (sal[outkey] * (val - min_val) + min_val) + 0.3 * val +\n0.4 * sal[outkey] * diff + min_val),\ntitle=\"{} {}\".format(key, outkey)))\nfor key in self.outputs:\ncolumns.append(BatchDisplay(image=masks[key], title=key, color_map=\"inferno\"))\nif smoothed:\ncolumns.append(BatchDisplay(image=smoothed[key], title=f\"Smoothed {key}\", color_map=\"inferno\"))\nif integrated:\ncolumns.append(BatchDisplay(image=integrated[key], title=f\"Integrated {key}\", color_map=\"inferno\"))\nif smint:\ncolumns.append(BatchDisplay(image=smint[key], title=f\"SmInt {key}\", color_map=\"inferno\"))\nresult = GridDisplay(columns=columns)\ndata.write_without_log(self.outputs[0], result)\n</code></pre>"}, {"location": "fastestimator/util/base_util.html", "title": "base_util", "text": ""}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.DefaultKeyDict", "title": "<code>DefaultKeyDict</code>", "text": "<p>         Bases: <code>Dict[KT, VT]</code></p> <p>Like collections.defaultdict but it passes the key argument to the default function.</p> <p>This class is intentionally not @traceable.</p> <pre><code>d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\nprint(d[\"a\"])  # 4\nprint(d[\"c\"])  # \"cc\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Callable[[Any], Any]</code> <p>A function which takes a key and returns a default value based on the key.</p> required <code>**kwargs</code> <p>Initial key/value pairs for the dictionary.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class DefaultKeyDict(Dict[KT, VT]):\n\"\"\"Like collections.defaultdict but it passes the key argument to the default function.\n    This class is intentionally not @traceable.\n    ```python\n    d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\n    print(d[\"a\"])  # 4\n    print(d[\"c\"])  # \"cc\"\n    ```\n    Args:\n        default: A function which takes a key and returns a default value based on the key.\n        **kwargs: Initial key/value pairs for the dictionary.\n    \"\"\"\ndef __init__(self, default: Callable[[Any], Any], **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.factory = default\ndef __missing__(self, key: Any) -&gt; Any:\nres = self[key] = self.factory(key)\nreturn res\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.FEID", "title": "<code>FEID</code>", "text": "<p>An int wrapper class that can change how it's values are printed.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>int</code> <p>An integer id to be wrapped.</p> required Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class FEID:\n\"\"\"An int wrapper class that can change how it's values are printed.\n    This class is intentionally not @traceable.\n    Args:\n        val: An integer id to be wrapped.\n    \"\"\"\n__slots__ = ['_val']\n_translation_dict = {}\ndef __init__(self, val: int):\nself._val = val\ndef __hash__(self) -&gt; int:\nreturn hash(self._val)\ndef __eq__(self, other: Any) -&gt; bool:\nif isinstance(other, FEID):\nreturn self._val == other._val\nelse:\nreturn int.__eq__(self._val, other)\ndef __lt__(self, other: Any) -&gt; bool:\nif isinstance(other, FEID):\nother = other._val\nreturn int.__lt__(self._val, other)\ndef __str__(self) -&gt; str:\nreturn f\"{self._translation_dict.get(self._val, self._val)}\"\ndef __repr__(self) -&gt; str:\nreturn f\"{self._translation_dict.get(self._val, self._val)}\"\n@classmethod\ndef set_translation_dict(cls, mapping: Dict[int, Any]) -&gt; None:\n\"\"\"Provide a lookup table to be invoked during value printing.\n        Args:\n            mapping: A mapping of id: printable id.\n        \"\"\"\ncls._translation_dict.clear()\ncls._translation_dict.update(mapping)\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.FEID.set_translation_dict", "title": "<code>set_translation_dict</code>  <code>classmethod</code>", "text": "<p>Provide a lookup table to be invoked during value printing.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Dict[int, Any]</code> <p>A mapping of id: printable id.</p> required Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>@classmethod\ndef set_translation_dict(cls, mapping: Dict[int, Any]) -&gt; None:\n\"\"\"Provide a lookup table to be invoked during value printing.\n    Args:\n        mapping: A mapping of id: printable id.\n    \"\"\"\ncls._translation_dict.clear()\ncls._translation_dict.update(mapping)\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.FigureFE", "title": "<code>FigureFE</code>", "text": "<p>         Bases: <code>Figure</code></p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class FigureFE(Figure):\n@classmethod\ndef from_figure(cls, fig: Figure) -&gt; 'FigureFE':\nnew_fig = FigureFE()\nnew_fig.__dict__ = fig.__dict__.copy()\nreturn new_fig\ndef show(self,\nsave_path: Optional[str] = None,\nverbose: bool = True,\nscale: int = 1,\ninteractive: bool = True) -&gt; None:\n\"\"\"A function which will save or display plotly figures.\n        Args:\n            save_path: The path where the figure should be saved, or None to display the figure to the screen.\n            verbose: Whether to print out the save location.\n            scale: A scaling factor to apply when exporting to static images (to increase resolution).\n            interactive: Whether the figure should be interactive or static. This is only applicable when\n                save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the\n                resulting jupyter notebook can be dramatically reduced.\n        \"\"\"\nconfig = {\n'displaylogo': False,\n'toImageButtonOptions': {\n'format': 'png',  # one of png, svg, jpeg, webp\n'height': None,\n'width': None,\n'filename': 'figure',\n'scale': scale  # Multiply title/legend/axis/canvas sizes by this factor (high resolution save)\n}}\nif save_path is None:\nif not interactive and in_notebook():\nfrom IPython.display import Image, display\ndisplay(Image(self.to_image(format='png', scale=scale)))\nelse:\nsuper().show(config=config)\nelse:\nsave_path = os.path.normpath(save_path)\nroot_dir = os.path.dirname(save_path)\nif root_dir == \"\":\nroot_dir = \".\"\nos.makedirs(root_dir, exist_ok=True)\nsave_file = os.path.join(root_dir, os.path.basename(save_path) or 'figure.html')\nconfig['toImageButtonOptions']['filename'] = os.path.splitext(os.path.basename(save_file))[0]\next = os.path.splitext(save_file)[1]\nif ext == '':\next = '.html'\nsave_file = save_file + ext  # Use html by default\nif verbose:\nprint(\"Saving to {}\".format(save_file))\nif ext == '.html':\nself.write_html(save_file, config=config)\nelse:\nself.write_image(save_file, width=1920, height=1080, scale=scale)\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.FigureFE.show", "title": "<code>show</code>", "text": "<p>A function which will save or display plotly figures.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> <code>scale</code> <code>int</code> <p>A scaling factor to apply when exporting to static images (to increase resolution).</p> <code>1</code> <code>interactive</code> <code>bool</code> <p>Whether the figure should be interactive or static. This is only applicable when save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the resulting jupyter notebook can be dramatically reduced.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def show(self,\nsave_path: Optional[str] = None,\nverbose: bool = True,\nscale: int = 1,\ninteractive: bool = True) -&gt; None:\n\"\"\"A function which will save or display plotly figures.\n    Args:\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n        scale: A scaling factor to apply when exporting to static images (to increase resolution).\n        interactive: Whether the figure should be interactive or static. This is only applicable when\n            save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the\n            resulting jupyter notebook can be dramatically reduced.\n    \"\"\"\nconfig = {\n'displaylogo': False,\n'toImageButtonOptions': {\n'format': 'png',  # one of png, svg, jpeg, webp\n'height': None,\n'width': None,\n'filename': 'figure',\n'scale': scale  # Multiply title/legend/axis/canvas sizes by this factor (high resolution save)\n}}\nif save_path is None:\nif not interactive and in_notebook():\nfrom IPython.display import Image, display\ndisplay(Image(self.to_image(format='png', scale=scale)))\nelse:\nsuper().show(config=config)\nelse:\nsave_path = os.path.normpath(save_path)\nroot_dir = os.path.dirname(save_path)\nif root_dir == \"\":\nroot_dir = \".\"\nos.makedirs(root_dir, exist_ok=True)\nsave_file = os.path.join(root_dir, os.path.basename(save_path) or 'figure.html')\nconfig['toImageButtonOptions']['filename'] = os.path.splitext(os.path.basename(save_file))[0]\next = os.path.splitext(save_file)[1]\nif ext == '':\next = '.html'\nsave_file = save_file + ext  # Use html by default\nif verbose:\nprint(\"Saving to {}\".format(save_file))\nif ext == '.html':\nself.write_html(save_file, config=config)\nelse:\nself.write_image(save_file, width=1920, height=1080, scale=scale)\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.Flag", "title": "<code>Flag</code>", "text": "<p>A mutable wrapper around a boolean.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>bool</code> <p>The initial value for the Flag.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class Flag:\n\"\"\"A mutable wrapper around a boolean.\n    This class is intentionally not @traceable.\n    Args:\n        val: The initial value for the Flag.\n    \"\"\"\n__slots__ = ['_val']\ndef __init__(self, val: bool = False):\nself._val = val\ndef set_true(self):\nself._val = True\ndef set_false(self):\nself._val = False\ndef __bool__(self):\nreturn self._val\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.LogSplicer", "title": "<code>LogSplicer</code>", "text": "<p>A class to send stdout information into a file before passing it along to the normal stdout.</p> <p>Parameters:</p> Name Type Description Default <code>log_path</code> <code>str</code> <p>The path/filename into which to append the current stdout.</p> required Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class LogSplicer:\n\"\"\"A class to send stdout information into a file before passing it along to the normal stdout.\n    Args:\n        log_path: The path/filename into which to append the current stdout.\n    \"\"\"\ndef __init__(self, log_path: str):\nself.log_path = log_path\nself.stdout = None\nself.log_file = None\ndef __enter__(self) -&gt; None:\nself.log_file = open(self.log_path, 'a')\nself.stdout = sys.stdout\nsys.stdout = self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nsys.stdout = self.stdout\nself.log_file.close()\ndef write(self, output: str) -&gt; None:\nself.log_file.write(output)\nself.stdout.write(output)\ndef flush(self) -&gt; None:\nself.stdout.flush()\nself.log_file.flush()\ndef getvalue(self) -&gt; str:\nreturn self.stdout.getvalue()\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.NonContext", "title": "<code>NonContext</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which is used to make nothing unusual happen.</p> <p>This class is intentionally not @traceable.</p> <pre><code>a = 5\nwith fe.util.NonContext():\na = a + 37\nprint(a)  # 42\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class NonContext(object):\n\"\"\"A class which is used to make nothing unusual happen.\n    This class is intentionally not @traceable.\n    ```python\n    a = 5\n    with fe.util.NonContext():\n        a = a + 37\n    print(a)  # 42\n    ```\n    \"\"\"\ndef __enter__(self) -&gt; None:\npass\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.Suppressor", "title": "<code>Suppressor</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which can be used to silence output of function calls.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>allow_pyprint</code> <code>bool</code> <p>Whether to allow python printing to occur still within this scope (and therefore only silence printing from non-python sources like c).</p> <code>False</code> <pre><code>x = lambda: print(\"hello\")\nx()  # \"hello\"\nwith fe.util.Suppressor():\nx()  #\nx()  # \"hello\"\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>class Suppressor(object):\n\"\"\"A class which can be used to silence output of function calls.\n    This class is intentionally not @traceable.\n    Args:\n        allow_pyprint: Whether to allow python printing to occur still within this scope (and therefore only silence\n            printing from non-python sources like c).\n    ```python\n    x = lambda: print(\"hello\")\n    x()  # \"hello\"\n    with fe.util.Suppressor():\n        x()  #\n    x()  # \"hello\"\n    ```\n    \"\"\"\ndef __init__(self, allow_pyprint: bool = False):\nself.allow_pyprint = allow_pyprint\ndef __enter__(self) -&gt; None:\n# This is not necessary to block printing, but lets the system know what's happening\nself.py_reals = [sys.stdout, sys.stderr]\nsys.stdout = sys.stderr = self\n# This part does the heavy lifting\nself.fakes = [os.open(os.devnull, os.O_RDWR), os.open(os.devnull, os.O_RDWR)]\nself.reals = [os.dup(1), os.dup(2)]  # [stdout, stderr]\nos.dup2(self.fakes[0], 1)\nos.dup2(self.fakes[1], 2)\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nos.dup2(self.reals[0], 1)\nos.dup2(self.reals[1], 2)\nfor fd in self.fakes + self.reals:\nos.close(fd)\n# Set the python pointers back too\nsys.stdout, sys.stderr = self.py_reals[0], self.py_reals[1]\ndef write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n        Args:\n            dummy: The string which wanted to be printed.\n        \"\"\"\nif self.allow_pyprint:\nos.write(self.reals[0], dummy.encode('utf-8'))\ndef flush(self) -&gt; None:\n\"\"\"A function to empty the current print buffer. No-op in this case.\n        \"\"\"\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.Suppressor.flush", "title": "<code>flush</code>", "text": "<p>A function to empty the current print buffer. No-op in this case.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def flush(self) -&gt; None:\n\"\"\"A function to empty the current print buffer. No-op in this case.\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.Suppressor.write", "title": "<code>write</code>", "text": "<p>A function which is invoked during print calls.</p> <p>Parameters:</p> Name Type Description Default <code>dummy</code> <code>str</code> <p>The string which wanted to be printed.</p> required Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n    Args:\n        dummy: The string which wanted to be printed.\n    \"\"\"\nif self.allow_pyprint:\nos.write(self.reals[0], dummy.encode('utf-8'))\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.check_ds_id", "title": "<code>check_ds_id</code>", "text": "<p>A function to check whether ds_ids inputs are correct inputs.</p> <p>ds_ids should either be defined through whitelist, like {\"ds1\", \"ds2\"} or blacklist, like {\"!ds1\", \"!ds2\"}.</p> <pre><code>m = fe.util.parse_ds_id({\"ds1\"})  # {\"ds1\"}\nm = fe.util.parse_ds_id({\"!ds1\"})  # {\"!ds1\"}\nm = fe.util.parse_ds_id({\"ds1\", \"ds2\"})  # {\"ds1\", \"ds2\"}\nm = fe.util.parse_ds_id({\"!ds1\", \"!ds2\"})  # {\"!ds1\", \"!ds2\"}\nm = fe.util.parse_ds_id({\"!ds1\", \"ds2\"})  # Raises Assertion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ds_ids</code> <code>Set[str]</code> <p>The desired ds_id to run on (possibly containing blacklisted ds_ids).</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The ds_ids to run or to avoid.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if blacklisted modes and whitelisted modes are mixed.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def check_ds_id(ds_ids: Set[str]) -&gt; Set[str]:\n\"\"\"A function to check whether ds_ids inputs are correct inputs.\n    ds_ids should either be defined through whitelist, like {\"ds1\", \"ds2\"} or blacklist, like {\"!ds1\", \"!ds2\"}.\n    ```python\n    m = fe.util.parse_ds_id({\"ds1\"})  # {\"ds1\"}\n    m = fe.util.parse_ds_id({\"!ds1\"})  # {\"!ds1\"}\n    m = fe.util.parse_ds_id({\"ds1\", \"ds2\"})  # {\"ds1\", \"ds2\"}\n    m = fe.util.parse_ds_id({\"!ds1\", \"!ds2\"})  # {\"!ds1\", \"!ds2\"}\n    m = fe.util.parse_ds_id({\"!ds1\", \"ds2\"})  # Raises Assertion\n    ```\n    Args:\n        ds_ids: The desired ds_id to run on (possibly containing blacklisted ds_ids).\n    Returns:\n        The ds_ids to run or to avoid.\n    Raises:\n        AssertionError: if blacklisted modes and whitelisted modes are mixed.\n    \"\"\"\nnegation = set([ds_id.startswith(\"!\") for ds_id in ds_ids])\nassert len(negation) &lt; 2, \"cannot mix !ds_id with ds_id, found {}\".format(ds_ids)\nforbidden_ds_id_chars = {\":\", \";\", \"|\"}\nfor ds_id in ds_ids:\nassert isinstance(ds_id, str) and len(ds_id) &gt; 0, \"dataset id must be a string, found {}\".format(ds_id)\nassert not any(char in ds_id for char in forbidden_ds_id_chars), \\\n            \"dataset id should not contain forbidden characters like ':', ';', '|', found {}\".format(ds_id)\nreturn ds_ids\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.get_colors", "title": "<code>get_colors</code>", "text": "<p>Get a list of colors to use in plotting.</p> <p>Parameters:</p> Name Type Description Default <code>n_colors</code> <code>int</code> <p>How many colors to return.</p> required <code>alpha</code> <code>float</code> <p>What opacity value to use (0 to 1).</p> <code>1.0</code> <code>as_numbers</code> <code>bool</code> <p>Whether to return the values as a list of numbers [r,g,b,a] or as a string</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Union[str, Tuple[float, float, float, float]]]</code> <p>A list of rgba string colors.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def get_colors(n_colors: int,\nalpha: float = 1.0,\nas_numbers: bool = False) -&gt; List[Union[str, Tuple[float, float, float, float]]]:\n\"\"\"Get a list of colors to use in plotting.\n    Args:\n        n_colors: How many colors to return.\n        alpha: What opacity value to use (0 to 1).\n        as_numbers: Whether to return the values as a list of numbers [r,g,b,a] or as a string\n    Returns:\n        A list of rgba string colors.\n    \"\"\"\nif n_colors &lt;= 10:\ncolors = [f'rgba(1,115,178,{alpha})', f'rgba(222,143,5,{alpha})', f'rgba(2,158,115,{alpha})',\nf'rgba(213,94,0,{alpha})', f'rgba(204,120,188,{alpha})', f'rgba(202,145,97,{alpha})',\nf'rgba(251,175,228,{alpha})', f'rgba(148,148,148,{alpha})', f'rgba(236,225,51,{alpha})',\nf'rgba(86,180,233,{alpha})']\nelse:\ncolors = [(i + 0.01) / n_colors for i in range(n_colors)]\ncolors = [color - 1 if color &gt;= 1 else color for color in colors]\ncolors = [colorsys.hls_to_rgb(color, 0.6, 0.95) for color in colors]\ncolors = [f'rgba({int(256*r)},{int(256*g)},{int(256*b)},{alpha})' for r, g, b in colors]\ncolors = colors[:n_colors]\nif as_numbers:\ncolors = [[float(x) for x in elem.strip('rgba(').strip(')').split(',')] for elem in colors]\nreturn colors\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.get_shape", "title": "<code>get_shape</code>", "text": "<p>A function to find the shapes of an object or sequence of objects.</p> <p>Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an attempt will be made to determine which of the interior dimensions are ragged.</p> <pre><code>x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\nx = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data to infer the shape of.</p> required <p>Returns:</p> Type Description <code>List[Optional[int]]</code> <p>A list representing the shape of the data.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def get_shape(obj: Any) -&gt; List[Optional[int]]:\n\"\"\"A function to find the shapes of an object or sequence of objects.\n    Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have\n    mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an\n    attempt will be made to determine which of the interior dimensions are ragged.\n    ```python\n    x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\n    x = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n    ```\n    Args:\n        obj: Data to infer the shape of.\n    Returns:\n        A list representing the shape of the data.\n    \"\"\"\nif hasattr(obj, \"shape\"):\nresult = list(obj.shape)\nelif isinstance(obj, (List, Tuple)):\nshapes = [get_shape(ob) for ob in obj]\nresult = [None]\nif shapes:\nrank = len(shapes[0])\nif any((len(shape) != rank for shape in shapes)):\nreturn result\nresult.extend(shapes[0])\nfor shape in shapes[1:]:\nfor idx, dim in enumerate(shape):\nif result[idx + 1] != dim:\nresult[idx + 1] = None\nelse:\nresult = []\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.get_type", "title": "<code>get_type</code>", "text": "<p>A function to try and infer the types of data within containers.</p> <pre><code>x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\nx = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\nx = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\nx = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\nx = fe.util.get_type(27)  # \"int\"\n</code></pre> <p>For container to look into its element's type, its type needs to be either list or tuple, and the return string will be List[...]. All container elements need to have the same data type becuase it will only check its first element.</p> <pre><code>x = fe.util.get_type({\"a\":1, \"b\":2})  # \"dict\"\nx = fe.util.get_type([1, \"a\"]) # \"List[int]\"\nx = fe.util.get_type([[[1]]]) # \"List[List[List[int]]]\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data which may be wrapped in some kind of container.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the data type of the <code>obj</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def get_type(obj: Any) -&gt; str:\n\"\"\"A function to try and infer the types of data within containers.\n    ```python\n    x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\n    x = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\n    x = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\n    x = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\n    x = fe.util.get_type(27)  # \"int\"\n    ```\n    For container to look into its element's type, its type needs to be either list or tuple, and the return string will\n    be List[...]. All container elements need to have the same data type becuase it will only check its first element.\n    ```python\n    x = fe.util.get_type({\"a\":1, \"b\":2})  # \"dict\"\n    x = fe.util.get_type([1, \"a\"]) # \"List[int]\"\n    x = fe.util.get_type([[[1]]]) # \"List[List[List[int]]]\"\n    ```\n    Args:\n        obj: Data which may be wrapped in some kind of container.\n    Returns:\n        A string representation of the data type of the `obj`.\n    \"\"\"\nif hasattr(obj, \"dtype\"):\nresult = str(obj.dtype)\nelif isinstance(obj, (List, Tuple)):\nif len(obj) &gt; 0:\nresult = \"List[{}]\".format(get_type(obj[0]))\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.in_notebook", "title": "<code>in_notebook</code>", "text": "<p>Determine whether the code is running inside a jupyter notebook</p> <p>Returns:</p> Type Description <code>bool</code> <p>True iff the code is executing inside a Jupyter notebook</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def in_notebook() -&gt; bool:\n\"\"\"Determine whether the code is running inside a jupyter notebook\n    Returns:\n        True iff the code is executing inside a Jupyter notebook\n    \"\"\"\ntry:\nfrom IPython import get_ipython\nshell = get_ipython().__class__.__name__\nif shell == 'ZMQInteractiveShell':\nreturn True  # Jupyter notebook or qtconsole\nreturn False\nexcept (ImportError, NameError):\nreturn False\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.is_number", "title": "<code>is_number</code>", "text": "<p>Check if a given string can be converted into a number.</p> <pre><code>x = fe.util.is_number(\"13.7\")  # True\nx = fe.util.is_number(\"ae13.7\")  # False\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>str</code> <p>A potentially numeric input string.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True iff <code>arg</code> represents a number.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def is_number(arg: str) -&gt; bool:\n\"\"\"Check if a given string can be converted into a number.\n    ```python\n    x = fe.util.is_number(\"13.7\")  # True\n    x = fe.util.is_number(\"ae13.7\")  # False\n    ```\n    Args:\n        arg: A potentially numeric input string.\n    Returns:\n        True iff `arg` represents a number.\n    \"\"\"\ntry:\nfloat(arg)\nreturn True\nexcept (ValueError, TypeError):\nreturn False\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.list_files", "title": "<code>list_files</code>", "text": "<p>Get the paths of all files in a particular root directory subject to a particular file extension.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data.</p> required <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> <code>recursive_search</code> <code>bool</code> <p>Whether to search within subdirectories for files.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of file paths found within the directory.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided path isn't a directory.</p> <code>ValueError</code> <p>If the directory has an invalid structure.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def list_files(root_dir: str,\nfile_extension: Optional[str] = None,\nrecursive_search: bool = True) -&gt; List[str]:\n\"\"\"Get the paths of all files in a particular root directory subject to a particular file extension.\n    Args:\n        root_dir: The path to the directory containing data.\n        file_extension: If provided then only files ending with the file_extension will be included.\n        recursive_search: Whether to search within subdirectories for files.\n    Returns:\n        A list of file paths found within the directory.\n    Raises:\n        AssertionError: If the provided path isn't a directory.\n        ValueError: If the directory has an invalid structure.\n    \"\"\"\npaths = []\nroot_dir = os.path.normpath(root_dir)\nif not os.path.isdir(root_dir):\nraise AssertionError(\"Provided path is not a directory\")\ntry:\nfor root, _, files in os.walk(root_dir):\nfor file_name in files:\nif file_name.startswith(\".\") or (file_extension is not None\nand not file_name.endswith(file_extension)):\ncontinue\npaths.append(os.path.join(root, file_name))\nif not recursive_search:\nbreak\nexcept StopIteration:\nraise ValueError(\"Invalid directory structure for DirDataset at root: {}\".format(root_dir))\nreturn paths\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.param_to_range", "title": "<code>param_to_range</code>", "text": "<p>Convert a single int or float value to a tuple signifying a range.</p> <pre><code>x = fe.util.param_to_tuple(7)  # (-7, 7)\nx = fe.util.param_to_tuple([7, 8])  # (7,8))\nx = fe.util.param_to_tuple((3.1, 4.3))  # (3.1, 4.3)\nx = fe.util.to_set((-3.2))  # (-3.2, 3.2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[int, float, Tuple[int, int], Tuple[float, float]]</code> <p>Input data.</p> required <p>Returns:</p> Type Description <code>Union[Tuple[int, int], Tuple[float, float]]</code> <p>The input <code>data</code> but in tuple form for a range.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def param_to_range(\ndata: Union[int, float, Tuple[int, int], Tuple[float, float]]) -&gt; Union[Tuple[int, int], Tuple[float, float]]:\n\"\"\"Convert a single int or float value to a tuple signifying a range.\n    ```python\n    x = fe.util.param_to_tuple(7)  # (-7, 7)\n    x = fe.util.param_to_tuple([7, 8])  # (7,8))\n    x = fe.util.param_to_tuple((3.1, 4.3))  # (3.1, 4.3)\n    x = fe.util.to_set((-3.2))  # (-3.2, 3.2)\n    ```\n    Args:\n        data: Input data.\n    Returns:\n        The input `data` but in tuple form for a range.\n    \"\"\"\nif isinstance(data, (int, float)):\nif data &gt; 0:\ndata = -data, data\nelse:\ndata = data, -data\nelif isinstance(data, (list, tuple)):\ndata = tuple(data)\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.parse_modes", "title": "<code>parse_modes</code>", "text": "<p>A function to determine which modes to run on based on a set of modes potentially containing blacklist values.</p> <pre><code>m = fe.util.parse_modes({\"train\"})  # {\"train\"}\nm = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\nm = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\nm = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modes</code> <code>Set[str]</code> <p>The desired modes to run on (possibly containing blacklisted modes).</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes to run on (converted to a whitelist).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def parse_modes(modes: Set[str]) -&gt; Set[str]:\n\"\"\"A function to determine which modes to run on based on a set of modes potentially containing blacklist values.\n    ```python\n    m = fe.util.parse_modes({\"train\"})  # {\"train\"}\n    m = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\n    m = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\n    m = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n    ```\n    Args:\n        modes: The desired modes to run on (possibly containing blacklisted modes).\n    Returns:\n        The modes to run on (converted to a whitelist).\n    Raises:\n        AssertionError: If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.\n    \"\"\"\nvalid_fields = {\"train\", \"eval\", \"test\", \"infer\", \"!train\", \"!eval\", \"!test\", \"!infer\"}\nassert modes.issubset(valid_fields), \"Invalid modes argument {}\".format(modes - valid_fields)\nnegation = set([mode.startswith(\"!\") for mode in modes])\nassert len(negation) &lt; 2, \"cannot mix !mode with mode, found {}\".format(modes)\nif True in negation:\nnew_modes = {\"train\", \"eval\", \"test\", \"infer\"}\nfor mode in modes:\nnew_modes.discard(mode.strip(\"!\"))\nmodes = new_modes\nreturn modes\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.prettify_metric_name", "title": "<code>prettify_metric_name</code>", "text": "<p>Add spaces to camel case words, then swap _ for space, and capitalize each word.</p> <pre><code>x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>A string to be formatted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted version of 'metric'.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def prettify_metric_name(metric: str) -&gt; str:\n\"\"\"Add spaces to camel case words, then swap _ for space, and capitalize each word.\n    ```python\n    x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n    ```\n    Args:\n        metric: A string to be formatted.\n    Returns:\n        The formatted version of 'metric'.\n    \"\"\"\nreturn string.capwords(re.sub(\"([a-z])([A-Z])\", r\"\\g&lt;1&gt; \\g&lt;2&gt;\", metric).replace(\"_\", \" \"))\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.strip_prefix", "title": "<code>strip_prefix</code>", "text": "<p>Remove the given <code>prefix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\nx = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>prefix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def strip_prefix(target: Optional[str], prefix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `prefix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\n    x = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        prefix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif prefix is None or target is None:\nreturn target\ns_len = len(prefix)\nif target[:s_len] == prefix:\nreturn target[s_len:]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.strip_suffix", "title": "<code>strip_suffix</code>", "text": "<p>Remove the given <code>suffix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\nx = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def strip_suffix(target: Optional[str], suffix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `suffix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\n    x = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        suffix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif suffix is None or target is None:\nreturn target\ns_len = len(suffix)\nif target[-s_len:] == suffix:\nreturn target[:-s_len]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.to_list", "title": "<code>to_list</code>", "text": "<p>Convert data to a list. A single None value will be converted to the empty list.</p> <pre><code>x = fe.util.to_list(None)  # []\nx = fe.util.to_list([None])  # [None]\nx = fe.util.to_list(7)  # [7]\nx = fe.util.to_list([7, 8])  # [7,8]\nx = fe.util.to_list({7})  # [7]\nx = fe.util.to_list((7))  # [7]\nx = fe.util.to_list({'a': 7})  # [{'a': 7}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>The input <code>data</code> but inside a list instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def to_list(data: Any) -&gt; List[Any]:\n\"\"\"Convert data to a list. A single None value will be converted to the empty list.\n    ```python\n    x = fe.util.to_list(None)  # []\n    x = fe.util.to_list([None])  # [None]\n    x = fe.util.to_list(7)  # [7]\n    x = fe.util.to_list([7, 8])  # [7,8]\n    x = fe.util.to_list({7})  # [7]\n    x = fe.util.to_list((7))  # [7]\n    x = fe.util.to_list({'a': 7})  # [{'a': 7}]\n    ```\n    Args:\n        data: Input data, within or without a python container.\n    Returns:\n        The input `data` but inside a list instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn []\nif not isinstance(data, list):\nif isinstance(data, (tuple, set)):\ndata = list(data)\nelse:\ndata = [data]\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/base_util.html#fastestimator.fastestimator.util.base_util.to_set", "title": "<code>to_set</code>", "text": "<p>Convert data to a set. A single None value will be converted to the empty set.</p> <pre><code>x = fe.util.to_set(None)  # set()\nx = fe.util.to_set([None])  # {None}\nx = fe.util.to_set(7)  # {7}\nx = fe.util.to_set([7, 8])  # {7,8}\nx = fe.util.to_set({7})  # {7}\nx = fe.util.to_set((7))  # {7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container. The <code>data</code> must be hashable.</p> required <p>Returns:</p> Type Description <code>Set[Any]</code> <p>The input <code>data</code> but inside a set instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\base_util.py</code> <pre><code>def to_set(data: Any) -&gt; Set[Any]:\n\"\"\"Convert data to a set. A single None value will be converted to the empty set.\n    ```python\n    x = fe.util.to_set(None)  # set()\n    x = fe.util.to_set([None])  # {None}\n    x = fe.util.to_set(7)  # {7}\n    x = fe.util.to_set([7, 8])  # {7,8}\n    x = fe.util.to_set({7})  # {7}\n    x = fe.util.to_set((7))  # {7}\n    ```\n    Args:\n        data: Input data, within or without a python container. The `data` must be hashable.\n    Returns:\n        The input `data` but inside a set instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn set()\nif not isinstance(data, set):\nif isinstance(data, (tuple, list, KeysView)):\ndata = set(data)\nelse:\ndata = {data}\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/cli_util.html", "title": "cli_util", "text": ""}, {"location": "fastestimator/util/cli_util.html#fastestimator.fastestimator.util.cli_util.SaveAction", "title": "<code>SaveAction</code>", "text": "<p>         Bases: <code>argparse.Action</code></p> <p>A customized save action for use with argparse.</p> <p>A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file is invoked directly during argument parsing.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>option_strings</code> <code>Sequence[str]</code> <p>A list of command-line option strings which should be associated with this action.</p> required <code>dest</code> <code>str</code> <p>The name of the attribute to hold the created object(s).</p> required <code>nargs</code> <code>Union[int, str, None]</code> <p>The number of command line arguments to be consumed.</p> <code>'?'</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Pass-through keyword arguments.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\cli_util.py</code> <pre><code>class SaveAction(argparse.Action):\n\"\"\"A customized save action for use with argparse.\n    A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file\n    is invoked directly during argument parsing.\n    This class is intentionally not @traceable.\n    Args:\n        option_strings: A list of command-line option strings which should be associated with this action.\n        dest: The name of the attribute to hold the created object(s).\n        nargs: The number of command line arguments to be consumed.\n        **kwargs: Pass-through keyword arguments.\n    \"\"\"\ndef __init__(self,\noption_strings: Sequence[str],\ndest: str,\nnargs: Union[int, str, None] = '?',\n**kwargs: Dict[str, Any]) -&gt; None:\nif '?' != nargs:\nraise ValueError(\"nargs must be \\'?\\'\")\nsuper().__init__(option_strings, dest, nargs, **kwargs)\ndef __call__(self,\nparser: argparse.ArgumentParser,\nnamespace: argparse.Namespace,\nvalues: Optional[str],\noption_string: Optional[str] = None) -&gt; None:\n\"\"\"Invokes the save action, writing two values into the namespace.\n        Args:\n            parser: The active argument parser (ignored by this implementation).\n            namespace: The current namespace to be written to.\n            values: The value to write into the namespace.\n            option_string: An option_string (ignored by this implementation).\n        \"\"\"\nsetattr(namespace, self.dest, True)\nsetattr(namespace, self.dest + '_dir', values if values is None else os.path.join(values, ''))\n</code></pre>"}, {"location": "fastestimator/util/cli_util.html#fastestimator.fastestimator.util.cli_util.parse_cli_to_dictionary", "title": "<code>parse_cli_to_dictionary</code>", "text": "<p>Convert a list of strings into a dictionary with python objects as values.</p> <pre><code>a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"])\n# {'epochs': 5, 'test': 'this', 'lr': 0.74}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[str]</code> <p>A list of input strings from the cli.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary constructed from the <code>input_list</code>, with values converted to python objects where applicable.</p> Source code in <code>fastestimator\\fastestimator\\util\\cli_util.py</code> <pre><code>def parse_cli_to_dictionary(input_list: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Convert a list of strings into a dictionary with python objects as values.\n    ```python\n    a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"])\n    # {'epochs': 5, 'test': 'this', 'lr': 0.74}\n    ```\n    Args:\n        input_list: A list of input strings from the cli.\n    Returns:\n        A dictionary constructed from the `input_list`, with values converted to python objects where applicable.\n    \"\"\"\nresult = {}\nif input_list is None:\nreturn result\nkey = \"\"\nval = \"\"\nidx = 0\nwhile idx &lt; len(input_list):\nif input_list[idx].startswith(\"--\"):\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nval = \"\"\nkey = input_list[idx].strip('--')\nelse:\nval += input_list[idx]\nidx += 1\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/cli_util.html#fastestimator.fastestimator.util.cli_util.parse_string_to_python", "title": "<code>parse_string_to_python</code>", "text": "<p>Convert a string into a python object.</p> <pre><code>x = fe.util.parse_string_to_python(\"5\")  # 5\nx = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\nx = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>str</code> <p>An input string.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A python object version of the input string.</p> Source code in <code>fastestimator\\fastestimator\\util\\cli_util.py</code> <pre><code>def parse_string_to_python(val: str) -&gt; Any:\n\"\"\"Convert a string into a python object.\n    ```python\n    x = fe.util.parse_string_to_python(\"5\")  # 5\n    x = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\n    x = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n    ```\n    Args:\n        val: An input string.\n    Returns:\n        A python object version of the input string.\n    \"\"\"\nif val is None or not val:\nreturn \"\"\ntry:\nreturn literal_eval(val)\nexcept (ValueError, SyntaxError):\ntry:\nreturn json.loads(val)\nexcept json.JSONDecodeError:\nreturn val\n</code></pre>"}, {"location": "fastestimator/util/data.html", "title": "data", "text": ""}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data", "title": "<code>Data</code>", "text": "<p>         Bases: <code>ChainMap[str, Any]</code></p> <p>A class which contains prediction and batch data.</p> <p>This class is intentionally not @traceable.</p> <p>Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not. We therefore provide helper methods to write entries into <code>Data</code> which are intended or not intended for logging.</p> <pre><code>d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\na = d[\"a\"]  # 0\nd.write_with_log(\"d\", 3)\nd.write_without_log(\"e\", 5)\nd.write_with_log(\"a\", 4)\na = d[\"a\"]  # 4\nr = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Optional[MutableMapping[str, Any]]</code> <p>The batch data dictionary. In practice this is itself often a ChainMap containing separate prediction and batch dictionaries.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>class Data(ChainMap[str, Any]):\n\"\"\"A class which contains prediction and batch data.\n    This class is intentionally not @traceable.\n    Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of\n    two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data\n    written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not.\n    We therefore provide helper methods to write entries into `Data` which are intended or not intended for logging.\n    ```python\n    d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\n    a = d[\"a\"]  # 0\n    d.write_with_log(\"d\", 3)\n    d.write_without_log(\"e\", 5)\n    d.write_with_log(\"a\", 4)\n    a = d[\"a\"]  # 4\n    r = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n    ```\n    Args:\n        batch_data: The batch data dictionary. In practice this is itself often a ChainMap containing separate\n            prediction and batch dictionaries.\n    \"\"\"\nmaps: List[MutableMapping[str, Any]]\ndef __init__(self, batch_data: Optional[MutableMapping[str, Any]] = None) -&gt; None:\nsuper().__init__({}, batch_data or {}, {})\nself.per_instance_enabled = True  # Can be toggled if you need to block traces from recording detailed info\ndef write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n        Args:\n            key: The key to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.__setitem__(key, value)\ndef write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n        Args:\n            key: The key to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.maps[1][key] = value\ndef write_per_instance_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given per-instance `value` into the `Data` dictionary for use with detailed loggers (ex. CSVLogger).\n        Args:\n            key: The key to associate with the new entry.\n            value: The new per-instance entry to be written.\n        \"\"\"\nif self.per_instance_enabled:\nself.maps[2][key] = value\ndef read_logs(self) -&gt; MutableMapping[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n        Returns:\n            A dictionary of all of the keys and values to be logged.\n        \"\"\"\nreturn self.maps[0]\ndef read_per_instance_logs(self) -&gt; MutableMapping[str, Any]:\n\"\"\"Read all per-instance values from the `Data` dictionary for detailed logging.\n        Returns:\n             A dictionary of the keys and values to be logged.\n        \"\"\"\nreturn self.maps[2]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.read_logs", "title": "<code>read_logs</code>", "text": "<p>Read all values from the <code>Data</code> dictionary which were intended to be logged.</p> <p>Returns:</p> Type Description <code>MutableMapping[str, Any]</code> <p>A dictionary of all of the keys and values to be logged.</p> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def read_logs(self) -&gt; MutableMapping[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n    Returns:\n        A dictionary of all of the keys and values to be logged.\n    \"\"\"\nreturn self.maps[0]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.read_per_instance_logs", "title": "<code>read_per_instance_logs</code>", "text": "<p>Read all per-instance values from the <code>Data</code> dictionary for detailed logging.</p> <p>Returns:</p> Type Description <code>MutableMapping[str, Any]</code> <p>A dictionary of the keys and values to be logged.</p> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def read_per_instance_logs(self) -&gt; MutableMapping[str, Any]:\n\"\"\"Read all per-instance values from the `Data` dictionary for detailed logging.\n    Returns:\n         A dictionary of the keys and values to be logged.\n    \"\"\"\nreturn self.maps[2]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_per_instance_log", "title": "<code>write_per_instance_log</code>", "text": "<p>Write a given per-instance <code>value</code> into the <code>Data</code> dictionary for use with detailed loggers (ex. CSVLogger).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new per-instance entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_per_instance_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given per-instance `value` into the `Data` dictionary for use with detailed loggers (ex. CSVLogger).\n    Args:\n        key: The key to associate with the new entry.\n        value: The new per-instance entry to be written.\n    \"\"\"\nif self.per_instance_enabled:\nself.maps[2][key] = value\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_with_log", "title": "<code>write_with_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n    Args:\n        key: The key to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.__setitem__(key, value)\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_without_log", "title": "<code>write_without_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it not be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n    Args:\n        key: The key to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.maps[1][key] = value\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.FilteredData", "title": "<code>FilteredData</code>", "text": "<p>A placeholder to indicate that this data instance should not be used.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>replacement</code> <code>bool</code> <p>Whether to replace the filtered element with another (thus maintaining the number of steps in an epoch but potentially increasing data repetition) or else shortening the epoch by the number of filtered data points (fewer steps per epoch than expected, but no extra data repetition). Either way, the number of data points within an individual batch will remain the same. Even if <code>replacement</code> is true, data will not be repeated until all of the given epoch's data has been traversed (except for at most 1 batch of data which might not appear until after the re-shuffle has occurred).</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>class FilteredData:\n\"\"\"A placeholder to indicate that this data instance should not be used.\n    This class is intentionally not @traceable.\n    Args:\n        replacement: Whether to replace the filtered element with another (thus maintaining the number of steps in an\n            epoch but potentially increasing data repetition) or else shortening the epoch by the number of filtered\n            data points (fewer steps per epoch than expected, but no extra data repetition). Either way, the number of\n            data points within an individual batch will remain the same. Even if `replacement` is true, data will not be\n            repeated until all of the given epoch's data has been traversed (except for at most 1 batch of data which\n            might not appear until after the re-shuffle has occurred).\n    \"\"\"\ndef __init__(self, replacement: bool = True):\nself.replacement = replacement\ndef __repr__(self):\nreturn \"FilteredData\"\n</code></pre>"}, {"location": "fastestimator/util/img_data.html", "title": "img_data", "text": ""}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.BatchDisplay", "title": "<code>BatchDisplay</code>", "text": "<p>         Bases: <code>Display</code></p> <p>An object to combine various batched image components for visualization</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[None, Tensor, Sequence[Tensor]]</code> <p>A batch of image to be displayed. 4-dimensional torch tensors are generally assumed to be channel first, while tf and np are assumed to be channel last. Either way, only 1 or 3 channel images are supported.</p> <code>None</code> <code>text</code> <code>Union[None, Sequence[str], Tensor, Sequence[Tensor]]</code> <p>Text which will be printed in the center of each figure.</p> <code>None</code> <code>masks</code> <code>Union[None, Tensor, Sequence[Tuple[Tensor, str]], Sequence[Sequence[Tensor]], Sequence[Sequence[Tuple[Tensor, str]]]]</code> <p>Batches of one or more 2-dimensional tensors. They may be combined with labels if desired: Bx(, ). Tensors may be 3-dimensional with the second dimension indicating multiple different masks: BxNxx. <code>None</code> <code>bboxes</code> <code>Union[None, Sequence[BoundingBox], Tensor, Sequence[Tensor], Sequence[Sequence[BoundingBox]], Sequence[Sequence[Tensor]]]</code> <p>Batches of one or more bounding boxes of the form (x0, y0, width, height [, label]), where (x0, y0) is the top left corner of the box. These may also be encoded in a tensor of shape (4,) or (N,4) for multiple boxes.</p> <code>None</code> <code>keypoints</code> <code>Union[None, Sequence[KeyPoint], Tensor, Sequence[Tensor], Sequence[Sequence[KeyPoint]], Sequence[Sequence[Tensor]]]</code> <p>Batches of one or more keypoints of the form (x, y [, label]). These may also be encoded in a tensor of shape (2,) or (N,2) for multiple keypoints.</p> <code>None</code> <code>title</code> <code>Union[None, str]</code> <p>What should the title of this figure be.</p> <code>None</code> <code>color_map</code> <code>str</code> <p>How to color 1-channel images. Options from: https://plotly.com/python/builtin-colorscales/</p> <code>'greys'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided arguments violate expected type/shape constraints.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>class BatchDisplay(Display):\n\"\"\"An object to combine various batched image components for visualization\n    Args:\n        image: A batch of image to be displayed. 4-dimensional torch tensors are generally assumed to be channel first,\n            while tf and np are assumed to be channel last. Either way, only 1 or 3 channel images are supported.\n        text: Text which will be printed in the center of each figure.\n        masks: Batches of one or more 2-dimensional tensors. They may be combined with labels if desired:\n            Bx(&lt;mask&gt;, &lt;label&gt;). Tensors may be 3-dimensional with the second dimension indicating multiple different\n            masks: BxNx&lt;mask&gt;x&lt;label&gt;.\n        bboxes: Batches of one or more bounding boxes of the form (x0, y0, width, height [, label]), where (x0, y0) is\n            the top left corner of the box. These may also be encoded in a tensor of shape (4,) or (N,4) for multiple\n            boxes.\n        keypoints: Batches of one or more keypoints of the form (x, y [, label]). These may also be encoded in a tensor\n            of shape (2,) or (N,2) for multiple keypoints.\n        title: What should the title of this figure be.\n        color_map: How to color 1-channel images. Options from: https://plotly.com/python/builtin-colorscales/\n    Raises:\n        AssertionError: If the provided arguments violate expected type/shape constraints.\n    \"\"\"\ndef __init__(self,\nimage: Union[None, 'Tensor', Sequence['Tensor']] = None,\ntext: Union[None, Sequence[str], 'Tensor', Sequence['Tensor']] = None,\nmasks: Union[None, 'Tensor', Sequence[Tuple['Tensor', str]], Sequence[Sequence['Tensor']],\nSequence[Sequence[Tuple['Tensor', str]]]] = None,\nbboxes: Union[None, Sequence['BoundingBox'], 'Tensor', Sequence['Tensor'],\nSequence[Sequence['BoundingBox']], Sequence[Sequence['Tensor']]] = None,\nkeypoints: Union[None, Sequence['KeyPoint'], 'Tensor', Sequence['Tensor'],\nSequence[Sequence['KeyPoint']], Sequence[Sequence['Tensor']]] = None,\ntitle: Union[None, str] = None,\ncolor_map: str = \"greys\"\n):\nself.batch = []\nfor img, txt, mask, bbox, keypoint in zip_longest([] if image is None else image,\n[] if text is None else text,\n[] if masks is None else masks,\n[] if bboxes is None else bboxes,\n[] if keypoints is None else keypoints,\nfillvalue=None):\nself.batch.append(ImageDisplay(image=img,\ntext=txt,\nmasks=mask,\nbboxes=bbox,\nkeypoints=keypoint,\ntitle=None,\ncolor_map=color_map))\nself.batch_size = len(self.batch) or 1\nself.title = title or ''\ndef prepare(self,\nfig: Optional[Figure] = None,\ncol: Optional[int] = None,\ncol_width: int = 400) -&gt; FigureFE:\nif fig is None:\nfig = make_subplots(rows=self.batch_size,\ncols=1,\ncolumn_titles=[self.title],\nvertical_spacing=0.005)\n# Update the figure title text size\nfig.for_each_annotation(lambda a: a.update(font={\n'size': min(20, 3 + int(1 + col_width) // len(a.text or ' ')),\n'family': 'monospace'\n}))\nif col is None:\ncol = 1\nfor row, elem in enumerate(self.batch, start=1):\nfig = elem.prepare(fig=fig, axis=(row, col), col_width=col_width)\nfig.update_layout(width=col_width,\nheight=col_width * self.batch_size,\nmargin={'l': 0, 'r': 0, 'b': 40, 't': 60}\n)\nif not isinstance(fig, FigureFE):\nfig = FigureFE.from_figure(fig)\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.Display", "title": "<code>Display</code>", "text": "<p>         Bases: <code>ABC</code></p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>class Display(ABC):\n@abstractmethod\ndef prepare(self, **kwargs) -&gt; FigureFE:\nraise NotImplementedError()\ndef show(self,\nsave_path: Optional[str] = None,\nverbose: bool = True,\nscale: int = 1,\ninteractive: bool = False) -&gt; None:\n\"\"\"A function which will save or display the image as a plotly figure.\n        Args:\n            save_path: The path where the figure should be saved, or None to display the figure to the screen.\n            verbose: Whether to print out the save location.\n            scale: A scaling factor to apply when exporting to static images (to increase resolution).\n            interactive: Whether the figure should be interactive or static. This is only applicable when\n                save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the\n                resulting jupyter notebook can be dramatically reduced.\n        \"\"\"\nfig = self.prepare()\nfig.show(save_path=save_path, verbose=verbose, scale=scale, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.Display.show", "title": "<code>show</code>", "text": "<p>A function which will save or display the image as a plotly figure.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Optional[str]</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print out the save location.</p> <code>True</code> <code>scale</code> <code>int</code> <p>A scaling factor to apply when exporting to static images (to increase resolution).</p> <code>1</code> <code>interactive</code> <code>bool</code> <p>Whether the figure should be interactive or static. This is only applicable when save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the resulting jupyter notebook can be dramatically reduced.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>def show(self,\nsave_path: Optional[str] = None,\nverbose: bool = True,\nscale: int = 1,\ninteractive: bool = False) -&gt; None:\n\"\"\"A function which will save or display the image as a plotly figure.\n    Args:\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        verbose: Whether to print out the save location.\n        scale: A scaling factor to apply when exporting to static images (to increase resolution).\n        interactive: Whether the figure should be interactive or static. This is only applicable when\n            save_path is None and when running inside a jupyter notebook. The advantage is that the file size of the\n            resulting jupyter notebook can be dramatically reduced.\n    \"\"\"\nfig = self.prepare()\nfig.show(save_path=save_path, verbose=verbose, scale=scale, interactive=interactive)\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImageDisplay", "title": "<code>ImageDisplay</code>", "text": "<p>         Bases: <code>Display</code></p> <p>An object to combine various image components for visualization</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[None, Tensor]</code> <p>An image to be displayed. 3-dimensional torch tensors are generally assumed to be channel first, while tf and np are assumed to be channel last. Either way, only 1 or 3 channel images are supported.</p> <code>None</code> <code>text</code> <code>Union[None, str, Tensor]</code> <p>Text which will be printed in the center of this figure.</p> <code>None</code> <code>masks</code> <code>Union[None, Tensor, Tuple[Tensor, str], Sequence[Tensor], Sequence[Tuple[Tensor, str]]]</code> <p>One or more 2-dimensional tensors. They may be combined with labels if desired (, ). Tensors may be 3-dimensional with the last dimension indicating multiple different masks. <code>None</code> <code>bboxes</code> <code>Union[None, BoundingBox, Tensor, Sequence[BoundingBox], Sequence[Tensor]]</code> <p>One or more bounding boxes of the form (x0, y0, width, height [, label]), where (x0, y0) is the top left corner of the box. These may also be encoded in a tensor of shape (4,) or (N,4) for multiple boxes.</p> <code>None</code> <code>keypoints</code> <code>Union[None, KeyPoint, Tensor, Sequence[KeyPoint], Sequence[Tensor]]</code> <p>One or more keypoints of the form (x, y [, label]). These may also be encoded in a tensor of shape (2,) or (N,2) for multiple keypoints.</p> <code>None</code> <code>title</code> <code>Union[None, str]</code> <p>What should the title of this figure be.</p> <code>None</code> <code>color_map</code> <code>str</code> <p>How to color 1-channel images. Options from: https://plotly.com/python/builtin-colorscales/</p> <code>'gray'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the provided arguments violate expected type/shape constraints.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>class ImageDisplay(Display):\n\"\"\"An object to combine various image components for visualization\n    Args:\n        image: An image to be displayed. 3-dimensional torch tensors are generally assumed to be channel first,\n            while tf and np are assumed to be channel last. Either way, only 1 or 3 channel images are supported.\n        text: Text which will be printed in the center of this figure.\n        masks: One or more 2-dimensional tensors. They may be combined with labels if desired (&lt;mask&gt;, &lt;label&gt;).\n            Tensors may be 3-dimensional with the last dimension indicating multiple different masks.\n        bboxes: One or more bounding boxes of the form (x0, y0, width, height [, label]), where (x0, y0) is the top\n            left corner of the box. These may also be encoded in a tensor of shape (4,) or (N,4) for multiple boxes.\n        keypoints: One or more keypoints of the form (x, y [, label]). These may also be encoded in a tensor of shape\n            (2,) or (N,2) for multiple keypoints.\n        title: What should the title of this figure be.\n        color_map: How to color 1-channel images. Options from: https://plotly.com/python/builtin-colorscales/\n    Raises:\n        AssertionError: If the provided arguments violate expected type/shape constraints.\n    \"\"\"\ndef __init__(self,\nimage: Union[None, 'Tensor'] = None,\ntext: Union[None, str, 'Tensor'] = None,\nmasks: Union[None, 'Tensor', Tuple['Tensor', str], Sequence['Tensor'],\nSequence[Tuple['Tensor', str]]] = None,\nbboxes: Union[None, 'BoundingBox', 'Tensor', Sequence['BoundingBox'], Sequence['Tensor']] = None,\nkeypoints: Union[None, 'KeyPoint', 'Tensor', Sequence['KeyPoint'], Sequence['Tensor']] = None,\ntitle: Union[None, str] = None,\ncolor_map: str = \"gray\"\n):\nif image is not None:\nshape = image.shape\nassert len(shape) in (2, 3), f\"Image must have 2 or 3 dimensions, but found {len(shape)}\"\nif len(image.shape) == 3:\nif isinstance(image, torch.Tensor) and image.shape[0] in (1, 3) and image.shape[2] &gt; 3:\n# Move channel first to channel last\nchannels = list(range(len(image.shape)))\nchannels.append(channels.pop(0))\nimage = image.permute(*channels)\nassert image.shape[2] in (1, 3), f\"Image must have either 1 or 3 channels, but found {image.shape[2]}\"\nif image.shape[2] == 1:\n# pyplot doesn't support (x,y,1) images, so convert them to (x,y)\nimage = np.reshape(image, (image.shape[0], image.shape[1]))  # This works on tf and torch tensors\n# Convert to numpy for consistency\nimage = to_number(image)\nself.image = image\nif text is not None:\n# Convert to numpy for consistency\ntext = to_number(text)\nassert len(text.shape) &lt;= 1, f\"A text tensor can have at most one value, but found {len(text.shape)}\"\nif len(text.shape) == 1:\ntext = text[0]\ntext = text.item()\nif isinstance(text, bytes):\ntext = text.decode('utf8')\ntext = \"{}\".format(text)\nself.text = text\nmasks = to_list(masks)\nmasks = [(mask, '') if not isinstance(mask, tuple) else mask for mask in masks]\nself.masks = []\nself.n_masks = 0\nfor mask_tuple in masks:\nassert isinstance(mask_tuple, tuple), \\\n                \"Masks must be tuples of the form (&lt;tensor&gt;, &lt;label&gt;) or else simply a raw tensor\"\nassert len(mask_tuple) == 2, \"Masks must be tuples of the form (&lt;tensor&gt;, &lt;label&gt;)\"\nassert isinstance(mask_tuple[1], str), \"Masks must be tuples of the form (&lt;tensor&gt;, &lt;label&gt;)\"\nmask = to_number(mask_tuple[0])\nassert len(mask.shape) in (2, 3), \"Masks must be 2 dimensional, or 3 dimensional with the last \" \\\n                                              f\"dimension indicating multiple masks, but found {len(mask.shape)}\"\n# Give all masks a channel dimension for consistency\nif len(mask.shape) == 2:\nmask = np.expand_dims(mask, axis=-1)\n# Move the channels to the front for easy iteration\nmask = np.moveaxis(mask, -1, 0)\nself.n_masks = max(self.n_masks, mask.shape[0])\n# Add an axis on the end which will be used for colors later\nmask = np.expand_dims(mask, -1)\nself.masks.append((mask, mask_tuple[1]))\nbboxes = to_list(bboxes)\nself.bboxes = []\nself.n_bboxes = 0\nfor bbox in bboxes:\nif hasattr(bbox, 'shape'):\nassert len(bbox.shape) in (1, 2), \"Bounding box tensors must be 1 dimensional, or 2 dimensional \" \\\n                                                  \"with the first dimension indicating multiple boxes, but found \" \\\n                                                  f\"{len(bbox.shape)}\"\nassert bbox.shape[-1] in (4, 5), \"Bounding boxes must contain either 4 or 5 elements: (x0, y0, width,\" \\\n                                                 f\" height [,label]), but found {bbox.shape[-1]}\"\nbbox = to_number(bbox)\n# Give all the bboxes a channel dimension for consistency\nif len(bbox.shape) == 1:\nbbox = np.expand_dims(bbox, axis=0)\nself.n_bboxes = max(self.n_bboxes, bbox.shape[0])\nelse:\nself.n_bboxes = max(self.n_bboxes, 1)\nassert len(bbox) in (4, 5), \"Bounding boxes must contain either 4 or 5 elements: (x0, y0, width,\" \\\n                                            f\" height [,label]), but found {len(bbox)}\"\n# Add a channel dimension for consistency\nbbox = [bbox]  # TODO - non-tensor bbox should stack together to get different colors?\nself.bboxes.append(bbox)\n# TODO - keypoint handling\nself.keypoints = keypoints\nself.title = title or ''\nself.color_map = color_map\ndef _make_image(self, im: np.ndarray) -&gt; Image:\nim_max = np.max(im)\nim_min = np.min(im)\nif np.issubdtype(im.dtype, np.integer):\n# im is already in int format\nim = im.astype(np.uint8)\nelif 0 &lt;= im_min &lt;= im_max &lt;= 1:  # im is [0,1]\nim = (im * 255).astype(np.uint8)\nelif -0.5 &lt;= im_min &lt; 0 &lt; im_max &lt;= 0.5:  # im is [-0.5, 0.5]\nim = ((im + 0.5) * 255).astype(np.uint8)\nelif -1 &lt;= im_min &lt; 0 &lt; im_max &lt;= 1:  # im is [-1, 1]\nim = ((im + 1) * 127.5).astype(np.uint8)\nelse:  # im is in some arbitrary range, probably due to the Normalize Op\nma = abs(np.max(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nmi = abs(np.min(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nim = (((im + mi) / (ma + mi)) * 255).astype(np.uint8)\n# Convert (x,y) into (x,y,1) for consistency\nif len(im.shape) == 2:\nim = np.expand_dims(im, axis=-1)\n# Manually apply a colormap to 1-channel images\nif im.shape[2] == 1:\nim = np.array(sample_colorscale(colorscale=self.color_map,\nsamplepoints=np.reshape(im, (-1)) / 255.0,\ncolortype='tuple')).reshape((im.shape[0], im.shape[1], 3))\nim = np.rint(im * 255)\nreturn Image(z=im)\ndef prepare(self,\nfig: Optional[Figure] = None,\naxis: Optional[Tuple[int, int]] = None,\ncol_width: int = 280) -&gt; FigureFE:\nif axis is None:\naxis = (1, 1)\nrow, col = axis\ntitle_size = min(20, col_width // len(self.title or ' '))\nif fig is None:\nfig = make_subplots(rows=1, cols=1, subplot_titles=[self.title] if self.title else None)\nif self.title:\nfig['layout']['annotations'][0]['font'] = {'size': title_size, 'family': 'monospace'}\nfig.update_layout({'plot_bgcolor': '#FFF'})\nx_axis_name = fig.get_subplot(row=row, col=col).xaxis.plotly_name\ny_axis_name = fig.get_subplot(row=row, col=col).yaxis.plotly_name\nfig['layout'][x_axis_name]['showticklabels'] = False\nfig['layout'][y_axis_name]['showticklabels'] = False\n# make y0 the top of the image instead of bottom (this is done automatically for Image)\nfig['layout'][y_axis_name]['autorange'] = 'reversed'\nx_axis_domain = f\"{x_axis_name.split('axis')[0]} domain\"\ny_axis_domain = f\"{y_axis_name.split('axis')[0]} domain\"\n# Put an invisible element on the plot to make other stuff work\nfig.add_annotation(text='', showarrow=False, row=row, col=col)\nif self.image is not None:\nim = self._make_image(im=self.image)\nfig.add_trace(im,\nrow=row,\ncol=col)\nempty_color = np.array((0.0, 0.0, 0.0, 0.0))  # RGBA\nmask_colors = get_colors(n_colors=self.n_masks, alpha=0.4, as_numbers=True)\nmask_colors = [np.array(color) for color in mask_colors]\nfor mask_tuple in self.masks:\nmask, label = mask_tuple\n# Mask will be channel x width x height x 1\nfor color_idx, msk in enumerate(mask):\npositive_color = mask_colors[color_idx]\nmsk = np.where(msk, positive_color, empty_color)\n# TODO - handle labeling\nmsk = Image(z=msk, colormodel='rgba', hoverinfo=None)\nfig.add_trace(msk, row=row, col=col)\n# # Works, and legend interactivity, but slow\n# mask_legend = defaultdict(lambda: True)\n# mask_colors = get_colors(n_colors=self.n_masks, alpha=0.3)\n# for mask_tuple in self.masks:\n#     mask, label = mask_tuple\n#     # Mask will be channel x width x height x 1\n#     for color_idx, msk in enumerate(mask):\n#         y, x = np.where(np.squeeze(msk))\n#         mask_title = label or f\"{color_idx}\"\n#         for y_c, x_c in zip(y, x):\n#             point = Scatter(x=[x_c-0.5, x_c+0.5, x_c+0.5, x_c-0.5],\n#                             y=[y_c+0.5, y_c+0.5, y_c-0.5, y_c-0.5],\n#                             mode='lines',\n#                             fill='toself',\n#                             fillcolor=mask_colors[color_idx],\n#                             name=mask_title,\n#                             legendgroup=mask_title,\n#                             showlegend=mask_legend[mask_title],\n#                             text=mask_title,\n#                             line={'width': 0})\n#             mask_legend[mask_title] = False\n#             fig.add_trace(point, row=row, col=col)\nbbox_colors = get_colors(n_colors=self.n_bboxes)\nfor bbox_set in self.bboxes:\nfor color_idx, bbox in enumerate(bbox_set):\n# Bounding Box Data. Should be (x0, y0, w, h, &lt;label&gt;)\n# Unpack the box, which may or may not have a label\nx0 = float(bbox[0])\ny0 = float(bbox[1])\nwidth = float(bbox[2])\nheight = float(bbox[3])\ncolor = bbox_colors[color_idx]\nlabel = None if len(bbox) &lt; 5 else str(bbox[4])\n# Don't draw empty boxes, or invalid box\nif width &lt;= 0 or height &lt;= 0:\ncontinue\nfig.add_shape({'type': 'rect',\n'x0': x0,\n'x1': x0 + width,\n'y0': y0,\n'y1': y0 + height,\n'line_color': color,\n'line_width': 3},\nrow=row,\ncol=col,\nexclude_empty_subplots=False)\nif label is not None:\nfont_size = max(8, min(14, int(width // len(label or ' '))))\n# One annotation with translucent background\nfig.add_annotation(x=x0,\ny=y0,\nxshift=len(label)*font_size/2,\nyshift=font_size,\ntext=label,\nshowarrow=False,\nfont={'size': font_size,\n'color': 'white',\n'family': 'monospace'},\nbgcolor='white',\nopacity=0.6,\nexclude_empty_subplots=False,\nrow=row,\ncol=col)\n# Another to make the text opaque\nfig.add_annotation(x=x0,\ny=y0,\nxshift=len(label)*font_size/2,\nyshift=font_size,\ntext=label,\nshowarrow=False,\nfont={'size': font_size,\n'color': color,\n'family': 'monospace'},\nexclude_empty_subplots=False,\nrow=row,\ncol=col)\nif self.text:\nfig.add_annotation(text=self.text,\nfont={'size': min(45, col_width // len(self.text or ' ')),\n'color': 'Black',\n'family': 'monospace'},\nshowarrow=False,\nxref=x_axis_domain,\nxanchor='center',\nx=0.5,\nyref=y_axis_domain,\nyanchor='middle',\ny=0.5,\nexclude_empty_subplots=False,\nrow=row,\ncol=col)\nif not isinstance(fig, FigureFE):\nfig = FigureFE.from_figure(fig)\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html", "title": "latex_util", "text": ""}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.AdjustBox", "title": "<code>AdjustBox</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to adjust the size of boxes.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class AdjustBox(Environment):\n\"\"\"A class to adjust the size of boxes.\n    This class is intentionally not @traceable.\n    \"\"\"\npackages = [Package('adjustbox')]\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Center", "title": "<code>Center</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to center content in a page.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Center(Environment):\n\"\"\"A class to center content in a page.\n    This class is intentionally not @traceable.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.ContainerList", "title": "<code>ContainerList</code>", "text": "<p>         Bases: <code>Container</code></p> <p>A class to expedite combining pieces of latex together.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class ContainerList(Container):\n\"\"\"A class to expedite combining pieces of latex together.\n    This class is intentionally not @traceable.\n    \"\"\"\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this container.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nreturn self.dumps_content()\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.ContainerList.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this container.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this container.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nreturn self.dumps_content()\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Form", "title": "<code>Form</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to allow Form elements.</p> <p>This class is intentionally not @traceable. Only one Form is allowed per document.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Form(Environment):\n\"\"\"A class to allow Form elements.\n    This class is intentionally not @traceable. Only one Form is allowed per document.\n    \"\"\"\n_latex_name = 'Form'\npackages = [Package('hyperref', options='hidelinks')]\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.HrefFEID", "title": "<code>HrefFEID</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to represent a colored and underlined hyperref based on a given fe_id.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>fe_id</code> <code>FEID</code> <p>The id used to link this hyperref.</p> required <code>name</code> <code>str</code> <p>A string suffix to be printed as part of the link text.</p> required <code>link_prefix</code> <code>str</code> <p>The prefix for the hyperlink.</p> <code>'tbl'</code> <code>id_in_name</code> <code>bool</code> <p>Whether to include the id in front of the name text.</p> <code>True</code> <code>bold_name</code> <code>bool</code> <p>Whether to bold the name.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class HrefFEID(ContainerList):\n\"\"\"A class to represent a colored and underlined hyperref based on a given fe_id.\n    This class is intentionally not @traceable.\n    Args:\n        fe_id: The id used to link this hyperref.\n        name: A string suffix to be printed as part of the link text.\n        link_prefix: The prefix for the hyperlink.\n        id_in_name: Whether to include the id in front of the name text.\n        bold_name: Whether to bold the name.\n    \"\"\"\ndef __init__(self,\nfe_id: FEID,\nname: str,\nlink_prefix: str = 'tbl',\nid_in_name: bool = True,\nbold_name: bool = False,\ncolor: str = 'blue'):\nself.content_separator = ''\nself.packages.add(Package('hyperref', options='hidelinks'))\nself.packages.add(Package('ulem'))\nself.packages.add(Package('xcolor', options='table'))\nself.fe_id = fe_id\nself.name = name\ndata = [\nNoEscape(r'\\hyperref['),\nescape_latex(f\"{link_prefix}:\"), fe_id,\nNoEscape(r']{\\textcolor{' + color + r'}{\\uline{')\n]\nif id_in_name:\ndata.append(fe_id)\nif name:\ndata.append(\": \")\nif name:\ndata.append(bold(escape_latex(name)) if bold_name else escape_latex(name))\ndata.append(NoEscape(\"}}}\"))\nsuper().__init__(data=data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.IterJoin", "title": "<code>IterJoin</code>", "text": "<p>         Bases: <code>Container</code></p> <p>A class to convert an iterable to a latex representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable</code> <p>Data of the cell.</p> required <code>token</code> <code>str</code> <p>String to serve as separator among items of <code>data</code>.</p> required Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class IterJoin(Container):\n\"\"\"A class to convert an iterable to a latex representation.\n    Args:\n        data: Data of the cell.\n        token: String to serve as separator among items of `data`.\n    \"\"\"\ndef __init__(self, data: Iterable, token: str):\nsuper().__init__(data=data)\nself.token = token\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nreturn dumps_list(self, token=self.token)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.IterJoin.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this cell.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nreturn dumps_list(self, token=self.token)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.PyContainer", "title": "<code>PyContainer</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to convert python containers to a LaTeX representation.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, tuple, set, dict]</code> <p>The python object to be converted to LaTeX.</p> required <code>truncate</code> <code>Optional[int]</code> <p>How many values to display before truncating with an ellipsis. This should be a positive integer or None to disable truncation.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class PyContainer(ContainerList):\n\"\"\"A class to convert python containers to a LaTeX representation.\n    This class is intentionally not @traceable.\n    Args:\n        data: The python object to be converted to LaTeX.\n        truncate: How many values to display before truncating with an ellipsis. This should be a positive integer or\n            None to disable truncation.\n    \"\"\"\ndef __init__(self, data: Union[list, tuple, set, dict], truncate: Optional[int] = None):\nself.packages.add(Package('enumitem', options='inline'))\nassert isinstance(data, (list, tuple, set, dict)), f\"Unacceptable data type for PyContainer: {type(data)}\"\nopen_char = '[' if isinstance(data, list) else '(' if isinstance(data, tuple) else r'\\{'\nclose_char = ']' if isinstance(data, list) else ')' if isinstance(data, tuple) else r'\\}'\nltx = Enumerate(options=Options(NoEscape('label={}'), NoEscape('itemjoin={,}')))\nltx._star_latex_name = True  # Converts this to an inline list\nself.raw_input = data\nif isinstance(data, dict):\nfor key, val in list(data.items())[:truncate]:\nltx.add_item(ContainerList(data=[key, \": \", val]))\nelse:\nfor val in list(data)[:truncate]:\nltx.add_item(val)\nif truncate and len(data) &gt; truncate:\nltx.add_item(NoEscape(r'\\ldots'))\nsuper().__init__(data=[NoEscape(open_char), ltx, NoEscape(close_char)])\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.TextField", "title": "<code>TextField</code>", "text": "<p>         Bases: <code>ContainerCommand</code></p> <p>A class to create editable text fields.</p> <p>This class is intentionally not @traceable. It can only be used inside of a Form.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class TextField(ContainerCommand):\n\"\"\"A class to create editable text fields.\n    This class is intentionally not @traceable. It can only be used inside of a Form.\n    \"\"\"\n_latex_name = \"TextField\"\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.TextFieldBox", "title": "<code>TextFieldBox</code>", "text": "<p>         Bases: <code>ContainerList</code></p> <p>A class to wrap TextFields into padded boxes for use in nesting within tables.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to assign to this TextField. It should be unique within the document since changes to one box will impact all boxes with the same name.</p> required <code>height</code> <code>str</code> <p>How tall should the TextField box be? Note that it will be wrapped by 10pt space on the top and bottom.</p> <code>'2.5cm'</code> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class TextFieldBox(ContainerList):\n\"\"\"A class to wrap TextFields into padded boxes for use in nesting within tables.\n    Args:\n        name: The name to assign to this TextField. It should be unique within the document since changes to one box\n            will impact all boxes with the same name.\n        height: How tall should the TextField box be? Note that it will be wrapped by 10pt space on the top and bottom.\n    \"\"\"\npackages = [Package('xcolor', options='table')]\ndef __init__(self, name: str, height: str = '2.5cm'):\ndata = [\nNoEscape(r\"\\begin{minipage}{\\linewidth}\"),\nNoEscape(r\"\\vspace{3pt}\"),\nTextField(options=[\nNoEscape(r'width=\\linewidth'),\nNoEscape(f'height={height}'),\nNoEscape('backgroundcolor={0.97 0.97 0.97}'),\n'bordercolor=white',\n'multiline=true',\nf'name={name}'\n]),\nNoEscape(r\"\\vspace{3pt}\"),\nNoEscape(r\"\\end{minipage}\")\n]\nsuper().__init__(data=data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.Verbatim", "title": "<code>Verbatim</code>", "text": "<p>         Bases: <code>Environment</code></p> <p>A class to put a string inside the latex verbatim environment.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>The string to be wrapped.</p> required Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class Verbatim(Environment):\n\"\"\"A class to put a string inside the latex verbatim environment.\n    This class is intentionally not @traceable.\n    Args:\n        data: The string to be wrapped.\n    \"\"\"\ndef __init__(self, data: str):\nsuper().__init__(options=None, arguments=None, start_arguments=None, data=NoEscape(data))\nself.content_separator = '\\n'\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.WrapText", "title": "<code>WrapText</code>", "text": "<p>         Bases: <code>LatexObject</code></p> <p>A class to convert strings or numbers to wrappable latex representation.</p> <p>This class will first convert the data to string, and then to a wrappable latex representation if its length is too long. This fixes an issue which prevents the first element placed into a latex X column from wrapping correctly.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, int, float]</code> <p>Input data to be converted.</p> required <code>threshold</code> <code>int</code> <p>When the length of <code>data</code> is greater than <code>threshold</code>, the resulting string will be made wrappable.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>data</code> is not a string, int, or float.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>class WrapText(LatexObject):\n\"\"\"A class to convert strings or numbers to wrappable latex representation.\n    This class will first convert the data to string, and then to a wrappable latex representation if its length is too\n    long. This fixes an issue which prevents the first element placed into a latex X column from wrapping correctly.\n    Args:\n        data: Input data to be converted.\n        threshold: When the length of `data` is greater than `threshold`, the resulting string will be made wrappable.\n    Raises:\n        AssertionError: If `data` is not a string, int, or float.\n    \"\"\"\ndef __init__(self, data: Union[str, int, float], threshold: int):\nassert isinstance(data, (str, int, float)), \"the self.data type needs to be str, int, float\"\nself.threshold = threshold\nself.data = str(data)\nsuper().__init__()\ndef dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n        Returns:\n            A string representation of itself.\n        \"\"\"\nif len(self.data) &gt; self.threshold:\nreturn NoEscape(r'\\seqsplit{' + escape_latex(self.data) + '}')\nelse:\nreturn escape_latex(self.data)\n</code></pre>"}, {"location": "fastestimator/util/latex_util.html#fastestimator.fastestimator.util.latex_util.WrapText.dumps", "title": "<code>dumps</code>", "text": "<p>Get a string representation of this cell.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of itself.</p> Source code in <code>fastestimator\\fastestimator\\util\\latex_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Get a string representation of this cell.\n    Returns:\n        A string representation of itself.\n    \"\"\"\nif len(self.data) &gt; self.threshold:\nreturn NoEscape(r'\\seqsplit{' + escape_latex(self.data) + '}')\nelse:\nreturn escape_latex(self.data)\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html", "title": "traceability_util", "text": ""}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeInputSpec", "title": "<code>FeInputSpec</code>", "text": "<p>A class to keep track of a model's input so that fake inputs can be generated.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>model_input</code> <code>Any</code> <p>The input to the model.</p> required <code>model</code> <code>Model</code> <p>The model which corresponds to the given <code>model_input</code>.</p> required Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeInputSpec:\n\"\"\"A class to keep track of a model's input so that fake inputs can be generated.\n    This class is intentionally not @traceable.\n    Args:\n        model_input: The input to the model.\n        model: The model which corresponds to the given `model_input`.\n    \"\"\"\ndef __init__(self, model_input: Any, model: Model):\nself.shape = to_shape(model_input)\nself.dtype = to_type(model_input)\nself.device = self._get_device(model_input)\nself.tensor_func = tf.ones if isinstance(model, tf.keras.Model) else torch.ones\ndef _get_device(self, data: Any) -&gt; Union[None, str, torch.device]:\n\"\"\"Get the device on which a tensor or collection of tensors is residing.\n        Args:\n            data: A tensor or collection of tensors.\n        Returns:\n            The device on which the tensors are residing\n        \"\"\"\nif tf.is_tensor(data) or isinstance(data, torch.Tensor):\nreturn data.device\nelif isinstance(data, dict):\nreturn self._get_device(list(data.values()))\nelif isinstance(data, (list, tuple, set)):\nfor val in data:\ndevice = self._get_device(val)\nif device is not None:\nreturn device\nelse:\nreturn None\ndef get_dummy_input(self) -&gt; Any:\n\"\"\"Get fake input for the model.\n        Returns:\n            Input of the correct shape and dtype for the model.\n        \"\"\"\nreturn self._from_shape_and_type(self.shape, self.dtype)\ndef _from_shape_and_type(self, shape: Any, dtype: Any) -&gt; Any:\n\"\"\"Constructs tensor(s) with the specified shape and dtype.\n        It is assumed that the `shape` and `dtype` arguments have the same container structure. That is to say, if\n        `shape` is a list of 5 elements, it is required that `dtype` also be a list of 5 elements.\n        Args:\n            shape: A shape or (possibly nested) container of shapes.\n            dtype: A dtype or (possibly nested) container of dtypes.\n        Returns:\n            A tensor or collection of tensors corresponding to the shape and dtype arguments.\n        \"\"\"\nif isinstance(dtype, dict):\nreturn {key: self._from_shape_and_type(value, dtype[key]) for key, value in shape.items()}\nelif isinstance(dtype, list):\nreturn [self._from_shape_and_type(shape[i], dtype[i]) for i in range(len(shape))]\nelif isinstance(dtype, tuple):\nreturn tuple([self._from_shape_and_type(shape[i], dtype[i]) for i in range(len(shape))])\nelif isinstance(dtype, set):\nreturn set([self._from_shape_and_type(s, t) for s, t in zip(shape, dtype)])\nelse:\nretval = self.tensor_func(shape, dtype=dtype)\nif isinstance(self.device, torch.device):\nretval = retval.to(self.device)\nreturn retval\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeInputSpec.get_dummy_input", "title": "<code>get_dummy_input</code>", "text": "<p>Get fake input for the model.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Input of the correct shape and dtype for the model.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def get_dummy_input(self) -&gt; Any:\n\"\"\"Get fake input for the model.\n    Returns:\n        Input of the correct shape and dtype for the model.\n    \"\"\"\nreturn self._from_shape_and_type(self.shape, self.dtype)\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary", "title": "<code>FeSplitSummary</code>", "text": "<p>         Bases: <code>LatexObject</code></p> <p>A class to summarize splits performed on an FE Dataset.</p> <p>This class is intentionally not @traceable.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeSplitSummary(LatexObject):\n\"\"\"A class to summarize splits performed on an FE Dataset.\n    This class is intentionally not @traceable.\n    \"\"\"\ndef __init__(self):\nsuper().__init__()\nself.data = []\ndef add_split(self, parent: Union[FEID, str], fraction: str, seed: Optional[int], stratify: Optional[str]) -&gt; None:\n\"\"\"Record another split on this dataset.\n        Args:\n            parent: The id of the parent involved in the split (or 'self' if you are the parent).\n            fraction: The string representation of the split fraction that was used.\n            seed: The random seed used during the split.\n            stratify: The stratify key used during the split.\n        \"\"\"\nself.data.append((parent, fraction, seed, stratify))\ndef dumps(self) -&gt; str:\n\"\"\"Generate a LaTeX formatted representation of this object.\n        Returns:\n            A LaTeX string representation of this object.\n        \"\"\"\nreturn \" $\\\\rightarrow$ \".join([\nf\"{HrefFEID(parent, name='').dumps() if isinstance(parent, FEID) else parent}({escape_latex(fraction)}\" +\n(f\", seed={seed}\" if seed is not None else \"\") +\n(f\", stratify=`{escape_latex(stratify)}'\" if stratify is not None else \"\") + \")\" for parent,\nfraction,\nseed,\nstratify in self.data\n])\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary.add_split", "title": "<code>add_split</code>", "text": "<p>Record another split on this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>Union[FEID, str]</code> <p>The id of the parent involved in the split (or 'self' if you are the parent).</p> required <code>fraction</code> <code>str</code> <p>The string representation of the split fraction that was used.</p> required <code>seed</code> <code>Optional[int]</code> <p>The random seed used during the split.</p> required <code>stratify</code> <code>Optional[str]</code> <p>The stratify key used during the split.</p> required Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def add_split(self, parent: Union[FEID, str], fraction: str, seed: Optional[int], stratify: Optional[str]) -&gt; None:\n\"\"\"Record another split on this dataset.\n    Args:\n        parent: The id of the parent involved in the split (or 'self' if you are the parent).\n        fraction: The string representation of the split fraction that was used.\n        seed: The random seed used during the split.\n        stratify: The stratify key used during the split.\n    \"\"\"\nself.data.append((parent, fraction, seed, stratify))\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSplitSummary.dumps", "title": "<code>dumps</code>", "text": "<p>Generate a LaTeX formatted representation of this object.</p> <p>Returns:</p> Type Description <code>str</code> <p>A LaTeX string representation of this object.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def dumps(self) -&gt; str:\n\"\"\"Generate a LaTeX formatted representation of this object.\n    Returns:\n        A LaTeX string representation of this object.\n    \"\"\"\nreturn \" $\\\\rightarrow$ \".join([\nf\"{HrefFEID(parent, name='').dumps() if isinstance(parent, FEID) else parent}({escape_latex(fraction)}\" +\n(f\", seed={seed}\" if seed is not None else \"\") +\n(f\", stratify=`{escape_latex(stratify)}'\" if stratify is not None else \"\") + \")\" for parent,\nfraction,\nseed,\nstratify in self.data\n])\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSummaryTable", "title": "<code>FeSummaryTable</code>", "text": "<p>A class containing summaries of traceability information.</p> <p>This class is intentionally not @traceable.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The string to be used as the title line in the summary table.</p> required <code>fe_id</code> <code>FEID</code> <p>The id of this table, used for cross-referencing from other tables.</p> required <code>target_type</code> <code>Type</code> <p>The type of the object being summarized.</p> required <code>path</code> <code>Union[None, str, LatexObject]</code> <p>The import path of the object in question. Might be more complicated when methods/functions are involved.</p> <code>None</code> <code>kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>The keyword arguments used to instantiate the object being summarized.</p> <code>None</code> <code>**fields</code> <code>Any</code> <p>Any other information about the summarized object / function.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>class FeSummaryTable:\n\"\"\"A class containing summaries of traceability information.\n    This class is intentionally not @traceable.\n    Args:\n        name: The string to be used as the title line in the summary table.\n        fe_id: The id of this table, used for cross-referencing from other tables.\n        target_type: The type of the object being summarized.\n        path: The import path of the object in question. Might be more complicated when methods/functions are involved.\n        kwargs: The keyword arguments used to instantiate the object being summarized.\n        **fields: Any other information about the summarized object / function.\n    \"\"\"\nname: Union[str, LatexObject]\nfe_id: FEID\nfields: Dict[str, Any]\ndef __init__(self,\nname: str,\nfe_id: FEID,\ntarget_type: Type,\npath: Union[None, str, LatexObject] = None,\nkwargs: Optional[Dict[str, Any]] = None,\n**fields: Any):\nself.name = name\nself.fe_id = fe_id\nself.type = target_type\nself.path = path\nself.args = fields.pop(\"args\", None)\nself.kwargs = kwargs or {}\nself.fields = fields\ndef render_table(self,\ndoc: Document,\nname_override: Optional[LatexObject] = None,\ntoc_ref: Optional[str] = None,\nextra_rows: Optional[List[Tuple[str, Any]]] = None) -&gt; None:\n\"\"\"Write this table into a LaTeX document.\n        Args:\n            doc: The LaTeX document to be appended to.\n            name_override: An optional replacement for this table's name field.\n            toc_ref: A reference to be added to the table of contents.\n            extra_rows: Any extra rows to be added to the table before the kwargs.\n        \"\"\"\nwith doc.create(Table(position='htp!')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\ntable.append(Label(Marker(name=str(self.fe_id), prefix=\"tbl\")))\nif toc_ref:\ntable.append(NoEscape(r'\\addcontentsline{toc}{subsection}{' + escape_latex(toc_ref) + '}'))\nwith doc.create(Tabularx('|lX|', booktabs=True)) as tabular:\npackage = Package('xcolor', options='table')\nif package not in tabular.packages:\n# Need to invoke a table color before invoking TextColor (bug?)\ntabular.packages.append(package)\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\ntabular.add_row((name_override if name_override else bold(self.name),\nMultiColumn(size=1, align='r|', data=TextColor('blue', self.fe_id))))\ntabular.add_hline()\ntype_str = f\"{self.type}\"\nmatch = re.fullmatch(r'^&lt;.* \\'(?P&lt;typ&gt;.*)\\'&gt;$', type_str)\ntype_str = match.group(\"typ\") if match else type_str\ntabular.add_row((\"Type: \", escape_latex(type_str)))\nif self.path:\nif isinstance(self.path, LatexObject):\ntabular.add_row((\"\", self.path))\nelse:\ntabular.add_row((\"\", escape_latex(self.path)))\nfor k, v in self.fields.items():\ntabular.add_hline()\ntabular.add_row((f\"{k.capitalize()}: \", v))\nif self.args:\ntabular.add_hline()\ntabular.add_row((\"Args: \", self.args))\nif extra_rows:\nfor (key, val) in extra_rows:\ntabular.add_hline()\ntabular.add_row(key, val)\nif self.kwargs:\ntabular.add_hline()\nfor idx, (kwarg, val) in enumerate(self.kwargs.items()):\ntabular.add_row((italic(kwarg), val), color='white' if idx % 2 else 'black!5')\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.FeSummaryTable.render_table", "title": "<code>render_table</code>", "text": "<p>Write this table into a LaTeX document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>The LaTeX document to be appended to.</p> required <code>name_override</code> <code>Optional[LatexObject]</code> <p>An optional replacement for this table's name field.</p> <code>None</code> <code>toc_ref</code> <code>Optional[str]</code> <p>A reference to be added to the table of contents.</p> <code>None</code> <code>extra_rows</code> <code>Optional[List[Tuple[str, Any]]]</code> <p>Any extra rows to be added to the table before the kwargs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def render_table(self,\ndoc: Document,\nname_override: Optional[LatexObject] = None,\ntoc_ref: Optional[str] = None,\nextra_rows: Optional[List[Tuple[str, Any]]] = None) -&gt; None:\n\"\"\"Write this table into a LaTeX document.\n    Args:\n        doc: The LaTeX document to be appended to.\n        name_override: An optional replacement for this table's name field.\n        toc_ref: A reference to be added to the table of contents.\n        extra_rows: Any extra rows to be added to the table before the kwargs.\n    \"\"\"\nwith doc.create(Table(position='htp!')) as table:\ntable.append(NoEscape(r'\\refstepcounter{table}'))\ntable.append(Label(Marker(name=str(self.fe_id), prefix=\"tbl\")))\nif toc_ref:\ntable.append(NoEscape(r'\\addcontentsline{toc}{subsection}{' + escape_latex(toc_ref) + '}'))\nwith doc.create(Tabularx('|lX|', booktabs=True)) as tabular:\npackage = Package('xcolor', options='table')\nif package not in tabular.packages:\n# Need to invoke a table color before invoking TextColor (bug?)\ntabular.packages.append(package)\npackage = Package('seqsplit')\nif package not in tabular.packages:\ntabular.packages.append(package)\ntabular.add_row((name_override if name_override else bold(self.name),\nMultiColumn(size=1, align='r|', data=TextColor('blue', self.fe_id))))\ntabular.add_hline()\ntype_str = f\"{self.type}\"\nmatch = re.fullmatch(r'^&lt;.* \\'(?P&lt;typ&gt;.*)\\'&gt;$', type_str)\ntype_str = match.group(\"typ\") if match else type_str\ntabular.add_row((\"Type: \", escape_latex(type_str)))\nif self.path:\nif isinstance(self.path, LatexObject):\ntabular.add_row((\"\", self.path))\nelse:\ntabular.add_row((\"\", escape_latex(self.path)))\nfor k, v in self.fields.items():\ntabular.add_hline()\ntabular.add_row((f\"{k.capitalize()}: \", v))\nif self.args:\ntabular.add_hline()\ntabular.add_row((\"Args: \", self.args))\nif extra_rows:\nfor (key, val) in extra_rows:\ntabular.add_hline()\ntabular.add_row(key, val)\nif self.kwargs:\ntabular.add_hline()\nfor idx, (kwarg, val) in enumerate(self.kwargs.items()):\ntabular.add_row((italic(kwarg), val), color='white' if idx % 2 else 'black!5')\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.fe_summary", "title": "<code>fe_summary</code>", "text": "<p>Return a summary of how this class was instantiated (for traceability).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>The bound class instance.</p> required <p>Returns:</p> Type Description <code>List[FeSummaryTable]</code> <p>A summary of the instance.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def fe_summary(self) -&gt; List[FeSummaryTable]:\n\"\"\"Return a summary of how this class was instantiated (for traceability).\n    Args:\n        self: The bound class instance.\n    Returns:\n        A summary of the instance.\n    \"\"\"\n# Delayed imports to avoid circular dependency\nfrom torch.utils.data import Dataset\nfrom fastestimator.estimator import Estimator\nfrom fastestimator.network import TFNetwork, TorchNetwork\nfrom fastestimator.op.op import Op\nfrom fastestimator.pipeline import Pipeline\nfrom fastestimator.schedule.schedule import Scheduler\nfrom fastestimator.trace.trace import Trace\n# re-number the references for nicer viewing\nordered_items = sorted(\nself._fe_traceability_summary.items(),\nkey=lambda x: 0 if issubclass(x[1].type, Estimator) else 1\nif issubclass(x[1].type, (TFNetwork, TorchNetwork)) else 2 if issubclass(x[1].type, Pipeline) else 3\nif issubclass(x[1].type, Scheduler) else 4 if issubclass(x[1].type, Trace) else 5\nif issubclass(x[1].type, Op) else 6 if issubclass(x[1].type, (Dataset, tf.data.Dataset)) else 7\nif issubclass(x[1].type, (tf.keras.Model, torch.nn.Module)) else 8\nif issubclass(x[1].type, types.FunctionType) else 9\nif issubclass(x[1].type, (np.ndarray, tf.Tensor, tf.Variable, torch.Tensor)) else 10)\nkey_mapping = {fe_id: f\"@FE{idx}\" for idx, (fe_id, val) in enumerate(ordered_items)}\nFEID.set_translation_dict(key_mapping)\nreturn [item[1] for item in ordered_items]\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.is_restorable", "title": "<code>is_restorable</code>", "text": "<p>Determine whether a given object can be restored easily via Pickle.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The object in question.</p> required <code>memory_limit</code> <code>int</code> <p>The maximum memory size (in bytes) to allow for an object (or 0 for no limit).</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>(result, memory size) where result is True iff <code>data</code> is only comprised of 'simple' objects and does not exceed</p> <code>int</code> <p>the <code>memory_limit</code>. If the result is False, then memory size will be &lt;= the true memory size of the <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def is_restorable(data: Any, memory_limit: int = 0) -&gt; Tuple[bool, int]:\n\"\"\"Determine whether a given object can be restored easily via Pickle.\n    Args:\n        data: The object in question.\n        memory_limit: The maximum memory size (in bytes) to allow for an object (or 0 for no limit).\n    Returns:\n        (result, memory size) where result is True iff `data` is only comprised of 'simple' objects and does not exceed\n        the `memory_limit`. If the result is False, then memory size will be &lt;= the true memory size of the `data`.\n    \"\"\"\nif isinstance(data, _RestorableClasses):\nsize = sys.getsizeof(data)\nif isinstance(data, tf.Tensor):\nsize = sys.getsizeof(data.numpy())\nelif isinstance(data, torch.Tensor):\nsize = data.element_size() * data.nelement()\nreturn True, size\nelif isinstance(data, dict):\nsize = 0\nfor key, value in data.items():\nkey_stat = is_restorable(key, memory_limit)\nif key_stat[0] is False:\nreturn False, size\nsize += key_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nval_stat = is_restorable(value, memory_limit)\nif val_stat[0] is False:\nreturn False, size\nsize += val_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nreturn True, size\nelif isinstance(data, (list, tuple, set)):\nsize = 0\nfor elem in data:\nelem_stat = is_restorable(elem, memory_limit)\nif elem_stat[0] is False:\nreturn False, size\nsize += elem_stat[1]\nif 0 &lt; memory_limit &lt; size:\nreturn False, size\nreturn True, size\nelse:\nreturn False, 0\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.trace_model", "title": "<code>trace_model</code>", "text": "<p>A function to add traceability information to an FE-compiled model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to be made traceable.</p> required <code>model_idx</code> <code>int</code> <p>Which of the return values from the <code>model_fn</code> is this model (or -1 if only a single return value).</p> required <code>model_fn</code> <code>Any</code> <p>The function used to generate this model.</p> required <code>optimizer_fn</code> <code>Any</code> <p>The thing used to define this model's optimizer.</p> required <code>weights_path</code> <code>Any</code> <p>The path to the weights for this model.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The <code>model</code>, but now with an fe_summary() method.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def trace_model(model: Model, model_idx: int, model_fn: Any, optimizer_fn: Any, weights_path: Any) -&gt; Model:\n\"\"\"A function to add traceability information to an FE-compiled model.\n    Args:\n        model: The model to be made traceable.\n        model_idx: Which of the return values from the `model_fn` is this model (or -1 if only a single return value).\n        model_fn: The function used to generate this model.\n        optimizer_fn: The thing used to define this model's optimizer.\n        weights_path: The path to the weights for this model.\n    Returns:\n        The `model`, but now with an fe_summary() method.\n    \"\"\"\ntables = {}\ndescription = {'definition': _trace_value(model_fn, tables, ret_ref=Flag())}\nif model_idx != -1:\ndescription['index'] = model_idx\nif optimizer_fn or isinstance(optimizer_fn, list) and optimizer_fn[0] is not None:\ndescription['optimizer'] = _trace_value(\noptimizer_fn[model_idx] if isinstance(optimizer_fn, list) else optimizer_fn, tables, ret_ref=Flag())\nif weights_path:\ndescription['weights'] = _trace_value(weights_path, tables, ret_ref=Flag())\nfe_id = FEID(id(model))\ntbl = FeSummaryTable(name=model.model_name, fe_id=fe_id, target_type=type(model), **description)\ntables[fe_id] = tbl\n# Have to put this in a ChainMap b/c dict gets put into model._layers automatically somehow\nmodel._fe_traceability_summary = ChainMap(tables)\n# Use MethodType to bind the method to the class instance\nsetattr(model, 'fe_summary', types.MethodType(fe_summary, model))\nreturn model\n</code></pre>"}, {"location": "fastestimator/util/traceability_util.html#fastestimator.fastestimator.util.traceability_util.traceable", "title": "<code>traceable</code>", "text": "<p>A decorator to be placed on classes in order to make them traceable and to enable a deep restore.</p> <p>Decorated classes will gain the .fe_summary() and .fe_state() methods.</p> <p>Parameters:</p> Name Type Description Default <code>whitelist</code> <code>Union[str, Tuple[str, ...]]</code> <p>Arguments which should be included in a deep restore of the decorated class.</p> <code>()</code> <code>blacklist</code> <code>Union[str, Tuple[str, ...]]</code> <p>Arguments which should be excluded from a deep restore of the decorated class.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The decorated class.</p> Source code in <code>fastestimator\\fastestimator\\util\\traceability_util.py</code> <pre><code>def traceable(whitelist: Union[str, Tuple[str, ...]] = (), blacklist: Union[str, Tuple[str, ...]] = ()) -&gt; Callable:\n\"\"\"A decorator to be placed on classes in order to make them traceable and to enable a deep restore.\n    Decorated classes will gain the .fe_summary() and .fe_state() methods.\n    Args:\n        whitelist: Arguments which should be included in a deep restore of the decorated class.\n        blacklist: Arguments which should be excluded from a deep restore of the decorated class.\n    Returns:\n        The decorated class.\n    \"\"\"\nif isinstance(whitelist, str):\nwhitelist = (whitelist, )\nif isinstance(blacklist, str):\nblacklist = (blacklist, )\nif whitelist and blacklist:\nraise ValueError(\"Traceable objects may specify a whitelist or a blacklist, but not both\")\ndef make_traceable(cls):\nbase_init = getattr(cls, '__init__')\nif hasattr(base_init, '__module__') and base_init.__module__ != 'fastestimator.util.traceability_util':\n# We haven't already overridden this class' init method\n@functools.wraps(base_init)  # to preserve the original class signature\ndef init(self, *args, **kwargs):\nif not hasattr(self, '_fe_state_whitelist'):\nself._fe_state_whitelist = whitelist\nelse:\nself._fe_state_whitelist = tuple(set(self._fe_state_whitelist).union(set(whitelist)))\nif not hasattr(self, '_fe_state_blacklist'):\nself._fe_state_blacklist = blacklist + (\n'_fe_state_whitelist', '_fe_state_blacklist', '_fe_traceability_summary')\nelse:\nself._fe_state_blacklist = tuple(set(self._fe_state_blacklist).union(set(blacklist)))\nif not hasattr(self, '_fe_traceability_summary'):\nbound_args = inspect.signature(base_init).bind(self, *args, **kwargs)\nbound_args.apply_defaults()\ntables = {}\n_trace_value(_BoundFn(self, bound_args), tables, ret_ref=Flag())\nself._fe_traceability_summary = tables\nbase_init(self, *args, **kwargs)\nsetattr(cls, '__init__', init)\nbase_func = getattr(cls, 'fe_summary', None)\nif base_func is None:\nsetattr(cls, 'fe_summary', fe_summary)\nbase_func = getattr(cls, '__getstate__', None)\nif base_func is None:\nsetattr(cls, '__getstate__', __getstate__)\nbase_func = getattr(cls, '__setstate__', None)\nif base_func is None:\nsetattr(cls, '__setstate__', __setstate__)\nreturn cls\nreturn make_traceable\n</code></pre>"}, {"location": "fastestimator/util/util.html", "title": "util", "text": "<p>Utilities for FastEstimator.</p>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Timer", "title": "<code>Timer</code>", "text": "<p>         Bases: <code>ContextDecorator</code></p> <p>A class that can be used to time things.</p> <p>This class is intentionally not @traceable.</p> <pre><code>x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\nwith fe.util.Timer():\nx()  # Task took 0.1639 seconds\n@fe.util.Timer(\"T2\")\ndef func():\nreturn x()\nfunc()  # T2 took 0.14819 seconds\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Timer(ContextDecorator):\n\"\"\"A class that can be used to time things.\n    This class is intentionally not @traceable.\n    ```python\n    x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\n    with fe.util.Timer():\n        x()  # Task took 0.1639 seconds\n    @fe.util.Timer(\"T2\")\n    def func():\n        return x()\n    func()  # T2 took 0.14819 seconds\n    ```\n    \"\"\"\ndef __init__(self, name=\"Task\") -&gt; None:\nself.name = name\nself.start = None\nself.end = None\nself.interval = None\ndef __enter__(self) -&gt; 'Timer':\nself.start = time.perf_counter()\nreturn self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nself.end = time.perf_counter()\nself.interval = self.end - self.start\ntf.print(\"{} took {} seconds\".format(self.name, self.interval))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.cpu_count", "title": "<code>cpu_count</code>", "text": "<p>Determine the nuber of available CPUs (correcting for docker container limits).</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>If provided, the TF and Torch backends will be told to use <code>limit</code> number of threads, or the available number of cpus if the latter is lower (<code>limit</code> cannot raise the number of threads). A limit can only be enforced once per python session, before starting anything like pipeline which requires multiprocessing.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The nuber of available CPUs (correcting for docker container limits), or the user provided <code>limit</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a <code>limit</code> is provided which doesn't match previously enforced limits.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def cpu_count(limit: Optional[int] = None) -&gt; int:\n\"\"\"Determine the nuber of available CPUs (correcting for docker container limits).\n    Args:\n        limit: If provided, the TF and Torch backends will be told to use `limit` number of threads, or the available\n            number of cpus if the latter is lower (`limit` cannot raise the number of threads). A limit can only be\n            enforced once per python session, before starting anything like pipeline which requires multiprocessing.\n    Returns:\n        The nuber of available CPUs (correcting for docker container limits), or the user provided `limit`.\n    Raises:\n        ValueError: If a `limit` is provided which doesn't match previously enforced limits.\n    \"\"\"\nexisting_limit = os.environ.get('FE_NUM_THREADS_', None)  # This variable is used internally to indicate whether cpu\n# limits have already been enforced in this python session\nif existing_limit:\ntry:\nexisting_limit = int(existing_limit)\nexcept ValueError as err:\nprint(\"FastEstimator-Error: FE_NUM_THREADS_ is an internal variable. Use FE_NUM_THREADS (no underscore)\")\nraise err\nif limit and limit != existing_limit:\nraise ValueError(f\"Tried to enforce a cpu limit of {limit}, but {existing_limit} was already set.\")\nreturn existing_limit\n# Check if user provided an environment variable limit on the number of threads\nenv_limit = os.environ.get('FE_NUM_THREADS', None)  # User might set this one in a bash script\nif env_limit:\ntry:\nenv_limit = int(env_limit)\nexcept ValueError as err:\nprint(f\"FastEstimator-Warn: FE_NUM_THREADS variable must be an integer, but was set to: {env_limit}\")\nraise err\ntry:\n# In docker containers which have --cpuset-cpus, the limit won't be reflected by normal os.cpu_count() call\ncores = len(os.sched_getaffinity(0))\nexcept AttributeError:\n# Running on Mac or Windows where the above method isn't available, so use the regular way\ncores = os.cpu_count()\ncores = min(cores, limit or cores, env_limit or cores)\nif cores &lt; 1:\nraise ValueError(f\"At least 1 core is required for training, but found {cores}\")\nos.environ['FE_NUM_THREADS_'] = f\"{cores}\"  # Remember the value so we don't try to re-set the frameworks later\nos.environ['OMP_NUM_THREADS'] = f\"{cores}\"\nos.environ['MKL_NUM_THREADS'] = f\"{cores}\"\nos.environ['TF_NUM_INTEROP_THREADS'] = f\"{cores}\"\nos.environ['TF_NUM_INTRAOP_THREADS'] = f\"{cores}\"\ntorch.set_num_threads(cores)\ntorch.set_num_interop_threads(cores)\nreturn cores\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.draw", "title": "<code>draw</code>", "text": "<p>Print our name.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def draw() -&gt; None:\n\"\"\"Print our name.\n    \"\"\"\nprint(Figlet(font=\"slant\").renderText(\"FastEstimator\"))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_batch_size", "title": "<code>get_batch_size</code>", "text": "<p>Infer batch size from a batch dictionary. It will ignore all dictionary value with data type that doesn't have \"shape\" attribute.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The batch dictionary.</p> required <p>Returns:</p> Type Description <code>int</code> <p>batch size.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_batch_size(data: Dict[str, Any]) -&gt; int:\n\"\"\"Infer batch size from a batch dictionary. It will ignore all dictionary value with data type that\n    doesn't have \"shape\" attribute.\n    Args:\n        data: The batch dictionary.\n    Returns:\n        batch size.\n    \"\"\"\nassert isinstance(data, dict), \"data input must be a dictionary\"\nbatch_size = set(data[key].shape[0] for key in data if hasattr(data[key], \"shape\") and list(data[key].shape))\nassert len(batch_size) == 1, \"invalid batch size: {}\".format(batch_size)\nreturn batch_size.pop()\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_num_devices", "title": "<code>get_num_devices</code>", "text": "<p>Determine the number of available GPUs.</p> <p>Returns:</p> Type Description <p>The number of available GPUs, or 1 if none are found.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_num_devices():\n\"\"\"Determine the number of available GPUs.\n    Returns:\n        The number of available GPUs, or 1 if none are found.\n    \"\"\"\nreturn max(torch.cuda.device_count(), 1)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_batch", "title": "<code>pad_batch</code>", "text": "<p>A function to pad a batch of data in-place by appending to the ends of the tensors. Tensor type needs to be numpy array otherwise would get ignored. (tf.Tensor and torch.Tensor will cause error)</p> <pre><code>data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\nfe.util.pad_batch(data, pad_value=0)\nprint(data)  # [{'x': [[1., 1.], [1., 1.], [0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[MutableMapping[str, np.ndarray]]</code> <p>A list of data to be padded.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to pad with.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the data within the batch do not have matching rank, or have different keys</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_batch(batch: List[MutableMapping[str, np.ndarray]], pad_value: Union[float, int]) -&gt; None:\n\"\"\"A function to pad a batch of data in-place by appending to the ends of the tensors. Tensor type needs to be\n    numpy array otherwise would get ignored. (tf.Tensor and torch.Tensor will cause error)\n    ```python\n    data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\n    fe.util.pad_batch(data, pad_value=0)\n    print(data)  # [{'x': [[1., 1.], [1., 1.], [0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n    ```\n    Args:\n        batch: A list of data to be padded.\n        pad_value: The value to pad with.\n    Raises:\n        AssertionError: If the data within the batch do not have matching rank, or have different keys\n    \"\"\"\nkeys = batch[0].keys()\nfor one_batch in batch:\nassert one_batch.keys() == keys, \"data within batch must have same keys\"\nfor key in keys:\nshapes = [data[key].shape for data in batch if hasattr(data[key], \"shape\")]\nif len(set(shapes)) &gt; 1:\nassert len(set(len(shape) for shape in shapes)) == 1, \"data within batch must have same rank\"\nmax_shapes = tuple(np.max(np.array(shapes), axis=0))\nfor data in batch:\ndata[key] = pad_data(data[key], max_shapes, pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_data", "title": "<code>pad_data</code>", "text": "<p>Pad <code>data</code> by appending <code>pad_value</code>s along it's dimensions until the <code>target_shape</code> is reached. All entries of target_shape should be larger than the data.shape, and have the same rank.</p> <pre><code>x = np.ones((1,2))\nx = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\nx = fe.util.pad_data(x, target_shape=(3, 3, 3), pad_value = -2) # error\nx = fe.util.pad_data(x, target_shape=(4, 1), pad_value = -2) # error\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The data to be padded.</p> required <code>target_shape</code> <code>Tuple[int, ...]</code> <p>The desired shape for <code>data</code>. Should have the same rank as <code>data</code>, with each dimension being &gt;= the size of the <code>data</code> dimension.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to insert into <code>data</code> if padding is required to achieve the <code>target_shape</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The <code>data</code>, padded to the <code>target_shape</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_data(data: np.ndarray, target_shape: Tuple[int, ...], pad_value: Union[float, int]) -&gt; np.ndarray:\n\"\"\"Pad `data` by appending `pad_value`s along it's dimensions until the `target_shape` is reached. All entries of\n    target_shape should be larger than the data.shape, and have the same rank.\n    ```python\n    x = np.ones((1,2))\n    x = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\n    x = fe.util.pad_data(x, target_shape=(3, 3, 3), pad_value = -2) # error\n    x = fe.util.pad_data(x, target_shape=(4, 1), pad_value = -2) # error\n    ```\n    Args:\n        data: The data to be padded.\n        target_shape: The desired shape for `data`. Should have the same rank as `data`, with each dimension being &gt;=\n            the size of the `data` dimension.\n        pad_value: The value to insert into `data` if padding is required to achieve the `target_shape`.\n    Returns:\n        The `data`, padded to the `target_shape`.\n    \"\"\"\nshape_difference = np.array(target_shape) - np.array(data.shape)\npadded_shape = np.array([np.zeros_like(shape_difference), shape_difference]).T\nreturn np.pad(data, padded_shape, 'constant', constant_values=pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_number", "title": "<code>to_number</code>", "text": "<p>Convert an input value into a Numpy ndarray.</p> <p>This method can be used with Python and Numpy data: <pre><code>b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\nb = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\nn = np.array([1, 2, 3])\nb = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([1, 2, 3])\nb = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([1, 2, 3])\nb = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[tf.Tensor, torch.Tensor, np.ndarray, int, float, str]</code> <p>The value to be converted into a np.ndarray.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An ndarray corresponding to the given <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_number(data: Union[tf.Tensor, torch.Tensor, np.ndarray, int, float, str]) -&gt; np.ndarray:\n\"\"\"Convert an input value into a Numpy ndarray.\n    This method can be used with Python and Numpy data:\n    ```python\n    b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\n    b = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\n    n = np.array([1, 2, 3])\n    b = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([1, 2, 3])\n    b = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([1, 2, 3])\n    b = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    Args:\n        data: The value to be converted into a np.ndarray.\n    Returns:\n        An ndarray corresponding to the given `data`.\n    \"\"\"\nif tf.is_tensor(data):\ndata = data.numpy()\nelif isinstance(data, torch.Tensor):\nif data.requires_grad:\ndata = data.detach().numpy()\nelse:\ndata = data.numpy()\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html", "title": "wget_util", "text": ""}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.bar_custom", "title": "<code>bar_custom</code>", "text": "<p>Return progress bar string for given values in one of three styles depending on available width.</p> <p>This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.</p> The bar will be one of the following formats depending on available width <p>[..  ] downloaded / total downloaded / total [.. ]</p> <p>If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:     %s / unknown     %s</p> <p>If there is not enough space on the screen, do not display anything. The returned string doesn't include control characters like   used to place cursor at the beginning of the line to erase previous content.</p> <p>This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.</p> <pre><code>wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>float</code> <p>The current amount of progress.</p> required <code>total</code> <code>float</code> <p>The total amount of progress required by the task.</p> required <code>width</code> <code>int</code> <p>The available width.</p> <code>80</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string to display the current progress.</p> Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def bar_custom(current: float, total: float, width: int = 80) -&gt; str:\n\"\"\"Return progress bar string for given values in one of three styles depending on available width.\n    This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.\n    The bar will be one of the following formats depending on available width:\n        [..  ] downloaded / total\n        downloaded / total\n        [.. ]\n    If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:\n        %s / unknown\n        %s\n    If there is not enough space on the screen, do not display anything. The returned string doesn't include control\n    characters like \\r used to place cursor at the beginning of the line to erase previous content.\n    This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.\n    ```python\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        current: The current amount of progress.\n        total: The total amount of progress required by the task.\n        width: The available width.\n    Returns:\n        A formatted string to display the current progress.\n    \"\"\"\n# process special case when total size is unknown and return immediately\nif not total or total &lt; 0:\nmsg = \"{} / unknown\".format(current)\nif len(msg) &lt; width:  # leaves one character to avoid linefeed\nreturn msg\nif len(\"{}\".format(current)) &lt; width:\nreturn \"{}\".format(current)\n# --- adaptive layout algorithm ---\n#\n# [x] describe the format of the progress bar\n# [x] describe min width for each data field\n# [x] set priorities for each element\n# [x] select elements to be shown\n#   [x] choose top priority element min_width &lt; avail_width\n#   [x] lessen avail_width by value if min_width\n#   [x] exclude element from priority list and repeat\n#  10% [.. ]  10/100\n# pppp bbbbb sssssss\nmin_width = {\n'percent': 4,  # 100%\n'bar': 3,  # [.]\n'size': len(\"{}\".format(total)) * 2 + 3,  # 'xxxx / yyyy'\n}\npriority = ['percent', 'bar', 'size']\n# select elements to show\nselected = []\navail = width\nfor field in priority:\nif min_width[field] &lt; avail:\nselected.append(field)\navail -= min_width[field] + 1  # +1 is for separator or for reserved space at\n# the end of line to avoid linefeed on Windows\n# render\noutput = ''\nfor field in selected:\nif field == 'percent':\n# fixed size width for percentage\noutput += \"{}%\".format(100 * current // total).rjust(min_width['percent'])\nelif field == 'bar':  # [. ]\n# bar takes its min width + all available space\noutput += wget.bar_thermometer(current, total, min_width['bar'] + avail)\nelif field == 'size':\n# size field has a constant width (min == max)\noutput += \"{:.2f} / {:.2f} MB\".format(current / 1e6, total / 1e6).rjust(min_width['size'])\nselected = selected[1:]\nif selected:\noutput += ' '  # add field separator\nreturn output\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.callback_progress", "title": "<code>callback_progress</code>", "text": "<p>Callback function for urlretrieve that is called when a connection is created and then once for each block.</p> <p>Draws adaptive progress bar in terminal/console.</p> <p>Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a linefeed on Windows.</p> <pre><code>import wget\nwget.callback_progress = fe.util.callback_progress\nwget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocks</code> <code>int</code> <p>number of blocks transferred so far.</p> required <code>block_size</code> <code>int</code> <p>in bytes.</p> required <code>total_size</code> <code>int</code> <p>in bytes, can be -1 if server doesn't return it.</p> required <code>bar_function</code> <code>Callable[[int, int, int], str]</code> <p>another callback function to visualize progress.</p> required Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def callback_progress(blocks: int, block_size: int, total_size: int, bar_function: Callable[[int, int, int],\nstr]) -&gt; None:\n\"\"\"Callback function for urlretrieve that is called when a connection is created and then once for each block.\n    Draws adaptive progress bar in terminal/console.\n    Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a\n    linefeed on Windows.\n    ```python\n    import wget\n    wget.callback_progress = fe.util.callback_progress\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        blocks: number of blocks transferred so far.\n        block_size: in bytes.\n        total_size: in bytes, can be -1 if server doesn't return it.\n        bar_function: another callback function to visualize progress.\n    \"\"\"\nwidth = min(100, wget.get_console_width())\nif width == 0:  # sys.stdout.fileno() in get_console_width() is not supported in jupyter notebook\nwidth = 80\ncurrent_size = min(blocks * block_size, total_size)\nprogress = bar_function(current_size, total_size, width)\nif progress:\nsys.stdout.write(\"\\r{}\".format(progress))\nif current_size &gt;= total_size:\nsys.stdout.write(\"\\n\")\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet", "title": "<code>SaliencyNet</code>", "text": "<p>A class to generate saliency masks from a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model, compiled with fe.build, which is to be inspected.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model inputs within the data dictionary.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model outputs which are written into the data dictionary.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The keys(s) under which to write the generated saliency images.</p> <code>'saliency'</code> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>@traceable()\nclass SaliencyNet:\n\"\"\"A class to generate saliency masks from a given model.\n    Args:\n        model: The model, compiled with fe.build, which is to be inspected.\n        model_inputs: The key(s) corresponding to the model inputs within the data dictionary.\n        model_outputs: The key(s) corresponding to the model outputs which are written into the data dictionary.\n        outputs: The keys(s) under which to write the generated saliency images.\n    \"\"\"\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\noutputs: Union[str, List[str]] = \"saliency\"):\nmode = \"test\"\nself.model_op = ModelOp(model=model, mode=mode, inputs=model_inputs, outputs=model_outputs, trainable=False)\nself.outputs = to_list(outputs)\nself.mode = mode\nself.gather_keys = [\"SaliencyNet_Target_Index_{}\".format(key) for key in self.model_outputs]\nself.network = Network(ops=[\nWatch(inputs=self.model_inputs, mode=mode),\nself.model_op,\nGather(inputs=self.model_outputs,\nindices=self.gather_keys,\noutputs=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\nmode=mode),\nGradientOp(inputs=self.model_inputs,\nfinals=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\noutputs=deepcopy(self.outputs),\nmode=mode),\n])\n@property\ndef model_inputs(self):\nreturn deepcopy(self.model_op.inputs)\n@property\ndef model_outputs(self):\nreturn deepcopy(self.model_op.outputs)\n@staticmethod\ndef _convert_for_visualization(tensor: Tensor, tile: int = 99) -&gt; np.ndarray:\n\"\"\"Modify the range of data in a given input `tensor` to be appropriate for visualization.\n        Args:\n            tensor: Input masks, whose channel values are to be reduced by absolute value summation.\n            tile: The percentile [0-100] used to set the max value of the image.\n        Returns:\n            A (batch X width X height) image after visualization clipping is applied.\n        \"\"\"\nif isinstance(tensor, torch.Tensor):\nchannel_axis = 1\nelse:\nchannel_axis = -1\nflattened_mask = reduce_sum(abs(tensor), axis=channel_axis, keepdims=True)\nnon_batch_axes = list(range(len(flattened_mask.shape)))[1:]\nvmax = percentile(flattened_mask, tile, axis=non_batch_axes, keepdims=True)\nvmin = reduce_min(flattened_mask, axis=non_batch_axes, keepdims=True)\nreturn clip_by_value((flattened_mask - vmin) / (vmax - vmin), 0, 1)\ndef get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\ndef _get_mask(self, batch: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model outputs and the raw saliency mask(s) for the given `batch` of data. Model predictions are reduced\n            via argmax.\n        \"\"\"\nfor key in self.gather_keys:\n# If there's no target key, use an empty array which will cause the max-likelihood class to be selected\nbatch.setdefault(key, [])\nprediction = self.network.transform(data=batch, mode=self.mode)\nfor key in self.model_outputs:\nprediction[key] = argmax(prediction[key], axis=1)\nreturn prediction\ndef _get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw integrated saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n            nsamples: How many samples to consider during integration.\n        Returns:\n            The raw integrated saliency mask(s) for the given `batch` of data.\n        \"\"\"\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\n# Use a random uniform baseline as advised in https://distill.pub/2020/attribution-baselines/\ninput_baselines = [\nrandom_uniform_like(ins, minval=reduce_min(ins), maxval=reduce_max(ins)) for ins in model_inputs\n]\ninput_diffs = [\nmodel_input - input_baseline for model_input, input_baseline in zip(model_inputs, input_baselines)\n]\nresponse = {}\nfor alpha in np.linspace(0.0, 1.0, nsamples):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nx_step = input_baselines[idx] + alpha * input_diffs[idx]\nnoisy_batch[input_name] = x_step\ngrads_and_preds = self._get_mask(noisy_batch)\nfor key in self.outputs:\nif key in response:\nresponse[key] += grads_and_preds[key]\nelse:\nresponse[key] = grads_and_preds[key]\nfor key in self.outputs:\ngrad = response[key]\nfor diff in input_diffs:\ngrad = grad * diff\nresponse[key] = grad\nreturn response\ndef get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: An input batch of data.\n            stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n            nsamples: Number of samples to average across to get the smooth gradient.\n            nintegration: Number of samples to compute when integrating (None to disable).\n            magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n        Returns:\n            Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nclean_batch = {key: val for key, val in noisy_batch.items()}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nclean_batch[input_name] = model_inputs[idx]\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nclean_batch, nsamples=nintegration)  # Integration introduces its own noise pattern\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\ndef get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n        See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n        Args:\n            batch: An input batch of data.\n            nsamples: Number of samples to average across to get the integrated gradient.\n        Returns:\n            Greyscale saliency masks smoothed via the IntegratedGradient method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_integrated_masks", "title": "<code>get_integrated_masks</code>", "text": "<p>Generates integrated greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the integrated gradient.</p> <code>25</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency masks smoothed via the IntegratedGradient method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n    See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n    Args:\n        batch: An input batch of data.\n        nsamples: Number of samples to average across to get the integrated gradient.\n    Returns:\n        Greyscale saliency masks smoothed via the IntegratedGradient method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_masks", "title": "<code>get_masks</code>", "text": "<p>Generates greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>A batch of input data to be fed to the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>The model's classification decisions and greyscale saliency mask(s) for the given <code>batch</code> of data.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: A batch of input data to be fed to the model.\n    Returns:\n        The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_smoothed_masks", "title": "<code>get_smoothed_masks</code>", "text": "<p>Generates smoothed greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>stdev_spread</code> <code>float</code> <p>Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).</p> <code>0.15</code> <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the smooth gradient.</p> <code>25</code> <code>nintegration</code> <code>Optional[int]</code> <p>Number of samples to compute when integrating (None to disable).</p> <code>None</code> <code>magnitude</code> <code>bool</code> <p>If true, computes the sum of squares of gradients instead of just the sum.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency mask(s) smoothed via the SmoothGrad method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: An input batch of data.\n        stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n        nsamples: Number of samples to average across to get the smooth gradient.\n        nintegration: Number of samples to compute when integrating (None to disable).\n        magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n    Returns:\n        Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nclean_batch = {key: val for key, val in noisy_batch.items()}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nclean_batch[input_name] = model_inputs[idx]\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nclean_batch, nsamples=nintegration)  # Integration introduces its own noise pattern\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\n</code></pre>"}, {"location": "tutorial/advanced/t01_dataset.html", "title": "Advanced Tutorial 1: Dataset", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\ntrain_data, eval_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data train_data, eval_data = load_data() In\u00a0[2]: Copied! <pre>train_data.summary()\n</pre> train_data.summary() Out[2]: <pre>&lt;DatasetSummary {'num_instances': 60000, 'keys': {'x': &lt;KeySummary {'shape': [28, 28], 'dtype': 'uint8'}&gt;, 'y': &lt;KeySummary {'num_unique_values': 10, 'shape': [], 'dtype': 'uint8'}&gt;}}&gt;</pre> <p>Or even more simply, by invoking the print function:</p> In\u00a0[3]: Copied! <pre>print(train_data)\n</pre> print(train_data) <pre>{\"num_instances\": 60000, \"keys\": {\"x\": {\"shape\": [28, 28], \"dtype\": \"uint8\"}, \"y\": {\"num_unique_values\": 10, \"shape\": [], \"dtype\": \"uint8\"}}}\n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>test_data = eval_data.split(0.5)\n</pre> test_data = eval_data.split(0.5) <p>Or if I want to split evaluation data into two test datasets with 20% of the evaluation data each:</p> In\u00a0[5]: Copied! <pre>test_data1, test_data2 = eval_data.split(0.2, 0.2)\n</pre> test_data1, test_data2 = eval_data.split(0.2, 0.2) <p></p> In\u00a0[6]: Copied! <pre>test_data3 = eval_data.split(100)\n</pre> test_data3 = eval_data.split(100) <p>And of course, we can generate multiple datasets by providing multiple inputs:</p> In\u00a0[7]: Copied! <pre>test_data4, test_data5 = eval_data.split(100, 100)\n</pre> test_data4, test_data5 = eval_data.split(100, 100) <p></p> In\u00a0[8]: Copied! <pre>test_data6 = eval_data.split([0,1,100])\n</pre> test_data6 = eval_data.split([0,1,100]) <p>If you just want continuous index, here's an easy way to provide index:</p> In\u00a0[9]: Copied! <pre>test_data7 = eval_data.split(range(100))\n</pre> test_data7 = eval_data.split(range(100)) <p>Needless to say, you can provide multiple inputs too:</p> In\u00a0[10]: Copied! <pre>test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5])\n</pre> test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5]) <p></p> In\u00a0[11]: Copied! <pre>from fastestimator.dataset.data.breast_cancer import load_data\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_data, eval_data = load_data()\nscaler = StandardScaler()\n\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\n</pre> from fastestimator.dataset.data.breast_cancer import load_data from sklearn.preprocessing import StandardScaler  train_data, eval_data = load_data() scaler = StandardScaler()  train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) <p></p> <p></p> In\u00a0[12]: Copied! <pre>from fastestimator.dataset.data import mnist, cifair10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_deterministic = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[4,4])\n# ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifair10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")  dataset_deterministic = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[4,4]) # ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[13]: Copied! <pre>from fastestimator.dataset.data import mnist, cifair10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_distribution = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=8, probability=[0.5, 0.5])\n# ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifair10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifair_data, _ = cifair10.load_data(image_key=\"x\", label_key=\"y\")  dataset_distribution = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=8, probability=[0.5, 0.5]) # ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[14]: Copied! <pre>from fastestimator.dataset.data import mnist, cifair10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\")\ncifair_data, _ = cifair10.load_data(image_key=\"x_cifair\", label_key=\"y_cifair\")\n\ndataset_unpaired = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[1,1])\n# ready to use dataset_unpaired in Pipeline\n</pre> from fastestimator.dataset.data import mnist, cifair10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\") cifair_data, _ = cifair10.load_data(image_key=\"x_cifair\", label_key=\"y_cifair\")  dataset_unpaired = BatchDataset(datasets=[mnist_data, cifair_data], num_samples=[1,1]) # ready to use dataset_unpaired in Pipeline <p></p>"}, {"location": "tutorial/advanced/t01_dataset.html#advanced-tutorial-1-dataset", "title": "Advanced Tutorial 1: Dataset\u00b6", "text": ""}, {"location": "tutorial/advanced/t01_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following topics:</p> <ul> <li>Dataset Summary</li> <li>Dataset Splitting<ul> <li>Random Fraction Split</li> <li>Random Count Split</li> <li>Index Split</li> </ul> </li> <li>Global Dataset Editing</li> <li>BatchDataset<ul> <li>Deterministic Batching</li> <li>Distribution Batching</li> <li>Unpaired Dataset</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>Before going through the tutorial, it is recommended to check Beginner Tutorial 2 for basic understanding of <code>dataset</code> from PyTorch and FastEstimator. We will talk about more details about <code>fe.dataset</code> API in this tutorial.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-summary", "title": "Dataset summary\u00b6", "text": "<p>As we have mentioned in previous tutorial, users can import our inherited dataset class for easy use in <code>Pipeline</code>. But how do we know what keys are available in the dataset?   Well, obviously one easy way is just call <code>dataset[0]</code> and check the keys. However, there's a more elegant way to check information of dataset: <code>dataset.summary()</code>.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-splitting", "title": "Dataset Splitting\u00b6", "text": "<p>Dataset splitting is nothing new in machine learning. In FastEstimator, users can easily split their data in different ways.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-fraction-split", "title": "Random Fraction Split\u00b6", "text": "<p>Let's say we want to randomly split 50% of the evaluation data into test data. This is easily accomplished:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-count-split", "title": "Random Count Split\u00b6", "text": "<p>Sometimes instead of fractions, we want an actual number of examples to split; for example, randomly splitting 100 samples from the evaluation dataset:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#index-split", "title": "Index Split\u00b6", "text": "<p>There are times when we need to split the dataset in a specific way. For that, you can provide a list of indexes. For example, if we want to split the 0th, 1st and 100th element of evaluation dataset into new test set:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#global-dataset-editing", "title": "Global Dataset Editing\u00b6", "text": "<p>In deep learning, we usually process the dataset batch by batch. However, when we are handling tabular data, we might need to apply some transformation globally before the training. For example, we may want to standardize the tabular data using <code>sklearn</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#batchdataset", "title": "BatchDataset\u00b6", "text": "<p>There might be scenarios where we need to combine multiple datasets together into one dataset in a specific way. Let's consider three such use-cases now:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#deterministic-batching", "title": "Deterministic Batching\u00b6", "text": "<p>Let's say we have <code>mnist</code> and <code>cifair</code> datasets, and want to combine them with a total batch size of 8. If we always want 4 examples from <code>mnist</code> and the rest from <code>cifair</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#distribution-batching", "title": "Distribution Batching\u00b6", "text": "<p>Some people prefer randomness in a batch. For example, given total batch size of 8, let's say we want 0.5 probability of <code>mnist</code> and the other 0.5 from <code>cifair</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#unpaired-dataset", "title": "Unpaired Dataset\u00b6", "text": "<p>Some deep learning tasks require random unpaired datasets. For example, in image-to-image translation (like Cycle-GAN), the system needs to randomly sample one horse image and one zebra image for every batch. In FastEstimator, <code>BatchDataset</code> can also handle unpaired datasets. The only restriction is that: keys from two different datasets must be unique for unpaired datasets.</p> <p>For example, let's sample one image from <code>mnist</code> and one image from <code>cifair</code> for every batch:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>DNN</li> </ul>"}, {"location": "tutorial/advanced/t02_pipeline.html", "title": "Advanced Tutorial 2: Pipeline", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Iterating Through Pipeline<ul> <li>Basic Concept</li> <li>Example</li> </ul> </li> <li>Advanced Batching Control<ul> <li>Dropping Last Batch</li> <li>Padding Batch Data</li> <li>Numpy Ops on Batches of Data</li> <li>Filtering Data</li> </ul> </li> <li>Benchmark Pipeline Speed</li> </ul> <p>In the Beginner Tutorial 4, we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the <code>Pipeline</code>, we will demonstrate some advanced concepts and how to leverage them to create efficient <code>Pipelines</code> in this tutorial.</p> <p></p> <p></p> <p>In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom fastestimator.dataset.data import cifair10\n    \n# sample numpy array to later create datasets from them\nx_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np from fastestimator.dataset.data import cifair10      # sample numpy array to later create datasets from them x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1))) train_data = {\"x\": x_train, \"y\": y_train} In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# create NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)\n</pre> import fastestimator as fe from fastestimator.dataset.numpy_dataset import NumpyDataset  # create NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3) <p>Let's get the loader object for the <code>Pipeline</code>, then iterate through the loader with a for loop:</p> In\u00a0[3]: Copied! <pre>with pipeline_fe(mode=\"train\") as loader_fe:\n    for batch in loader_fe:\n        print(batch)\n</pre> with pipeline_fe(mode=\"train\") as loader_fe:     for batch in loader_fe:         print(batch) <pre>{'x': tensor([[0.7792, 0.4546],\n        [0.6361, 0.7613],\n        [0.5676, 0.9048]], dtype=torch.float64), 'y': tensor([[0.9328],\n        [0.9089],\n        [0.1312]], dtype=torch.float64)}\n{'x': tensor([[0.0175, 0.2374],\n        [0.3992, 0.8328],\n        [0.7125, 0.0620]], dtype=torch.float64), 'y': tensor([[0.4130],\n        [0.9074],\n        [0.3998]], dtype=torch.float64)}\n{'x': tensor([[0.0584, 0.7026],\n        [0.9152, 0.5944],\n        [0.5536, 0.4152]], dtype=torch.float64), 'y': tensor([[0.0863],\n        [0.5301],\n        [0.6771]], dtype=torch.float64)}\n{'x': tensor([[0.4141, 0.0859]], dtype=torch.float64), 'y': tensor([[0.5408]], dtype=torch.float64)}\n</pre> <p></p> <p>Let's say we have the ciFAIR-10 dataset and we want to find global average pixel value over three channels:</p> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import cifair10\n\ncifair_train, _ = cifair10.load_data()\n</pre> from fastestimator.dataset.data import cifair10  cifair_train, _ = cifair10.load_data() <p>We will take the <code>batch_size</code> 64 and load the data into <code>Pipeline</code></p> In\u00a0[5]: Copied! <pre>pipeline_cifair = fe.Pipeline(train_data=cifair_train, batch_size=64)\n</pre> pipeline_cifair = fe.Pipeline(train_data=cifair_train, batch_size=64) <p>Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset.</p> In\u00a0[6]: Copied! <pre>with pipeline_cifair(mode=\"train\", shuffle=False) as loader_fe:\n    mean_arr = np.zeros((3))\n    for i, batch in enumerate(loader_fe):\n        mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\n    mean_arr = mean_arr / (i+1)\n</pre> with pipeline_cifair(mode=\"train\", shuffle=False) as loader_fe:     mean_arr = np.zeros((3))     for i, batch in enumerate(loader_fe):         mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))     mean_arr = mean_arr / (i+1) In\u00a0[7]: Copied! <pre>print(\"Mean pixel value over the channels are: \", mean_arr)\n</pre> print(\"Mean pixel value over the channels are: \", mean_arr) <pre>Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n</pre> <p></p> <p></p> <p>If the total number of dataset elements is not divisible by the <code>batch_size</code>, by default, the last batch will have less data than other batches.  To drop the last batch we can set <code>drop_last</code> to <code>True</code>. Therefore, if the last batch is incomplete it will be dropped.</p> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop import Batch\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, drop_last=True)])\n</pre> from fastestimator.op.numpyop import Batch  pipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, drop_last=True)]) <p>Since <code>Batch</code> is an op, you can schedule it's behavior to change over different epochs (see Advanced Tutorial 5), as well as for specific modes or datasets (see Advanced Tutorial 13).</p> <p></p> <p>There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch.</p> <p>To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre># define numpy arrays with different shapes\nelem1 = np.array([4, 5])\nelem2 = np.array([1, 2, 6])\nelem3 = np.array([3])\n\n# create train dataset\nx_train = np.array([elem1, elem2, elem3], dtype=object)\ntrain_data = {\"x\": x_train}\ndataset_fe = NumpyDataset(train_data)\n</pre> # define numpy arrays with different shapes elem1 = np.array([4, 5]) elem2 = np.array([1, 2, 6]) elem3 = np.array([3])  # create train dataset x_train = np.array([elem1, elem2, elem3], dtype=object) train_data = {\"x\": x_train} dataset_fe = NumpyDataset(train_data) <p>We will set any <code>pad_value</code> that we want to append at the end of the tensor data. <code>pad_value</code> can be either <code>int</code> or <code>float</code>:</p> In\u00a0[10]: Copied! <pre>pipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, pad_value=0)])\n</pre> pipeline_fe = fe.Pipeline(train_data=dataset_fe, ops=[Batch(batch_size=3, pad_value=0)]) <p>Now let's print the batch data after padding:</p> In\u00a0[11]: Copied! <pre>with pipeline_fe(mode=\"train\", shuffle=False) as loader_fe:\n    for elem in loader_fe:\n        print(elem)\n</pre> with pipeline_fe(mode=\"train\", shuffle=False) as loader_fe:     for elem in loader_fe:         print(elem) <pre>{'x': tensor([[4, 5, 0],\n        [1, 2, 6],\n        [3, 0, 0]])}\n</pre> <p></p> In\u00a0[12]: Copied! <pre># create train dataset\ntrain_data = {\"x\": np.array([[1.0, 2.0, 3.0], [4, 5, 6], [7, 8 ,9]])}\ndataset_fe = NumpyDataset(train_data)\n\n#Imports\nfrom fastestimator.op.numpyop import LambdaOp\nfrom fastestimator.backend import reduce_mean\n\n# Set up the pipeline\npipeline_fe = fe.Pipeline(train_data=dataset_fe, \n                          ops=[Batch(batch_size=3),\n                               LambdaOp(inputs=\"x\", outputs=\"x\", fn=lambda x: x-reduce_mean(x))\n                              ])\n\n# Check the results\npipeline_fe.get_results()\n</pre> # create train dataset train_data = {\"x\": np.array([[1.0, 2.0, 3.0], [4, 5, 6], [7, 8 ,9]])} dataset_fe = NumpyDataset(train_data)  #Imports from fastestimator.op.numpyop import LambdaOp from fastestimator.backend import reduce_mean  # Set up the pipeline pipeline_fe = fe.Pipeline(train_data=dataset_fe,                            ops=[Batch(batch_size=3),                                LambdaOp(inputs=\"x\", outputs=\"x\", fn=lambda x: x-reduce_mean(x))                               ])  # Check the results pipeline_fe.get_results() Out[12]: <pre>{'x': tensor([[-4., -3., -2.],\n         [-1.,  0.,  1.],\n         [ 2.,  3.,  4.]], dtype=torch.float64)}</pre> <p>As you can see, the batch mean (5) was successfully subtracted from each sample</p> <p></p> In\u00a0[13]: Copied! <pre>from fastestimator.backend import reduce_max\nfrom fastestimator.dataset import GeneratorDataset\nfrom fastestimator.op.numpyop import RemoveIf\n\n# Let's start with a dataset that generates random 5x5x3 'images'\nimage_generator = ({'x':np.random.rand(5,5,3)} for _ in iter(int, 1))\ntrain_data = GeneratorDataset(samples_per_epoch=5, generator=image_generator)\n\n# Now let's remove individual images if they don't have at least 1 value greater than 0.9\n# Let's also remove batches of images if the mean of the batch is less than 0.6\npipeline_fe = fe.Pipeline(train_data=train_data,\n                          ops=[RemoveIf(inputs='x', fn=lambda x: reduce_max(x) &lt;= 0.9),\n                               Batch(batch_size=4),\n                               RemoveIf(inputs='x', fn=lambda x: reduce_mean(x) &lt; 0.5)\n                              ])\n\n# Check the results\nbatches = pipeline_fe.get_results(num_steps=2)\nfor batch in batches:\n    print(f\"batch mean: {reduce_mean(batch['x'])}\")\n    for sample in batch['x']:\n        print(f\"sample max: {reduce_max(sample)}\")\n    print('---')\n</pre> from fastestimator.backend import reduce_max from fastestimator.dataset import GeneratorDataset from fastestimator.op.numpyop import RemoveIf  # Let's start with a dataset that generates random 5x5x3 'images' image_generator = ({'x':np.random.rand(5,5,3)} for _ in iter(int, 1)) train_data = GeneratorDataset(samples_per_epoch=5, generator=image_generator)  # Now let's remove individual images if they don't have at least 1 value greater than 0.9 # Let's also remove batches of images if the mean of the batch is less than 0.6 pipeline_fe = fe.Pipeline(train_data=train_data,                           ops=[RemoveIf(inputs='x', fn=lambda x: reduce_max(x) &lt;= 0.9),                                Batch(batch_size=4),                                RemoveIf(inputs='x', fn=lambda x: reduce_mean(x) &lt; 0.5)                               ])  # Check the results batches = pipeline_fe.get_results(num_steps=2) for batch in batches:     print(f\"batch mean: {reduce_mean(batch['x'])}\")     for sample in batch['x']:         print(f\"sample max: {reduce_max(sample)}\")     print('---') <pre>batch mean: 0.5012458402998198\nsample max: 0.959835264378248\nsample max: 0.9998099671968909\nsample max: 0.9553317550086283\nsample max: 0.9985566586444284\n---\nbatch mean: 0.5615718094782679\nsample max: 0.9934757632773238\n---\n</pre> <p>Note that since the dataset specified that it contained 5 samples (samples_per_epoch=5), there were still 5 samples in the output after filtering. The <code>RemoveIf</code> op defaults to filtering with replacement, meaning that discarded samples are replaced with other samples from the dataset. If you wish to discard without replacement (for example, in eval mode), you can set replacement=False. When replacement is True the system will still draw all of the available data once before repeating samples. See the <code>RemoveIf</code> docs for more detailed information.</p> <p></p> <p>It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a <code>Pipeline</code> in order to help diagnose any potential problems. The way to benchmark <code>Pipeline</code> speed in FastEstimator is very simple: call <code>Pipeline.benchmark</code>.</p> <p>For illustration, we will create a <code>Pipeline</code> for the ciFAIR-10 dataset with list of Numpy operators that expand dimensions, apply <code>Minmax</code> and finally <code>Rotate</code> the input images:</p> In\u00a0[14]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\nfrom fastestimator.op.numpyop.multivariate import Rotate\n\npipeline = fe.Pipeline(train_data=cifair_train,\n                       ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),\n                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),\n                            ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],\n                       batch_size=64)\n</pre> from fastestimator.op.numpyop.univariate import Minmax, ExpandDims from fastestimator.op.numpyop.multivariate import Rotate  pipeline = fe.Pipeline(train_data=cifair_train,                        ops=[Minmax(inputs=\"x\", outputs=\"x_out\"),                             Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180),                             ExpandDims(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")],                        batch_size=64) <p>Let's benchmark the pre-processing speed for this pipeline in training mode:</p> In\u00a0[15]: Copied! <pre>pipeline.benchmark(mode=\"train\")\n</pre> pipeline.benchmark(mode=\"train\") <pre>FastEstimator-Benchmark: Dataset: , Step: 100, Epoch: 1, Steps/sec: 189.0564267416377\nFastEstimator-Benchmark: Dataset: , Step: 200, Epoch: 1, Steps/sec: 254.93623924284935\nFastEstimator-Benchmark: Dataset: , Step: 300, Epoch: 1, Steps/sec: 243.89103090633392\nFastEstimator-Benchmark: Dataset: , Step: 400, Epoch: 1, Steps/sec: 281.2587460839808\nFastEstimator-Benchmark: Dataset: , Step: 500, Epoch: 1, Steps/sec: 262.6512477872427\nFastEstimator-Benchmark: Dataset: , Step: 600, Epoch: 1, Steps/sec: 260.0439313025715\nFastEstimator-Benchmark: Dataset: , Step: 700, Epoch: 1, Steps/sec: 235.81544039811544\nFastEstimator-Benchmark: Dataset: , Step: 800, Epoch: 1, Steps/sec: 231.6578051316986\nFastEstimator-Benchmark: Dataset: , Step: 900, Epoch: 1, Steps/sec: 228.99282500888296\nFastEstimator-Benchmark: Dataset: , Step: 1000, Epoch: 1, Steps/sec: 221.91052155741298\nBreakdown of time taken by Pipeline Operations (mode:train epoch:1, ds_id:)\nOp         : Inputs : Outputs :  Time\n--------------------------------------\nMinmax     : x      : x_out   : 39.91%\nRotate     : x_out  : x_out   : 49.76%\nExpandDims : x_out  : x_out   : 10.33%\n\n\n</pre>"}, {"location": "tutorial/advanced/t02_pipeline.html#advanced-tutorial-2-pipeline", "title": "Advanced Tutorial 2: Pipeline\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#overview", "title": "Overview\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#iterating-through-pipeline", "title": "Iterating Through Pipeline\u00b6", "text": "<p>In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the <code>ImageNet</code> dataset, people usually use a precomputed global pixel average for each channel to normalize the images.</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#basic-concept", "title": "Basic Concept\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#example", "title": "Example\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#advanced-batching-control", "title": "Advanced Batching Control\u00b6", "text": "<p>Sometimes you may need advanced control over pipeline batching behavior, or even to run pipeline ops on an entire batch of data at once rather than on individual samples. Both of these use cases are enabled through the <code>Batch</code> Op. We'll start with looking at how you can customize batching behavior using the op:</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#dropping-last-batch", "title": "Dropping Last Batch\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#padding-batch-data", "title": "Padding Batch Data\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#numpy-ops-on-batches-of-data", "title": "Numpy Ops on Batches of Data\u00b6", "text": "<p>Normally <code>Pipeline</code> ops run on individual data instances before they are combined together into a batch. There might, however, be certain instances where you need to run an op on the entire batch of data at once. You could use a <code>TensorOp</code> in the <code>Network</code> to accomplish this, but it is also possible in the <code>Pipeline</code> by placing your <code>NumpyOp</code> after the <code>Batch</code> Op in the op list. This is generally less efficient than performing pre-processing on a per-instance level though, so we recommend only using the feature if you are certain that you need it. This process uses the forward_batch method of <code>NumpyOp</code>s, which has a default implementation that breaks the batch apart and runs the forward method on each individual instance before recombining the batch. A handful of Ops override this default behavior to take advantage of the full batch information. If you want to implement a custom op that leverages all of the available batch information, take a look at the <code>NumpyOp</code> base class implementation for more information.</p> <p>For now, let's consider a simple example using a <code>LambdaOp</code> which will subtract the batch-global mean from each sample in the batch:</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#filtering-data", "title": "Filtering Data\u00b6", "text": "<p>Suppose that you want more control over the composition of a particular batch of data. For example, you might have some bad data you want to exclude, or difficult samples that you want to save for later during training. While it would be more computationally efficient to modify your dataset to exclude undesirable samples, you can also apply a filter inside the <code>Pipeline</code> using the <code>RemoveIf</code> Op. This can be applied either before or after the <code>Batch</code> Op depending on your requirements. Let's take a look at an example:</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#benchmark-pipeline-speed", "title": "Benchmark Pipeline Speed\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html", "title": "Advanced Tutorial 3: Operator", "text": "<p>Here's one simple example of an operator:</p> In\u00a0[1]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddOne(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        x, y = data\n        x = x + 1\n        y = y + 1\n        return x, y\n    \nAddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\"))\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddOne(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         x, y = data         x = x + 1         y = y + 1         return x, y      AddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\")) <p>An <code>Op</code> interacts with the required portion of this data using the keys specified through the <code>inputs</code> key, processes the data through the <code>forward</code> function and writes the values returned from the <code>forward</code> function to this data dictionary using the <code>outputs</code> key. The processes are illustrated in the diagram below:</p> <p></p> <p></p> <p></p> <p></p> <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate\nfrom fastestimator.op.numpyop.univariate import Minmax\n\ntrain_data, eval_data = cifair10.load_data() \n\npipeline1 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])\n\npipeline2 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45), \n                               Delete(keys=\"x_mid\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate from fastestimator.op.numpyop.univariate import Minmax  train_data, eval_data = cifair10.load_data()   pipeline1 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])  pipeline2 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45),                                 Delete(keys=\"x_mid\")]) In\u00a0[3]: Copied! <pre>data1 = pipeline1.get_results()\nprint(\"Keys in pipeline: \", data1.keys())\n\ndata2 = pipeline2.get_results()\nprint(\"Keys in pipeline with Delete Op: \", data2.keys())\n</pre> data1 = pipeline1.get_results() print(\"Keys in pipeline: \", data1.keys())  data2 = pipeline2.get_results() print(\"Keys in pipeline with Delete Op: \", data2.keys()) <pre>Keys in pipeline:  dict_keys(['x', 'x_mid', 'y'])\nKeys in pipeline with Delete Op:  dict_keys(['x', 'y'])\n</pre> <p></p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop import LambdaOp\n\npipeline3 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [LambdaOp(fn=lambda: 5, outputs=\"z\"), #create a new key\n                               LambdaOp(fn=lambda a: a*3, inputs=\"z\", outputs=\"z3\")\n                               ])\n</pre> from fastestimator.op.numpyop import LambdaOp  pipeline3 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [LambdaOp(fn=lambda: 5, outputs=\"z\"), #create a new key                                LambdaOp(fn=lambda a: a*3, inputs=\"z\", outputs=\"z3\")                                ]) In\u00a0[5]: Copied! <pre>data3 = pipeline3.get_results()\nprint(f\"z: {data3['z']}\")\nprint(f\"z3: {data3['z3']}\")\n</pre> data3 = pipeline3.get_results() print(f\"z: {data3['z']}\") print(f\"z3: {data3['z3']}\") <pre>z: tensor([5, 5, 5, 5])\nz3: tensor([15, 15, 15, 15])\n</pre> <p></p> In\u00a0[6]: Copied! <pre>from albumentations.augmentations import RandomCrop\nimport numpy as np\n\nclass Patch(NumpyOp):\n    def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):\n        super().__init__(inputs, outputs, mode)\n        self.num_patch = num_patch\n        self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)\n\n    def forward(self, data, state):\n        image, label = data\n        image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)\n        label = np.array([label for _ in range(self.num_patch)])\n        return [image, label]\n    \n    def _gen_patch(self, data):\n        data = self.crop_fn(image=data)\n        return data[\"image\"].astype(np.float32)\n</pre> from albumentations.augmentations import RandomCrop import numpy as np  class Patch(NumpyOp):     def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):         super().__init__(inputs, outputs, mode)         self.num_patch = num_patch         self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)      def forward(self, data, state):         image, label = data         image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)         label = np.array([label for _ in range(self.num_patch)])         return [image, label]          def _gen_patch(self, data):         data = self.crop_fn(image=data)         return data[\"image\"].astype(np.float32) <p>Let's create a pipeline and visualize the results.</p> In\u00a0[7]: Copied! <pre>pipeline4 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=8,\n                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"], \n                                   num_patch=4)])\n</pre> pipeline4 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=8,                         ops=[Minmax(inputs=\"x\", outputs=\"x\"),                              Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"],                                     num_patch=4)]) In\u00a0[8]: Copied! <pre>from fastestimator.util import BatchDisplay, GridDisplay\n\ndata4 = pipeline4.get_results()\n\nfig = GridDisplay([BatchDisplay(image=data4[\"x\"], title=\"Input Image\"),\n                   BatchDisplay(image=data4[\"x_out\"][:,0,:,:,:], title=\"Patch 0\"),\n                   BatchDisplay(image=data4[\"x_out\"][:,1,:,:,:], title=\"Patch 1\"),\n                   BatchDisplay(image=data4[\"x_out\"][:,2,:,:,:], title=\"Patch 2\"),\n                   BatchDisplay(image=data4[\"x_out\"][:,3,:,:,:], title=\"Patch 3\")\n                  ])\nfig.show()\n</pre> from fastestimator.util import BatchDisplay, GridDisplay  data4 = pipeline4.get_results()  fig = GridDisplay([BatchDisplay(image=data4[\"x\"], title=\"Input Image\"),                    BatchDisplay(image=data4[\"x_out\"][:,0,:,:,:], title=\"Patch 0\"),                    BatchDisplay(image=data4[\"x_out\"][:,1,:,:,:], title=\"Patch 1\"),                    BatchDisplay(image=data4[\"x_out\"][:,2,:,:,:], title=\"Patch 2\"),                    BatchDisplay(image=data4[\"x_out\"][:,3,:,:,:], title=\"Patch 3\")                   ]) fig.show() <p></p> <p></p> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import LambdaOp\nimport tensorflow as tf\n\nnetwork = fe.Network(ops=[\n    LambdaOp(inputs='x', outputs='y', fn=lambda a: a*5)\n])\n\ndata = network.transform(data={'x':tf.ones((2,2))}, mode='train')\nprint(f\"x: \\n{data['x']}\")\nprint(f\"y: \\n{data['y']}\")\n</pre> from fastestimator.op.tensorop import LambdaOp import tensorflow as tf  network = fe.Network(ops=[     LambdaOp(inputs='x', outputs='y', fn=lambda a: a*5) ])  data = network.transform(data={'x':tf.ones((2,2))}, mode='train') print(f\"x: \\n{data['x']}\") print(f\"y: \\n{data['y']}\") <pre>x: \n[[1. 1.]\n [1. 1.]]\ny: \n[[5. 5.]\n [5. 5.]]\n</pre> <p></p> In\u00a0[10]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass DimensionAdjustment(TensorOp):\n    def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.reduce_dim = reduce_dim\n        self.reshape_fn = None\n    \n    def build(self, framework, device):\n        if framework=='tf':\n            self.reshape_fn = lambda tensor: tf.reshape(tensor, shape=self._new_shape(tensor))\n        elif framework=='torch':\n            self.reshape_fn = lambda tensor: torch.reshape(tensor, shape=self._new_shape(tensor))\n    \n    def forward(self, data, state):\n        image, label = data\n        image_out = self.reshape_fn(image)\n        label_out = self.reshape_fn(label)\n        return [image_out, label_out]\n    \n    def _new_shape(self, data):\n        return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim]\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class DimensionAdjustment(TensorOp):     def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):         super().__init__(inputs, outputs, mode)         self.reduce_dim = reduce_dim         self.reshape_fn = None          def build(self, framework, device):         if framework=='tf':             self.reshape_fn = lambda tensor: tf.reshape(tensor, shape=self._new_shape(tensor))         elif framework=='torch':             self.reshape_fn = lambda tensor: torch.reshape(tensor, shape=self._new_shape(tensor))          def forward(self, data, state):         image, label = data         image_out = self.reshape_fn(image)         label_out = self.reshape_fn(label)         return [image_out, label_out]          def _new_shape(self, data):         return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim] In\u00a0[11]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\npipeline5 = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=8,\n                       ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                            Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"], \n                                  num_patch=4)])\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> from fastestimator.architecture.tensorflow import LeNet from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  pipeline5 = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=8,                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"],                                    num_patch=4)])  model = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) <p>Let's check the dimensions the of Pipeline output and DimensionAdjustment TensorOp output.</p> In\u00a0[12]: Copied! <pre>data5 = pipeline5.get_results()\nresult = network.transform(data5, mode=\"infer\")\n\nprint(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\")\nprint(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\")\n</pre> data5 = pipeline5.get_results() result = network.transform(data5, mode=\"infer\")  print(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\") print(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\") <pre>Pipeline Output, Image Shape: torch.Size([8, 4, 24, 24, 3]), Label Shape: torch.Size([8, 4, 1])\nResult Image Shape: (32, 24, 24, 3), Label Shape: (32, 1)\n</pre> <p><code>TensorOps</code> have three other methods which are much less commonly used, but may be overridden if you are working on a complex Op:</p> <ol> <li>get_fe_models(self) -&gt; Set</li> <li>get_fe_loss_keys(self) -&gt; Set</li> <li>fe_retain_graph(self, retain) -&gt; bool</li> </ol> <p>If your custom <code>TensorOp</code> contains one or more neural network models, you should override the get_fe_models() method to return all of those models. An example where this is done is in our <code>ModelOp</code>.</p> <p>If your custom <code>TensorOp</code> is being used to apply a loss value to a model, you should override the get_fe_loss_keys() method to return the string name(s) of all the keys which are being used as losses. An example where this is done is in our <code>UpdateOp</code>.</p> <p>Finally, if your custom Op computes gradients, it should override the fe_retain_graph method such that it can control whether or not your Op will keep the computation graph in memory or erase it after completing its forward pass. An example where this is done is in our <code>UpdateOp</code>.</p> <p></p>"}, {"location": "tutorial/advanced/t03_operator.html#advanced-tutorial-3-operator", "title": "Advanced Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Operator Mechanism<ul> <li>data</li> <li>state</li> </ul> </li> <li>NumpyOp<ul> <li>DeleteOp</li> <li>LambdaOp</li> <li>Customizing NumpyOps</li> </ul> </li> <li>TensorOp<ul> <li>LambdaOp</li> <li>Customizing TensorOps</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t03_operator.html#operator-mechanism", "title": "Operator Mechanism\u00b6", "text": "<p>We learned about the operator structure in Beginner Tutorial 3. Operators are used to build complex computation graphs in FastEstimator.</p> <p>In FastEstimator, all the available data is held in a data dictionary during execution. An <code>Op</code> runs when it's <code>mode</code> matches the current execution mode. For more information on mode, you can go through Beginner Tutorial 8.</p>"}, {"location": "tutorial/advanced/t03_operator.html#data", "title": "data\u00b6", "text": "<p>The data argument in the <code>forward</code> function passes the portion of data dictionary corresponding to the Operator's <code>inputs</code> into the forward function. If multiple keys are provided as <code>inputs</code>, the data will be a list of corresponding to the values of those keys.</p>"}, {"location": "tutorial/advanced/t03_operator.html#state", "title": "state\u00b6", "text": "<p>The state argument in the <code>forward</code> function stores meta information about training like the current mode, GradientTape for tensorflow, etc. It is very unlikely that you would need to interact with it.</p>"}, {"location": "tutorial/advanced/t03_operator.html#numpyop", "title": "NumpyOp\u00b6", "text": "<p>NumpyOp is used in <code>Pipeline</code> for data pre-processing and augmentation. You can go through Beginner Tutorial 4 to get an overview of NumpyOp and their usage. Here, we will talk about some advanced NumpyOps.</p>"}, {"location": "tutorial/advanced/t03_operator.html#deleteop", "title": "DeleteOp\u00b6", "text": "<p>Delete op is used to delete keys from the data dictionary which are no longer required by the user. This helps in improving processing speed as we are holding only the required data in the memory. Let's see its usage:</p>"}, {"location": "tutorial/advanced/t03_operator.html#lambdaop", "title": "LambdaOp\u00b6", "text": "<p>The <code>LambdaOp</code> is a flexible Op which allows you to execute arbitrary lambda functions. This can be especially useful for adding new keys to the data dictionary, or for performing simple computations without having to write a new NumpyOp. If your lambda function has a return value, it should be in the form of an np.ndarrary.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-numpyops", "title": "Customizing NumpyOps\u00b6", "text": "<p>We can create a custom NumpyOp which suits our needs. Below, we showcase a custom NumpyOp which creates multiple random patches (crops) of images from each image.</p>"}, {"location": "tutorial/advanced/t03_operator.html#tensorop", "title": "TensorOp\u00b6", "text": "<p><code>TensorOps</code> are used to process tensor data. They are used within a <code>Network</code> for graph-based operations. You can go through Beginner Tutorial 6 to get an overview of <code>TensorOps</code> and their usages.</p>"}, {"location": "tutorial/advanced/t03_operator.html#lambdaop", "title": "LambdaOp\u00b6", "text": "<p>Just like with NumpyOps, TensorOps have a <code>LambdaOp</code> too. The TensorOp version differs in that it should return a tf.Tensor or torch.Tensor rather than an np.ndarray.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-tensorops", "title": "Customizing TensorOps\u00b6", "text": "<p>We can create a custom <code>TensorOp</code> using TensorFlow or Pytorch library calls according to our requirements. Below, we showcase a custom <code>TensorOp</code> which combines the batch dimension and patch dimension from the output of the above <code>Pipeline</code> to make it compatible to the <code>Network</code>. <code>TensorOps</code> also have a .build() method which will be invoked before the Network runs so that you can make the op compatible with multiple different backends (though you don't have to do this if you don't care about cross-framework compatibility).</p>"}, {"location": "tutorial/advanced/t03_operator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>Convolutional Variational AutoEncoder</li> <li>Semantic Segmentation</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html", "title": "Advanced Tutorial 4: Trace", "text": "<p>Let's create a function to generate a pipeline, model and network to be used for the tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    \n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=batch_size,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)          pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=batch_size,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.util import to_number\nfrom fastestimator.trace import Trace\nfrom sklearn.metrics import fbeta_score\nimport numpy as np\n\nclass FBetaScore(Trace):\n    def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):\n        super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        self.beta = beta\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_batch_end(self, data):\n        y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\n        y_pred = np.argmax(y_pred, axis=-1)\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n        \n    def on_epoch_end(self, data):\n        score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")\n        data.write_with_log(self.outputs[0], score)\n</pre> from fastestimator.util import to_number from fastestimator.trace import Trace from sklearn.metrics import fbeta_score import numpy as np  class FBetaScore(Trace):     def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):         super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)         self.true_key = true_key         self.pred_key = pred_key         self.beta = beta         self.y_true = []         self.y_pred = []              def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []              def on_batch_end(self, data):         y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])         y_pred = np.argmax(y_pred, axis=-1)         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())              def on_epoch_end(self, data):         score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")         data.write_with_log(self.outputs[0], score) <p>Now let's calculate the f2-score using our custom <code>Trace</code>. f2-score gives more importance to recall.</p> In\u00a0[3]: Copied! <pre>pipeline, model, network = get_pipeline_model_network()\n\ntraces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)\n\nestimator.fit()\n</pre> pipeline, model, network = get_pipeline_model_network()  traces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)  estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.3083596; \nFastEstimator-Train: step: 1000; ce: 0.16284753; steps/sec: 656.26; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 3.55 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.035797507; f2_score: 0.9885909522565743; \nFastEstimator-Train: step: 2000; ce: 0.020546585; steps/sec: 615.78; \nFastEstimator-Train: step: 3000; ce: 0.0059753414; steps/sec: 713.25; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.69 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03689827; f2_score: 0.9877924021686296; \nFastEstimator-Train: step: 4000; ce: 0.02098944; steps/sec: 680.01; \nFastEstimator-Train: step: 5000; ce: 0.22268356; steps/sec: 741.56; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 2.65 sec; \nFastEstimator-Eval: step: 5625; epoch: 3; ce: 0.032033153; f2_score: 0.9901934586365465; \nFastEstimator-Train: step: 6000; ce: 0.0055854702; steps/sec: 677.84; \nFastEstimator-Train: step: 7000; ce: 0.0013257915; steps/sec: 679.31; \nFastEstimator-Train: step: 7500; epoch: 4; epoch_time: 2.8 sec; \nFastEstimator-Eval: step: 7500; epoch: 4; ce: 0.029642625; f2_score: 0.9913968204671144; \nFastEstimator-Finish: step: 7500; total_time: 17.99 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Let's see an example where we utilize the outputs of the <code>Precision</code> and <code>Recall</code> <code>Traces</code> to generate f1-score:</p> In\u00a0[4]: Copied! <pre>from fastestimator.trace.metric import Precision, Recall\n\nclass CustomF1Score(Trace):\n    def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):\n        super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)\n        self.precision_key = precision_key\n        self.recall_key = recall_key\n        \n    def on_epoch_end(self, data):\n        precision = data[self.precision_key]\n        recall = data[self.recall_key]\n        score = 2*(precision*recall)/(precision+recall)\n        data.write_with_log(self.outputs[0], score)\n        \n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = [\n    Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),\n    Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),\n    CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\")\n]\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000)\n</pre> from fastestimator.trace.metric import Precision, Recall  class CustomF1Score(Trace):     def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):         super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)         self.precision_key = precision_key         self.recall_key = recall_key              def on_epoch_end(self, data):         precision = data[self.precision_key]         recall = data[self.recall_key]         score = 2*(precision*recall)/(precision+recall)         data.write_with_log(self.outputs[0], score)           pipeline, model, network = get_pipeline_model_network()  traces = [     Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),     Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),     CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\") ] estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000) In\u00a0[5]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.305337; \nFastEstimator-Train: step: 1000; ce: 0.024452677; steps/sec: 734.32; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.76 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.0569705; \nprecision:\n[0.97585513,0.98211091,0.9752381 ,0.98080614,0.99562363,0.96210526,\n 1.        ,0.98137803,1.        ,0.97504798];\nrecall:\n[0.99589322,1.        ,0.99224806,0.99223301,0.98484848,0.9827957 ,\n 0.95850622,0.98137803,0.95503212,0.97692308];\nf1_score:\n[0.98577236,0.99097473,0.98366955,0.98648649,0.99020675,0.97234043,\n 0.97881356,0.98137803,0.9769989 ,0.97598463];\nFastEstimator-Train: step: 2000; ce: 0.0021102745; steps/sec: 674.01; \nFastEstimator-Train: step: 3000; ce: 0.0089770565; steps/sec: 688.42; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 2.8 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.034781747; \nprecision:\n[0.98780488,0.99097473,0.98843931,0.98841699,0.99349241,0.98908297,\n 0.99375   ,0.9905303 ,0.97468354,0.98449612];\nrecall:\n[0.99794661,1.        ,0.99418605,0.99417476,0.99134199,0.97419355,\n 0.98962656,0.97392924,0.98929336,0.97692308];\nf1_score:\n[0.99284985,0.99546691,0.99130435,0.99128751,0.99241603,0.9815818 ,\n 0.99168399,0.98215962,0.98193411,0.98069498];\nFastEstimator-Finish: step: 3750; total_time: 8.76 sec; LeNet_lr: 0.001; \n</pre> <p><code>Note:</code> precision, recall, and f1-score are displayed for each class</p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class MonitorPred(Trace):\n    def __init__(self, true_key, pred_key, mode=\"train\"):\n        super().__init__(inputs=(true_key, pred_key), mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        \n    def on_batch_end(self, data):\n        print(\"Global Step Index: \", self.system.global_step)\n        print(\"Batch Index: \", self.system.batch_idx)\n        print(\"Epoch: \", self.system.epoch_idx)\n        print(\"Batch data has following keys: \", list(data.keys()))\n        print(\"Batch true labels: \", data[self.true_key])\n        print(\"Batch predictictions: \", data[self.pred_key])\n\npipeline, model, network = get_pipeline_model_network(batch_size=4)\n\ntraces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, train_steps_per_epoch=2, log_steps=None)\n</pre> class MonitorPred(Trace):     def __init__(self, true_key, pred_key, mode=\"train\"):         super().__init__(inputs=(true_key, pred_key), mode=mode)         self.true_key = true_key         self.pred_key = pred_key              def on_batch_end(self, data):         print(\"Global Step Index: \", self.system.global_step)         print(\"Batch Index: \", self.system.batch_idx)         print(\"Epoch: \", self.system.epoch_idx)         print(\"Batch data has following keys: \", list(data.keys()))         print(\"Batch true labels: \", data[self.true_key])         print(\"Batch predictictions: \", data[self.pred_key])  pipeline, model, network = get_pipeline_model_network(batch_size=4)  traces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, train_steps_per_epoch=2, log_steps=None) In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nGlobal Step Index:  1\nBatch Index:  1\nEpoch:  1\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [1 5 8 5]\nBatch predictictions:  [[0.09878654 0.11280762 0.10882236 0.0953772  0.09711165 0.09277759\n  0.09783419 0.09401798 0.10111833 0.10134653]\n [0.10425894 0.11605782 0.11004242 0.09267453 0.08793817 0.09537386\n  0.10757758 0.08135056 0.09903805 0.10568804]\n [0.1016297  0.11371672 0.10940187 0.09458858 0.09116017 0.09185343\n  0.10174091 0.08704273 0.10234813 0.10651773]\n [0.10281158 0.10875763 0.10668261 0.08935054 0.09368025 0.10163527\n  0.10554942 0.08158974 0.09799404 0.11194893]]\nGlobal Step Index:  2\nBatch Index:  2\nEpoch:  1\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [9 7 0 9]\nBatch predictictions:  [[0.10153595 0.11117928 0.10700106 0.09030598 0.09056976 0.10074646\n  0.10491277 0.08370153 0.10058438 0.10946291]\n [0.09943405 0.11675353 0.10615741 0.09357058 0.09498165 0.09680846\n  0.09997059 0.08461777 0.09770196 0.11000396]\n [0.10712261 0.11406822 0.10380837 0.09336544 0.08995877 0.09921383\n  0.10175668 0.08751085 0.09903854 0.10415668]\n [0.10325367 0.10959569 0.10525871 0.08968467 0.09167413 0.10499243\n  0.10512233 0.08271552 0.09867672 0.10902614]]\nGlobal Step Index:  3\nBatch Index:  1\nEpoch:  2\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [4 9 5 0]\nBatch predictictions:  [[0.10507825 0.10794099 0.10248892 0.08767187 0.08906174 0.10877317\n  0.10675651 0.08316758 0.09733932 0.11172164]\n [0.10452065 0.10935836 0.10143676 0.08643056 0.08772491 0.11231022\n  0.10028692 0.08151487 0.09872114 0.11769552]\n [0.10281294 0.11222194 0.1011567  0.08917599 0.093499   0.10987655\n  0.10295148 0.08328241 0.09753096 0.10749206]\n [0.11502377 0.10897078 0.10094845 0.08484171 0.08951931 0.10733136\n  0.09949591 0.08294778 0.09814924 0.11277179]]\nGlobal Step Index:  4\nBatch Index:  2\nEpoch:  2\nBatch data has following keys:  ['y', 'ce', 'x', 'y_pred']\nBatch true labels:  [2 9 5 9]\nBatch predictictions:  [[0.10447924 0.11029453 0.09903328 0.08642756 0.09253392 0.11049397\n  0.10054693 0.08330047 0.09570859 0.11718156]\n [0.10390399 0.11127824 0.10138535 0.08615676 0.09266223 0.11076459\n  0.10240171 0.08131735 0.09794777 0.11218196]\n [0.10628477 0.10850214 0.09937814 0.08383881 0.0902461  0.11622549\n  0.103737   0.07806063 0.09677587 0.11695106]\n [0.10669366 0.10886899 0.09865166 0.08427355 0.0894412  0.117375\n  0.10394516 0.07848874 0.09449891 0.11776313]]\n</pre> <p>As you can see, we can visualize information like the global step, batch number, epoch, keys in the data dictionary, true labels, and predictions at batch level using our <code>Trace</code>.</p> <p></p>"}, {"location": "tutorial/advanced/t04_trace.html#advanced-tutorial-4-trace", "title": "Advanced Tutorial 4: Trace\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing Traces<ul> <li>Example</li> </ul> </li> <li>More About Traces<ul> <li>Inputs, Outputs, and Mode</li> <li>Data</li> <li>System</li> </ul> </li> <li>Trace Communication</li> <li>Other Trace Usages<ul> <li>Debugging/Monitoring</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html#customizing-traces", "title": "Customizing Traces\u00b6", "text": "<p>In Beginner Tutorial 7, we talked about the basic concept and structure of <code>Traces</code> and used a few <code>Traces</code> provided by FastEstimator. We can also customize a Trace to suit our needs. Let's look at an example of a custom trace implementation:</p>"}, {"location": "tutorial/advanced/t04_trace.html#example", "title": "Example\u00b6", "text": "<p>We can utilize traces to calculate any custom metric needed for monitoring or controlling training. Below, we implement a trace for calculating the F-beta score of our model.</p>"}, {"location": "tutorial/advanced/t04_trace.html#more-about-traces", "title": "More About Traces\u00b6", "text": "<p>As we have now seen a custom Trace implementaion, let's delve deeper into the structure of <code>Traces</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#inputs-outputs-and-mode", "title": "Inputs, Outputs, and Mode\u00b6", "text": "<p>These Trace arguments are similar to the Operator. To recap, the keys from the data dictionary which are required by the Trace can be specified using the <code>inputs</code> argument. The <code>outputs</code> argument is used to specify the keys which the Trace wants to write into the system buffer. Unlike with Ops, the Trace <code>inputs</code> and <code>outputs</code> are essentially on an honor system. FastEstimator will not check whether a Trace is really only reading values listed in its <code>inputs</code> and writing values listed in its <code>outputs</code>. If you are developing a new <code>Trace</code> and want your code to work well with the features provided by FastEstimator, it is important to use these fields correctly. The <code>mode</code> argument is used to specify the mode(s) for trace execution as with <code>Ops</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#data", "title": "Data\u00b6", "text": "<p>Through its data argument, Trace has access to the current data dictionary. You can use any keys which the Trace declared as its <code>inputs</code> to access information from the data dictionary. You can write the outputs into the <code>Data</code> dictionary with or without logging using the <code>write_with_log</code> and <code>write_without_log</code> methods respectively.</p>"}, {"location": "tutorial/advanced/t04_trace.html#system", "title": "System\u00b6", "text": "<p>Traces have access to the current <code>System</code> instance which has information about the <code>Network</code> and training process. The information contained in <code>System</code> is listed below:</p> <ul> <li>global_step</li> <li>num_devices</li> <li>log_steps</li> <li>total_epochs</li> <li>epoch_idx</li> <li>batch_idx</li> <li>stop_training</li> <li>network</li> <li>train_steps_per_epoch</li> <li>eval_steps_per_epoch</li> <li>summary</li> <li>experiment_time</li> </ul> <p>We will showcase <code>System</code> usage in the other trace usages section of this tutorial.</p>"}, {"location": "tutorial/advanced/t04_trace.html#trace-communication", "title": "Trace Communication\u00b6", "text": "<p>We can have multiple traces in a network where the output of one trace is utilized as an input for another, as depicted below:</p>"}, {"location": "tutorial/advanced/t04_trace.html#other-trace-usages", "title": "Other Trace Usages\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#debuggingmonitoring", "title": "Debugging/Monitoring\u00b6", "text": "<p>Lets implement a custom trace to monitor a model's predictions. Using this, any discrepancy from the expected behavior can be checked and the relevant corrections can be made:</p>"}, {"location": "tutorial/advanced/t04_trace.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html", "title": "Advanced Tutorial 5: Scheduler", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.schedule import EpochScheduler\nbatch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64})\n</pre> from fastestimator.schedule import EpochScheduler batch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64}) In\u00a0[2]: Copied! <pre>for epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 16\nAt epoch 2, batch size is 32\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 64\n</pre> In\u00a0[3]: Copied! <pre>from fastestimator.schedule import RepeatScheduler\nbatch_size = RepeatScheduler(repeat_list=[32, 64])\n\nfor epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> from fastestimator.schedule import RepeatScheduler batch_size = RepeatScheduler(repeat_list=[32, 64])  for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 32\nAt epoch 2, batch size is 64\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 32\n</pre> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mnist\nfrom fastestimator.schedule import EpochScheduler\n\ntrain_data1, eval_data = mnist.load_data()\ntrain_data2, _ = mnist.load_data()\ntrain_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2})\n</pre> from fastestimator.dataset.data import mnist from fastestimator.schedule import EpochScheduler  train_data1, eval_data = mnist.load_data() train_data2, _ = mnist.load_data() train_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2}) In\u00a0[5]: Copied! <pre>batch_size = RepeatScheduler(repeat_list=[32,64])\n</pre> batch_size = RepeatScheduler(repeat_list=[32,64]) In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.numpyop.multivariate import Rotate\nimport fastestimator as fe\n\nrotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})\n\npipeline = fe.Pipeline(train_data=train_data, \n                       eval_data=eval_data,\n                       batch_size=batch_size, \n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.numpyop.multivariate import Rotate import fastestimator as fe  rotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})  pipeline = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                        batch_size=batch_size,                         ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\n\nmodel_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")\n\nmodel_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"), \n             3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}\n\nupdate_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}\n\nnetwork = fe.Network(ops=[EpochScheduler(model_map),\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n                          EpochScheduler(update_map)])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop.loss import CrossEntropy  model_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")  model_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"),               3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}  update_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}  network = fe.Network(ops=[EpochScheduler(model_map),                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),                           EpochScheduler(update_map)]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.io import ModelSaver\nimport tempfile\n\nsave_folder = tempfile.mkdtemp()\n\n#Disable model saving by setting None on 3rd epoch:\nmodelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})\n\nmodelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})\n\ntraces=[modelsaver1, modelsaver2]\n</pre> from fastestimator.trace.io import ModelSaver import tempfile  save_folder = tempfile.mkdtemp()  #Disable model saving by setting None on 3rd epoch: modelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})  modelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})  traces=[modelsaver1, modelsaver2] In\u00a0[10]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300)\nestimator.fit()\n</pre> estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300) estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 300; \nFastEstimator-Train: step: 1; ce: 0.016785031; \nFastEstimator-Train: step: 300; ce: 0.16430134; steps/sec: 697.26; \nFastEstimator-Train: step: 600; ce: 0.023913195; steps/sec: 728.45; \nFastEstimator-Train: step: 900; ce: 0.042380013; steps/sec: 732.24; \nFastEstimator-Train: step: 1200; ce: 0.0014684915; steps/sec: 723.88; \nFastEstimator-Train: step: 1500; ce: 0.020901386; steps/sec: 728.1; \nFastEstimator-Train: step: 1800; ce: 0.0114256; steps/sec: 724.26; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 2.66 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.054206606; \nFastEstimator-Train: step: 2100; ce: 0.023387551; steps/sec: 546.57; \nFastEstimator-Train: step: 2400; ce: 0.0030879583; steps/sec: 627.71; \nFastEstimator-Train: step: 2700; ce: 0.10354612; steps/sec: 631.19; \nFastEstimator-ModelSaver: Saved model to /tmp/tmph72ava81/m1_epoch_2.h5\nFastEstimator-Train: step: 2813; epoch: 2; epoch_time: 1.58 sec; \nFastEstimator-Eval: step: 2813; epoch: 2; ce: 0.040080495; \nFastEstimator-Train: step: 3000; ce: 0.0011174735; steps/sec: 627.96; \nFastEstimator-Train: step: 3300; ce: 0.019162945; steps/sec: 792.26; \nFastEstimator-Train: step: 3600; ce: 0.21189407; steps/sec: 796.04; \nFastEstimator-Train: step: 3900; ce: 0.0007937134; steps/sec: 811.5; \nFastEstimator-Train: step: 4200; ce: 0.002208311; steps/sec: 818.86; \nFastEstimator-Train: step: 4500; ce: 0.005765636; steps/sec: 815.32; \nFastEstimator-ModelSaver: Saved model to /tmp/tmph72ava81/m2_epoch_3.h5\nFastEstimator-Train: step: 4688; epoch: 3; epoch_time: 2.4 sec; \nFastEstimator-Eval: step: 4688; epoch: 3; ce: 0.033545353; \nFastEstimator-Finish: step: 4688; total_time: 10.79 sec; m2_lr: 0.001; m1_lr: 0.01; \n</pre>"}, {"location": "tutorial/advanced/t05_scheduler.html#advanced-tutorial-5-scheduler", "title": "Advanced Tutorial 5: Scheduler\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Scheduler<ul> <li>Concept</li> <li>EpochScheduler</li> <li>RepeatScheduler</li> </ul> </li> <li>Things You Can Schedule<ul> <li>Datasets</li> <li>Batch Size</li> <li>NumpyOps</li> <li>Optimizers</li> <li>TensorOps</li> <li>Traces</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#scheduler", "title": "Scheduler\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#concept", "title": "Concept\u00b6", "text": "<p>Deep learning training is getting more complicated every year. One major aspect of this complexity is time-dependent training. For example:</p> <ul> <li>Using different datasets for different training epochs.</li> <li>Applying different preprocessing for different epochs.</li> <li>Training different networks on different epochs.</li> <li>...</li> </ul> <p>The list goes on and on. In order to provide an easy way for users to accomplish time-dependent training, we provide the <code>Scheduler</code> class which can help you schedule any part of the training.</p> <p>Please note that the basic time unit that <code>Scheduler</code> can handle is <code>epochs</code>. If users want arbitrary scheduling cycles, the simplest way is to customize the length of one epoch in <code>Estimator</code> using train_steps_per_epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#epochscheduler", "title": "EpochScheduler\u00b6", "text": "<p>The most straightforward way to schedule things is through an epoch-value mapping. For example, If users want to schedule the batch size in the following way:</p> <ul> <li>epoch 1 - batchsize 16</li> <li>epoch 2 - batchsize 32</li> <li>epoch 3 - batchsize 32</li> <li>epoch 4 - batchsize 64</li> <li>epoch 5 - batchsize 64</li> </ul> <p>You can do the following:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#repeatscheduler", "title": "RepeatScheduler\u00b6", "text": "<p>If your schedule follows a repeating pattern, then you don't want to specify that for all epochs. <code>RepeatScheduler</code> is here to help you. Let's say we want the batch size on odd epochs to be 32, and on even epochs to be 64:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#things-you-can-schedule", "title": "Things You Can Schedule:\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#datasets", "title": "Datasets\u00b6", "text": "<p>Scheduling training or evaluation datasets is very common in deep learning. For example, in curriculum learning people will train on an easy dataset first and then gradually move on to harder datasets. For illustration purposes, let's use two different instances of the same MNIST dataset:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#batch-size", "title": "Batch Size\u00b6", "text": "<p>We can also schedule the batch size on different epochs, which may help resolve GPU resource constraints.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#numpyops", "title": "NumpyOps\u00b6", "text": "<p>Preprocessing operators can also be scheduled. For illustration purpose, we will apply a <code>Rotation</code> for the first two epochs and then not apply it for the third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#optimizers", "title": "Optimizers\u00b6", "text": "<p>For fast convergence, some people like to use different optimizers at different training phases. In our example, we will use <code>adam</code> for the first epoch and <code>sgd</code> for the second epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#tensorops", "title": "TensorOps\u00b6", "text": "<p>We can schedule <code>TensorOps</code> just like <code>NumpyOps</code>. Let's define another model <code>model_2</code> such that:</p> <ul> <li>epoch 1-2: train <code>model_1</code></li> <li>epoch 3: train <code>model_2</code></li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#traces", "title": "Traces\u00b6", "text": "<p><code>Traces</code> can also be scheduled. For example, we will save <code>model_1</code> at the end of second epoch and save <code>model_3</code> at the end of third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#let-the-training-begin", "title": "Let the training begin\u00b6", "text": "<p>Nothing special in here, create the estimator then start the training:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PGGAN</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html", "title": "Advanced Tutorial 6: Summary", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import TensorBoard\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import TensorBoard  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ] In\u00a0[2]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120)\nest.fit()\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 120; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2963042; model_lr: 0.0009999998;\nFastEstimator-Train: step: 120; ce: 0.1863498; model_lr: 0.000997478; steps/sec: 70.22;\nFastEstimator-Train: step: 240; ce: 0.051570907; model_lr: 0.0009899376; steps/sec: 70.85;\nFastEstimator-Train: step: 360; ce: 0.14517793; model_lr: 0.0009774548; steps/sec: 69.82;\nFastEstimator-Train: step: 480; ce: 0.16006204; model_lr: 0.0009601558; steps/sec: 71.39;\nFastEstimator-Train: step: 600; ce: 0.014987067; model_lr: 0.0009382152; steps/sec: 70.44;\nFastEstimator-Train: step: 720; ce: 0.14745927; model_lr: 0.00091185456; steps/sec: 67.46;\nFastEstimator-Train: step: 840; ce: 0.06190053; model_lr: 0.00088134; steps/sec: 69.75;\nFastEstimator-Train: step: 960; ce: 0.0073453495; model_lr: 0.00084697985; steps/sec: 68.31;\nFastEstimator-Train: step: 1080; ce: 0.0628736; model_lr: 0.0008091209; steps/sec: 68.8;\nFastEstimator-Train: step: 1200; ce: 0.09909142; model_lr: 0.0007681455; steps/sec: 68.23;\nFastEstimator-Train: step: 1320; ce: 0.006691601; model_lr: 0.0007244674; steps/sec: 68.03;\nFastEstimator-Train: step: 1440; ce: 0.031444274; model_lr: 0.00067852775; steps/sec: 64.0;\nFastEstimator-Train: step: 1560; ce: 0.0026731049; model_lr: 0.0006307903; steps/sec: 66.83;\nFastEstimator-Train: step: 1680; ce: 0.2757488; model_lr: 0.00058173726; steps/sec: 61.66;\nFastEstimator-Train: step: 1800; ce: 0.1322045; model_lr: 0.0005318639; steps/sec: 56.45;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 29.41 sec;\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9876; ce: 0.035917815;\nFastEstimator-Finish: step: 1875; model_lr: 0.0005005; total_time: 30.63 sec;\n</pre> In\u00a0[3]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500)\nsummary = est.fit(\"experiment1\")\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500) summary = est.fit(\"experiment1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 500; num_device: 0;\nFastEstimator-Train: step: 1; ce: 0.12709273; model_lr: 0.0009999998;\nFastEstimator-Train: step: 500; ce: 0.0039903466; model_lr: 0.00095681596; steps/sec: 63.74;\nFastEstimator-Train: step: 1000; ce: 0.1970906; model_lr: 0.00083473074; steps/sec: 66.85;\nFastEstimator-Train: step: 1500; ce: 0.00413731; model_lr: 0.000654854; steps/sec: 66.22;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 29.32 sec;\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.988; ce: 0.03613729;\nFastEstimator-Finish: step: 1875; model_lr: 0.0005005; total_time: 30.65 sec;\n</pre> <p>Lets take a look at what sort of information is contained within our <code>Summary</code> object:</p> In\u00a0[4]: Copied! <pre>summary.name\n</pre> summary.name Out[4]: <pre>'experiment1'</pre> In\u00a0[5]: Copied! <pre>summary.history\n</pre> summary.history Out[5]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'logging_interval': {0: array(500)},\n                          'num_device': {0: array(0)},\n                          'ce': {1: array(0.12709273, dtype=float32),\n                           500: array(0.00399035, dtype=float32),\n                           1000: array(0.1970906, dtype=float32),\n                           1500: array(0.00413731, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095682, dtype=float32),\n                           1000: array(0.00083473, dtype=float32),\n                           1500: array(0.00065485, dtype=float32),\n                           1875: array(0.0005005, dtype=float32)},\n                          'steps/sec': {500: array(63.74),\n                           1000: array(66.85),\n                           1500: array(66.22)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('29.32 sec', dtype='&lt;U9')},\n                          'total_time': {1875: array('30.65 sec', dtype='&lt;U9')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'accuracy': {1875: array(0.988)},\n                          'ce': {1875: array(0.03613729, dtype=float32)}})})</pre> <p>The history field can appear a little daunting, but it is simply a dictionary laid out as follows: {mode: {key: {step: value}}}. Once you have invoked the .fit() method with an experiment name, subsequent calls to .test() will add their results into the same summary dictionary:</p> In\u00a0[6]: Copied! <pre>summary = est.test()\n</pre> summary = est.test() <pre>FastEstimator-Test: step: 1875; epoch: 1; accuracy: 0.9896; ce: 0.03368678;\n</pre> In\u00a0[7]: Copied! <pre>summary.history\n</pre> summary.history Out[7]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'logging_interval': {0: array(500)},\n                          'num_device': {0: array(0)},\n                          'ce': {1: array(0.12709273, dtype=float32),\n                           500: array(0.00399035, dtype=float32),\n                           1000: array(0.1970906, dtype=float32),\n                           1500: array(0.00413731, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095682, dtype=float32),\n                           1000: array(0.00083473, dtype=float32),\n                           1500: array(0.00065485, dtype=float32),\n                           1875: array(0.0005005, dtype=float32)},\n                          'steps/sec': {500: array(63.74),\n                           1000: array(66.85),\n                           1500: array(66.22)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('29.32 sec', dtype='&lt;U9')},\n                          'total_time': {1875: array('30.65 sec', dtype='&lt;U9')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'accuracy': {1875: array(0.988)},\n                          'ce': {1875: array(0.03613729, dtype=float32)}}),\n             'test': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'accuracy': {1875: array(0.9896)},\n                          'ce': {1875: array(0.03368678, dtype=float32)}})})</pre> <p>Even if an experiment name was not provided during the .fit() call, it may be provided during the .test() call. The resulting summary object will, however, only contain information from the Test mode.</p> <p></p> In\u00a0[8]: Copied! <pre>summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\")\n</pre> summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\") In\u00a0[9]: Copied! <pre>summary.name\n</pre> summary.name Out[9]: <pre>'t06a_exp1'</pre> In\u00a0[10]: Copied! <pre>summary.history['eval']\n</pre> summary.history['eval'] Out[10]: <pre>defaultdict(dict,\n            {'epoch': {1875: 1.0, 3750: 2.0, 5625: 3.0},\n             'ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02382297},\n             'min_ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02343675},\n             'since_best': {1875: 0.0, 3750: 0.0, 5625: 1.0},\n             'accuracy': {1875: 0.9882, 3750: 0.992, 5625: 0.9922}})</pre> <p></p> In\u00a0[11]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary])\n</pre> fe.summary.logs.visualize_logs(experiments=[summary]) <p>If you are only interested in visualizing a subset of these log values, it is also possible to whitelist or blacklist values via the 'include_metrics' and 'ignore_metrics' arguments respectively:</p> In\u00a0[12]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"})\n</pre> fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"}) <p>It is also possible to compare logs from different experiments, which can be especially useful when fiddling with hyper-parameter values to determine their effects on training:</p> In\u00a0[13]: Copied! <pre>fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\")\n</pre> fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\") <p>All of the log files within a given directory can also be compared at the same time, either by using the parse_log_dir() method or via the command line as follows: fastestimator logs --extension .txt --smooth 0 ../resources</p> <p></p> In\u00a0[14]: Copied! <pre>fe.summary.logs.parse_log_dir(dir_path='../resources/t06a_logs', smooth_factor=0)\n</pre> fe.summary.logs.parse_log_dir(dir_path='../resources/t06a_logs', smooth_factor=0) <p>While this is certainly an option, it is not very easy to tell at a glance which of lossA or lossB is superior. Let's use log grouping in order to get a cleaner picture:</p> In\u00a0[15]: Copied! <pre>fe.summary.logs.parse_log_dir(dir_path='../resources/t06a_logs', smooth_factor=0, group_by=r'(.*)_[\\d]+\\.txt')\n</pre> fe.summary.logs.parse_log_dir(dir_path='../resources/t06a_logs', smooth_factor=0, group_by=r'(.*)_[\\d]+\\.txt') <p>Now we are displaying the mean values for lossA and lossB, plus or minus their standard deviations over the 5 experiments. This makes it easy to see that lossA results in a better mcc score and calibration error, whereas lossB has slightly faster training, but the speeds are typically within 1 standard deviation so that might be noise. The group_by argument can take any regex pattern, and if you are using it from the command line, you can simply pass <code>--group_by _n</code> as a shortcut to get the regex pattern used above.</p> <p></p> In\u00a0[16]: Copied! <pre>import tempfile\nlog_dir = tempfile.mkdtemp()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),\n    TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\")\n]\nest = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000)\nest.fit()\n</pre> import tempfile log_dir = tempfile.mkdtemp()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),     TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\") ] est = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Tensorboard: writing logs to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpnb971yiz/20220413-150813\nFastEstimator-Start: step: 1; logging_interval: 1000; num_device: 0;\nWARNING:tensorflow:5 out of the last 160 calls to &lt;function TFNetwork._forward_step_static at 0x1814cf790&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nFastEstimator-Train: step: 1; ce: 2.3051662; model1_lr: 0.0009999998;\nFastEstimator-Train: step: 1000; ce: 0.3109427; model1_lr: 0.00083473074; steps/sec: 57.51;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 33.49 sec;\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9888; ce: 0.037164193;\nFastEstimator-Train: step: 2000; ce: 0.0102050435; model1_lr: 0.00044828805; steps/sec: 53.36;\nFastEstimator-Train: step: 3000; ce: 0.013866902; model1_lr: 9.639601e-05; steps/sec: 47.11;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 40.77 sec;\nFastEstimator-Eval: step: 3750; epoch: 2; accuracy: 0.9914; ce: 0.026301745;\nFastEstimator-Train: step: 4000; ce: 0.010989559; model1_lr: 0.0009890847; steps/sec: 45.03;\nFastEstimator-Train: step: 5000; ce: 0.007876373; model1_lr: 0.00075025; steps/sec: 45.57;\nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 41.5 sec;\nFastEstimator-Eval: step: 5625; epoch: 3; accuracy: 0.9888; ce: 0.034312796;\nFastEstimator-Finish: step: 5625; model1_lr: 0.0005005; total_time: 121.21 sec;\n</pre> <p>Now let's launch TensorBoard to visualize our logs. Note that this call will prevent any subsequent Jupyter Notebook cells from running until you manually terminate it.</p> In\u00a0[17]: Copied! <pre>#!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe\n</pre> #!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe <p>The TensorBoard display should look something like this:</p> <p></p> <p></p>"}, {"location": "tutorial/advanced/t06_summary.html#advanced-tutorial-6-summary", "title": "Advanced Tutorial 6: Summary\u00b6", "text": ""}, {"location": "tutorial/advanced/t06_summary.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Experiment Logging</li> <li>Experiment Summaries</li> <li>Log Parsing</li> <li>Summary Visualization</li> <li>Visualizing Repeat Trials</li> <li>TensorBoard Visualization</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>We will first set up a basic MNIST example for the rest of the demonstrations:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-logging", "title": "Experiment Logging\u00b6", "text": "<p>As you may have noticed if you have used FastEstimator, log messages are printed to the screen during training. If you want to persist these log messages for later records, you can simply pipe them into a file when launching training from the command line, or else just copy and paste the messages from the console into a persistent file on the disk. FastEstimator allows logging to be controlled via arguments passed to the <code>Estimator</code> class, as described in the Beginner Tutorial 7. Let's see an example logging every 120 steps:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-summaries", "title": "Experiment Summaries\u00b6", "text": "<p>Having log messages on the screen can be handy, but what if you want to access these messages within python? Enter the <code>Summary</code> class. <code>Summary</code> objects contain information about the training over time, and will be automatically generated when the <code>Estimator</code> fit() method is invoked with an experiment name:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-parsing", "title": "Log Parsing\u00b6", "text": "<p>Suppose that you have a log file saved to disk, and you want to create an in-memory <code>Summary</code> representation of it. This can be done through FastEstimator logging utilities:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-visualization", "title": "Log Visualization\u00b6", "text": "<p>While seeing log data as numbers can be informative, visualizations of data are often more useful. FastEstimator provides several ways to visualize log data: from python using <code>Summary</code> objects or log files, as well as through the command line.</p>"}, {"location": "tutorial/advanced/t06_summary.html#visualizing-repeat-trials", "title": "Visualizing Repeat Trials\u00b6", "text": "<p>Suppose you are running some experiments like the ones above to try and decide which of several experimental configurations is best. For example, suppose you are trying to decide between lossA and lossB. You run 5 experiments with each loss in order to account for randomness, and save the logs as lossA_1.txt, lossA_2.txt, lossB_1.txt, etc. You could use the method described above, for example:</p>"}, {"location": "tutorial/advanced/t06_summary.html#tensorboard", "title": "TensorBoard\u00b6", "text": "<p>Of course, no modern AI framework would be complete without TensorBoard integration. In FastEstimator, all that is required to achieve TensorBoard integration is to add the TensorBoard <code>Trace</code> to the list of traces passed to the <code>Estimator</code>:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html", "title": "Advanced Tutorial 7: Learning Rate Scheduling", "text": "<p>Learning rate schedules can be implemented using the <code>LRScheduler</code> <code>Trace</code>. <code>LRScheduler</code> takes the model and learning schedule through the lr_fn parameter. lr_fn should be a function/lambda function with 'step' or 'epoch' as its input parameter. This determines whether the learning schedule will be applied at a step or epoch level.</p> <p>For more details on traces, you can visit Beginner Tutorial 7 and Advanced Tutorial 4.</p> <p>Let's create a function to generate the pipeline, model, and network to be used for this tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\"):\n    train_data, _ = mnist.load_data()\n\n    pipeline = fe.Pipeline(train_data=train_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\"):     train_data, _ = mnist.load_data()      pipeline = fe.Pipeline(train_data=train_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import LRScheduler\n\ndef lr_schedule(epoch):\n    lr = 0.001*(20-epoch+1)/20\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)\n\nhistory = estimator.fit(summary=\"Experiment_1\")\n</pre> from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import LRScheduler  def lr_schedule(epoch):     lr = 0.001*(20-epoch+1)/20     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)  history = estimator.fit(summary=\"Experiment_1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3051612; LeNet_lr: 0.001;\nFastEstimator-Train: step: 100; ce: 0.0635467; LeNet_lr: 0.001; steps/sec: 68.63;\nFastEstimator-Train: step: 200; ce: 0.19281179; LeNet_lr: 0.001; steps/sec: 69.69;\nFastEstimator-Train: step: 300; ce: 0.08815661; LeNet_lr: 0.001; steps/sec: 69.59;\nFastEstimator-Train: step: 400; ce: 0.10132183; LeNet_lr: 0.001; steps/sec: 73.27;\nFastEstimator-Train: step: 500; ce: 0.07849489; LeNet_lr: 0.001; steps/sec: 72.99;\nFastEstimator-Train: step: 600; ce: 0.05470478; LeNet_lr: 0.001; steps/sec: 70.92;\nFastEstimator-Train: step: 700; ce: 0.07486844; LeNet_lr: 0.001; steps/sec: 71.82;\nFastEstimator-Train: step: 800; ce: 0.13934246; LeNet_lr: 0.001; steps/sec: 69.99;\nFastEstimator-Train: step: 900; ce: 0.08544485; LeNet_lr: 0.001; steps/sec: 63.82;\nFastEstimator-Train: step: 1000; ce: 0.053602487; LeNet_lr: 0.001; steps/sec: 69.94;\nFastEstimator-Train: step: 1100; ce: 0.09014913; LeNet_lr: 0.001; steps/sec: 70.25;\nFastEstimator-Train: step: 1200; ce: 0.07134402; LeNet_lr: 0.001; steps/sec: 69.68;\nFastEstimator-Train: step: 1300; ce: 0.012463177; LeNet_lr: 0.001; steps/sec: 71.6;\nFastEstimator-Train: step: 1400; ce: 0.041746277; LeNet_lr: 0.001; steps/sec: 71.23;\nFastEstimator-Train: step: 1500; ce: 0.026363641; LeNet_lr: 0.001; steps/sec: 71.07;\nFastEstimator-Train: step: 1600; ce: 0.00073903985; LeNet_lr: 0.001; steps/sec: 68.05;\nFastEstimator-Train: step: 1700; ce: 0.001534229; LeNet_lr: 0.001; steps/sec: 70.16;\nFastEstimator-Train: step: 1800; ce: 0.05694783; LeNet_lr: 0.001; steps/sec: 72.08;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 28.25 sec;\nFastEstimator-Train: step: 1900; ce: 0.015518977; LeNet_lr: 0.00095; steps/sec: 53.08;\nFastEstimator-Train: step: 2000; ce: 0.08641896; LeNet_lr: 0.00095; steps/sec: 67.71;\nFastEstimator-Train: step: 2100; ce: 0.024979822; LeNet_lr: 0.00095; steps/sec: 68.86;\nFastEstimator-Train: step: 2200; ce: 0.028013688; LeNet_lr: 0.00095; steps/sec: 67.55;\nFastEstimator-Train: step: 2300; ce: 0.15737121; LeNet_lr: 0.00095; steps/sec: 67.49;\nFastEstimator-Train: step: 2400; ce: 0.05255642; LeNet_lr: 0.00095; steps/sec: 68.04;\nFastEstimator-Train: step: 2500; ce: 0.03364688; LeNet_lr: 0.00095; steps/sec: 67.09;\nFastEstimator-Train: step: 2600; ce: 0.06446718; LeNet_lr: 0.00095; steps/sec: 65.47;\nFastEstimator-Train: step: 2700; ce: 0.003595281; LeNet_lr: 0.00095; steps/sec: 68.34;\nFastEstimator-Train: step: 2800; ce: 0.047859844; LeNet_lr: 0.00095; steps/sec: 68.43;\nFastEstimator-Train: step: 2900; ce: 0.0088707255; LeNet_lr: 0.00095; steps/sec: 67.91;\nFastEstimator-Train: step: 3000; ce: 0.09855172; LeNet_lr: 0.00095; steps/sec: 67.98;\nFastEstimator-Train: step: 3100; ce: 0.012705317; LeNet_lr: 0.00095; steps/sec: 66.99;\nFastEstimator-Train: step: 3200; ce: 0.012674243; LeNet_lr: 0.00095; steps/sec: 66.29;\nFastEstimator-Train: step: 3300; ce: 0.0048475517; LeNet_lr: 0.00095; steps/sec: 67.02;\nFastEstimator-Train: step: 3400; ce: 0.077167764; LeNet_lr: 0.00095; steps/sec: 67.14;\nFastEstimator-Train: step: 3500; ce: 0.030703163; LeNet_lr: 0.00095; steps/sec: 65.63;\nFastEstimator-Train: step: 3600; ce: 0.015752632; LeNet_lr: 0.00095; steps/sec: 67.31;\nFastEstimator-Train: step: 3700; ce: 0.10233892; LeNet_lr: 0.00095; steps/sec: 67.73;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 28.34 sec;\nFastEstimator-Train: step: 3800; ce: 0.043337934; LeNet_lr: 0.0009; steps/sec: 50.85;\nFastEstimator-Train: step: 3900; ce: 0.002429512; LeNet_lr: 0.0009; steps/sec: 67.46;\nFastEstimator-Train: step: 4000; ce: 0.009731653; LeNet_lr: 0.0009; steps/sec: 69.08;\nFastEstimator-Train: step: 4100; ce: 0.06496129; LeNet_lr: 0.0009; steps/sec: 70.65;\nFastEstimator-Train: step: 4200; ce: 0.0048102853; LeNet_lr: 0.0009; steps/sec: 64.19;\nFastEstimator-Train: step: 4300; ce: 0.02214607; LeNet_lr: 0.0009; steps/sec: 64.36;\nFastEstimator-Train: step: 4400; ce: 0.0017155888; LeNet_lr: 0.0009; steps/sec: 70.27;\nFastEstimator-Train: step: 4500; ce: 0.0050323857; LeNet_lr: 0.0009; steps/sec: 65.19;\nFastEstimator-Train: step: 4600; ce: 0.0005132223; LeNet_lr: 0.0009; steps/sec: 66.96;\nFastEstimator-Train: step: 4700; ce: 0.2890709; LeNet_lr: 0.0009; steps/sec: 68.09;\nFastEstimator-Train: step: 4800; ce: 0.0011604289; LeNet_lr: 0.0009; steps/sec: 69.16;\nFastEstimator-Train: step: 4900; ce: 0.007527765; LeNet_lr: 0.0009; steps/sec: 69.86;\nFastEstimator-Train: step: 5000; ce: 0.06803949; LeNet_lr: 0.0009; steps/sec: 67.5;\nFastEstimator-Train: step: 5100; ce: 0.01932398; LeNet_lr: 0.0009; steps/sec: 63.75;\nFastEstimator-Train: step: 5200; ce: 0.0017156545; LeNet_lr: 0.0009; steps/sec: 68.0;\nFastEstimator-Train: step: 5300; ce: 0.00885325; LeNet_lr: 0.0009; steps/sec: 60.41;\nFastEstimator-Train: step: 5400; ce: 0.020371906; LeNet_lr: 0.0009; steps/sec: 68.71;\nFastEstimator-Train: step: 5500; ce: 0.021073567; LeNet_lr: 0.0009; steps/sec: 66.44;\nFastEstimator-Train: step: 5600; ce: 0.0105727; LeNet_lr: 0.0009; steps/sec: 66.34;\nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 28.49 sec;\nFastEstimator-Finish: step: 5625; LeNet_lr: 0.0009; total_time: 85.12 sec;\n</pre> <p>The learning rate is available in the training log at steps specified using the log_steps parameter in the <code>Estimator</code>. By default, training is logged every 100 steps.</p> In\u00a0[3]: Copied! <pre>visualize_logs(history, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history, include_metrics=\"LeNet_lr\") <p>As you can see, the learning rate changes only after every epoch.</p> <p></p> In\u00a0[4]: Copied! <pre>def lr_schedule(step):\n    lr = 0.001*(7500-step+1)/7500\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory2 = estimator.fit(summary=\"Experiment_2\")\n</pre> def lr_schedule(step):     lr = 0.001*(7500-step+1)/7500     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history2 = estimator.fit(summary=\"Experiment_2\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3127494; LeNet_lr: 0.001;\nFastEstimator-Train: step: 100; ce: 0.48936948; LeNet_lr: 0.0009868; steps/sec: 69.11;\nFastEstimator-Train: step: 200; ce: 0.20337304; LeNet_lr: 0.00097346667; steps/sec: 68.69;\nFastEstimator-Train: step: 300; ce: 0.0806367; LeNet_lr: 0.00096013333; steps/sec: 67.94;\nFastEstimator-Train: step: 400; ce: 0.2164749; LeNet_lr: 0.0009468; steps/sec: 67.96;\nFastEstimator-Train: step: 500; ce: 0.035739463; LeNet_lr: 0.00093346665; steps/sec: 68.37;\nFastEstimator-Train: step: 600; ce: 0.13662587; LeNet_lr: 0.0009201333; steps/sec: 68.3;\nFastEstimator-Train: step: 700; ce: 0.03717831; LeNet_lr: 0.0009068; steps/sec: 67.53;\nFastEstimator-Train: step: 800; ce: 0.051307462; LeNet_lr: 0.00089346664; steps/sec: 65.37;\nFastEstimator-Train: step: 900; ce: 0.26505423; LeNet_lr: 0.00088013336; steps/sec: 65.54;\nFastEstimator-Train: step: 1000; ce: 0.020806756; LeNet_lr: 0.0008668; steps/sec: 65.7;\nFastEstimator-Train: step: 1100; ce: 0.33482748; LeNet_lr: 0.0008534667; steps/sec: 64.84;\nFastEstimator-Train: step: 1200; ce: 0.06784122; LeNet_lr: 0.00084013335; steps/sec: 64.64;\nFastEstimator-Train: step: 1300; ce: 0.008510821; LeNet_lr: 0.0008268; steps/sec: 62.37;\nFastEstimator-Train: step: 1400; ce: 0.0026529469; LeNet_lr: 0.0008134667; steps/sec: 63.92;\nFastEstimator-Train: step: 1500; ce: 0.10912442; LeNet_lr: 0.00080013333; steps/sec: 63.4;\nFastEstimator-Train: step: 1600; ce: 0.017952079; LeNet_lr: 0.0007868; steps/sec: 65.31;\nFastEstimator-Train: step: 1700; ce: 0.0029014135; LeNet_lr: 0.00077346666; steps/sec: 65.12;\nFastEstimator-Train: step: 1800; ce: 0.014999421; LeNet_lr: 0.0007601333; steps/sec: 64.47;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 29.07 sec;\nFastEstimator-Train: step: 1900; ce: 0.025787469; LeNet_lr: 0.0007468; steps/sec: 49.44;\nFastEstimator-Train: step: 2000; ce: 0.12261312; LeNet_lr: 0.00073346664; steps/sec: 66.56;\nFastEstimator-Train: step: 2100; ce: 0.113973364; LeNet_lr: 0.0007201333; steps/sec: 66.32;\nFastEstimator-Train: step: 2200; ce: 0.013463736; LeNet_lr: 0.0007068; steps/sec: 68.12;\nFastEstimator-Train: step: 2300; ce: 0.017395472; LeNet_lr: 0.0006934667; steps/sec: 67.15;\nFastEstimator-Train: step: 2400; ce: 0.0009720749; LeNet_lr: 0.00068013335; steps/sec: 66.65;\nFastEstimator-Train: step: 2500; ce: 0.04773684; LeNet_lr: 0.0006668; steps/sec: 65.45;\nFastEstimator-Train: step: 2600; ce: 0.0063042343; LeNet_lr: 0.0006534667; steps/sec: 64.76;\nFastEstimator-Train: step: 2700; ce: 0.0079148635; LeNet_lr: 0.00064013334; steps/sec: 66.14;\nFastEstimator-Train: step: 2800; ce: 0.046359185; LeNet_lr: 0.0006268; steps/sec: 64.41;\nFastEstimator-Train: step: 2900; ce: 0.019742897; LeNet_lr: 0.00061346666; steps/sec: 66.24;\nFastEstimator-Train: step: 3000; ce: 0.011395022; LeNet_lr: 0.0006001333; steps/sec: 67.24;\nFastEstimator-Train: step: 3100; ce: 0.12286997; LeNet_lr: 0.0005868; steps/sec: 66.07;\nFastEstimator-Train: step: 3200; ce: 0.0017318464; LeNet_lr: 0.00057346665; steps/sec: 65.83;\nFastEstimator-Train: step: 3300; ce: 0.10378831; LeNet_lr: 0.0005601333; steps/sec: 63.85;\nFastEstimator-Train: step: 3400; ce: 0.0010544249; LeNet_lr: 0.0005468; steps/sec: 63.47;\nFastEstimator-Train: step: 3500; ce: 0.013209468; LeNet_lr: 0.0005334667; steps/sec: 64.08;\nFastEstimator-Train: step: 3600; ce: 0.0063856095; LeNet_lr: 0.00052013336; steps/sec: 64.74;\nFastEstimator-Train: step: 3700; ce: 0.029427588; LeNet_lr: 0.0005068; steps/sec: 65.24;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 29.08 sec;\nFastEstimator-Finish: step: 3750; LeNet_lr: 0.0005001333; total_time: 58.17 sec;\n</pre> In\u00a0[5]: Copied! <pre>visualize_logs(history2, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history2, include_metrics=\"LeNet_lr\") <p></p> <p></p> In\u00a0[6]: Copied! <pre>from fastestimator.schedule import cosine_decay\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3))\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory3 = estimator.fit(summary=\"Experiment_3\")\n</pre> from fastestimator.schedule import cosine_decay  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3)) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history3 = estimator.fit(summary=\"Experiment_3\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.297008; LeNet_lr: 0.0009999993;\nFastEstimator-Train: step: 100; ce: 0.33457872; LeNet_lr: 0.000993005; steps/sec: 65.37;\nFastEstimator-Train: step: 200; ce: 0.17324731; LeNet_lr: 0.000972216; steps/sec: 67.94;\nFastEstimator-Train: step: 300; ce: 0.068810925; LeNet_lr: 0.0009382152; steps/sec: 69.28;\nFastEstimator-Train: step: 400; ce: 0.1458275; LeNet_lr: 0.00089195487; steps/sec: 68.89;\nFastEstimator-Train: step: 500; ce: 0.025297957; LeNet_lr: 0.00083473074; steps/sec: 67.89;\nFastEstimator-Train: step: 600; ce: 0.093824804; LeNet_lr: 0.0007681455; steps/sec: 66.97;\nFastEstimator-Train: step: 700; ce: 0.043740712; LeNet_lr: 0.000694064; steps/sec: 65.74;\nFastEstimator-Train: step: 800; ce: 0.27958778; LeNet_lr: 0.00061456126; steps/sec: 66.3;\nFastEstimator-Train: step: 900; ce: 0.061571077; LeNet_lr: 0.0005318639; steps/sec: 64.48;\nFastEstimator-Train: step: 1000; ce: 0.033488706; LeNet_lr: 0.00044828805; steps/sec: 65.23;\nFastEstimator-Train: step: 1100; ce: 0.14927314; LeNet_lr: 0.00036617456; steps/sec: 64.95;\nFastEstimator-Train: step: 1200; ce: 0.029459432; LeNet_lr: 0.00028782323; steps/sec: 64.45;\nFastEstimator-Train: step: 1300; ce: 0.2428919; LeNet_lr: 0.00021542858; steps/sec: 65.25;\nFastEstimator-Train: step: 1400; ce: 0.05267974; LeNet_lr: 0.00015101816; steps/sec: 64.99;\nFastEstimator-Train: step: 1500; ce: 0.026721286; LeNet_lr: 9.639601e-05; steps/sec: 64.68;\nFastEstimator-Train: step: 1600; ce: 0.16975759; LeNet_lr: 5.3091975e-05; steps/sec: 65.42;\nFastEstimator-Train: step: 1700; ce: 0.0128316255; LeNet_lr: 2.231891e-05; steps/sec: 66.23;\nFastEstimator-Train: step: 1800; ce: 0.03704284; LeNet_lr: 4.9387068e-06; steps/sec: 67.37;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 29.06 sec;\nFastEstimator-Train: step: 1900; ce: 0.017644638; LeNet_lr: 0.0009995619; steps/sec: 47.01;\nFastEstimator-Train: step: 2000; ce: 0.062515415; LeNet_lr: 0.0009890847; steps/sec: 67.26;\nFastEstimator-Train: step: 2100; ce: 0.036463827; LeNet_lr: 0.00096492335; steps/sec: 67.81;\nFastEstimator-Train: step: 2200; ce: 0.03142055; LeNet_lr: 0.00092775445; steps/sec: 65.61;\nFastEstimator-Train: step: 2300; ce: 0.07728143; LeNet_lr: 0.00087861903; steps/sec: 64.32;\nFastEstimator-Train: step: 2400; ce: 0.0017153291; LeNet_lr: 0.00081889326; steps/sec: 64.85;\nFastEstimator-Train: step: 2500; ce: 0.16212359; LeNet_lr: 0.00075025; steps/sec: 64.87;\nFastEstimator-Train: step: 2600; ce: 0.02398505; LeNet_lr: 0.0006746117; steps/sec: 65.58;\nFastEstimator-Train: step: 2700; ce: 0.08713155; LeNet_lr: 0.00059409696; steps/sec: 65.2;\nFastEstimator-Train: step: 2800; ce: 0.043791365; LeNet_lr: 0.00051096076; steps/sec: 67.37;\nFastEstimator-Train: step: 2900; ce: 0.0008412299; LeNet_lr: 0.00042753152; steps/sec: 67.45;\nFastEstimator-Train: step: 3000; ce: 0.00043671875; LeNet_lr: 0.000346146; steps/sec: 67.15;\nFastEstimator-Train: step: 3100; ce: 0.004842498; LeNet_lr: 0.00026908363; steps/sec: 65.77;\nFastEstimator-Train: step: 3200; ce: 0.0051019713; LeNet_lr: 0.00019850275; steps/sec: 64.36;\nFastEstimator-Train: step: 3300; ce: 0.00468382; LeNet_lr: 0.00013638017; steps/sec: 63.97;\nFastEstimator-Train: step: 3400; ce: 0.14304858; LeNet_lr: 8.445584e-05; steps/sec: 64.01;\nFastEstimator-Train: step: 3500; ce: 0.006549092; LeNet_lr: 4.4184046e-05; steps/sec: 55.52;\nFastEstimator-Train: step: 3600; ce: 0.074703395; LeNet_lr: 1.6692711e-05; steps/sec: 55.0;\nFastEstimator-Train: step: 3700; ce: 0.0007971178; LeNet_lr: 2.7518167e-06; steps/sec: 52.77;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 30.31 sec;\nFastEstimator-Finish: step: 3750; LeNet_lr: 1e-06; total_time: 59.39 sec;\n</pre> In\u00a0[7]: Copied! <pre>visualize_logs(history3, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history3, include_metrics=\"LeNet_lr\") <p></p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#advanced-tutorial-7-learning-rate-scheduling", "title": "Advanced Tutorial 7: Learning Rate Scheduling\u00b6", "text": ""}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing a Learning Rate Schedule Function<ul> <li>epoch-wise</li> <li>step-wise</li> </ul> </li> <li>Using a Built-In lr_schedule Function<ul> <li>cosine decay</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#customizing-a-learning-rate-schedule-function", "title": "Customizing a Learning Rate Schedule Function\u00b6", "text": "<p>We can specify a custom learning schedule by passing a custom function to the lr_fn parameter of <code>LRScheduler</code>. We can have this learning rate schedule applied at either the epoch or step level. Epoch and step both start from 1.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#epoch-wise", "title": "Epoch-wise\u00b6", "text": "<p>To apply learning rate scheduling at an epoch level, the custom function should have 'epoch' as its parameter. Let's look at the example below which demonstrates this. We will be using the summary parameter in the fit method to be able to visualize the learning rate later. You can go through Advanced Tutorial 6 for more details on accessing training history.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#step-wise", "title": "Step-wise\u00b6", "text": "<p>The custom function should have 'step' as its parameter for step-based learning rate schedules.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#using-built-in-lr_schedule-function", "title": "Using Built-In lr_schedule Function\u00b6", "text": "<p>Some learning rates schedules are widely popular in the deep learning community. We have implemented some of them in FastEstimator so that you don't need to write a custom schedule for them. We will be showcasing the <code>cosine decay</code> schedule below.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#cosine_decay", "title": "cosine_decay\u00b6", "text": "<p>We can specify the length of the decay cycle and initial learning rate using cycle_length and init_lr respectively. Similar to custom learning schedule, lr_fn should have step or epoch as a parameter. The FastEstimator cosine decay can be used as follows:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t08_xai.html", "title": "Advanced Tutorial 8: Explainable AI (XAI)", "text": ""}, {"location": "tutorial/advanced/t08_xai.html#advanced-tutorial-8-explainable-ai-xai", "title": "Advanced Tutorial 8: Explainable AI (XAI)\u00b6", "text": ""}, {"location": "tutorial/advanced/t08_xai.html#these-tutorials-have-moved-to-a-new-xai-tutorial-section", "title": "These tutorials have moved to a new XAI Tutorial Section\u00b6", "text": ""}, {"location": "tutorial/advanced/t09_meta_ops.html", "title": "Advanced Tutorial 9: Meta Ops", "text": "In\u00a0[1]: Copied! <pre>from fastestimator import Pipeline\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop import LambdaOp, NumpyOp\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip\nfrom fastestimator.op.numpyop.univariate import Blur\nfrom fastestimator.util import to_number, BatchDisplay, GridDisplay\n\ntrain_data, eval_data = cifair10.load_data()\n\nclass AddOne(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        return data + 1\n</pre> from fastestimator import Pipeline from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop import LambdaOp, NumpyOp from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip from fastestimator.op.numpyop.univariate import Blur from fastestimator.util import to_number, BatchDisplay, GridDisplay  train_data, eval_data = cifair10.load_data()  class AddOne(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         return data + 1 In\u00a0[2]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes  # Note that there is also a Sometimes in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), prob=0.5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Sometimes  # Note that there is also a Sometimes in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), prob=0.5)                         ]                    ) In\u00a0[3]: Copied! <pre>data = pipeline.get_results()\nfig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),\n                   BatchDisplay(image=data[\"x_out\"], title=\"x_out\")\n                  ])\nfig.show()\n</pre> data = pipeline.get_results() fig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),                    BatchDisplay(image=data[\"x_out\"], title=\"x_out\")                   ]) fig.show() In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import OneOf  # Note that there is also a OneOf in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         OneOf(Rotate(image_in=\"x\", image_out=\"x_out\", mode=\"train\", limit=45), \n                               VerticalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"), \n                               Blur(inputs=\"x\", outputs=\"x_out\", mode=\"train\", blur_limit=7))\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import OneOf  # Note that there is also a OneOf in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          OneOf(Rotate(image_in=\"x\", image_out=\"x_out\", mode=\"train\", limit=45),                                 VerticalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),                                 Blur(inputs=\"x\", outputs=\"x_out\", mode=\"train\", blur_limit=7))                         ]                    ) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\nfig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),\n                   BatchDisplay(image=data[\"x_out\"], title=\"x_out\")\n                  ])\nfig.show()\n</pre> data = pipeline.get_results() fig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),                    BatchDisplay(image=data[\"x_out\"], title=\"x_out\")                   ]) fig.show() In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),\n                         Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),                          Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=5)                         ]                    ) In\u00a0[7]: Copied! <pre>data = pipeline.get_results()\nprint(data['z'])\n</pre> data = pipeline.get_results() print(data['z']) <pre>tensor([5, 5, 5, 5])\n</pre> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),\n                         Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=lambda z: z &lt; 6.5)\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Repeat  # Note that there is also a Repeat in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda: 0, outputs=\"z\"),                          Repeat(AddOne(inputs=\"z\", outputs=\"z\"), repeat=lambda z: z &lt; 6.5)                         ]                    ) In\u00a0[9]: Copied! <pre>data = pipeline.get_results()\nprint(data['z'])\n</pre> data = pipeline.get_results() print(data['z']) <pre>tensor([7, 7, 7, 7])\n</pre> In\u00a0[10]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes, Fuse  # Note that Sometimes and Fuse are also available in tensorop.meta\n\npipeline = Pipeline(train_data=train_data,\n                    eval_data=eval_data,\n                    batch_size=4,\n                    ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),\n                         Sometimes(\n                             Fuse([\n                                 HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),\n                                 VerticalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")]))\n                        ]\n                   )\n</pre> from fastestimator.op.numpyop.meta import Sometimes, Fuse  # Note that Sometimes and Fuse are also available in tensorop.meta  pipeline = Pipeline(train_data=train_data,                     eval_data=eval_data,                     batch_size=4,                     ops=[LambdaOp(fn=lambda x: x, inputs=\"x\", outputs=\"x_out\"),                          Sometimes(                              Fuse([                                  HorizontalFlip(image_in=\"x\", image_out=\"x_out\", mode=\"train\"),                                  VerticalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")]))                         ]                    ) In\u00a0[11]: Copied! <pre>data = pipeline.get_results()\nfig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),\n                   BatchDisplay(image=data[\"x_out\"], title=\"x_out\")\n                  ])\nfig.show()\n</pre> data = pipeline.get_results() fig = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"x\"),                    BatchDisplay(image=data[\"x_out\"], title=\"x_out\")                   ]) fig.show()"}, {"location": "tutorial/advanced/t09_meta_ops.html#advanced-tutorial-9-meta-ops", "title": "Advanced Tutorial 9: Meta Ops\u00b6", "text": ""}, {"location": "tutorial/advanced/t09_meta_ops.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Meta Op Overview</li> <li>Sometimes</li> <li>OneOf</li> <li>Repeat<ul> <li>Static</li> <li>Dynamic</li> </ul> </li> <li>Fuse</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t09_meta_ops.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>Let's gather some datasets and get some imports out of the way</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#meta-op-overview", "title": "Meta Op Overview\u00b6", "text": "<p>We learned about the operator structure in Beginner Tutorial 3. Operators are used to build complex computation graphs in FastEstimator.</p> <p>Meta Ops are Operators which take other Operators as inputs and modify their functionality. These can allow for much more complicated computation graphs, as we will see in the following examples. They are available as both NumpyOps for use in a <code>Pipeline</code>, and as TensorOps for use in a <code>Network</code>.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#sometimes", "title": "Sometimes\u00b6", "text": "<p><code>Sometimes</code> is a meta op which applies a given Op with a specified probability, by default 50% of the time. The <code>Sometimes</code> Op cannot be used to create keys which do not already exist in the data dictionary, since then it would not be clear what should be done when the Op decides not to execute. One convenient way to create default values is to first use a <code>LambdaOp</code>, as described in Advanced Tutorial 3.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#oneof", "title": "OneOf\u00b6", "text": "<p><code>OneOf</code> takes a list of Ops for input, and randomly chooses one of them every step to be executed. The Ops to be selected between must all share the same inputs, outputs, and modes.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#repeat", "title": "Repeat\u00b6", "text": "<p><code>Repeat</code> takes an Op and runs it multiple times in a row. It can be set to repeat for a fixed (static) number of times, or to repeat until a given input function evaluates to False (dynamic). <code>Repeat</code> will always evaluate at least once. After performing each forward pass, it will check to see whether the stopping criteria have been met. If using an input function to determine the stopping criteria, any input arguments to that function will be looked up by name from the data dictionary and passed through to the function for evaluation.</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#static", "title": "Static\u00b6", "text": "<p>We will start with a static example of the <code>Repeat</code> Op, which will always run 5 times:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#dynamic", "title": "Dynamic\u00b6", "text": "<p>Now lets see an example of a dynamic repeat op, which uses a lambda function to determine when it should stop. In this case, the repeat will continue so long as z is less than 6.5:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#fuse", "title": "Fuse\u00b6", "text": "<p><code>Fuse</code> takes a list of Ops and combines them together into a single Op. All of the fused Ops must have the same mode. This can be useful in conjunction with the other Meta Ops. For example, suppose you have Op A and Op B, and want to run Sometimes(A) but only want B to execute when A is chosen to run by the Sometimes. You could then run Sometimes(Fuse(A,B)). Or if you wanted to perform mini-batch training within a network, you could do something like Repeat(Fuse(Model, Loss, Update)). Let's try an example where we either leave an image alone, or perform both a horizontal and vertical flip on it:</p>"}, {"location": "tutorial/advanced/t09_meta_ops.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Semantic Segmentation</li> </ul>"}, {"location": "tutorial/advanced/t10_report_generation.html", "title": "Advanced Tutorial 10: Automated Report Generation", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\nroot_output_dir = tempfile.mkdtemp()\n\ndef get_estimator(extra_traces):\n    # step 1\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(100)\n    test_data['id'] = [i for i in range(len(test_data))]  # Assign some data ids for the test report to look at\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n\n    # step 2\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    # step 3\n    traces = [\n        Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n        BestModelSaver(model=model, save_dir=root_output_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n        LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    ]\n    traces.extend(extra_traces)\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=2,\n                             traces=traces,\n                             train_steps_per_epoch=100,\n                             eval_steps_per_epoch=100,\n                             log_steps=10)\n    return estimator\n</pre> import tempfile import os import numpy as np import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  root_output_dir = tempfile.mkdtemp()  def get_estimator(extra_traces):     # step 1     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(100)     test_data['id'] = [i for i in range(len(test_data))]  # Assign some data ids for the test report to look at     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])      # step 2     model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     # step 3     traces = [         Accuracy(true_key=\"y\", pred_key=\"y_pred\"),         BestModelSaver(model=model, save_dir=root_output_dir, metric=\"accuracy\", save_best_mode=\"max\"),         LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))     ]     traces.extend(extra_traces)     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=2,                              traces=traces,                              train_steps_per_epoch=100,                              eval_steps_per_epoch=100,                              log_steps=10)     return estimator In\u00a0[2]: Copied! <pre>from fastestimator.trace.io import Traceability\n\nsave_dir = os.path.join(root_output_dir, 'report')\nest = get_estimator([Traceability(save_dir)])\n\nprint(f\"The root save directory is: {root_output_dir}\")\nprint(f\"The traceability report will be written to: {save_dir}\")\nprint(f\"Logs and images from the report will be written to: {os.path.join(save_dir, 'resources')}\")\n</pre> from fastestimator.trace.io import Traceability  save_dir = os.path.join(root_output_dir, 'report') est = get_estimator([Traceability(save_dir)])  print(f\"The root save directory is: {root_output_dir}\") print(f\"The traceability report will be written to: {save_dir}\") print(f\"Logs and images from the report will be written to: {os.path.join(save_dir, 'resources')}\") <pre>The root save directory is: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na\nThe traceability report will be written to: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report\nLogs and images from the report will be written to: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report/resources\n</pre> <p>When using Traceability, you must pass a summary name to the Estimator.fit() call. This will become the name of your report.</p> In\u00a0[3]: Copied! <pre>est.fit(\"Sample MNIST Report\")\n</pre> est.fit(\"Sample MNIST Report\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 10; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.29492; model_lr: 0.0009999998;\nFastEstimator-Train: step: 10; ce: 2.2441; model_lr: 0.0009999825; steps/sec: 70.13;\nFastEstimator-Train: step: 20; ce: 1.6545596; model_lr: 0.0009999298; steps/sec: 65.46;\nFastEstimator-Train: step: 30; ce: 0.7863882; model_lr: 0.0009998423; steps/sec: 68.01;\nFastEstimator-Train: step: 40; ce: 0.8883647; model_lr: 0.0009997196; steps/sec: 67.27;\nFastEstimator-Train: step: 50; ce: 0.57198644; model_lr: 0.0009995619; steps/sec: 64.75;\nFastEstimator-Train: step: 60; ce: 0.96408165; model_lr: 0.0009993691; steps/sec: 59.96;\nFastEstimator-Train: step: 70; ce: 0.6204411; model_lr: 0.0009991414; steps/sec: 65.0;\nFastEstimator-Train: step: 80; ce: 0.46853542; model_lr: 0.0009988786; steps/sec: 62.4;\nFastEstimator-Train: step: 90; ce: 0.6022613; model_lr: 0.0009985808; steps/sec: 63.77;\nFastEstimator-Train: step: 100; ce: 0.31527373; model_lr: 0.0009982482; steps/sec: 67.53;\nFastEstimator-Train: step: 100; epoch: 1; epoch_time: 2.97 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/model_best_accuracy.h5\nFastEstimator-Eval: step: 100; epoch: 1; accuracy: 0.88; ce: 0.4036578; max_accuracy: 0.88; since_best_accuracy: 0;\nFastEstimator-Train: step: 110; ce: 0.2963678; model_lr: 0.0009978806; steps/sec: 16.23;\nFastEstimator-Train: step: 120; ce: 0.7164498; model_lr: 0.000997478; steps/sec: 64.31;\nFastEstimator-Train: step: 130; ce: 0.45244628; model_lr: 0.0009970407; steps/sec: 63.75;\nFastEstimator-Train: step: 140; ce: 0.12593144; model_lr: 0.0009965684; steps/sec: 65.46;\nFastEstimator-Train: step: 150; ce: 0.24340093; model_lr: 0.0009960613; steps/sec: 64.74;\nFastEstimator-Train: step: 160; ce: 0.19809574; model_lr: 0.0009955195; steps/sec: 64.78;\nFastEstimator-Train: step: 170; ce: 0.2975997; model_lr: 0.0009949428; steps/sec: 61.15;\nFastEstimator-Train: step: 180; ce: 0.18596129; model_lr: 0.0009943316; steps/sec: 57.29;\nFastEstimator-Train: step: 190; ce: 0.16816229; model_lr: 0.0009936856; steps/sec: 51.65;\nFastEstimator-Train: step: 200; ce: 0.1781869; model_lr: 0.000993005; steps/sec: 60.26;\nFastEstimator-Train: step: 200; epoch: 2; epoch_time: 2.09 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/model_best_accuracy.h5\nFastEstimator-Eval: step: 200; epoch: 2; accuracy: 0.936875; ce: 0.20415045; max_accuracy: 0.936875; since_best_accuracy: 0;\nFastEstimator-Finish: step: 200; model_lr: 0.000993005; total_time: 6.86 sec;\nFastEstimator-Traceability: Report written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report/sample_mnist_report.pdf\n</pre> Out[3]: <pre>&lt;fastestimator.summary.summary.Summary at 0x17a55be80&gt;</pre> <p>If everything went according to plan, then inside your root save directory you should now have the following files:</p> <pre><code>/report\n    sample_mnist_report.pdf\n    sample_mnist_report.tex\n    /resources\n        sample_mnist_report_logs.png\n        sample_mnist_report_model.pdf\n        sample_mnist_report.txt\n</code></pre> <p>You could then switch up your experiment parameters and call .fit() with a new experiment name in order to write more reports into the same folder. A call to <code>fastestimator logs ./resources</code> would then allow you to easily compare these experiments, as described in Advanced Tutorial 6</p> <p>Our report should look something like this (use Chrome or Firefox to view):</p> In\u00a0[4]: Copied! <pre>from IPython.display import IFrame\nIFrame('../resources/t10a_traceability.pdf', width=600, height=800)\n</pre> from IPython.display import IFrame IFrame('../resources/t10a_traceability.pdf', width=600, height=800) Out[4]: <p></p> In\u00a0[5]: Copied! <pre>from fastestimator.trace.io.test_report import TestCase, TestReport\n\nsave_dir = os.path.join(root_output_dir, 'report2')\n\n# Note that the name of the input to the 'criteria' function must match a key in the data dictionary\nagg_test_easy = TestCase(description='Accuracy should be greater than 1%', criteria=lambda accuracy: accuracy &gt; 0.01)\nagg_test_hard = TestCase(description='Accuracy should be greater than 99%', criteria=lambda accuracy: accuracy &gt; 0.99)\n\ninst_test_hard = TestCase(description='All Data should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=0)\ninst_test_easy = TestCase(description='At least one image should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=len(est.pipeline.data['test'])-1)\n\nreport = TestReport(test_cases=[agg_test_easy, agg_test_hard, inst_test_easy, inst_test_hard], save_path=save_dir, data_id='id')\n\nest = get_estimator([report])\n\nprint(f\"The root save directory is: {root_output_dir}\")\nprint(f\"The test report will be written to: {save_dir}\")\nprint(f\"A json summary of the report will be written to: {os.path.join(save_dir, 'resources')}\")\n</pre> from fastestimator.trace.io.test_report import TestCase, TestReport  save_dir = os.path.join(root_output_dir, 'report2')  # Note that the name of the input to the 'criteria' function must match a key in the data dictionary agg_test_easy = TestCase(description='Accuracy should be greater than 1%', criteria=lambda accuracy: accuracy &gt; 0.01) agg_test_hard = TestCase(description='Accuracy should be greater than 99%', criteria=lambda accuracy: accuracy &gt; 0.99)  inst_test_hard = TestCase(description='All Data should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=0) inst_test_easy = TestCase(description='At least one image should be correctly classified', criteria=lambda y, y_pred: np.equal(y,np.argmax(y_pred, axis=-1)), aggregate=False, fail_threshold=len(est.pipeline.data['test'])-1)  report = TestReport(test_cases=[agg_test_easy, agg_test_hard, inst_test_easy, inst_test_hard], save_path=save_dir, data_id='id')  est = get_estimator([report])  print(f\"The root save directory is: {root_output_dir}\") print(f\"The test report will be written to: {save_dir}\") print(f\"A json summary of the report will be written to: {os.path.join(save_dir, 'resources')}\") <pre>The root save directory is: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na\nThe test report will be written to: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report2\nA json summary of the report will be written to: /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report2/resources\n</pre> In\u00a0[6]: Copied! <pre>est.fit(\"MNIST\")\nest.test()\n</pre> est.fit(\"MNIST\") est.test() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 10; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3143482; model1_lr: 0.0009999998;\nFastEstimator-Train: step: 10; ce: 2.1496544; model1_lr: 0.0009999825; steps/sec: 52.26;\nFastEstimator-Train: step: 20; ce: 1.8073351; model1_lr: 0.0009999298; steps/sec: 53.76;\nFastEstimator-Train: step: 30; ce: 1.1469091; model1_lr: 0.0009998423; steps/sec: 54.22;\nFastEstimator-Train: step: 40; ce: 0.58918655; model1_lr: 0.0009997196; steps/sec: 51.02;\nFastEstimator-Train: step: 50; ce: 0.51719546; model1_lr: 0.0009995619; steps/sec: 50.71;\nFastEstimator-Train: step: 60; ce: 1.4550462; model1_lr: 0.0009993691; steps/sec: 53.08;\nFastEstimator-Train: step: 70; ce: 0.30743462; model1_lr: 0.0009991414; steps/sec: 54.5;\nFastEstimator-Train: step: 80; ce: 0.42680097; model1_lr: 0.0009988786; steps/sec: 49.95;\nFastEstimator-Train: step: 90; ce: 0.40482238; model1_lr: 0.0009985808; steps/sec: 53.35;\nFastEstimator-Train: step: 100; ce: 0.48199874; model1_lr: 0.0009982482; steps/sec: 51.36;\nFastEstimator-Train: step: 100; epoch: 1; epoch_time: 2.67 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/model1_best_accuracy.h5\nFastEstimator-Eval: step: 100; epoch: 1; accuracy: 0.8759375; ce: 0.39020747; max_accuracy: 0.8759375; since_best_accuracy: 0;\nFastEstimator-Train: step: 110; ce: 0.42094475; model1_lr: 0.0009978806; steps/sec: 13.26;\nFastEstimator-Train: step: 120; ce: 0.40665025; model1_lr: 0.000997478; steps/sec: 53.11;\nFastEstimator-Train: step: 130; ce: 0.3574287; model1_lr: 0.0009970407; steps/sec: 51.02;\nFastEstimator-Train: step: 140; ce: 0.14667201; model1_lr: 0.0009965684; steps/sec: 54.13;\nFastEstimator-Train: step: 150; ce: 0.41308922; model1_lr: 0.0009960613; steps/sec: 54.12;\nFastEstimator-Train: step: 160; ce: 0.12534323; model1_lr: 0.0009955195; steps/sec: 53.03;\nFastEstimator-Train: step: 170; ce: 0.18102367; model1_lr: 0.0009949428; steps/sec: 51.0;\nFastEstimator-Train: step: 180; ce: 0.13506973; model1_lr: 0.0009943316; steps/sec: 52.05;\nFastEstimator-Train: step: 190; ce: 0.10690997; model1_lr: 0.0009936856; steps/sec: 51.8;\nFastEstimator-Train: step: 200; ce: 0.14696348; model1_lr: 0.000993005; steps/sec: 53.67;\nFastEstimator-Train: step: 200; epoch: 2; epoch_time: 2.51 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/model1_best_accuracy.h5\nFastEstimator-Eval: step: 200; epoch: 2; accuracy: 0.9275; ce: 0.22968279; max_accuracy: 0.9275; since_best_accuracy: 0;\nFastEstimator-Finish: step: 200; model1_lr: 0.000993005; total_time: 7.65 sec;\nFastEstimator-Test: step: 200; epoch: 2; accuracy: 0.97; ce: 0.14020246;\nFastEstimator-TestReport: Report written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmptrgna3na/report2/mnist_TestReport.pdf\n</pre> Out[6]: <pre>&lt;fastestimator.summary.summary.Summary at 0x17ba5ddc0&gt;</pre> <p>If everything went according to plan, then inside your root save directory you should now have the following files:</p> <pre><code>/report2\n    mnist_TestReport.pdf\n    mnist_TestReport.tex\n    /resources\n        mnist_TestReport.json\n</code></pre> <p>Our report should look something like this (use Chrome or Firefox to view):</p> In\u00a0[7]: Copied! <pre>from IPython.display import IFrame\nIFrame('../resources/t10a_test.pdf', width=600, height=800)\n</pre> from IPython.display import IFrame IFrame('../resources/t10a_test.pdf', width=600, height=800) Out[7]:"}, {"location": "tutorial/advanced/t10_report_generation.html#advanced-tutorial-10-automated-report-generation", "title": "Advanced Tutorial 10: Automated Report Generation\u00b6", "text": ""}, {"location": "tutorial/advanced/t10_report_generation.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Overview and Dependencies</li> <li>Traceability</li> <li>Test Report</li> </ul>"}, {"location": "tutorial/advanced/t10_report_generation.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>Let's get some imports and object construction out of the way:</p>"}, {"location": "tutorial/advanced/t10_report_generation.html#overview-and-dependencies", "title": "Overview and Dependencies\u00b6", "text": "<p>FastEstimator provides Traces which allow you to automatically generate traceability documents and test reports. These reports are written in the LaTeX file format, and then automatically compiled into PDF documents if you have LaTeX installed on your machine. If you don't have LaTeX installed on your training machine, you can still generate the report files and then move them to a different computer in order to compile them manually. Generating traceability documents also requires GraphViz which, unlike LaTeX, must be installed in order for training to proceed.</p> <pre><code>Installing Dependencies:\n    On Linux: \n        apt-get install -y graphviz texlive-latex-base texlive-latex-extra\n    On SageMaker:\n        unset PYTHONPATH\n        export DEBIAN_FRONTEND=noninteractive\n        apt-get install -y graphviz texlive-latex-base texlive-latex-extra\n    On Mac:\n        brew install graphviz\n        brew cask install mactex\n    On Windows:\n        winget install graphviz\n        winget install TeXLive\n</code></pre>"}, {"location": "tutorial/advanced/t10_report_generation.html#traceability", "title": "Traceability\u00b6", "text": "<p>Traceability reports are designed to capture all the information about the state of your system when an experiment was run. The report will include training graphs, operator architecture diagrams, model architecture diagrams, a summary of your system configuration, and the values of all variables used to instantiate objects during training. It will also automatically save a copy of your log output to disk, which can be especially useful for comparing different experiment configurations without worrying about forgetting what settings were used for each run. To generate this report, simply add a Traceability trace to your list of traces:</p>"}, {"location": "tutorial/advanced/t10_report_generation.html#test-report", "title": "Test Report\u00b6", "text": "<p>Test Reports can provide an automatically generated overview summary of how well your model is performing. This could be useful if, for example, you needed to submit documentation to a regulatory agency. Test Reports can also be used to highlight particular failure cases so that you can investigate problematic data points in more detail.</p> <p>The <code>TestReport</code> trace takes a list of <code>TestCase</code> objects as input. These are further subdivided into two types: aggregate and per-instance. Aggregate test cases run at the end of the test epoch and deal with aggregated information (typically metrics such as accuracy). Per-instance tests run at the end of every step during testing, and are meant to evaluate every element within a batch independently. If your data dictionary happens to contain data instance ids, you can also use these to find problematic inputs.</p>"}, {"location": "tutorial/advanced/t11_model_calibration.html", "title": "Advanced Tutorial 11: Model Calibration", "text": "<p>We'll start by getting the imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tempfile\nimport os\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Calibrate\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import PBMCalibrator\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import CalibrationError, MCC\nfrom fastestimator.util import to_list\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n</pre> import tempfile import os  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Calibrate from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import PBMCalibrator from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import CalibrationError, MCC from fastestimator.util import to_list  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 } <p>And let's define a function to build a generic ciFAIR10 estimator. We will show how to use combinations of extra traces and post-processing ops to enhance this estimator throughout the tutorial.</p> In\u00a0[2]: Copied! <pre>def build_estimator(extra_traces = None, postprocessing_ops = None):\n    batch_size=128\n    save_dir = tempfile.mkdtemp()\n    extra_traces = to_list(extra_traces)\n    postprocessing_ops = to_list(postprocessing_ops)\n    train_data, eval_data = cifair10.load_data()\n    test_data = eval_data.split(range(len(eval_data) // 2))\n    pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        test_data=test_data,\n        batch_size=batch_size,\n        ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n             PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n             RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n             Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n             CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n             ],\n        num_process=0)\n\n    model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\n    network = fe.Network(\n        ops=[\n            ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n            CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n            UpdateOp(model=model, loss_name=\"ce\")\n        ], \n        pops=postprocessing_ops)  # &lt;---- Some of the secret sauce will go here\n\n    traces = [\n        MCC(true_key=\"y\", pred_key=\"y_pred\"),\n        BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),\n    ]\n    traces = traces + extra_traces  # &lt;---- Most of the secret sauce will go here\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=21,\n                             traces=traces,\n                             log_steps=300)\n    return estimator\n</pre> def build_estimator(extra_traces = None, postprocessing_ops = None):     batch_size=128     save_dir = tempfile.mkdtemp()     extra_traces = to_list(extra_traces)     postprocessing_ops = to_list(postprocessing_ops)     train_data, eval_data = cifair10.load_data()     test_data = eval_data.split(range(len(eval_data) // 2))     pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         test_data=test_data,         batch_size=batch_size,         ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),              PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),              RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),              Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),              CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),              ],         num_process=0)      model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")     network = fe.Network(         ops=[             ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),             CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),             UpdateOp(model=model, loss_name=\"ce\")         ],          pops=postprocessing_ops)  # &lt;---- Some of the secret sauce will go here      traces = [         MCC(true_key=\"y\", pred_key=\"y_pred\"),         BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),     ]     traces = traces + extra_traces  # &lt;---- Most of the secret sauce will go here     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=21,                              traces=traces,                              log_steps=300)     return estimator <p></p> In\u00a0[3]: Copied! <pre>estimator = build_estimator(extra_traces=CalibrationError(true_key=\"y\", pred_key=\"y_pred\", confidence_interval=95))\n</pre> estimator = build_estimator(extra_traces=CalibrationError(true_key=\"y\", pred_key=\"y_pred\", confidence_interval=95)) In\u00a0[4]: Copied! <pre>summary = estimator.fit(\"experiment1\")\n</pre> summary = estimator.fit(\"experiment1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.364285;\nFastEstimator-Train: step: 300; ce: 1.5092063; steps/sec: 13.04;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 31.73 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 391; epoch: 1; calibration_error: (0.0349, 0.0394, 0.045); ce: 1.3210957; max_mcc: 0.4815202560109812; mcc: 0.4815202560109812; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 1.2845359; steps/sec: 13.74;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 26.76 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; calibration_error: (0.0277, 0.0323, 0.0372); ce: 1.1139303; max_mcc: 0.5592140800930085; mcc: 0.5592140800930085; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 1.2871141; steps/sec: 14.45;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 27.92 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; calibration_error: (0.0318, 0.0358, 0.0402); ce: 1.0209823; max_mcc: 0.5947684007258542; mcc: 0.5947684007258542; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 1.1260216; steps/sec: 13.86;\nFastEstimator-Train: step: 1500; ce: 1.174921; steps/sec: 14.19;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 28.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; calibration_error: (0.0256, 0.0288, 0.0334); ce: 0.97435844; max_mcc: 0.6225507938118597; mcc: 0.6225507938118597; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 0.9238555; steps/sec: 11.79;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 31.89 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; calibration_error: (0.0208, 0.026, 0.0295); ce: 0.8851633; max_mcc: 0.6524498253308791; mcc: 0.6524498253308791; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 1.1505089; steps/sec: 13.71;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 27.85 sec;\nFastEstimator-Eval: step: 2346; epoch: 6; calibration_error: (0.0399, 0.0454, 0.0513); ce: 0.8992597; max_mcc: 0.6524498253308791; mcc: 0.6422697701389847; since_best_mcc: 1;\nFastEstimator-Train: step: 2400; ce: 0.88649476; steps/sec: 13.89;\nFastEstimator-Train: step: 2700; ce: 0.83883905; steps/sec: 14.12;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 27.7 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; calibration_error: (0.0311, 0.0374, 0.0427); ce: 0.8641518; max_mcc: 0.6573990501670419; mcc: 0.6573990501670419; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 0.91415524; steps/sec: 14.43;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 27.35 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 3128; epoch: 8; calibration_error: (0.0419, 0.0467, 0.0505); ce: 0.86572677; max_mcc: 0.6622589033779989; mcc: 0.6622589033779989; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: 0.84909713; steps/sec: 14.06;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 29.29 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 3519; epoch: 9; calibration_error: (0.0227, 0.0284, 0.0349); ce: 0.7914774; max_mcc: 0.6860117005983222; mcc: 0.6860117005983222; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: 0.90386593; steps/sec: 13.1;\nFastEstimator-Train: step: 3900; ce: 1.0241306; steps/sec: 13.93;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 28.02 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 3910; epoch: 10; calibration_error: (0.0276, 0.0317, 0.0365); ce: 0.78860706; max_mcc: 0.6993861770313364; mcc: 0.6993861770313364; since_best_mcc: 0;\nFastEstimator-Train: step: 4200; ce: 0.8331722; steps/sec: 14.46;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 27.1 sec;\nFastEstimator-Eval: step: 4301; epoch: 11; calibration_error: (0.0298, 0.0346, 0.0399); ce: 0.7831341; max_mcc: 0.6993861770313364; mcc: 0.6894049337786957; since_best_mcc: 1;\nFastEstimator-Train: step: 4500; ce: 0.8837549; steps/sec: 14.11;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 28.27 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 4692; epoch: 12; calibration_error: (0.0166, 0.0218, 0.0265); ce: 0.7365888; max_mcc: 0.7091949717856824; mcc: 0.7091949717856824; since_best_mcc: 0;\nFastEstimator-Train: step: 4800; ce: 0.9474237; steps/sec: 13.8;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 28.08 sec;\nFastEstimator-Eval: step: 5083; epoch: 13; calibration_error: (0.0397, 0.045, 0.0491); ce: 0.7789904; max_mcc: 0.7091949717856824; mcc: 0.7014650315495937; since_best_mcc: 1;\nFastEstimator-Train: step: 5100; ce: 1.0256269; steps/sec: 13.7;\nFastEstimator-Train: step: 5400; ce: 0.9247025; steps/sec: 11.6;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 33.01 sec;\nFastEstimator-Eval: step: 5474; epoch: 14; calibration_error: (0.0362, 0.0399, 0.0442); ce: 0.755077; max_mcc: 0.7091949717856824; mcc: 0.7070422768702824; since_best_mcc: 2;\nFastEstimator-Train: step: 5700; ce: 0.70286965; steps/sec: 13.75;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 29.03 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 5865; epoch: 15; calibration_error: (0.0232, 0.0276, 0.0324); ce: 0.7251368; max_mcc: 0.7163199141358709; mcc: 0.7163199141358709; since_best_mcc: 0;\nFastEstimator-Train: step: 6000; ce: 0.86999273; steps/sec: 13.33;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 28.46 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 6256; epoch: 16; calibration_error: (0.0139, 0.0188, 0.0246); ce: 0.6940484; max_mcc: 0.7354250490842272; mcc: 0.7354250490842272; since_best_mcc: 0;\nFastEstimator-Train: step: 6300; ce: 0.7672814; steps/sec: 13.6;\nFastEstimator-Train: step: 6600; ce: 0.73419267; steps/sec: 13.72;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 28.82 sec;\nFastEstimator-Eval: step: 6647; epoch: 17; calibration_error: (0.0114, 0.0151, 0.0198); ce: 0.70021456; max_mcc: 0.7354250490842272; mcc: 0.7238136191672119; since_best_mcc: 1;\nFastEstimator-Train: step: 6900; ce: 0.72660446; steps/sec: 12.8;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 30.39 sec;\nFastEstimator-Eval: step: 7038; epoch: 18; calibration_error: (0.0347, 0.0385, 0.0451); ce: 0.71250015; max_mcc: 0.7354250490842272; mcc: 0.7180530704737343; since_best_mcc: 2;\nFastEstimator-Train: step: 7200; ce: 0.8266677; steps/sec: 13.58;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 28.03 sec;\nFastEstimator-Eval: step: 7429; epoch: 19; calibration_error: (0.0181, 0.0244, 0.0309); ce: 0.68665934; max_mcc: 0.7354250490842272; mcc: 0.7327478409220392; since_best_mcc: 3;\nFastEstimator-Train: step: 7500; ce: 0.71262574; steps/sec: 13.92;\nFastEstimator-Train: step: 7800; ce: 0.7178256; steps/sec: 14.22;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 27.46 sec;\nFastEstimator-Eval: step: 7820; epoch: 20; calibration_error: (0.0226, 0.0274, 0.0326); ce: 0.68733853; max_mcc: 0.7354250490842272; mcc: 0.7334799441801962; since_best_mcc: 4;\nFastEstimator-Train: step: 8100; ce: 0.5832919; steps/sec: 14.46;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 27.44 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Eval: step: 8211; epoch: 21; calibration_error: (0.0285, 0.0319, 0.0374); ce: 0.67022; max_mcc: 0.7361255970251015; mcc: 0.7361255970251015; since_best_mcc: 0;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpeksb1mr2/model_best_mcc.h5\nFastEstimator-Finish: step: 8211; model_lr: 0.001; total_time: 793.06 sec;\n</pre> In\u00a0[5]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 8211; epoch: 21; calibration_error: (0.0208, 0.0258, 0.0301); ce: 0.7097068; mcc: 0.7399028376036656;\n</pre> Out[5]: <pre>&lt;fastestimator.summary.summary.Summary at 0x108064d30&gt;</pre> <p>Let's take a look at how the calibration error changed over training:</p> In\u00a0[6]: Copied! <pre>visualize_logs([summary], include_metrics={'calibration_error', 'mcc', 'ce'})\n</pre> visualize_logs([summary], include_metrics={'calibration_error', 'mcc', 'ce'}) <p>As we can see from the graph above, calibration error is significantly more noisy than classical metrics like mcc or accuracy. In this case it does seem to have improved somewhat with training, though the correlation isn't strong enough to expect to be able to eliminate your calibration error just by training longer. Instead, we will see how you can effectively calibrate a model after-the-fact:</p> <p></p> In\u00a0[7]: Copied! <pre>save_path = os.path.join(tempfile.mkdtemp(), 'calibrator.pkl')\nestimator = build_estimator(extra_traces=[CalibrationError(true_key=\"y\", pred_key=\"y_pred\", confidence_interval=95), \n                                          PBMCalibrator(true_key=\"y\", pred_key=\"y_pred\", save_path=save_path, save_if_key=\"since_best_mcc\", mode=\"eval\"),\n                                          # We will also compare the MCC and calibration error between the original and calibrated samples:\n                                          MCC(true_key=\"y\", pred_key=\"y_pred_calibrated\", output_name=\"mcc (calibrated)\", mode=\"test\"),\n                                          CalibrationError(true_key=\"y\", pred_key=\"y_pred_calibrated\", output_name=\"calibration_error (calibrated)\", confidence_interval=95, mode=\"test\"), \n                                          ],\n                           postprocessing_ops = Calibrate(inputs=\"y_pred\", outputs=\"y_pred_calibrated\", calibration_fn=save_path, mode=\"test\"))\n</pre> save_path = os.path.join(tempfile.mkdtemp(), 'calibrator.pkl') estimator = build_estimator(extra_traces=[CalibrationError(true_key=\"y\", pred_key=\"y_pred\", confidence_interval=95),                                            PBMCalibrator(true_key=\"y\", pred_key=\"y_pred\", save_path=save_path, save_if_key=\"since_best_mcc\", mode=\"eval\"),                                           # We will also compare the MCC and calibration error between the original and calibrated samples:                                           MCC(true_key=\"y\", pred_key=\"y_pred_calibrated\", output_name=\"mcc (calibrated)\", mode=\"test\"),                                           CalibrationError(true_key=\"y\", pred_key=\"y_pred_calibrated\", output_name=\"calibration_error (calibrated)\", confidence_interval=95, mode=\"test\"),                                            ],                            postprocessing_ops = Calibrate(inputs=\"y_pred\", outputs=\"y_pred_calibrated\", calibration_fn=save_path, mode=\"test\")) In\u00a0[8]: Copied! <pre>summary = estimator.fit(\"experiment2\")\n</pre> summary = estimator.fit(\"experiment2\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nWARNING:tensorflow:5 out of the last 43 calls to &lt;function TFNetwork._forward_step_static at 0x17a27d670&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nFastEstimator-Train: step: 1; ce: 2.2869081;\nFastEstimator-Train: step: 300; ce: 1.4376379; steps/sec: 15.34;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 26.92 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 391; epoch: 1; calibration_error: (0.0282, 0.033, 0.0391); ce: 1.2963489; max_mcc: 0.47887144783984326; mcc: 0.47887144783984326; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 1.2960017; steps/sec: 14.03;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 27.64 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 782; epoch: 2; calibration_error: (0.033, 0.0385, 0.0417); ce: 1.1607816; max_mcc: 0.5413508452419946; mcc: 0.5413508452419946; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 1.3636127; steps/sec: 13.55;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 30.64 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 1173; epoch: 3; calibration_error: (0.0224, 0.0271, 0.0339); ce: 1.0179937; max_mcc: 0.6101771269094974; mcc: 0.6101771269094974; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 1.084051; steps/sec: 12.39;\nFastEstimator-Train: step: 1500; ce: 1.0849717; steps/sec: 11.78;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 33.79 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 1564; epoch: 4; calibration_error: (0.0199, 0.0254, 0.0301); ce: 0.9683536; max_mcc: 0.6222706205728137; mcc: 0.6222706205728137; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 1.002698; steps/sec: 11.22;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 35.22 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 1955; epoch: 5; calibration_error: (0.033, 0.037, 0.0426); ce: 0.9345478; max_mcc: 0.6267778416655716; mcc: 0.6267778416655716; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 1.0518067; steps/sec: 10.68;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 35.07 sec;\nFastEstimator-Eval: step: 2346; epoch: 6; calibration_error: (0.044, 0.049, 0.0534); ce: 0.94733584; max_mcc: 0.6267778416655716; mcc: 0.6233546799342925; since_best_mcc: 1;\nFastEstimator-Train: step: 2400; ce: 0.9290854; steps/sec: 11.98;\nFastEstimator-Train: step: 2700; ce: 0.8167516; steps/sec: 13.0;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 29.83 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 2737; epoch: 7; calibration_error: (0.0268, 0.0312, 0.0355); ce: 0.882226; max_mcc: 0.6533327147622581; mcc: 0.6533327147622581; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 0.8312993; steps/sec: 13.76;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 28.22 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 3128; epoch: 8; calibration_error: (0.0315, 0.0353, 0.0407); ce: 0.86187744; max_mcc: 0.6627038830783283; mcc: 0.6627038830783283; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: 1.00288; steps/sec: 14.13;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 27.96 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 3519; epoch: 9; calibration_error: (0.0194, 0.023, 0.0279); ce: 0.80819094; max_mcc: 0.6796538303957567; mcc: 0.6796538303957567; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: 0.86661005; steps/sec: 13.73;\nFastEstimator-Train: step: 3900; ce: 0.9506703; steps/sec: 14.18;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 27.83 sec;\nFastEstimator-Eval: step: 3910; epoch: 10; calibration_error: (0.0382, 0.0426, 0.0465); ce: 0.8241831; max_mcc: 0.6796538303957567; mcc: 0.6746589510196896; since_best_mcc: 1;\nFastEstimator-Train: step: 4200; ce: 0.66708326; steps/sec: 14.2;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 27.53 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 4301; epoch: 11; calibration_error: (0.026, 0.031, 0.0372); ce: 0.7791259; max_mcc: 0.6915740268068222; mcc: 0.6915740268068222; since_best_mcc: 0;\nFastEstimator-Train: step: 4500; ce: 0.78781855; steps/sec: 14.01;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 31.77 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 4692; epoch: 12; calibration_error: (0.0317, 0.0363, 0.0405); ce: 0.7906082; max_mcc: 0.694238202567274; mcc: 0.694238202567274; since_best_mcc: 0;\nFastEstimator-Train: step: 4800; ce: 0.87651783; steps/sec: 10.46;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 37.82 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 5083; epoch: 13; calibration_error: (0.021, 0.0252, 0.03); ce: 0.7387682; max_mcc: 0.7108036127220436; mcc: 0.7108036127220436; since_best_mcc: 0;\nFastEstimator-Train: step: 5100; ce: 0.8433088; steps/sec: 10.66;\nFastEstimator-Train: step: 5400; ce: 0.5917939; steps/sec: 13.79;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 28.78 sec;\nFastEstimator-Eval: step: 5474; epoch: 14; calibration_error: (0.0295, 0.0331, 0.0366); ce: 0.77765036; max_mcc: 0.7108036127220436; mcc: 0.6947198040176703; since_best_mcc: 1;\nFastEstimator-Train: step: 5700; ce: 0.8003416; steps/sec: 14.15;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 27.74 sec;\nFastEstimator-Eval: step: 5865; epoch: 15; calibration_error: (0.0376, 0.0426, 0.0477); ce: 0.7729173; max_mcc: 0.7108036127220436; mcc: 0.6948574421893091; since_best_mcc: 2;\nFastEstimator-Train: step: 6000; ce: 0.79758346; steps/sec: 14.07;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 27.36 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 6256; epoch: 16; calibration_error: (0.0202, 0.024, 0.03); ce: 0.7178384; max_mcc: 0.7197751708113554; mcc: 0.7197751708113554; since_best_mcc: 0;\nFastEstimator-Train: step: 6300; ce: 0.8341557; steps/sec: 14.05;\nFastEstimator-Train: step: 6600; ce: 0.73111624; steps/sec: 14.46;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 27.31 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 6647; epoch: 17; calibration_error: (0.0241, 0.0306, 0.0353); ce: 0.7144198; max_mcc: 0.7279127104193787; mcc: 0.7279127104193787; since_best_mcc: 0;\nFastEstimator-Train: step: 6900; ce: 0.8111193; steps/sec: 14.57;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 27.24 sec;\nFastEstimator-Eval: step: 7038; epoch: 18; calibration_error: (0.0292, 0.0348, 0.0407); ce: 0.71747106; max_mcc: 0.7279127104193787; mcc: 0.7237059364078412; since_best_mcc: 1;\nFastEstimator-Train: step: 7200; ce: 0.71593326; steps/sec: 14.33;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 27.45 sec;\nFastEstimator-Eval: step: 7429; epoch: 19; calibration_error: (0.0228, 0.0282, 0.0336); ce: 0.69454527; max_mcc: 0.7279127104193787; mcc: 0.7254248272218604; since_best_mcc: 2;\nFastEstimator-Train: step: 7500; ce: 0.92269725; steps/sec: 13.95;\nFastEstimator-Train: step: 7800; ce: 0.7913792; steps/sec: 11.81;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 32.34 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-PBMCalibrator: Calibrator written to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Eval: step: 7820; epoch: 20; calibration_error: (0.019, 0.0248, 0.0301); ce: 0.66156435; max_mcc: 0.7342423316872604; mcc: 0.7342423316872604; since_best_mcc: 0;\nFastEstimator-Train: step: 8100; ce: 0.72583616; steps/sec: 12.59;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 30.54 sec;\nFastEstimator-Eval: step: 8211; epoch: 21; calibration_error: (0.0253, 0.033, 0.0379); ce: 0.69342196; max_mcc: 0.7342423316872604; mcc: 0.7329696458591745; since_best_mcc: 1;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp8silmrs5/model1_best_mcc.h5\nFastEstimator-Finish: step: 8211; model1_lr: 0.001; total_time: 841.56 sec;\n</pre> In\u00a0[9]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Calibrate: calibration function loaded from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpimlnddtj/calibrator.pkl\nFastEstimator-Test: step: 8211; epoch: 21; calibration_error: (0.023, 0.0275, 0.0335); calibration_error (calibrated): (0.0015, 0.0054, 0.0106); ce: 0.69831085; mcc: 0.7473948072665257; mcc (calibrated): 0.7516498941076983;\n</pre> Out[9]: <pre>&lt;fastestimator.summary.summary.Summary at 0x17a297370&gt;</pre> In\u00a0[10]: Copied! <pre>visualize_logs([summary], include_metrics={'calibration_error', 'mcc', 'ce', \"calibration_error (calibrated)\", \"mcc (calibrated)\"})\n</pre> visualize_logs([summary], include_metrics={'calibration_error', 'mcc', 'ce', \"calibration_error (calibrated)\", \"mcc (calibrated)\"}) In\u00a0[11]: Copied! <pre>delta = summary.history['test']['mcc (calibrated)'][8211] - summary.history['test']['mcc'][8211]\nrelative_delta = delta / summary.history['test']['mcc'][8211]\nprint(f\"mcc change after calibration: {delta} ({relative_delta*100}%)\")\n</pre> delta = summary.history['test']['mcc (calibrated)'][8211] - summary.history['test']['mcc'][8211] relative_delta = delta / summary.history['test']['mcc'][8211] print(f\"mcc change after calibration: {delta} ({relative_delta*100}%)\") <pre>mcc change after calibration: 0.004255086841172595 (0.5693225052947423%)\n</pre> In\u00a0[12]: Copied! <pre>delta = summary.history['test']['calibration_error (calibrated)'][8211].y - summary.history['test']['calibration_error'][8211].y\nrelative_delta = delta / summary.history['test']['calibration_error'][8211].y\nprint(f\"calibration error change after calibration: {delta} ({relative_delta*100}%)\")\n</pre> delta = summary.history['test']['calibration_error (calibrated)'][8211].y - summary.history['test']['calibration_error'][8211].y relative_delta = delta / summary.history['test']['calibration_error'][8211].y print(f\"calibration error change after calibration: {delta} ({relative_delta*100}%)\") <pre>calibration error change after calibration: -0.0221 (-80.36363636363637%)\n</pre> <p>As we can see from the graphs and values above, with the use of a platt binning marginal calibrator we can dramatically reduce a model's calibration error (in this case by over 80%) while sacrificing only a very small amount of model performance (in this case less than a 1% reduction in MCC).</p>"}, {"location": "tutorial/advanced/t11_model_calibration.html#advanced-tutorial-11-model-calibration", "title": "Advanced Tutorial 11: Model Calibration\u00b6", "text": ""}, {"location": "tutorial/advanced/t11_model_calibration.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Calculating Calibration Error</li> <li>Generating and Applying a Model Calibrator</li> </ul>"}, {"location": "tutorial/advanced/t11_model_calibration.html#calculating-calibration-error", "title": "Calculating Calibration Error\u00b6", "text": "<p>Suppose you have a neural network that is performing image classification. For the sake of argument, let's imagine that the classification problem is to look at x-ray images and determine whether or not a patient has cancer. Let's further suppose that your model is very accurate: when it assigns a higher probability to 'cancer' the patient is almost always sick, and when it assigns a higher probability to 'healthy' the patient is almost always fine. It could be tempting to think that the job is done, but there is still a potential problem for real-world deployments of your model. Suppose a physician using your model runs an image and gets a report saying that it is 51% likely that the patient is healthy, and 49% likely that there is a cancerous tumor. In reality the patient is indeed healthy. From an accuracy point of view, your model is doing just fine. However, if the doctor sees that it is 49% likely that there is a tumor, they are likely to order a biopsy in order to be on the safe side. Taken to an extreme, suppose that your model always predicts a 49% probability of a tumor whenever it sees a healthy patient. Even though the model might have perfect accuracy, in practice it would always result in extra surgical procedures being performed. Ideally, if the model says that there is a 49% probability of a tumor, you would expect there to actually be a tumor in 49% of those cases. The discrepancy between a models predicted probability of a class and the true probability of that class conditioned on the prediction is measured as the calibration error. Calibration error is notoriously difficult to estimate correctly, but FE provides a <code>Trace</code> for this based on a 2019 NeurIPS spotlight paper titled \"Verified Uncertainty Calibration\".</p> <p>The <code>CalibrationError</code> trace can be used just like any other metric trace, though it also optionally can compute confidence intervals around the estimated error. Keep in mind that to measure calibration error you would want your validation dataset to have a reasonable real-world class distribution (only a small percentage of people in the population actually have cancer, for example). For the purpose of easy illustration we will be using the ciFAIR10 dataset, and computing a 95% confidence interval for the estimated calibration error of the model:</p>"}, {"location": "tutorial/advanced/t11_model_calibration.html#generating-and-applying-a-model-calibrator", "title": "Generating and Applying a Model Calibrator\u00b6", "text": "<p>While there have been many proposed approaches for model calibration, we will again be leveraging the Verified Uncertainty Calibration paper mentioned above to achieve highly sample-efficient model re-calibration. There are two steps involved here. The first step is that we will use the <code>PBMCalibrator</code> trace to generate a 'platt binner marginal calibrator'. This calibrator is separate from the neural network, but will take neural network outputs and return calibrated outputs. A consequence of performing this calibration is that the output vector for a prediction will no longer sum to 1, since each class is calibrated independently.</p> <p>Of course, simply having such a calibration object is not useful if we don't use it. To make use of our calibrator object we will use the <code>Calibrate</code> numpyOp, which can load any calibrator object from disk and then apply it during <code>Network</code> post-processing. Since we are using a best model saver, we will only save the calibrator object when our since_best is 0 so that when we re-load the best model we will also be loading the correct calibrator for that model.</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html", "title": "Advanced Tutorial 12: Hyperparameter Search", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.search import GridSearch\n\ndef objective_fn(search_idx, x):\n    return {\"objective\": (x-3)**2}\n\ngrid_search = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]})\n</pre> from fastestimator.search import GridSearch  def objective_fn(search_idx, x):     return {\"objective\": (x-3)**2}  grid_search = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]}) <p>Note that in the score function, one of the arguments must be <code>search_idx</code>. This is to help user differentiate multiple search runs. To run the search, simply call:</p> In\u00a0[2]: Copied! <pre>grid_search.fit()\n</pre> grid_search.fit() <pre>FastEstimator-Search: Evaluated {'x': 0.5, 'search_idx': 1}, result: {'objective': 6.25}\nFastEstimator-Search: Evaluated {'x': 1.5, 'search_idx': 2}, result: {'objective': 2.25}\nFastEstimator-Search: Evaluated {'x': 2.9, 'search_idx': 3}, result: {'objective': 0.010000000000000018}\nFastEstimator-Search: Evaluated {'x': 4, 'search_idx': 4}, result: {'objective': 1}\nFastEstimator-Search: Evaluated {'x': 5.3, 'search_idx': 5}, result: {'objective': 5.289999999999999}\n</pre> <p></p> In\u00a0[3]: Copied! <pre>print(\"best search result:\")\nprint(grid_search.get_best_results(best_mode=\"min\", optimize_field=\"objective\"))\n</pre> print(\"best search result:\") print(grid_search.get_best_results(best_mode=\"min\", optimize_field=\"objective\")) <pre>best search result:\n{'param': {'x': 2.9, 'search_idx': 3}, 'result': {'objective': 0.010000000000000018}}\n</pre> In\u00a0[4]: Copied! <pre>print(\"search history:\")\nprint(grid_search.get_search_summary())\n</pre> print(\"search history:\") print(grid_search.get_search_summary()) <pre>search history:\n[{'param': {'x': 0.5, 'search_idx': 1}, 'result': {'objective': 6.25}}, {'param': {'x': 1.5, 'search_idx': 2}, 'result': {'objective': 2.25}}, {'param': {'x': 2.9, 'search_idx': 3}, 'result': {'objective': 0.010000000000000018}}, {'param': {'x': 4, 'search_idx': 4}, 'result': {'objective': 1}}, {'param': {'x': 5.3, 'search_idx': 5}, 'result': {'objective': 5.289999999999999}}]\n</pre> <p></p> In\u00a0[5]: Copied! <pre>import tempfile\nsave_dir = tempfile.mkdtemp()\n\n# save the state to save_dir\ngrid_search.save(save_dir) \n\n# instantiate a new object\ngrid_search2 = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]}) \n\n# load the previously saved state\ngrid_search2.load(save_dir)\n\n# display the best result of the loaded instance\nprint(grid_search2.get_best_results(best_mode=\"min\", optimize_field=\"objective\")) \n\n# display the search summary of the loadeded instance\nprint(grid_search2.get_search_summary())\n</pre> import tempfile save_dir = tempfile.mkdtemp()  # save the state to save_dir grid_search.save(save_dir)   # instantiate a new object grid_search2 = GridSearch(eval_fn=objective_fn, params={\"x\": [0.5, 1.5, 2.9, 4, 5.3]})   # load the previously saved state grid_search2.load(save_dir)  # display the best result of the loaded instance print(grid_search2.get_best_results(best_mode=\"min\", optimize_field=\"objective\"))   # display the search summary of the loadeded instance print(grid_search2.get_search_summary()) <pre>FastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpy3xpegbx/grid_search.json\nFastEstimator-Search: Loading the search state from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpy3xpegbx/grid_search.json\n{'param': {'x': 2.9, 'search_idx': 3}, 'result': {'objective': 0.010000000000000018}}\n[{'param': {'x': 0.5, 'search_idx': 1}, 'result': {'objective': 6.25}}, {'param': {'x': 1.5, 'search_idx': 2}, 'result': {'objective': 2.25}}, {'param': {'x': 2.9, 'search_idx': 3}, 'result': {'objective': 0.010000000000000018}}, {'param': {'x': 4, 'search_idx': 4}, 'result': {'objective': 1}}, {'param': {'x': 5.3, 'search_idx': 5}, 'result': {'objective': 5.289999999999999}}]\n</pre> <p></p> In\u00a0[6]: Copied! <pre>from fastestimator.search import GoldenSection\nsave_dir2 = tempfile.mkdtemp()\n\ngs_search =  GoldenSection(eval_fn=objective_fn, \n                           x_min=0, \n                           x_max=6, \n                           max_iter=10, \n                           integer=False, \n                           optimize_field=\"objective\", \n                           best_mode=\"min\")\n\ngs_search.fit(save_dir=save_dir2)\n</pre> from fastestimator.search import GoldenSection save_dir2 = tempfile.mkdtemp()  gs_search =  GoldenSection(eval_fn=objective_fn,                             x_min=0,                             x_max=6,                             max_iter=10,                             integer=False,                             optimize_field=\"objective\",                             best_mode=\"min\")  gs_search.fit(save_dir=save_dir2) <pre>FastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.2917960675006306, 'search_idx': 1}, result: {'objective': 0.5015528100075713}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.7082039324993694, 'search_idx': 2}, result: {'objective': 0.5015528100075713}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 4.583592135001262, 'search_idx': 3}, result: {'objective': 2.5077640500378555}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.1671842700025232, 'search_idx': 4}, result: {'objective': 0.027950580136276586}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.832815729997476, 'search_idx': 5}, result: {'objective': 0.027950580136276885}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.3738353924943216, 'search_idx': 6}, result: {'objective': 0.1397529006813835}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0394668524892743, 'search_idx': 7}, result: {'objective': 0.0015576324454101358}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.9605331475107253, 'search_idx': 8}, result: {'objective': 0.0015576324454101708}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0882505650239747, 'search_idx': 9}, result: {'objective': 0.00778816222705078}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.009316860045425, 'search_idx': 10}, result: {'objective': 8.68038811060405e-05}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.9906831399545744, 'search_idx': 11}, result: {'objective': 8.680388110604876e-05}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0208331323984234, 'search_idx': 12}, result: {'objective': 0.0004340194055302403}\nFastEstimator-Search: Golden Section Search Finished, best parameters: {'x': 3.009316860045425, 'search_idx': 10}, best result: {'objective': 8.68038811060405e-05}\n</pre> <p>After interruption, we can create the instance and call <code>fit</code> on the same directory:</p> In\u00a0[7]: Copied! <pre>gs_search2 =  GoldenSection(eval_fn=objective_fn, \n                           x_min=0, \n                           x_max=6, \n                           max_iter=20, \n                           integer=False, \n                           optimize_field=\"objective\", \n                           best_mode=\"min\")\n\ngs_search2.fit(save_dir=save_dir2)\n</pre> gs_search2 =  GoldenSection(eval_fn=objective_fn,                             x_min=0,                             x_max=6,                             max_iter=20,                             integer=False,                             optimize_field=\"objective\",                             best_mode=\"min\")  gs_search2.fit(save_dir=save_dir2) <pre>FastEstimator-Search: Loading the search state from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.002199412307572, 'search_idx': 13}, result: {'objective': 4.8374144986998325e-06}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.997800587692428, 'search_idx': 14}, result: {'objective': 4.8374144986998325e-06}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0049180354302814, 'search_idx': 15}, result: {'objective': 2.4187072493502697e-05}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0005192108151366, 'search_idx': 16}, result: {'objective': 2.695798705548303e-07}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.9994807891848634, 'search_idx': 17}, result: {'objective': 2.695798705548303e-07}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.001160990677299, 'search_idx': 18}, result: {'objective': 1.3478993527749865e-06}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0001225690470252, 'search_idx': 19}, result: {'objective': 1.502317128867399e-08}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 2.9998774309529748, 'search_idx': 20}, result: {'objective': 1.502317128867399e-08}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.000274072721086, 'search_idx': 21}, result: {'objective': 7.511585644356621e-08}\nFastEstimator-Search: Saving the search summary to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpkyddkubn/golden_section_search.json\nFastEstimator-Search: Evaluated {'x': 3.0000289346270352, 'search_idx': 22}, result: {'objective': 8.372126416682983e-10}\nFastEstimator-Search: Golden Section Search Finished, best parameters: {'x': 3.0000289346270352, 'search_idx': 22}, best result: {'objective': 8.372126416682983e-10}\n</pre> <p>As we can see, the search started from search index 13 and proceeded for another 10 iterations.</p> <p></p> In\u00a0[8]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_hypara_tuning_estimator(batch_size, lr, choice):\n\n    pipeline_ops = []\n\n    if choice and isinstance(choice, str):\n        pipeline_ops = [RUA(inputs=\"x\", outputs=\"x\", mode=\"train\", choices=[choice])]\n\n    pipeline_ops = pipeline_ops + [ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]\n    train_data, test_data = mnist.load_data()\n    pipeline = fe.Pipeline(train_data=train_data,\n                           test_data=test_data,\n                           batch_size=batch_size,\n                           ops=pipeline_ops,\n                           num_process=0)\n    model = fe.build(model_fn=LeNet, optimizer_fn=lambda: tf.optimizers.Adam(lr))\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=1, train_steps_per_epoch=500)\n    return estimator\n</pre> import tensorflow as tf import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_hypara_tuning_estimator(batch_size, lr, choice):      pipeline_ops = []      if choice and isinstance(choice, str):         pipeline_ops = [RUA(inputs=\"x\", outputs=\"x\", mode=\"train\", choices=[choice])]      pipeline_ops = pipeline_ops + [ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]     train_data, test_data = mnist.load_data()     pipeline = fe.Pipeline(train_data=train_data,                            test_data=test_data,                            batch_size=batch_size,                            ops=pipeline_ops,                            num_process=0)     model = fe.build(model_fn=LeNet, optimizer_fn=lambda: tf.optimizers.Adam(lr))     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=1, train_steps_per_epoch=500)     return estimator <p>Given a batch size grid <code>[32, 64]</code>, we are interested in the optimial parameter that leads to the lowest test loss after 500 steps of training on MNIST dataset.</p> In\u00a0[9]: Copied! <pre>def eval_fn_v1(search_idx, batch_size):\n    est = get_hypara_tuning_estimator(batch_size, lr=1e-3, choice=None)\n    est.fit(warmup=False)\n    hist = est.test(summary=\"myexp\")\n    loss = float(hist.history[\"test\"][\"ce\"][500])\n    return {\"test_loss\": loss}\n\n\nmnist_grid_search_single = GridSearch(eval_fn=eval_fn_v1, params={\"batch_size\": [32, 64]})\n\nmnist_grid_search_single.fit()\n\nmnist_grid_search_single.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")\n</pre> def eval_fn_v1(search_idx, batch_size):     est = get_hypara_tuning_estimator(batch_size, lr=1e-3, choice=None)     est.fit(warmup=False)     hist = est.test(summary=\"myexp\")     loss = float(hist.history[\"test\"][\"ce\"][500])     return {\"test_loss\": loss}   mnist_grid_search_single = GridSearch(eval_fn=eval_fn_v1, params={\"batch_size\": [32, 64]})  mnist_grid_search_single.fit()  mnist_grid_search_single.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\") <pre>2022-05-23 15:55:03.944092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-05-23 15:55:04.016652: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2978234;\nFastEstimator-Train: step: 100; ce: 0.5928297; steps/sec: 68.95;\nFastEstimator-Train: step: 200; ce: 0.17800228; steps/sec: 67.77;\nFastEstimator-Train: step: 300; ce: 0.17582887; steps/sec: 66.81;\nFastEstimator-Train: step: 400; ce: 0.09949152; steps/sec: 69.79;\nFastEstimator-Train: step: 500; ce: 0.21450083; steps/sec: 68.93;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 10.47 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 10.48 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.1302412;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'search_idx': 1}, result: {'test_loss': 0.13024120032787323}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3029552;\nFastEstimator-Train: step: 100; ce: 0.24441192; steps/sec: 40.24;\nFastEstimator-Train: step: 200; ce: 0.13249156; steps/sec: 38.96;\nFastEstimator-Train: step: 300; ce: 0.081911236; steps/sec: 38.28;\nFastEstimator-Train: step: 400; ce: 0.10891421; steps/sec: 39.34;\nFastEstimator-Train: step: 500; ce: 0.12866744; steps/sec: 40.82;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 13.1 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 13.11 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.07181324;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'search_idx': 2}, result: {'test_loss': 0.07181324064731598}\n</pre> Out[9]: <pre>{'param': {'batch_size': 64, 'search_idx': 2},\n 'result': {'test_loss': 0.07181324064731598}}</pre> <p>Given a batch size grid <code>[32, 64]</code> and learning rate grid <code>[1e-2 and 1e-3]</code>, we are interested in the optimial parameter that leads to the lowest test loss after 500 steps of training on MNIST dataset.</p> In\u00a0[10]: Copied! <pre>def eval_fn_v2(search_idx, batch_size, lr):\n    est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=None)\n    est.fit(warmup=False)\n    hist = est.test(summary=\"myexp\")\n    loss = float(hist.history[\"test\"][\"ce\"][500])\n    return {\"test_loss\": loss}\n\nmnist_grid_search_double = GridSearch(eval_fn=eval_fn_v2, params={\"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3]})\n\nmnist_grid_search_double.fit()\n\nmnist_grid_search_double.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")\n</pre> def eval_fn_v2(search_idx, batch_size, lr):     est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=None)     est.fit(warmup=False)     hist = est.test(summary=\"myexp\")     loss = float(hist.history[\"test\"][\"ce\"][500])     return {\"test_loss\": loss}  mnist_grid_search_double = GridSearch(eval_fn=eval_fn_v2, params={\"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3]})  mnist_grid_search_double.fit()  mnist_grid_search_double.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.297504;\nFastEstimator-Train: step: 100; ce: 0.23087856; steps/sec: 64.66;\nFastEstimator-Train: step: 200; ce: 0.04770284; steps/sec: 59.33;\nFastEstimator-Train: step: 300; ce: 0.065143056; steps/sec: 63.28;\nFastEstimator-Train: step: 400; ce: 0.043231085; steps/sec: 63.76;\nFastEstimator-Train: step: 500; ce: 0.57282686; steps/sec: 61.11;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 8.44 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 8.45 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.1802756;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.01, 'search_idx': 1}, result: {'test_loss': 0.18027560412883759}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.288842;\nFastEstimator-Train: step: 100; ce: 0.22141448; steps/sec: 62.89;\nFastEstimator-Train: step: 200; ce: 0.30901936; steps/sec: 63.56;\nFastEstimator-Train: step: 300; ce: 0.18143213; steps/sec: 62.37;\nFastEstimator-Train: step: 400; ce: 0.29459214; steps/sec: 61.98;\nFastEstimator-Train: step: 500; ce: 0.27240336; steps/sec: 61.76;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 8.42 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 8.43 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.100863606;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.001, 'search_idx': 2}, result: {'test_loss': 0.10086360573768616}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.275618;\nFastEstimator-Train: step: 100; ce: 0.08152926; steps/sec: 39.58;\nFastEstimator-Train: step: 200; ce: 0.08025998; steps/sec: 39.01;\nFastEstimator-Train: step: 300; ce: 0.09793242; steps/sec: 37.88;\nFastEstimator-Train: step: 400; ce: 0.044547416; steps/sec: 38.41;\nFastEstimator-Train: step: 500; ce: 0.16613436; steps/sec: 38.42;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 13.39 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 13.4 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.08647069;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.01, 'search_idx': 3}, result: {'test_loss': 0.08647069334983826}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3088133;\nFastEstimator-Train: step: 100; ce: 0.28321943; steps/sec: 38.87;\nFastEstimator-Train: step: 200; ce: 0.3613764; steps/sec: 35.5;\nFastEstimator-Train: step: 300; ce: 0.059347313; steps/sec: 33.92;\nFastEstimator-Train: step: 400; ce: 0.19349068; steps/sec: 33.5;\nFastEstimator-Train: step: 500; ce: 0.26929265; steps/sec: 32.52;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 14.89 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 14.91 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.07576594;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.001, 'search_idx': 4}, result: {'test_loss': 0.0757659375667572}\n</pre> Out[10]: <pre>{'param': {'batch_size': 64, 'lr': 0.001, 'search_idx': 4},\n 'result': {'test_loss': 0.0757659375667572}}</pre> <p>Given a batch size grid <code>[32, 64]</code>, learning rate grid <code>[1e-2 and 1e-3]</code> and built-in augmentation <code>[\"Rotate\", \"Brightness\"]</code>, we are interested in the optimial parameter that leads to the lowest test loss after 500 steps of training on MNIST dataset.</p> In\u00a0[11]: Copied! <pre>def eval_fn_v3(search_idx, batch_size, lr, choices):\n    est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=choices)\n    est.fit(warmup=False)\n    hist = est.test(summary=\"myexp\")\n    loss = float(hist.history[\"test\"][\"ce\"][500])\n    return {\"test_loss\": loss}\n\nmnist_grid_search_multi = GridSearch(\n    eval_fn=eval_fn_v3, params={\n        \"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3], \"choices\": [\"Rotate\", \"Brightness\"]\n    })\n\nmnist_grid_search_multi.fit()\n\nmnist_grid_search_multi.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\")\n</pre> def eval_fn_v3(search_idx, batch_size, lr, choices):     est = get_hypara_tuning_estimator(batch_size, lr=lr, choice=choices)     est.fit(warmup=False)     hist = est.test(summary=\"myexp\")     loss = float(hist.history[\"test\"][\"ce\"][500])     return {\"test_loss\": loss}  mnist_grid_search_multi = GridSearch(     eval_fn=eval_fn_v3, params={         \"batch_size\": [32, 64], \"lr\": [1e-2, 1e-3], \"choices\": [\"Rotate\", \"Brightness\"]     })  mnist_grid_search_multi.fit()  mnist_grid_search_multi.get_best_results(best_mode=\"min\", optimize_field=\"test_loss\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.316989;\nFastEstimator-Train: step: 100; ce: 0.89612067; steps/sec: 61.37;\nFastEstimator-Train: step: 200; ce: 0.08497408; steps/sec: 58.87;\nFastEstimator-Train: step: 300; ce: 0.17728706; steps/sec: 55.34;\nFastEstimator-Train: step: 400; ce: 0.097662136; steps/sec: 54.78;\nFastEstimator-Train: step: 500; ce: 0.6184038; steps/sec: 54.37;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 9.33 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 9.34 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.15701194;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.01, 'choices': 'Rotate', 'search_idx': 1}, result: {'test_loss': 0.157011941075325}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.303073;\nFastEstimator-Train: step: 100; ce: 0.30278435; steps/sec: 46.5;\nFastEstimator-Train: step: 200; ce: 0.16040373; steps/sec: 46.01;\nFastEstimator-Train: step: 300; ce: 0.13413057; steps/sec: 46.08;\nFastEstimator-Train: step: 400; ce: 0.36688638; steps/sec: 42.99;\nFastEstimator-Train: step: 500; ce: 0.082864165; steps/sec: 41.49;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 11.76 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 11.78 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.12428217;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.01, 'choices': 'Brightness', 'search_idx': 2}, result: {'test_loss': 0.12428216636180878}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3268635;\nFastEstimator-Train: step: 100; ce: 0.7491451; steps/sec: 58.88;\nFastEstimator-Train: step: 200; ce: 0.3961307; steps/sec: 55.83;\nFastEstimator-Train: step: 300; ce: 0.4441402; steps/sec: 52.79;\nFastEstimator-Train: step: 400; ce: 0.11637384; steps/sec: 51.09;\nFastEstimator-Train: step: 500; ce: 0.19636872; steps/sec: 48.29;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 9.96 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 9.97 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.21185064;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.001, 'choices': 'Rotate', 'search_idx': 3}, result: {'test_loss': 0.21185064315795898}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3139453;\nFastEstimator-Train: step: 100; ce: 0.17447951; steps/sec: 42.92;\nFastEstimator-Train: step: 200; ce: 0.20068465; steps/sec: 42.0;\nFastEstimator-Train: step: 300; ce: 0.07889053; steps/sec: 41.77;\nFastEstimator-Train: step: 400; ce: 0.19607557; steps/sec: 41.3;\nFastEstimator-Train: step: 500; ce: 0.045420818; steps/sec: 40.3;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 12.57 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 12.59 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.10435359;\nFastEstimator-Search: Evaluated {'batch_size': 32, 'lr': 0.001, 'choices': 'Brightness', 'search_idx': 4}, result: {'test_loss': 0.10435359179973602}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3177319;\nFastEstimator-Train: step: 100; ce: 0.6719325; steps/sec: 32.37;\nFastEstimator-Train: step: 200; ce: 0.52074134; steps/sec: 28.28;\nFastEstimator-Train: step: 300; ce: 0.21777108; steps/sec: 28.32;\nFastEstimator-Train: step: 400; ce: 0.33475888; steps/sec: 27.19;\nFastEstimator-Train: step: 500; ce: 0.11661025; steps/sec: 26.83;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 18.17 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 18.18 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.15500401;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.01, 'choices': 'Rotate', 'search_idx': 5}, result: {'test_loss': 0.15500400960445404}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3164172;\nFastEstimator-Train: step: 100; ce: 0.21421057; steps/sec: 26.94;\nFastEstimator-Train: step: 200; ce: 0.250572; steps/sec: 26.67;\nFastEstimator-Train: step: 300; ce: 0.16062501; steps/sec: 24.45;\nFastEstimator-Train: step: 400; ce: 0.057134755; steps/sec: 24.38;\nFastEstimator-Train: step: 500; ce: 0.046086866; steps/sec: 24.82;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 20.29 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.01; total_time: 20.3 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.06220621;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.01, 'choices': 'Brightness', 'search_idx': 6}, result: {'test_loss': 0.0622062087059021}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.299392;\nFastEstimator-Train: step: 100; ce: 0.45705426; steps/sec: 34.21;\nFastEstimator-Train: step: 200; ce: 0.32576722; steps/sec: 30.81;\nFastEstimator-Train: step: 300; ce: 0.11331859; steps/sec: 29.75;\nFastEstimator-Train: step: 400; ce: 0.21821997; steps/sec: 28.17;\nFastEstimator-Train: step: 500; ce: 0.21981962; steps/sec: 27.09;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 17.35 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 17.37 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.15896726;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.001, 'choices': 'Rotate', 'search_idx': 7}, result: {'test_loss': 0.1589672565460205}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3077712;\nFastEstimator-Train: step: 100; ce: 0.31761616; steps/sec: 28.59;\nFastEstimator-Train: step: 200; ce: 0.16418545; steps/sec: 27.36;\nFastEstimator-Train: step: 300; ce: 0.108605824; steps/sec: 26.95;\nFastEstimator-Train: step: 400; ce: 0.082405984; steps/sec: 26.41;\nFastEstimator-Train: step: 500; ce: 0.04901053; steps/sec: 25.47;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 19.18 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 19.2 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 0.06811151;\nFastEstimator-Search: Evaluated {'batch_size': 64, 'lr': 0.001, 'choices': 'Brightness', 'search_idx': 8}, result: {'test_loss': 0.06811150908470154}\n</pre> Out[11]: <pre>{'param': {'batch_size': 64,\n  'lr': 0.01,\n  'choices': 'Brightness',\n  'search_idx': 6},\n 'result': {'test_loss': 0.0622062087059021}}</pre> <p></p> <p>Visualization of grid search with single hyperparameter:</p> In\u00a0[12]: Copied! <pre>from fastestimator.search.visualize import visualize_search\n\nvisualize_search(search=mnist_grid_search_single)\n</pre> from fastestimator.search.visualize import visualize_search  visualize_search(search=mnist_grid_search_single) <p>Visualization of grid search with two hyperparameters:</p> In\u00a0[13]: Copied! <pre>visualize_search(search=mnist_grid_search_double)\n</pre> visualize_search(search=mnist_grid_search_double) <p>Visualization of grid search with more than 2 hyperparameters:</p> In\u00a0[14]: Copied! <pre>visualize_search(search=mnist_grid_search_multi)\n</pre> visualize_search(search=mnist_grid_search_multi) <p></p> In\u00a0[15]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\ndef get_estimator(level):\n    train_data, test_data = cifair10.load_data()\n    pipeline = fe.Pipeline(train_data=train_data,\n                           test_data=test_data,\n                           batch_size=64,\n                           ops=[RUA(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")],\n                           num_process=0)\n    model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=1,\n                             train_steps_per_epoch=500)\n    return estimator\n\ndef eval_fn(search_idx, level):\n    est = get_estimator(level)\n    est.fit(warmup=False)\n    hist = est.test(summary=\"myexp\")\n    loss = float(hist.history[\"test\"][\"ce\"][500])\n    return {\"test_loss\": loss}\n\ncifair10_gs_search = GoldenSection(eval_fn=eval_fn, x_min=0, x_max=30, max_iter=5, best_mode=\"min\", optimize_field=\"test_loss\")\n</pre> import tensorflow as tf import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, RUA from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  def get_estimator(level):     train_data, test_data = cifair10.load_data()     pipeline = fe.Pipeline(train_data=train_data,                            test_data=test_data,                            batch_size=64,                            ops=[RUA(level=level, inputs=\"x\", outputs=\"x\", mode=\"train\"),                                  Minmax(inputs=\"x\", outputs=\"x\")],                            num_process=0)     model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=1,                              train_steps_per_epoch=500)     return estimator  def eval_fn(search_idx, level):     est = get_estimator(level)     est.fit(warmup=False)     hist = est.test(summary=\"myexp\")     loss = float(hist.history[\"test\"][\"ce\"][500])     return {\"test_loss\": loss}  cifair10_gs_search = GoldenSection(eval_fn=eval_fn, x_min=0, x_max=30, max_iter=5, best_mode=\"min\", optimize_field=\"test_loss\") In\u00a0[16]: Copied! <pre>cifair10_gs_search.fit()\n</pre> cifair10_gs_search.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3069172;\nFastEstimator-Train: step: 100; ce: 1.9433358; steps/sec: 15.57;\nFastEstimator-Train: step: 200; ce: 1.8442394; steps/sec: 15.36;\nFastEstimator-Train: step: 300; ce: 1.7987336; steps/sec: 14.66;\nFastEstimator-Train: step: 400; ce: 1.827171; steps/sec: 14.63;\nFastEstimator-Train: step: 500; ce: 1.6530949; steps/sec: 14.76;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 34.03 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 34.05 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.497365;\nFastEstimator-Search: Evaluated {'level': 11, 'search_idx': 1}, result: {'test_loss': 1.4973649978637695}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3124037;\nFastEstimator-Train: step: 100; ce: 2.1659584; steps/sec: 14.13;\nFastEstimator-Train: step: 200; ce: 2.0272436; steps/sec: 13.1;\nFastEstimator-Train: step: 300; ce: 1.8795973; steps/sec: 13.62;\nFastEstimator-Train: step: 400; ce: 1.8213081; steps/sec: 13.33;\nFastEstimator-Train: step: 500; ce: 1.6495254; steps/sec: 13.51;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 37.59 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 37.6 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.5087045;\nFastEstimator-Search: Evaluated {'level': 18, 'search_idx': 2}, result: {'test_loss': 1.5087045431137085}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3144321;\nFastEstimator-Train: step: 100; ce: 1.9611168; steps/sec: 17.91;\nFastEstimator-Train: step: 200; ce: 1.7303221; steps/sec: 15.36;\nFastEstimator-Train: step: 300; ce: 1.8715479; steps/sec: 15.63;\nFastEstimator-Train: step: 400; ce: 1.8697963; steps/sec: 15.95;\nFastEstimator-Train: step: 500; ce: 1.7709255; steps/sec: 15.99;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 31.61 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 31.62 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.4535282;\nFastEstimator-Search: Evaluated {'level': 7, 'search_idx': 3}, result: {'test_loss': 1.4535281658172607}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3262901;\nFastEstimator-Train: step: 100; ce: 1.8632274; steps/sec: 21.01;\nFastEstimator-Train: step: 200; ce: 1.8241731; steps/sec: 18.28;\nFastEstimator-Train: step: 300; ce: 1.5723119; steps/sec: 17.2;\nFastEstimator-Train: step: 400; ce: 1.5143611; steps/sec: 16.94;\nFastEstimator-Train: step: 500; ce: 1.4949286; steps/sec: 16.51;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 28.6 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 28.62 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.4889473;\nFastEstimator-Search: Evaluated {'level': 4, 'search_idx': 4}, result: {'test_loss': 1.4889472723007202}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2915616;\nFastEstimator-Train: step: 100; ce: 1.9557471; steps/sec: 17.23;\nFastEstimator-Train: step: 200; ce: 1.7439098; steps/sec: 15.71;\nFastEstimator-Train: step: 300; ce: 1.9362915; steps/sec: 15.4;\nFastEstimator-Train: step: 400; ce: 1.6905106; steps/sec: 15.72;\nFastEstimator-Train: step: 500; ce: 1.5294566; steps/sec: 15.53;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 32.08 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 32.1 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.468676;\nFastEstimator-Search: Evaluated {'level': 8, 'search_idx': 5}, result: {'test_loss': 1.468675971031189}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.317153;\nFastEstimator-Train: step: 100; ce: 2.0944784; steps/sec: 19.89;\nFastEstimator-Train: step: 200; ce: 1.6180917; steps/sec: 17.54;\nFastEstimator-Train: step: 300; ce: 1.730228; steps/sec: 17.11;\nFastEstimator-Train: step: 400; ce: 1.5884643; steps/sec: 16.26;\nFastEstimator-Train: step: 500; ce: 1.6063898; steps/sec: 16.33;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 29.45 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 29.47 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.5012085;\nFastEstimator-Search: Evaluated {'level': 5, 'search_idx': 6}, result: {'test_loss': 1.5012085437774658}\n    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3051028;\nFastEstimator-Train: step: 100; ce: 1.8751833; steps/sec: 19.09;\nFastEstimator-Train: step: 200; ce: 1.884644; steps/sec: 16.76;\nFastEstimator-Train: step: 300; ce: 1.774396; steps/sec: 16.69;\nFastEstimator-Train: step: 400; ce: 1.6647091; steps/sec: 16.47;\nFastEstimator-Train: step: 500; ce: 1.5038598; steps/sec: 16.44;\nFastEstimator-Train: step: 500; epoch: 1; epoch_time: 29.98 sec;\nFastEstimator-Finish: step: 500; model_lr: 0.001; total_time: 29.99 sec;\nFastEstimator-Test: step: 500; epoch: 1; ce: 1.4427035;\nFastEstimator-Search: Evaluated {'level': 6, 'search_idx': 7}, result: {'test_loss': 1.4427034854888916}\nFastEstimator-Search: Golden Section Search Finished, best parameters: {'level': 6, 'search_idx': 7}, best result: {'test_loss': 1.4427034854888916}\n</pre> <p>In this example, the optimial level we found is 5. We can then train the model again using <code>level=5</code> to get the final model. In a real use case you will want to perform parameter search on a held-out evaluation set, and test the best parameters on the test set.</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#advanced-tutorial-12-hyperparameter-search", "title": "Advanced Tutorial 12: Hyperparameter Search\u00b6", "text": ""}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>FastEstimator Search API<ul> <li>Getting the search results</li> <li>Saving and loading search results</li> <li>Interruption-resilient search</li> </ul> </li> <li>Example 1: Hyperparameter Tuning by Grid Search<ul> <li>Search Visualization</li> </ul> </li> <li>Example 2: RUA Augmentation via Golden-Section Search</li> </ul>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#search-api", "title": "Search API\u00b6", "text": "<p>There are many things in life that requires searching for an optimal solution in a given space, regardless of whether deep learning is involved. For example:</p> <ul> <li>what is the <code>x</code> that leads to the minimal value of <code>(x-3)**2</code>?</li> <li>what is the best <code>learning rate</code> and <code>batch size</code> combo that can produce the lowest evaluation loss after 2 epochs of training?</li> <li>what is the best augmentation magnitude that can lead to the highest evaluation accuracy?</li> </ul> <p>The <code>fe.search</code> API is designed to make the search easier, the API can be used independently for any search problem, as it only requires the following two components:</p> <ol> <li>objective function to measure the score of a solution.</li> <li>whether a maximum or minimum score is desired.</li> </ol> <p>We will start with a simple example using <code>Grid Search</code>. Say we want to find the <code>x</code> that produces the minimal value of <code>(x-3)**2</code>, where x is chosen from the list: <code>[0.5, 1.5, 2.9, 4, 5.3]</code></p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#getting-the-search-results", "title": "Getting the search results\u00b6", "text": "<p>After the search is done, you can also call the <code>search.get_best_results</code> or <code>search.get_search_results</code> to see the best and overall search history:</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#saving-and-loading-search-results", "title": "Saving and loading search results\u00b6", "text": "<p>Once the search is done, you can also save the search results into the disk and later load them back using <code>save</code> and <code>load</code> methods:</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#interruption-resilient-search", "title": "Interruption-resilient search\u00b6", "text": "<p>When you run search on a hardware that can be interrupted (like an AWS spot instance), you can provide a <code>save_dir</code> argument when calling <code>fit</code>. As a result, the search will automatically back up its result after each evaluation. Furthermore, when calling <code>fit</code> using the same <code>save_dir</code> the second time, it will first load the search results and then pick up from where it left off.</p> <p>To demonstrate this, we will use golden-section search on the same optimization problem. To simulate interruption, we will first iterate 10 times, then create a new instance and iterate another 10 times.</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#example-1-hyperparameter-tuning-by-grid-search", "title": "Example 1: Hyperparameter Tuning by Grid Search\u00b6", "text": "<p>In this example, we will use <code>GridSearch</code> on a real deep learning task to illustrate its usage. Based on number of hyperparameters, the  grid search is performed accordingly.</p>"}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#search-visualization", "title": "Search Visualization\u00b6", "text": ""}, {"location": "tutorial/advanced/t12_hyperparameter_search.html#example-2-rua-augmentation-via-golden-section-search", "title": "Example 2: RUA Augmentation via Golden-Section Search\u00b6", "text": "<p>In this example, we will use a built-in augmentation NumpyOp - RUA - and find the optimial level between 0 to 30 using <code>Golden-Section</code> search. The test result will be evaluated on the ciFAIR10 dataset after 500 steps of training.</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html", "title": "Advanced Tutorial 13: Multi-Dataset Training and Evaluation", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.dataset.data import mnist, svhn_cropped\n\ntrain_mnist, eval_mnist = mnist.load_data()\n_, eval_svhn = svhn_cropped.load_data()\nprint(\"mnist evaluation dataset summary:\")\nprint(eval_mnist.summary())\n\nprint(\"svhn_cropped evaluation dataset summary:\")\nprint(eval_svhn.summary())\n</pre> from fastestimator.dataset.data import mnist, svhn_cropped  train_mnist, eval_mnist = mnist.load_data() _, eval_svhn = svhn_cropped.load_data() print(\"mnist evaluation dataset summary:\") print(eval_mnist.summary())  print(\"svhn_cropped evaluation dataset summary:\") print(eval_svhn.summary()) <pre>mnist evaluation dataset summary:\n{\"num_instances\": 10000, \"keys\": {\"x\": {\"shape\": [28, 28], \"dtype\": \"uint8\"}, \"y\": {\"num_unique_values\": 10, \"shape\": [], \"dtype\": \"uint8\"}}}\nsvhn_cropped evaluation dataset summary:\n{\"num_instances\": 26032, \"keys\": {\"x\": {\"shape\": [32, 32, 3], \"dtype\": \"uint8\"}, \"y\": {\"shape\": [1], \"dtype\": \"uint8\"}}}\n</pre> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, ToGray\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop import LambdaOp\n\npipeline = fe.Pipeline(train_data={\"mnist\": train_mnist}, \n                       eval_data={\"mnist\": eval_mnist, \"svhn\": eval_svhn},\n                       batch_size=32,\n                       ops= [\n                             Resize(image_in=\"x\", image_out=\"x\", height=28, width=28, ds_id=\"svhn\"),\n                             ToGray(inputs=\"x\", outputs=\"x\", ds_id=\"svhn\"), # after ToGray, the output is still 3 channel\n                             LambdaOp(fn=lambda x: x[..., 0:1], inputs=\"x\", outputs=\"x\", ds_id=\"svhn\"), # select the first channel for svhn\n                             ExpandDims(inputs=\"x\", outputs=\"x\", ds_id=\"mnist\"), # (28, 28) -&gt; (28, 28, 1) for mnist\n                             LambdaOp(fn=fe.backend.squeeze, inputs=\"y\", outputs=\"y\", ds_id=\"svhn\"), # Match the mnist y shape\n                             Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> import fastestimator as fe from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, ToGray from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop import LambdaOp  pipeline = fe.Pipeline(train_data={\"mnist\": train_mnist},                         eval_data={\"mnist\": eval_mnist, \"svhn\": eval_svhn},                        batch_size=32,                        ops= [                              Resize(image_in=\"x\", image_out=\"x\", height=28, width=28, ds_id=\"svhn\"),                              ToGray(inputs=\"x\", outputs=\"x\", ds_id=\"svhn\"), # after ToGray, the output is still 3 channel                              LambdaOp(fn=lambda x: x[..., 0:1], inputs=\"x\", outputs=\"x\", ds_id=\"svhn\"), # select the first channel for svhn                              ExpandDims(inputs=\"x\", outputs=\"x\", ds_id=\"mnist\"), # (28, 28) -&gt; (28, 28, 1) for mnist                              LambdaOp(fn=fe.backend.squeeze, inputs=\"y\", outputs=\"y\", ds_id=\"svhn\"), # Match the mnist y shape                              Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[3]: Copied! <pre>from fastestimator.util import BatchDisplay, GridDisplay\n\nmnist_eval_data = pipeline.get_results(mode=\"eval\", ds_id=\"mnist\")\nfig = GridDisplay([BatchDisplay(image=mnist_eval_data[\"x\"][:2], title=\"image\"),\n                   BatchDisplay(text=mnist_eval_data[\"y\"][:2], title=\"label\")\n                  ])\nfig.show()\n</pre> from fastestimator.util import BatchDisplay, GridDisplay  mnist_eval_data = pipeline.get_results(mode=\"eval\", ds_id=\"mnist\") fig = GridDisplay([BatchDisplay(image=mnist_eval_data[\"x\"][:2], title=\"image\"),                    BatchDisplay(text=mnist_eval_data[\"y\"][:2], title=\"label\")                   ]) fig.show() In\u00a0[4]: Copied! <pre>svhn_eval_data = pipeline.get_results(mode=\"eval\", ds_id=\"svhn\")\nfig = GridDisplay([BatchDisplay(image=svhn_eval_data[\"x\"][:2], title=\"image\"),\n                   BatchDisplay(text=svhn_eval_data[\"y\"][:2], title=\"label\")\n                  ])\nfig.show()\n</pre> svhn_eval_data = pipeline.get_results(mode=\"eval\", ds_id=\"svhn\") fig = GridDisplay([BatchDisplay(image=svhn_eval_data[\"x\"][:2], title=\"image\"),                    BatchDisplay(text=svhn_eval_data[\"y\"][:2], title=\"label\")                   ]) fig.show() In\u00a0[5]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n</pre> from fastestimator.architecture.tensorflow import LeNet from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ]) <pre>2022-06-01 12:09:21.725055: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> In\u00a0[6]: Copied! <pre>from fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=4,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n                         train_steps_per_epoch=200)\n\nestimator.fit()\n</pre> from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy   estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=4,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                          train_steps_per_epoch=200)  estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce|mnist: 2.297002;\nFastEstimator-Train: step: 100; ce|mnist: 0.62006235; steps/sec|mnist: 68.14;\nFastEstimator-Train: step: 200; ce|mnist: 0.100964725; steps/sec|mnist: 72.17;\nFastEstimator-Train: step: 200; epoch: 1; epoch_time: 4.28 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 195.59;\nEval Progress: 208/312; steps/sec: 194.24;\nEval Progress: 312/312; steps/sec: 206.18;\nFastEstimator-Eval: step: 200; epoch: 1; accuracy: 0.38574045293072823; accuracy|mnist: 0.9399; accuracy|svhn: 0.1728641671788568; ce: 2.479175; ce|mnist: 0.20342466; ce|svhn: 3.3542488;\nFastEstimator-Train: step: 300; ce|mnist: 0.14703166; steps/sec|mnist: 54.67;\nFastEstimator-Train: step: 400; ce|mnist: 0.03327415; steps/sec|mnist: 71.45;\nFastEstimator-Train: step: 400; epoch: 2; epoch_time: 3.24 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 208.84;\nEval Progress: 208/312; steps/sec: 208.1;\nEval Progress: 312/312; steps/sec: 211.65;\nFastEstimator-Eval: step: 400; epoch: 2; accuracy: 0.4563721136767318; accuracy|mnist: 0.9722; accuracy|svhn: 0.25822065150583895; ce: 1.9564626; ce|mnist: 0.09087765; ce|svhn: 2.6738186;\nFastEstimator-Train: step: 500; ce|mnist: 0.26206714; steps/sec|mnist: 52.52;\nFastEstimator-Train: step: 600; ce|mnist: 0.20257875; steps/sec|mnist: 69.8;\nFastEstimator-Train: step: 600; epoch: 3; epoch_time: 3.33 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 209.52;\nEval Progress: 208/312; steps/sec: 216.23;\nEval Progress: 312/312; steps/sec: 209.66;\nFastEstimator-Eval: step: 600; epoch: 3; accuracy: 0.44879551509769094; accuracy|mnist: 0.9719; accuracy|svhn: 0.24784880147510757; ce: 1.7997541; ce|mnist: 0.09163932; ce|svhn: 2.4565597;\nFastEstimator-Train: step: 700; ce|mnist: 0.025095956; steps/sec|mnist: 52.47;\nFastEstimator-Train: step: 800; ce|mnist: 0.12707141; steps/sec|mnist: 70.46;\nFastEstimator-Train: step: 800; epoch: 4; epoch_time: 3.32 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 207.01;\nEval Progress: 208/312; steps/sec: 207.07;\nEval Progress: 312/312; steps/sec: 209.36;\nFastEstimator-Eval: step: 800; epoch: 4; accuracy: 0.4620892539964476; accuracy|mnist: 0.9741; accuracy|svhn: 0.2654041180086048; ce: 1.7328861; ce|mnist: 0.083745226; ce|svhn: 2.3670154;\nFastEstimator-Finish: step: 800; model_lr: 0.001; total_time: 41.48 sec;\n</pre> <p>As you can see in the training log, the <code>Accuracy</code> Trace created 3 keys: <code>accuracy|mnist</code>, <code>accuracy|svhn</code>, and <code>accuracy</code>. The <code>accuracy|mnist</code> and <code>accuracy|svhn</code> are measured on individual datasets, and <code>accuracy</code> is measured on the overall combined evaluation set.</p> In\u00a0[7]: Copied! <pre>from fastestimator.trace.meta import per_ds\nfrom sklearn import metrics\nimport numpy as np\n\n\n@per_ds  # Without this annotation the trace would only compute the aggregate metric\nclass AUC(fe.trace.Trace):\n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n\n    def on_batch_end(self, data):\n        y_pred, y_true = np.argmax(data[\"y_pred\"].numpy(), axis=-1), data[\"y\"].numpy()\n        y_pred, y_true = np.where(y_pred &lt; 5, 0, 1), np.where(y_true &lt; 5, 0, 1)\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n\n    def on_epoch_end(self, data):\n        fpr, tpr, _ = metrics.roc_curve(self.y_true, self.y_pred)\n        auc = metrics.auc(fpr, tpr)\n        data.write_with_log(\"auc\", auc)\n</pre> from fastestimator.trace.meta import per_ds from sklearn import metrics import numpy as np   @per_ds  # Without this annotation the trace would only compute the aggregate metric class AUC(fe.trace.Trace):     def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []      def on_batch_end(self, data):         y_pred, y_true = np.argmax(data[\"y_pred\"].numpy(), axis=-1), data[\"y\"].numpy()         y_pred, y_true = np.where(y_pred &lt; 5, 0, 1), np.where(y_true &lt; 5, 0, 1)         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())      def on_epoch_end(self, data):         fpr, tpr, _ = metrics.roc_curve(self.y_true, self.y_pred)         auc = metrics.auc(fpr, tpr)         data.write_with_log(\"auc\", auc) In\u00a0[8]: Copied! <pre>import tempfile\nfrom fastestimator.trace.io import BestModelSaver\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=4,\n                         traces=[Accuracy(true_key=\"y\", pred_key=\"y_pred\"), \n                                 AUC(inputs=(\"y\", \"y_pred\"), outputs=\"auc\", mode=\"eval\"),\n                                 BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"auc|svhn\", save_best_mode=\"max\")],\n                         train_steps_per_epoch=200)\n\nestimator.fit()\n</pre> import tempfile from fastestimator.trace.io import BestModelSaver  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=4,                          traces=[Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                                   AUC(inputs=(\"y\", \"y_pred\"), outputs=\"auc\", mode=\"eval\"),                                  BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"auc|svhn\", save_best_mode=\"max\")],                          train_steps_per_epoch=200)  estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce|mnist: 0.062433735;\nFastEstimator-Train: step: 100; ce|mnist: 0.2802775; steps/sec|mnist: 63.92;\nFastEstimator-Train: step: 200; ce|mnist: 0.23590814; steps/sec|mnist: 64.12;\nFastEstimator-Train: step: 200; epoch: 1; epoch_time: 3.7 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 180.97;\nEval Progress: 208/312; steps/sec: 193.39;\nEval Progress: 312/312; steps/sec: 179.7;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp95pnqff2/model_best_auc|svhn.h5\nFastEstimator-Eval: step: 200; epoch: 1; accuracy: 0.4888154973357016; accuracy|mnist: 0.9837; accuracy|svhn: 0.29870928088506454; auc: 0.7571002319848853; auc|mnist: 0.9892388633631616; auc|svhn: 0.6624670574144985; ce: 1.6440561; ce|mnist: 0.055526573; ce|svhn: 2.254879; max_auc|svhn: 0.6624670574144985; since_best_auc|svhn: 0;\nFastEstimator-Train: step: 300; ce|mnist: 0.037645765; steps/sec|mnist: 44.2;\nFastEstimator-Train: step: 400; ce|mnist: 0.05922611; steps/sec|mnist: 63.98;\nFastEstimator-Train: step: 400; epoch: 2; epoch_time: 3.79 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 150.98;\nEval Progress: 208/312; steps/sec: 166.17;\nEval Progress: 312/312; steps/sec: 187.0;\nFastEstimator-Eval: step: 400; epoch: 2; accuracy: 0.5021369893428064; accuracy|mnist: 0.9801; accuracy|svhn: 0.31853103872157346; auc: 0.7618485829159409; auc|mnist: 0.9859111115434452; auc|svhn: 0.6552186362897082; ce: 1.6789757; ce|mnist: 0.061825372; ce|svhn: 2.3008037; max_auc|svhn: 0.6624670574144985; since_best_auc|svhn: 1;\nFastEstimator-Train: step: 500; ce|mnist: 0.037580382; steps/sec|mnist: 45.06;\nFastEstimator-Train: step: 600; ce|mnist: 0.042693608; steps/sec|mnist: 58.93;\nFastEstimator-Train: step: 600; epoch: 3; epoch_time: 3.93 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 172.32;\nEval Progress: 208/312; steps/sec: 185.12;\nEval Progress: 312/312; steps/sec: 172.22;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmp95pnqff2/model_best_auc|svhn.h5\nFastEstimator-Eval: step: 600; epoch: 3; accuracy: 0.5132937388987566; accuracy|mnist: 0.98; accuracy|svhn: 0.3340119852489244; auc: 0.7671458888899824; auc|mnist: 0.9866311280009643; auc|svhn: 0.6683979301971492; ce: 1.530564; ce|mnist: 0.06489386; ce|svhn: 2.0941448; max_auc|svhn: 0.6683979301971492; since_best_auc|svhn: 0;\nFastEstimator-Train: step: 700; ce|mnist: 0.03424043; steps/sec|mnist: 43.93;\nFastEstimator-Train: step: 800; ce|mnist: 0.12813392; steps/sec|mnist: 63.89;\nFastEstimator-Train: step: 800; epoch: 4; epoch_time: 3.81 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 168.38;\nEval Progress: 208/312; steps/sec: 172.82;\nEval Progress: 312/312; steps/sec: 151.1;\nFastEstimator-Eval: step: 800; epoch: 4; accuracy: 0.48159968916518653; accuracy|mnist: 0.9787; accuracy|svhn: 0.2906422864167179; auc: 0.7624943008308487; auc|mnist: 0.9883770373095143; auc|svhn: 0.6621539867415974; ce: 1.7956299; ce|mnist: 0.06795667; ce|svhn: 2.4599562; max_auc|svhn: 0.6683979301971492; since_best_auc|svhn: 1;\nFastEstimator-Finish: step: 800; model_lr: 0.001; total_time: 47.55 sec;\n</pre> <p>Now during evaluation we can see <code>auc|svhn</code>, <code>auc|mnist</code>, and <code>auc</code> printing in the log. Moreover, our model saving is based on the best evaluation auc on the svhn dataset.</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#advanced-tutorial-13-multi-dataset-training-and-evaluation", "title": "Advanced Tutorial 13: Multi-Dataset Training and Evaluation\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Multi-Dataset Concept and API<ul> <li>Adding multiple datasets to a pipeline</li> <li>Dataset-specific Ops</li> <li>Dataset-specific Traces</li> <li>Specifying multiple datasets in Ops or Traces</li> </ul> </li> <li>Multi-dataset Example</li> </ul>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#multi-dataset-concept-and-api", "title": "Multi-Dataset Concept and API\u00b6", "text": "<p>When you are training a deep learning model, you may sometimes want to train/evaluate on multiple datasets. For example, we might be interested in knowing evaluation metrics separately for two datasets. In this section we show how to do that conveniently in FastEstimator.</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#adding-multiple-datasets-to-a-pipeline", "title": "Adding multiple datasets to a Pipeline\u00b6", "text": "<p>If you have multiple datasets, then in <code>Pipeline</code> you can simply provide a dictionary to the <code>train_data</code>, <code>eval_data</code>, and/or <code>test_data</code> arguments, with keys being the names of the datasets and values being the data instances.</p> <p>For example:</p> <pre>pipeline = fe.Pipeline(eval_data={\"ds1\": eval_data1, \"ds2\": eval_data2}, ...)\n</pre> <p>In the above example, <code>ds1</code> and <code>ds2</code> are the names of those datasets. These can be any other arbitrary names.</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#dataset-specific-ops", "title": "Dataset-specific Ops\u00b6", "text": "<p>Sometimes different datasets might require specific NumpyOps or TensorOps. For example, when we train a gray-scale model and have both gray-scale and colored evaluation sets, we only need to apply the gray-scale conversion to the colored dataset.</p> <p>In FastEstimator, To indicate that an Op is only applied for a specific dataset (say <code>ds1</code>), one only needs to do:</p> <pre>Op(..., ds_id=\"ds1\") # run the op on ds1\n</pre> <p><code>ds_id</code> works similarly to the <code>mode</code> argument in Ops. The operator will only execute if <code>ds_id</code> matches the specific dataset. If <code>ds_id</code> is None (default), then it will execute on all datasets.</p> <p>The <code>ds_id</code> argument works in conjunction with <code>mode</code>. For example, <code>Op(mode=\"train\", ds_id=\"myds1\")</code> would only run during training for dataset named \"myds1\".</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#dataset-specific-traces", "title": "Dataset-specific Traces\u00b6", "text": "<p>To only execute a Trace for a specific dataset, simply pass:</p> <pre>Trace(..., ds_id=\"ds1\") # run the trace on ds1\n</pre> <p>When using multiple datasets, the built-in FastEstimator metric traces will automatically report their outputs for each dataset individually, as well as the overall metric aggregated over all datasets. We will demonstrate this behavior in detail in the example section.</p>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#specifying-multiple-datasets-in-ops-or-traces", "title": "Specifying multiple datasets in Ops or Traces\u00b6", "text": "<p>When an Op or Trace needs to execute on multiple datasets, simply provide a list, tuple, or set of dataset names to the <code>ds_id</code> argument.</p> <p>For example:</p> <pre>Op(..., ds_id=[\"ds1\", \"ds2\"])  # run on both ds1 and ds2\n</pre> <p>When there are many datasets such that listing every name becomes undesirable, you can use <code>!</code> in front of the dataset name to indicate <code>all except</code>.</p> <p>For example:</p> <pre>Op(..., ds_id=[\"!ds1\", \"!ds2\"])  # run on all datasets except ds1 and ds2\n</pre>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#multi-dataset-example", "title": "Multi-dataset Example\u00b6", "text": "<p>In this example, we will train on the MNIST dataset but evaluate on both the MNIST test set and the SVHN-Cropped test set. Here are the dataset-specific items we will do in this example:</p> <ul> <li>Resize images in the SVHN-Cropped dataset from 32x32 to 28x28 to match the MNIST data</li> <li>Convert the SVHN-Cropped dataseet to gray-scale</li> <li>Measure dataset-specific Accuracy as well as combined Accuracy</li> <li>Customize an AUC metric that works on a per-dataset level</li> <li>Save the best model based on evaluation AUC of a specific dataset</li> </ul>"}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#prepare-dataset", "title": "Prepare Dataset\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#preprocessing", "title": "Preprocessing\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#visualize-mnist-preprocessing-results", "title": "Visualize MNIST preprocessing results\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#visualize-svhn_cropped-preprocessing-results", "title": "Visualize SVHN_cropped preprocessing results\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#define-model-and-networks", "title": "Define Model and Networks\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#start-training-with-only-accuracy-trace", "title": "Start Training with Only Accuracy Trace\u00b6", "text": ""}, {"location": "tutorial/advanced/t13_multi-dataset_training_evaluation.html#customize-an-auc-metric-that-works-for-every-dataset-then-save-model-based-on-auc-of-a-specific-dataset", "title": "Customize an AUC metric that works for every dataset, then save model based on AUC of a specific dataset\u00b6", "text": "<p>Since this is a 10-class classification task, to simplify AUC calculation, we will count any label &lt; 5 as 0 and the rest of labels as 1.  When a trace is initializing the data during <code>on_epoch_begin</code> and outputting the data during <code>on_epoch_end</code>, we only need a <code>per_ds</code> decorator to enable multi-dataset support as shown below.</p>"}, {"location": "tutorial/advanced/t14_custom_loaders.html", "title": "Advanced Tutorial 14: Ops with Custom Data Loaders", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.dataset.data import mnist\nfrom fastestimator.dataset.op_dataset import OpDataset\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n</pre> from fastestimator.dataset.data import mnist from fastestimator.dataset.op_dataset import OpDataset from fastestimator.op.numpyop.univariate import ExpandDims, Minmax In\u00a0[2]: Copied! <pre># Let's start by getting a simple dataset\ntrain_data, eval_data = mnist.load_data()\n</pre> # Let's start by getting a simple dataset train_data, eval_data = mnist.load_data() In\u00a0[3]: Copied! <pre># Now we can manually put this dataset into an OpDataset, along with our Op list\nop_ds = OpDataset(dataset=train_data,\n                  mode=\"train\",\n                  ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                       Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> # Now we can manually put this dataset into an OpDataset, along with our Op list op_ds = OpDataset(dataset=train_data,                   mode=\"train\",                   ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                         Minmax(inputs=\"x\", outputs=\"x\")]) <p>Note that while this will work for most use cases, the <code>Batch</code> Op and <code>RemoveIf</code> Op will not work as expected if you try to put them into your own custom OpDataset.</p> <p></p> In\u00a0[4]: Copied! <pre>import math\nimport random\n\nfrom torch.utils.data import Sampler\n</pre> import math import random  from torch.utils.data import Sampler In\u00a0[5]: Copied! <pre># A batch sampler that will increase the batch size based on the Fibonacci Sequence for a specified number of batches\nclass FibonacciSampler(Sampler):\n    def __init__(self, ds_length: int, n_batches: int):\n        self.ds_ln = ds_length\n        self.n_batches = n_batches\n        self.fib_fn = lambda n: round((math.pow((1+math.sqrt(5))/2, n) - math.pow((1-math.sqrt(5))/2, n))/math.sqrt(5))\n    def __len__(self):\n        return self.ds_ln\n    def __iter__(self):\n        indices = [random.sample(range(self.ds_ln), self.fib_fn(i)) for i in range(1, self.n_batches+1)]\n        return iter(indices)\n</pre> # A batch sampler that will increase the batch size based on the Fibonacci Sequence for a specified number of batches class FibonacciSampler(Sampler):     def __init__(self, ds_length: int, n_batches: int):         self.ds_ln = ds_length         self.n_batches = n_batches         self.fib_fn = lambda n: round((math.pow((1+math.sqrt(5))/2, n) - math.pow((1-math.sqrt(5))/2, n))/math.sqrt(5))     def __len__(self):         return self.ds_ln     def __iter__(self):         indices = [random.sample(range(self.ds_ln), self.fib_fn(i)) for i in range(1, self.n_batches+1)]         return iter(indices) In\u00a0[6]: Copied! <pre>my_sampler = FibonacciSampler(ds_length=len(op_ds), n_batches=10)\n</pre> my_sampler = FibonacciSampler(ds_length=len(op_ds), n_batches=10) In\u00a0[7]: Copied! <pre># Now let's build a custom data loader using this sampler:\nfrom torch.utils.data import DataLoader\nimport numpy as np\n</pre> # Now let's build a custom data loader using this sampler: from torch.utils.data import DataLoader import numpy as np In\u00a0[8]: Copied! <pre>loader = DataLoader(dataset=op_ds,\n                    batch_sampler=my_sampler,\n                    worker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),\n                    num_workers=4)\n\n#The worker_init_fn is needed to ensure that any randomness you have in your pipeline behaves properly across different threads\n</pre> loader = DataLoader(dataset=op_ds,                     batch_sampler=my_sampler,                     worker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),                     num_workers=4)  #The worker_init_fn is needed to ensure that any randomness you have in your pipeline behaves properly across different threads <p></p> In\u00a0[9]: Copied! <pre>from fastestimator import Pipeline\n\npipeline = Pipeline(train_data = loader)\n</pre> from fastestimator import Pipeline  pipeline = Pipeline(train_data = loader) In\u00a0[10]: Copied! <pre>data = pipeline.get_results(num_steps=10)\nfor idx, batch in enumerate(data):\n    print(f\"batch {idx}: {batch['x'].shape}\")\n</pre> data = pipeline.get_results(num_steps=10) for idx, batch in enumerate(data):     print(f\"batch {idx}: {batch['x'].shape}\") <pre>batch 0: torch.Size([1, 28, 28, 1])\nbatch 1: torch.Size([1, 28, 28, 1])\nbatch 2: torch.Size([2, 28, 28, 1])\nbatch 3: torch.Size([3, 28, 28, 1])\nbatch 4: torch.Size([5, 28, 28, 1])\nbatch 5: torch.Size([8, 28, 28, 1])\nbatch 6: torch.Size([13, 28, 28, 1])\nbatch 7: torch.Size([21, 28, 28, 1])\nbatch 8: torch.Size([34, 28, 28, 1])\nbatch 9: torch.Size([55, 28, 28, 1])\n</pre> <p>As expected, our batch size is now increasing every step following the Fibonacci sequence, but we have also successfully integrated FE Ops into our customized pipeline. Huzzah!</p>"}, {"location": "tutorial/advanced/t14_custom_loaders.html#advanced-tutorial-14-ops-with-custom-data-loaders", "title": "Advanced Tutorial 14: Ops with Custom Data Loaders\u00b6", "text": ""}, {"location": "tutorial/advanced/t14_custom_loaders.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Using Ops with Custom Data Loaders<ul> <li>Op Dataset</li> <li>Custom Data Loaders</li> <li>Putting Things Together</li> </ul> </li> </ul>"}, {"location": "tutorial/advanced/t14_custom_loaders.html#using-ops-with-custom-data-loaders", "title": "Using Ops with Custom Data Loaders\u00b6", "text": "<p>After using FE for a while you will likely become attached to the operator paradigm, but might concievably encounter a usecase which is not well supported by the default FE Pipeline. As you may already be aware, one way to avoid any limitations imposed by the FE API is to pass your own PyTorch Dataloader (or TensorFlow dataset) directly into the FE Pipeline (instead of passing a PyTorch/FE Dataset). Normally this would prevent you from using FE Ops, but there is a way around this:</p>"}, {"location": "tutorial/advanced/t14_custom_loaders.html#op-dataset", "title": "Op Dataset\u00b6", "text": "<p>FE contains an object called an OpDataset which is what we use internally to chain Ops onto datasets within our dataloader. You can construct one youself as well for use within your own dataloader. Let's see an example.</p>"}, {"location": "tutorial/advanced/t14_custom_loaders.html#custom-data-loaders", "title": "Custom Data Loaders\u00b6", "text": "<p>Now let's construct a custom PyTorch data loader using our OpDataset. Suppose, for example, that you want your batch size to change every step following the Fibonacci sequence. Even though the FE API lacks support for this critically important feature, you can still implement it yourself using a custom PyTorch batch sampler:</p>"}, {"location": "tutorial/advanced/t14_custom_loaders.html#putting-things-together", "title": "Putting Things Together\u00b6", "text": "<p>Now that we have a custom data loader along with our op dataset, let's use them with an FE pipeline and see what happens:</p>"}, {"location": "tutorial/advanced/t15_finetuning.html", "title": "Advanced Tutorial 15: Finetuning Tutorial", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport tensorflow as tf\n\n# Since we will be mixing TF and Torch in the tutorial, we need to stop TF from taking all of the GPU memory.\n# Normally you would pick either TF or Torch, so you don't need to worry about this.\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor device in physical_devices:\n    try:\n        tf.config.experimental.set_memory_growth(device, True)\n    except:\n        pass\n\nimport fastestimator as fe\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.schedule.schedule import EpochScheduler\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.dataset.data import cifair100, cifair10\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nfrom fastestimator.architecture.tensorflow import LeNet as lenet_tf\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras import Model\nfrom fastestimator.architecture.pytorch import LeNet as lenet_torch\nimport torch.nn as nn\nfrom torch import load, Tensor, cuda, save\n\nimport torch.nn.functional as fn\n</pre> import os import tempfile  import tensorflow as tf  # Since we will be mixing TF and Torch in the tutorial, we need to stop TF from taking all of the GPU memory. # Normally you would pick either TF or Torch, so you don't need to worry about this. physical_devices = tf.config.list_physical_devices('GPU') for device in physical_devices:     try:         tf.config.experimental.set_memory_growth(device, True)     except:         pass  import fastestimator as fe from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.schedule.schedule import EpochScheduler from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.dataset.data import cifair100, cifair10 from fastestimator.op.tensorop.model import ModelOp, UpdateOp  from fastestimator.architecture.tensorflow import LeNet as lenet_tf from tensorflow.keras import Sequential, layers from tensorflow.keras import Model from fastestimator.architecture.pytorch import LeNet as lenet_torch import torch.nn as nn from torch import load, Tensor, cuda, save  import torch.nn.functional as fn In\u00a0[2]: Copied! <pre>def get_pipeline(dataset, num_classes, batch_size, mode='tf', min_height=40, min_width=40):\n\n    train_data, eval_data = dataset.load_data()\n\n    mean_value = (0.4914, 0.4822, 0.4465)\n    std_value = (0.2471, 0.2435, 0.2616)\n\n    ops = [ Normalize(inputs=\"x\", outputs=\"x\", mean=mean_value, std=std_value),\n            PadIfNeeded(min_height=min_height, min_width=min_width, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n            Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n            CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n            Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=num_classes, label_smoothing=0.2)]\n\n    if mode == 'torch':\n        ops.append(ChannelTranspose(inputs=\"x\", outputs=\"x\"))\n                \n    return fe.Pipeline(\n                train_data=train_data,\n                eval_data=eval_data,\n                batch_size=batch_size,\n                ops=ops)\n\ndef get_network(model):\n    return  fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")])\n\ndef get_estimator(pipeline, network, epochs):\n    traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\")]\n\n    return fe.Estimator(pipeline=pipeline,\n                                network=network,\n                                epochs=epochs,\n                                traces=traces,\n                                log_steps=0)\n</pre> def get_pipeline(dataset, num_classes, batch_size, mode='tf', min_height=40, min_width=40):      train_data, eval_data = dataset.load_data()      mean_value = (0.4914, 0.4822, 0.4465)     std_value = (0.2471, 0.2435, 0.2616)      ops = [ Normalize(inputs=\"x\", outputs=\"x\", mean=mean_value, std=std_value),             PadIfNeeded(min_height=min_height, min_width=min_width, image_in=\"x\", image_out=\"x\", mode=\"train\"),             RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),             Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),             CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),             Onehot(inputs=\"y\", outputs=\"y\", mode=\"train\", num_classes=num_classes, label_smoothing=0.2)]      if mode == 'torch':         ops.append(ChannelTranspose(inputs=\"x\", outputs=\"x\"))                      return fe.Pipeline(                 train_data=train_data,                 eval_data=eval_data,                 batch_size=batch_size,                 ops=ops)  def get_network(model):     return  fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")])  def get_estimator(pipeline, network, epochs):     traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\")]      return fe.Estimator(pipeline=pipeline,                                 network=network,                                 epochs=epochs,                                 traces=traces,                                 log_steps=0) In\u00a0[3]: Copied! <pre>#training parameters\nepochs_pretrain = 10\n\nepochs_finetune = 5\n\nbatch_size = 64\n\nbase_num_classes = 100 \n\nfinetune_num_classes = 10\n\nmodel_dir = tempfile.gettempdir()\n</pre> #training parameters epochs_pretrain = 10  epochs_finetune = 5  batch_size = 64  base_num_classes = 100   finetune_num_classes = 10  model_dir = tempfile.gettempdir() <p>Now that boring stuff is done, let's train our first base model. We are using tensorflow LeNet to train on cifar100 with 100 classes. We are training for 10 epochs and saving the model at the end of the training job.</p> In\u00a0[4]: Copied! <pre>tf_input_shape = (32, 32, 3)\n\nmodel_tf_pretrain = fe.build(model_fn=lambda: lenet_tf(input_shape=tf_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")\n\npipeline_tf_pretrain = get_pipeline(cifair100, base_num_classes, batch_size)\n\nnetwork_tf_pretrain = get_network(model_tf_pretrain)\n\nestimator_tf_pretrain = get_estimator(pipeline_tf_pretrain, network_tf_pretrain, epochs_pretrain)\n\nestimator_tf_pretrain.fit(warmup=False)\n\nfe.backend.save_model(model_tf_pretrain, save_dir=model_dir, model_name= \"lenet_tf\")\n</pre> tf_input_shape = (32, 32, 3)  model_tf_pretrain = fe.build(model_fn=lambda: lenet_tf(input_shape=tf_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")  pipeline_tf_pretrain = get_pipeline(cifair100, base_num_classes, batch_size)  network_tf_pretrain = get_network(model_tf_pretrain)  estimator_tf_pretrain = get_estimator(pipeline_tf_pretrain, network_tf_pretrain, epochs_pretrain)  estimator_tf_pretrain.fit(warmup=False)  fe.backend.save_model(model_tf_pretrain, save_dir=model_dir, model_name= \"lenet_tf\") <pre>2022-04-28 17:29:24.469363: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-28 17:29:26.182234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n2022-04-28 17:29:28.562053: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 0; num_device: 1;\n</pre> <pre>2022-04-28 17:29:35.097002: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n2022-04-28 17:29:37.270702: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n</pre> <pre>FastEstimator-Train: step: 782; epoch: 1;\nFastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.1402; ce: 3.6746857;\nFastEstimator-Train: step: 1564; epoch: 2;\nFastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.2204; ce: 3.2619572;\nFastEstimator-Train: step: 2346; epoch: 3;\nFastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.2469; ce: 3.1025422;\nFastEstimator-Train: step: 3128; epoch: 4;\nFastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.2879; ce: 2.9410963;\nFastEstimator-Train: step: 3910; epoch: 5;\nFastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.2944; ce: 2.8627439;\nFastEstimator-Train: step: 4692; epoch: 6;\nFastEstimator-Eval: step: 4692; epoch: 6; accuracy: 0.3167; ce: 2.7871962;\nFastEstimator-Train: step: 5474; epoch: 7;\nFastEstimator-Eval: step: 5474; epoch: 7; accuracy: 0.324; ce: 2.7451925;\nFastEstimator-Train: step: 6256; epoch: 8;\nFastEstimator-Eval: step: 6256; epoch: 8; accuracy: 0.3267; ce: 2.7209747;\nFastEstimator-Train: step: 7038; epoch: 9;\nFastEstimator-Eval: step: 7038; epoch: 9; accuracy: 0.3453; ce: 2.6295624;\nFastEstimator-Train: step: 7820; epoch: 10;\nFastEstimator-Eval: step: 7820; epoch: 10; accuracy: 0.3513; ce: 2.601092;\nFastEstimator-Finish: step: 7820; model_lr: 0.001; total_time: 108.44 sec;\n</pre> Out[4]: <pre>'/tmp/lenet_tf.h5'</pre> <p>For finetuning, We use FastEstimator API to load the ciFAIR-10 dataset. You can use your own dataset by updating <code>get_pipeline</code> method.</p> In\u00a0[5]: Copied! <pre>pipeline_tf_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size)\n</pre> pipeline_tf_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size) <p>Now we are ready to extend our base model with finetuning task.</p> <p>Let's load our pretrained weights saved in previous setup. The weights files are saved with <code>h5</code> extension, since we have given <code>lenet_tf</code> as model_name to the <code>save_model</code>  function the model weights are saved as <code>lenet_tf.h5</code>.</p> In\u00a0[6]: Copied! <pre>weights_path = os.path.join(model_dir, \"lenet_tf.h5\")\n\npretrained_lenet_tf = fe.build(model_fn=lambda: lenet_tf(input_shape=tf_input_shape, classes=base_num_classes), optimizer_fn=\"adam\", weights_path=weights_path)\n</pre> weights_path = os.path.join(model_dir, \"lenet_tf.h5\")  pretrained_lenet_tf = fe.build(model_fn=lambda: lenet_tf(input_shape=tf_input_shape, classes=base_num_classes), optimizer_fn=\"adam\", weights_path=weights_path) <p>Let's remove the classification head of pretrained model and build a backbone. We will be using <code>fe.build</code> to build a new fe model.</p> In\u00a0[7]: Copied! <pre>def get_tf_backbone(pretrained_model):\n\n    model = Model(inputs=pretrained_model.inputs, outputs=pretrained_model.layers[-3].output)\n\n    return model\n\nbackbone_tf = fe.build(model_fn=lambda: get_tf_backbone(pretrained_lenet_tf), optimizer_fn=\"adam\")\n</pre> def get_tf_backbone(pretrained_model):      model = Model(inputs=pretrained_model.inputs, outputs=pretrained_model.layers[-3].output)      return model  backbone_tf = fe.build(model_fn=lambda: get_tf_backbone(pretrained_lenet_tf), optimizer_fn=\"adam\") <p>Next, we will define a classification head that can be used for the finetuning task. This is simply two <code>Dense</code> layers.</p> In\u00a0[8]: Copied! <pre>def get_class_head(finetune_num_classes):\n    return Sequential([layers.Dense(64, activation='relu', input_shape=(1024,)), \n                       layers.Dense(finetune_num_classes, activation='softmax')])\n    \ncls_head_tf_finetune = fe.build(model_fn=lambda: get_class_head(finetune_num_classes), optimizer_fn=\"adam\")\n</pre> def get_class_head(finetune_num_classes):     return Sequential([layers.Dense(64, activation='relu', input_shape=(1024,)),                         layers.Dense(finetune_num_classes, activation='softmax')])      cls_head_tf_finetune = fe.build(model_fn=lambda: get_class_head(finetune_num_classes), optimizer_fn=\"adam\") <p>If you want to save the finetune model, we can combine the <code>Backbone Model</code> and the <code>Class Head Model</code> and provide it to ModelSaver later.</p> In\u00a0[9]: Copied! <pre>def combined_tf_model(backbone_model, cls_head_finetune):\n\n    backbone_output = backbone_model.layers[-1].output\n    x = cls_head_finetune.layers[0](backbone_output)\n    x = cls_head_finetune.layers[1](x)\n    model = Model(inputs=backbone_model.inputs, outputs=x)\n    return model\n\nfinal_model_tf = fe.build(model_fn=lambda: combined_tf_model(backbone_tf, cls_head_tf_finetune),  optimizer_fn=\"adam\")\n</pre> def combined_tf_model(backbone_model, cls_head_finetune):      backbone_output = backbone_model.layers[-1].output     x = cls_head_finetune.layers[0](backbone_output)     x = cls_head_finetune.layers[1](x)     model = Model(inputs=backbone_model.inputs, outputs=x)     return model  final_model_tf = fe.build(model_fn=lambda: combined_tf_model(backbone_tf, cls_head_tf_finetune),  optimizer_fn=\"adam\") <p>For Finetuning, we want to train different part of the network in the following manner:</p> <ul> <li>epoch 1-3: <code>freeze</code> backbone, <code>train</code> classification head only</li> <li>epoch 4-end: <code>train</code> backbone and classification head <code>together</code></li> </ul> <p>Let's use EpochScheduler to define when backbone and class head weights are updated. UpdateOp is responsible for weight updating.</p> In\u00a0[10]: Copied! <pre>network_tf_finetune = fe.Network(ops=[\n                                ModelOp(model=backbone_tf, inputs=\"x\", outputs=\"feature\"),\n                                ModelOp(model=cls_head_tf_finetune, inputs=\"feature\", outputs=\"y_pred\"),\n                                CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n                                EpochScheduler({1: None, 4: UpdateOp(model=backbone_tf, loss_name=\"ce\")}),\n                                EpochScheduler({1: UpdateOp(model=cls_head_tf_finetune, loss_name=\"ce\")})])\n\nestimator_tf_finetune = get_estimator(pipeline_tf_finetune, network_tf_finetune, epochs_finetune)\n</pre> network_tf_finetune = fe.Network(ops=[                                 ModelOp(model=backbone_tf, inputs=\"x\", outputs=\"feature\"),                                 ModelOp(model=cls_head_tf_finetune, inputs=\"feature\", outputs=\"y_pred\"),                                 CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),                                 EpochScheduler({1: None, 4: UpdateOp(model=backbone_tf, loss_name=\"ce\")}),                                 EpochScheduler({1: UpdateOp(model=cls_head_tf_finetune, loss_name=\"ce\")})])  estimator_tf_finetune = get_estimator(pipeline_tf_finetune, network_tf_finetune, epochs_finetune) <p>Let's train our finetune model using pretrained weights on our new dataset.</p> In\u00a0[11]: Copied! <pre>estimator_tf_finetune.fit(warmup=False)\n</pre> estimator_tf_finetune.fit(warmup=False) <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 0; num_device: 1;\n</pre> <pre>/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning:\n\n\"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n\n</pre> <pre>FastEstimator-Train: step: 782; epoch: 1;\n</pre> <pre>/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning:\n\n\"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n\n</pre> <pre>FastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.6009; ce: 1.1969987;\nFastEstimator-Train: step: 1564; epoch: 2;\nFastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.6269; ce: 1.1396555;\nFastEstimator-Train: step: 2346; epoch: 3;\nFastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.6305; ce: 1.1114084;\nFastEstimator-Train: step: 3128; epoch: 4;\nFastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.666; ce: 1.0342808;\nFastEstimator-Train: step: 3910; epoch: 5;\nFastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.674; ce: 1.0168855;\nFastEstimator-Finish: step: 3910; model2_lr: 0.001; model3_lr: 0.001; total_time: 50.76 sec;\n</pre> <p>Finally, let's save our finetuned model.</p> In\u00a0[12]: Copied! <pre>fe.backend.save_model(final_model_tf, save_dir=model_dir, model_name=\"final_tf_finetune\")\n</pre> fe.backend.save_model(final_model_tf, save_dir=model_dir, model_name=\"final_tf_finetune\") Out[12]: <pre>'/tmp/final_tf_finetune.h5'</pre> <p>Let's train our first pytorch base model. We are using pytorch LeNet to train on cifar100 with 100 classes. We are training for 10 epochs and saving the model at the end of the training job.</p> In\u00a0[13]: Copied! <pre>torch_input_shape = (3, 32, 32)\n\nmodel_torch_pretrain = fe.build(model_fn=lambda: lenet_torch(input_shape=torch_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")\n\npipeline_torch_pretrain = get_pipeline(cifair100, base_num_classes, batch_size, 'torch')\n\nnetwork_torch_pretrain = get_network(model_torch_pretrain)\n\nestimator_torch_pretrain = get_estimator(pipeline_torch_pretrain, network_torch_pretrain, epochs_pretrain)\n\nestimator_torch_pretrain.fit()\n\nfe.backend.save_model(model_torch_pretrain, save_dir=model_dir, model_name=\"lenet_torch\")\n</pre> torch_input_shape = (3, 32, 32)  model_torch_pretrain = fe.build(model_fn=lambda: lenet_torch(input_shape=torch_input_shape, classes=base_num_classes), optimizer_fn=\"adam\")  pipeline_torch_pretrain = get_pipeline(cifair100, base_num_classes, batch_size, 'torch')  network_torch_pretrain = get_network(model_torch_pretrain)  estimator_torch_pretrain = get_estimator(pipeline_torch_pretrain, network_torch_pretrain, epochs_pretrain)  estimator_torch_pretrain.fit()  fe.backend.save_model(model_torch_pretrain, save_dir=model_dir, model_name=\"lenet_torch\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 0; num_device: 1;\nFastEstimator-Train: step: 782; epoch: 1;\nFastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.1401; ce: 3.658039;\nFastEstimator-Train: step: 1564; epoch: 2;\nFastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.1941; ce: 3.3732774;\nFastEstimator-Train: step: 2346; epoch: 3;\nFastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.2451; ce: 3.1511996;\nFastEstimator-Train: step: 3128; epoch: 4;\nFastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.2643; ce: 3.025363;\nFastEstimator-Train: step: 3910; epoch: 5;\nFastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.2903; ce: 2.8853397;\nFastEstimator-Train: step: 4692; epoch: 6;\nFastEstimator-Eval: step: 4692; epoch: 6; accuracy: 0.2999; ce: 2.8033638;\nFastEstimator-Train: step: 5474; epoch: 7;\nFastEstimator-Eval: step: 5474; epoch: 7; accuracy: 0.3158; ce: 2.7701957;\nFastEstimator-Train: step: 6256; epoch: 8;\nFastEstimator-Eval: step: 6256; epoch: 8; accuracy: 0.3213; ce: 2.7396107;\nFastEstimator-Train: step: 7038; epoch: 9;\nFastEstimator-Eval: step: 7038; epoch: 9; accuracy: 0.334; ce: 2.7355165;\nFastEstimator-Train: step: 7820; epoch: 10;\nFastEstimator-Eval: step: 7820; epoch: 10; accuracy: 0.3326; ce: 2.655726;\nFastEstimator-Finish: step: 7820; model5_lr: 0.001; total_time: 115.39 sec;\n</pre> Out[13]: <pre>'/tmp/lenet_torch.pt'</pre> <p>For finetuning, We use FastEstimator API to load the ciFAIR-10 dataset. You can use your own dataset by changing <code>get_pipeline</code> method.</p> In\u00a0[14]: Copied! <pre>pipeline_torch_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size, 'torch')\n</pre> pipeline_torch_finetune = get_pipeline(cifair10, finetune_num_classes, batch_size, 'torch') <p>Now we are ready to extend our base model with finetuning task.</p> <p>Let's load our pretrained weights saved in our previous setup. The weights files are saved with h5 extension, since we have given <code>lenet_torch</code> as model_name to the <code>save_model</code>  function the model weights are saved as <code>lenet_torch.pt</code>. Replace it if you used different model_name in <code>save_model</code> method.</p> In\u00a0[15]: Copied! <pre>weights_path=os.path.join(model_dir, 'lenet_torch.pt')\n\nmodel_torch_pretrained = fe.build(model_fn=lambda: lenet_torch(input_shape=torch_input_shape, classes=base_num_classes), optimizer_fn=\"adam\", weights_path=weights_path)\n</pre> weights_path=os.path.join(model_dir, 'lenet_torch.pt')  model_torch_pretrained = fe.build(model_fn=lambda: lenet_torch(input_shape=torch_input_shape, classes=base_num_classes), optimizer_fn=\"adam\", weights_path=weights_path) <p>Let's remove the last layer of pretrained model and build a new backbone. We will be using fe.build to build a new fe model.</p> In\u00a0[16]: Copied! <pre>class BackboneTorch(nn.Module):\n    def __init__(self, model_torch_pretrained) -&gt; None:\n        super().__init__()\n        self.pool_kernel = 2\n        if isinstance(model_torch_pretrained, nn.DataParallel):\n            self.backbone_layers = nn.Sequential(*(list(model_torch_pretrained.module.children())[:-2]))\n        else:\n            self.backbone_layers = nn.Sequential(*(list(model_torch_pretrained.children())[:-2]))\n\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        x = fn.relu(self.backbone_layers[0](x))\n        x = fn.max_pool2d(x, self.pool_kernel)\n        x = fn.relu(self.backbone_layers[1](x))\n        x = fn.max_pool2d(x, self.pool_kernel)\n        x = fn.relu(self.backbone_layers[2](x))\n        return x\n\nbackbone_torch = fe.build(model_fn=lambda: BackboneTorch(model_torch_pretrained), optimizer_fn=\"adam\")\n</pre> class BackboneTorch(nn.Module):     def __init__(self, model_torch_pretrained) -&gt; None:         super().__init__()         self.pool_kernel = 2         if isinstance(model_torch_pretrained, nn.DataParallel):             self.backbone_layers = nn.Sequential(*(list(model_torch_pretrained.module.children())[:-2]))         else:             self.backbone_layers = nn.Sequential(*(list(model_torch_pretrained.children())[:-2]))       def forward(self, x: Tensor) -&gt; Tensor:         x = fn.relu(self.backbone_layers[0](x))         x = fn.max_pool2d(x, self.pool_kernel)         x = fn.relu(self.backbone_layers[1](x))         x = fn.max_pool2d(x, self.pool_kernel)         x = fn.relu(self.backbone_layers[2](x))         return x  backbone_torch = fe.build(model_fn=lambda: BackboneTorch(model_torch_pretrained), optimizer_fn=\"adam\")  <p>Next, we will define a classification head that can be used for the finetuning task. This is simply two <code>nn.Linear</code> layers.</p> In\u00a0[17]: Copied! <pre>class ClassifierHead(nn.Module):\n    def __init__(self, classes=10):\n        super().__init__()\n        self.fc1 = nn.Linear(1024, 64)\n        self.fc2 = nn.Linear(64, classes)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = fn.relu(self.fc1(x))\n        x = fn.softmax(self.fc2(x), dim=-1)\n        return x\n\ncls_head_torch_finetune = fe.build(model_fn=lambda: ClassifierHead(classes=finetune_num_classes), optimizer_fn=\"adam\")\n</pre> class ClassifierHead(nn.Module):     def __init__(self, classes=10):         super().__init__()         self.fc1 = nn.Linear(1024, 64)         self.fc2 = nn.Linear(64, classes)      def forward(self, x):         x = x.view(x.size(0), -1)         x = fn.relu(self.fc1(x))         x = fn.softmax(self.fc2(x), dim=-1)         return x  cls_head_torch_finetune = fe.build(model_fn=lambda: ClassifierHead(classes=finetune_num_classes), optimizer_fn=\"adam\") <p>If you want to save the finetune model, we can combine the <code>Backbone Model</code> and the <code>Class Head Model</code> and provide it to ModelSaver later.</p> In\u00a0[18]: Copied! <pre>class CombinedTorchModel(nn.Module):\n    def __init__(self, backbone, cls_head):\n        super().__init__()\n        self.backbone = backbone\n        self.cls_head = cls_head\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.cls_head(x)\n        return x\n\nfinal_torch_model = fe.build(model_fn=lambda: CombinedTorchModel(backbone_torch, cls_head_torch_finetune), optimizer_fn=None)\n</pre> class CombinedTorchModel(nn.Module):     def __init__(self, backbone, cls_head):         super().__init__()         self.backbone = backbone         self.cls_head = cls_head      def forward(self, x):         x = self.backbone(x)         x = self.cls_head(x)         return x  final_torch_model = fe.build(model_fn=lambda: CombinedTorchModel(backbone_torch, cls_head_torch_finetune), optimizer_fn=None)  <p>For Finetuning, we want to train different part of the network in the following manner:</p> <ul> <li>epoch 1-3: <code>freeze</code> backbone, <code>train</code> classification head only</li> <li>epoch 4-end: <code>train</code> backbone and classification head <code>together</code></li> </ul> <p>Let's use EpochScheduler to define when backbone and class head weights are updated. UpdateOp is responsible for weight updating.</p> In\u00a0[19]: Copied! <pre>network_torch_finetune = fe.Network(ops=[\n                                ModelOp(model=backbone_torch, inputs=\"x\", outputs=\"feature\"),\n                                ModelOp(model=cls_head_torch_finetune, inputs=\"feature\", outputs=\"y_pred\"),\n                                CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),\n                                EpochScheduler({1: None, 4: UpdateOp(model=backbone_torch, loss_name=\"ce\")}),\n                                EpochScheduler({1: UpdateOp(model=cls_head_torch_finetune, loss_name=\"ce\")})])\n\nestimator_torch_finetune = get_estimator(pipeline_torch_finetune, network_torch_finetune, epochs_finetune)\n</pre> network_torch_finetune = fe.Network(ops=[                                 ModelOp(model=backbone_torch, inputs=\"x\", outputs=\"feature\"),                                 ModelOp(model=cls_head_torch_finetune, inputs=\"feature\", outputs=\"y_pred\"),                                 CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", from_logits=True),                                 EpochScheduler({1: None, 4: UpdateOp(model=backbone_torch, loss_name=\"ce\")}),                                 EpochScheduler({1: UpdateOp(model=cls_head_torch_finetune, loss_name=\"ce\")})])  estimator_torch_finetune = get_estimator(pipeline_torch_finetune, network_torch_finetune, epochs_finetune) <p>Let's train our finetune model using pretrained weights on our new dataset.</p> In\u00a0[20]: Copied! <pre>estimator_torch_finetune.fit()\n</pre> estimator_torch_finetune.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 0; num_device: 1;\nFastEstimator-Train: step: 782; epoch: 1;\nFastEstimator-Eval: step: 782; epoch: 1; accuracy: 0.5877; ce: 1.8731252;\nFastEstimator-Train: step: 1564; epoch: 2;\nFastEstimator-Eval: step: 1564; epoch: 2; accuracy: 0.6221; ce: 1.838632;\nFastEstimator-Train: step: 2346; epoch: 3;\nFastEstimator-Eval: step: 2346; epoch: 3; accuracy: 0.6047; ce: 1.8550365;\nFastEstimator-Train: step: 3128; epoch: 4;\nFastEstimator-Eval: step: 3128; epoch: 4; accuracy: 0.6269; ce: 1.8336332;\nFastEstimator-Train: step: 3910; epoch: 5;\nFastEstimator-Eval: step: 3910; epoch: 5; accuracy: 0.6301; ce: 1.8286426;\nFastEstimator-Finish: step: 3910; model7_lr: 0.001; model8_lr: 0.001; total_time: 60.43 sec;\n</pre> <p>Finally, let's save our finetuned model.</p> In\u00a0[21]: Copied! <pre>fe.backend.save_model(final_torch_model, save_dir=model_dir, model_name=\"final_torch_finetune\")\n</pre> fe.backend.save_model(final_torch_model, save_dir=model_dir, model_name=\"final_torch_finetune\") Out[21]: <pre>'/tmp/final_torch_finetune.pt'</pre>"}, {"location": "tutorial/advanced/t15_finetuning.html#advanced-tutorial-15-finetuning-tutorial", "title": "Advanced Tutorial 15: Finetuning Tutorial\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover finetuning using FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Setting Things Up<ul> <li>Define Reusable Methods</li> </ul> </li> <li>Tensorflow Workflow<ul> <li>Train Base Model</li> <li>Extending Base Model for Finetuning<ul> <li>Import Pretrained Model</li> <li>Extending Base Model</li> <li>Combine Base Model and Finetune Model</li> </ul> </li> <li>Start Finetuning</li> </ul> </li> <li>Pytorch Workflow<ul> <li>Train Base Model</li> <li>Extending Base Model for Finetuning<ul> <li>Import Pretrained Model</li> <li>Extending Base Model</li> <li>Combine Base Model and Finetune Model</li> </ul> </li> <li>Start Finetuning</li> </ul> </li> </ul>"}, {"location": "tutorial/advanced/t15_finetuning.html#setting-things-up", "title": "Setting Things Up \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#first-lets-get-some-imports-out-of-the-way", "title": "First let's get some imports out of the way:\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#define-reusable-methods", "title": "Define Reusable Methods \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#lets-load-some-default-training-parameters-as-well", "title": "Let's load some default training parameters as well\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#tensorflow-workflow", "title": "Tensorflow Workflow \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#train-base-model", "title": "Train Base Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#load-a-new-dataset-for-finetuning", "title": "Load a new dataset for finetuning\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#extending-base-model-for-finetuning", "title": "Extending Base Model for Finetuning \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#import-pretrained-model", "title": "Import Pretrained Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#extending-base-model", "title": "Extending Base Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#combine-base-model-and-finetune-model", "title": "Combine Base Model and Finetune Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#start-finetuning", "title": "Start Finetuning \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#pytorch-workflow", "title": "Pytorch Workflow \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#train-base-model", "title": "Train Base Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#load-a-new-dataset-for-finetuning", "title": "Load a new dataset for finetuning\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#extending-base-model-for-finetuning", "title": "Extending Base Model for Finetuning \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#import-pretrained-model", "title": "Import Pretrained Model\u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#extending-base-model", "title": "Extending Base Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#combine-base-model-and-finetune-model", "title": "Combine Base Model and Finetune Model \u00b6", "text": ""}, {"location": "tutorial/advanced/t15_finetuning.html#start-finetuning", "title": "Start Finetuning \u00b6", "text": ""}, {"location": "tutorial/advanced/t16_robustness.html", "title": "Advanced Tutorial 16: Model Robustness", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport os\nimport tempfile\n\nfrom fastestimator.dataset.data import cifair10\n\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize\nfrom fastestimator.op.numpyop.multivariate import Rotate, Affine\nfrom fastestimator.architecture.pytorch import LeNet\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import ModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\n\n\ndef get_estimator(\n    save_dir,\n    weight_path=None,\n    model_name='robust',\n    train_rotate=None,\n    test_rotate=None,\n    train_shear=None,\n    epochs=24,\n    visualize=False):\n    \n    train_data, eval_data = cifair10.load_data()\n    test_data = eval_data.split(0.5, seed = 0)\n    \n    numpy_op = []\n    \n    if train_shear is not None:\n        numpy_op.append(Affine(image_in=\"x\", shear=train_shear, mode=\"train\", border_handling='constant', fill_value=0))\n    if train_rotate is not None:\n        numpy_op.append(Affine(image_in=\"x\", rotate=train_rotate, mode=\"train\", border_handling='constant', fill_value=0))\n\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=numpy_op +[Rotate(image_in=\"x\", limit=[test_rotate, test_rotate], mode=\"test\"),\n                                Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n                                ChannelTranspose(inputs=\"x\", outputs=\"x\")\n                            ])\n    \n    \n    model = fe.build(model_fn=lambda: LeNet(input_shape=(3, 32, 32)), optimizer_fn=\"adam\", weights_path=weight_path, model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")\n    ])\n\n    traces = [\n            Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n            ModelSaver(model=model, save_dir=save_dir, frequency=epochs)\n        ]\n    \n    estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=epochs, traces=traces)\n    if visualize:\n        return estimator, pipeline\n    else:\n        return estimator\n</pre> import fastestimator as fe import os import tempfile  from fastestimator.dataset.data import cifair10  from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize from fastestimator.op.numpyop.multivariate import Rotate, Affine from fastestimator.architecture.pytorch import LeNet from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import ModelSaver from fastestimator.trace.metric import Accuracy    def get_estimator(     save_dir,     weight_path=None,     model_name='robust',     train_rotate=None,     test_rotate=None,     train_shear=None,     epochs=24,     visualize=False):          train_data, eval_data = cifair10.load_data()     test_data = eval_data.split(0.5, seed = 0)          numpy_op = []          if train_shear is not None:         numpy_op.append(Affine(image_in=\"x\", shear=train_shear, mode=\"train\", border_handling='constant', fill_value=0))     if train_rotate is not None:         numpy_op.append(Affine(image_in=\"x\", rotate=train_rotate, mode=\"train\", border_handling='constant', fill_value=0))      pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=numpy_op +[Rotate(image_in=\"x\", limit=[test_rotate, test_rotate], mode=\"test\"),                                 Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),                                 ChannelTranspose(inputs=\"x\", outputs=\"x\")                             ])               model = fe.build(model_fn=lambda: LeNet(input_shape=(3, 32, 32)), optimizer_fn=\"adam\", weights_path=weight_path, model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")     ])      traces = [             Accuracy(true_key=\"y\", pred_key=\"y_pred\"),             ModelSaver(model=model, save_dir=save_dir, frequency=epochs)         ]          estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=epochs, traces=traces)     if visualize:         return estimator, pipeline     else:         return estimator  <pre>2023-07-28 00:34:24.278703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-28 00:34:24.370670: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n</pre> <p>Let's train a model without any augmentation.</p> In\u00a0[2]: Copied! <pre>save_dir = tempfile.mkdtemp()\nest, no_aug_pipe = get_estimator(save_dir, weight_path=None, model_name='Without_augmentation', epochs=3, visualize=True)\nest.fit()\n</pre> save_dir = tempfile.mkdtemp() est, no_aug_pipe = get_estimator(save_dir, weight_path=None, model_name='Without_augmentation', epochs=3, visualize=True) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.297823;\nFastEstimator-Train: step: 100; ce: 1.9229205; steps/sec: 20.72;\nFastEstimator-Train: step: 200; ce: 1.8488135; steps/sec: 37.45;\nFastEstimator-Train: step: 300; ce: 1.97854; steps/sec: 36.75;\nFastEstimator-Train: step: 400; ce: 1.4034971; steps/sec: 41.87;\nFastEstimator-Train: step: 500; ce: 1.634158; steps/sec: 36.72;\nFastEstimator-Train: step: 600; ce: 1.5278842; steps/sec: 40.42;\nFastEstimator-Train: step: 700; ce: 1.5716941; steps/sec: 20.76;\nFastEstimator-Train: step: 800; ce: 1.4656043; steps/sec: 19.85;\nFastEstimator-Train: step: 900; ce: 1.3644964; steps/sec: 22.87;\nFastEstimator-Train: step: 1000; ce: 1.2243099; steps/sec: 27.8;\nFastEstimator-Train: step: 1100; ce: 1.1308111; steps/sec: 30.43;\nFastEstimator-Train: step: 1200; ce: 1.4400374; steps/sec: 30.82;\nFastEstimator-Train: step: 1300; ce: 1.0056257; steps/sec: 27.23;\nFastEstimator-Train: step: 1400; ce: 1.337088; steps/sec: 20.05;\nFastEstimator-Train: step: 1500; ce: 1.532153; steps/sec: 10.55;\nFastEstimator-Train: step: 1563; epoch: 1; epoch_time(sec): 62.79;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 20.32;\nEval Progress: 104/157; steps/sec: 112.04;\nEval Progress: 157/157; steps/sec: 105.34;\nFastEstimator-Eval: step: 1563; epoch: 1; accuracy: 0.5198; ce: 1.3610618;\nFastEstimator-Train: step: 1600; ce: 1.3166769; steps/sec: 30.71;\nFastEstimator-Train: step: 1700; ce: 1.1115323; steps/sec: 36.85;\nFastEstimator-Train: step: 1800; ce: 1.2530088; steps/sec: 23.32;\nFastEstimator-Train: step: 1900; ce: 1.1652532; steps/sec: 34.8;\nFastEstimator-Train: step: 2000; ce: 1.0679957; steps/sec: 36.68;\nFastEstimator-Train: step: 2100; ce: 1.1941316; steps/sec: 24.23;\nFastEstimator-Train: step: 2200; ce: 0.9643715; steps/sec: 34.52;\nFastEstimator-Train: step: 2300; ce: 1.3712162; steps/sec: 36.0;\nFastEstimator-Train: step: 2400; ce: 1.1851138; steps/sec: 26.4;\nFastEstimator-Train: step: 2500; ce: 1.0683028; steps/sec: 53.08;\nFastEstimator-Train: step: 2600; ce: 1.2203339; steps/sec: 26.74;\nFastEstimator-Train: step: 2700; ce: 0.9800033; steps/sec: 22.31;\nFastEstimator-Train: step: 2800; ce: 1.1035116; steps/sec: 29.24;\nFastEstimator-Train: step: 2900; ce: 0.93366957; steps/sec: 20.45;\nFastEstimator-Train: step: 3000; ce: 0.9092339; steps/sec: 33.47;\nFastEstimator-Train: step: 3100; ce: 1.1804193; steps/sec: 8.85;\nFastEstimator-Train: step: 3126; epoch: 2; epoch_time(sec): 63.83;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 24.04;\nEval Progress: 104/157; steps/sec: 16.5;\nEval Progress: 157/157; steps/sec: 17.76;\nFastEstimator-Eval: step: 3126; epoch: 2; accuracy: 0.6108; ce: 1.1001097;\nFastEstimator-Train: step: 3200; ce: 1.0673318; steps/sec: 14.38;\nFastEstimator-Train: step: 3300; ce: 0.80209684; steps/sec: 36.37;\nFastEstimator-Train: step: 3400; ce: 1.0805223; steps/sec: 22.28;\nFastEstimator-Train: step: 3500; ce: 0.8957855; steps/sec: 22.97;\nFastEstimator-Train: step: 3600; ce: 0.8971278; steps/sec: 61.19;\nFastEstimator-Train: step: 3700; ce: 0.95779765; steps/sec: 27.5;\nFastEstimator-Train: step: 3800; ce: 1.0616807; steps/sec: 36.44;\nFastEstimator-Train: step: 3900; ce: 1.2883285; steps/sec: 31.54;\nFastEstimator-Train: step: 4000; ce: 1.0859238; steps/sec: 17.53;\nFastEstimator-Train: step: 4100; ce: 1.16119; steps/sec: 34.34;\nFastEstimator-Train: step: 4200; ce: 0.7867568; steps/sec: 49.79;\nFastEstimator-Train: step: 4300; ce: 0.8639324; steps/sec: 26.84;\nFastEstimator-Train: step: 4400; ce: 1.1114055; steps/sec: 10.92;\nFastEstimator-Train: step: 4500; ce: 0.58221227; steps/sec: 19.2;\nFastEstimator-Train: step: 4600; ce: 0.65876365; steps/sec: 32.65;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp26og0a4e/Without_augmentation_epoch_3.pt\nFastEstimator-Train: step: 4689; epoch: 3; epoch_time(sec): 60.23;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 115.84;\nEval Progress: 104/157; steps/sec: 23.7;\nEval Progress: 157/157; steps/sec: 38.77;\nFastEstimator-Eval: step: 4689; epoch: 3; accuracy: 0.6584; ce: 0.9637741;\nFastEstimator-Finish: step: 4689; total_time(sec): 204.87; Without_augmentation_lr: 0.001;\n</pre> <p>Now that we have trained our model without any augmentations, let's load the trained model and test its performance on test set while the input images are rotated at various degrees. We will use FastEstimator Search API to accomplish this.</p> <p>First, let's define a generic evaluation function for Grid Search</p> In\u00a0[3]: Copied! <pre>from fastestimator.search.visualize import visualize_search\nfrom fastestimator.search import GridSearch\n\ndef score_fn(search_idx, rotate, weight_path, save_dir, field_name):\n    est = get_estimator(save_dir, weight_path=weight_path, test_rotate=rotate, epochs=3, visualize=False)\n    hist = est.test(summary=\"myexp\")\n    acc = float(hist.history[\"test\"][\"accuracy\"][0])\n    return {field_name: acc}\n</pre> from fastestimator.search.visualize import visualize_search from fastestimator.search import GridSearch  def score_fn(search_idx, rotate, weight_path, save_dir, field_name):     est = get_estimator(save_dir, weight_path=weight_path, test_rotate=rotate, epochs=3, visualize=False)     hist = est.test(summary=\"myexp\")     acc = float(hist.history[\"test\"][\"accuracy\"][0])     return {field_name: acc}  <p>We take a range of rotation from 0 to 360 degrees(at an interval of 10) and use Grid Search to pass the rotation angles the estimator function</p> In\u00a0[4]: Copied! <pre>weight_path = os.path.join(save_dir, 'Without_augmentation_epoch_3.pt')\nrot = list(range(0, 360, 10))\n\nno_aug_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Accuracy without Augmentation\"), params={\"rotate\": rot})\n\nno_aug_grid_search.fit()\n</pre> weight_path = os.path.join(save_dir, 'Without_augmentation_epoch_3.pt') rot = list(range(0, 360, 10))  no_aug_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Accuracy without Augmentation\"), params={\"rotate\": rot})  no_aug_grid_search.fit()  <pre>FastEstimator-Test: step: None; epoch: 3; accuracy: 0.6664; ce: 0.9561062;\nFastEstimator-Search: Evaluated {'rotate': 0, 'search_idx': 1}, result: {'Accuracy without Augmentation': 0.6664}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.6422; ce: 1.028634;\nFastEstimator-Search: Evaluated {'rotate': 10, 'search_idx': 2}, result: {'Accuracy without Augmentation': 0.6422}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5804; ce: 1.198358;\nFastEstimator-Search: Evaluated {'rotate': 20, 'search_idx': 3}, result: {'Accuracy without Augmentation': 0.5804}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4862; ce: 1.5152924;\nFastEstimator-Search: Evaluated {'rotate': 30, 'search_idx': 4}, result: {'Accuracy without Augmentation': 0.4862}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3996; ce: 1.8845491;\nFastEstimator-Search: Evaluated {'rotate': 40, 'search_idx': 5}, result: {'Accuracy without Augmentation': 0.3996}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3414; ce: 2.1744254;\nFastEstimator-Search: Evaluated {'rotate': 50, 'search_idx': 6}, result: {'Accuracy without Augmentation': 0.3414}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3072; ce: 2.351854;\nFastEstimator-Search: Evaluated {'rotate': 60, 'search_idx': 7}, result: {'Accuracy without Augmentation': 0.3072}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2864; ce: 2.4515882;\nFastEstimator-Search: Evaluated {'rotate': 70, 'search_idx': 8}, result: {'Accuracy without Augmentation': 0.2864}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2818; ce: 2.5172606;\nFastEstimator-Search: Evaluated {'rotate': 80, 'search_idx': 9}, result: {'Accuracy without Augmentation': 0.2818}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2932; ce: 2.4945111;\nFastEstimator-Search: Evaluated {'rotate': 90, 'search_idx': 10}, result: {'Accuracy without Augmentation': 0.2932}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.256; ce: 2.734745;\nFastEstimator-Search: Evaluated {'rotate': 100, 'search_idx': 11}, result: {'Accuracy without Augmentation': 0.256}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2414; ce: 2.9066494;\nFastEstimator-Search: Evaluated {'rotate': 110, 'search_idx': 12}, result: {'Accuracy without Augmentation': 0.2414}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2352; ce: 3.0595553;\nFastEstimator-Search: Evaluated {'rotate': 120, 'search_idx': 13}, result: {'Accuracy without Augmentation': 0.2352}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2346; ce: 3.0900187;\nFastEstimator-Search: Evaluated {'rotate': 130, 'search_idx': 14}, result: {'Accuracy without Augmentation': 0.2346}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2496; ce: 2.9533136;\nFastEstimator-Search: Evaluated {'rotate': 140, 'search_idx': 15}, result: {'Accuracy without Augmentation': 0.2496}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2694; ce: 2.6867018;\nFastEstimator-Search: Evaluated {'rotate': 150, 'search_idx': 16}, result: {'Accuracy without Augmentation': 0.2694}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.301; ce: 2.4071147;\nFastEstimator-Search: Evaluated {'rotate': 160, 'search_idx': 17}, result: {'Accuracy without Augmentation': 0.301}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.316; ce: 2.240197;\nFastEstimator-Search: Evaluated {'rotate': 170, 'search_idx': 18}, result: {'Accuracy without Augmentation': 0.316}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3428; ce: 2.1289763;\nFastEstimator-Search: Evaluated {'rotate': 180, 'search_idx': 19}, result: {'Accuracy without Augmentation': 0.3428}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3158; ce: 2.2378516;\nFastEstimator-Search: Evaluated {'rotate': 190, 'search_idx': 20}, result: {'Accuracy without Augmentation': 0.3158}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3002; ce: 2.378881;\nFastEstimator-Search: Evaluated {'rotate': 200, 'search_idx': 21}, result: {'Accuracy without Augmentation': 0.3002}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2768; ce: 2.6192675;\nFastEstimator-Search: Evaluated {'rotate': 210, 'search_idx': 22}, result: {'Accuracy without Augmentation': 0.2768}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2486; ce: 2.8439047;\nFastEstimator-Search: Evaluated {'rotate': 220, 'search_idx': 23}, result: {'Accuracy without Augmentation': 0.2486}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2346; ce: 2.9615552;\nFastEstimator-Search: Evaluated {'rotate': 230, 'search_idx': 24}, result: {'Accuracy without Augmentation': 0.2346}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2364; ce: 2.9517527;\nFastEstimator-Search: Evaluated {'rotate': 240, 'search_idx': 25}, result: {'Accuracy without Augmentation': 0.2364}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2488; ce: 2.8593893;\nFastEstimator-Search: Evaluated {'rotate': 250, 'search_idx': 26}, result: {'Accuracy without Augmentation': 0.2488}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2634; ce: 2.761762;\nFastEstimator-Search: Evaluated {'rotate': 260, 'search_idx': 27}, result: {'Accuracy without Augmentation': 0.2634}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2904; ce: 2.538681;\nFastEstimator-Search: Evaluated {'rotate': 270, 'search_idx': 28}, result: {'Accuracy without Augmentation': 0.2904}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2768; ce: 2.636479;\nFastEstimator-Search: Evaluated {'rotate': 280, 'search_idx': 29}, result: {'Accuracy without Augmentation': 0.2768}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.277; ce: 2.5930767;\nFastEstimator-Search: Evaluated {'rotate': 290, 'search_idx': 30}, result: {'Accuracy without Augmentation': 0.277}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2798; ce: 2.5175831;\nFastEstimator-Search: Evaluated {'rotate': 300, 'search_idx': 31}, result: {'Accuracy without Augmentation': 0.2798}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3082; ce: 2.318865;\nFastEstimator-Search: Evaluated {'rotate': 310, 'search_idx': 32}, result: {'Accuracy without Augmentation': 0.3082}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3678; ce: 1.9600892;\nFastEstimator-Search: Evaluated {'rotate': 320, 'search_idx': 33}, result: {'Accuracy without Augmentation': 0.3678}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.478; ce: 1.527449;\nFastEstimator-Search: Evaluated {'rotate': 330, 'search_idx': 34}, result: {'Accuracy without Augmentation': 0.478}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5888; ce: 1.1805097;\nFastEstimator-Search: Evaluated {'rotate': 340, 'search_idx': 35}, result: {'Accuracy without Augmentation': 0.5888}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.646; ce: 1.0058187;\nFastEstimator-Search: Evaluated {'rotate': 350, 'search_idx': 36}, result: {'Accuracy without Augmentation': 0.646}\n</pre> <p>Let's visualize how our model performs when tested on rotated images</p> In\u00a0[5]: Copied! <pre>visualize_search(search=no_aug_grid_search, title=\"Model Robustness Without Augmentation\")\n</pre> visualize_search(search=no_aug_grid_search, title=\"Model Robustness Without Augmentation\") <p></p> In\u00a0[6]: Copied! <pre>est, shear_pipe = get_estimator(save_dir, weight_path=None, model_name='Shear', train_shear=45, epochs=3, visualize=True)\n</pre> est, shear_pipe = get_estimator(save_dir, weight_path=None, model_name='Shear', train_shear=45, epochs=3, visualize=True)  <pre>/usr/local/lib/python3.8/dist-packages/albumentations/imgaug/transforms.py:346: FutureWarning:\n\nThis IAAAffine is deprecated. Please use Affine instead\n\n</pre> In\u00a0[7]: Copied! <pre>from fastestimator.util import GridDisplay, BatchDisplay\n\nshear_results = shear_pipe.get_results()\nno_aug_results = no_aug_pipe.get_results()\n\nsample_num = 3\n\nfig = GridDisplay([\n    BatchDisplay(image=no_aug_results['x'][0:sample_num], title=\"Pipeline Input\"),\n    BatchDisplay(image=shear_results['x'][0:sample_num], title=\"Pipeline Output\")\n])\nfig.show()\n</pre> from fastestimator.util import GridDisplay, BatchDisplay  shear_results = shear_pipe.get_results() no_aug_results = no_aug_pipe.get_results()  sample_num = 3  fig = GridDisplay([     BatchDisplay(image=no_aug_results['x'][0:sample_num], title=\"Pipeline Input\"),     BatchDisplay(image=shear_results['x'][0:sample_num], title=\"Pipeline Output\") ]) fig.show()  <p>Now that we have visualized shear operation , let us train a model while applying random shear operation in range [-45, 45] degrees.</p> In\u00a0[8]: Copied! <pre>est.fit()\n</pre> est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2861838;\nFastEstimator-Train: step: 100; ce: 2.005511; steps/sec: 9.38;\nFastEstimator-Train: step: 200; ce: 1.6937112; steps/sec: 6.46;\nFastEstimator-Train: step: 300; ce: 1.8294314; steps/sec: 4.56;\nFastEstimator-Train: step: 400; ce: 1.4309876; steps/sec: 6.09;\nFastEstimator-Train: step: 500; ce: 1.5529174; steps/sec: 8.53;\nFastEstimator-Train: step: 600; ce: 1.8520718; steps/sec: 12.41;\nFastEstimator-Train: step: 700; ce: 1.3964385; steps/sec: 5.37;\nFastEstimator-Train: step: 800; ce: 1.5918491; steps/sec: 8.14;\nFastEstimator-Train: step: 900; ce: 1.7113352; steps/sec: 12.04;\nFastEstimator-Train: step: 1000; ce: 1.4144588; steps/sec: 3.41;\nFastEstimator-Train: step: 1100; ce: 1.6062101; steps/sec: 7.75;\nFastEstimator-Train: step: 1200; ce: 1.4574857; steps/sec: 10.48;\nFastEstimator-Train: step: 1300; ce: 1.7639947; steps/sec: 13.06;\nFastEstimator-Train: step: 1400; ce: 1.5409832; steps/sec: 11.45;\nFastEstimator-Train: step: 1500; ce: 1.4090402; steps/sec: 10.2;\nFastEstimator-Train: step: 1563; epoch: 1; epoch_time(sec): 208.05;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 14.17;\nEval Progress: 104/157; steps/sec: 11.21;\nEval Progress: 157/157; steps/sec: 27.28;\nFastEstimator-Eval: step: 1563; epoch: 1; accuracy: 0.5006; ce: 1.3493171;\nFastEstimator-Train: step: 1600; ce: 1.5268304; steps/sec: 8.59;\nFastEstimator-Train: step: 1700; ce: 1.5136603; steps/sec: 8.34;\nFastEstimator-Train: step: 1800; ce: 1.2788031; steps/sec: 6.0;\nFastEstimator-Train: step: 1900; ce: 1.2211283; steps/sec: 12.23;\nFastEstimator-Train: step: 2000; ce: 1.2741537; steps/sec: 11.08;\nFastEstimator-Train: step: 2100; ce: 1.327008; steps/sec: 6.86;\nFastEstimator-Train: step: 2200; ce: 1.7329004; steps/sec: 8.57;\nFastEstimator-Train: step: 2300; ce: 1.5304513; steps/sec: 11.48;\nFastEstimator-Train: step: 2400; ce: 1.1817951; steps/sec: 13.21;\nFastEstimator-Train: step: 2500; ce: 1.4848573; steps/sec: 13.29;\nFastEstimator-Train: step: 2600; ce: 1.4725429; steps/sec: 14.05;\nFastEstimator-Train: step: 2700; ce: 1.3283855; steps/sec: 8.0;\nFastEstimator-Train: step: 2800; ce: 1.3344322; steps/sec: 3.3;\nFastEstimator-Train: step: 2900; ce: 1.3632934; steps/sec: 3.59;\nFastEstimator-Train: step: 3000; ce: 1.4991808; steps/sec: 13.95;\nFastEstimator-Train: step: 3100; ce: 1.3375795; steps/sec: 9.72;\nFastEstimator-Train: step: 3126; epoch: 2; epoch_time(sec): 199.82;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 44.0;\nEval Progress: 104/157; steps/sec: 77.09;\nEval Progress: 157/157; steps/sec: 53.15;\nFastEstimator-Eval: step: 3126; epoch: 2; accuracy: 0.5242; ce: 1.3092657;\nFastEstimator-Train: step: 3200; ce: 1.2650018; steps/sec: 10.98;\nFastEstimator-Train: step: 3300; ce: 1.169062; steps/sec: 11.88;\nFastEstimator-Train: step: 3400; ce: 1.6375405; steps/sec: 13.0;\nFastEstimator-Train: step: 3500; ce: 1.2077605; steps/sec: 11.34;\nFastEstimator-Train: step: 3600; ce: 1.4420668; steps/sec: 6.71;\nFastEstimator-Train: step: 3700; ce: 1.0475363; steps/sec: 11.78;\nFastEstimator-Train: step: 3800; ce: 1.25505; steps/sec: 12.77;\nFastEstimator-Train: step: 3900; ce: 1.6666105; steps/sec: 14.69;\nFastEstimator-Train: step: 4000; ce: 1.1326865; steps/sec: 15.31;\nFastEstimator-Train: step: 4100; ce: 1.667225; steps/sec: 17.02;\nFastEstimator-Train: step: 4200; ce: 0.94082505; steps/sec: 14.59;\nFastEstimator-Train: step: 4300; ce: 1.6120632; steps/sec: 12.3;\nFastEstimator-Train: step: 4400; ce: 1.4536572; steps/sec: 6.06;\nFastEstimator-Train: step: 4500; ce: 1.0896178; steps/sec: 13.95;\nFastEstimator-Train: step: 4600; ce: 1.3797795; steps/sec: 15.07;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp26og0a4e/Shear_epoch_3.pt\nFastEstimator-Train: step: 4689; epoch: 3; epoch_time(sec): 133.64;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 103.06;\nEval Progress: 104/157; steps/sec: 38.39;\nEval Progress: 157/157; steps/sec: 108.79;\nFastEstimator-Eval: step: 4689; epoch: 3; accuracy: 0.5906; ce: 1.1530296;\nFastEstimator-Finish: step: 4689; Shear_lr: 0.001; total_time(sec): 559.21;\n</pre> <p>Now that we have trained this new model of ours, let's use Grid Search again to test the performance of the model while the input images are rotated in range 0 to 360 degrees(at an interval of 10)</p> In\u00a0[9]: Copied! <pre>weight_path = os.path.join(save_dir, 'Shear_epoch_3.pt')\n\nrot = list(range(0, 360, 10))\n\nshear_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Shear Accuracy\"), params={\"rotate\": rot})\n\nshear_grid_search.fit()\n</pre> weight_path = os.path.join(save_dir, 'Shear_epoch_3.pt')  rot = list(range(0, 360, 10))  shear_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Shear Accuracy\"), params={\"rotate\": rot})  shear_grid_search.fit() <pre>FastEstimator-Test: step: None; epoch: 3; accuracy: 0.5988; ce: 1.1352092;\nFastEstimator-Search: Evaluated {'rotate': 0, 'search_idx': 1}, result: {'Shear Accuracy': 0.5988}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5908; ce: 1.1508583;\nFastEstimator-Search: Evaluated {'rotate': 10, 'search_idx': 2}, result: {'Shear Accuracy': 0.5908}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5784; ce: 1.2101475;\nFastEstimator-Search: Evaluated {'rotate': 20, 'search_idx': 3}, result: {'Shear Accuracy': 0.5784}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5358; ce: 1.3203115;\nFastEstimator-Search: Evaluated {'rotate': 30, 'search_idx': 4}, result: {'Shear Accuracy': 0.5358}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.485; ce: 1.4910775;\nFastEstimator-Search: Evaluated {'rotate': 40, 'search_idx': 5}, result: {'Shear Accuracy': 0.485}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.427; ce: 1.7015682;\nFastEstimator-Search: Evaluated {'rotate': 50, 'search_idx': 6}, result: {'Shear Accuracy': 0.427}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3684; ce: 1.9068954;\nFastEstimator-Search: Evaluated {'rotate': 60, 'search_idx': 7}, result: {'Shear Accuracy': 0.3684}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3346; ce: 2.0805237;\nFastEstimator-Search: Evaluated {'rotate': 70, 'search_idx': 8}, result: {'Shear Accuracy': 0.3346}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2974; ce: 2.262713;\nFastEstimator-Search: Evaluated {'rotate': 80, 'search_idx': 9}, result: {'Shear Accuracy': 0.2974}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2952; ce: 2.2669344;\nFastEstimator-Search: Evaluated {'rotate': 90, 'search_idx': 10}, result: {'Shear Accuracy': 0.2952}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2602; ce: 2.533539;\nFastEstimator-Search: Evaluated {'rotate': 100, 'search_idx': 11}, result: {'Shear Accuracy': 0.2602}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2584; ce: 2.58759;\nFastEstimator-Search: Evaluated {'rotate': 110, 'search_idx': 12}, result: {'Shear Accuracy': 0.2584}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2612; ce: 2.600429;\nFastEstimator-Search: Evaluated {'rotate': 120, 'search_idx': 13}, result: {'Shear Accuracy': 0.2612}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.27; ce: 2.5737288;\nFastEstimator-Search: Evaluated {'rotate': 130, 'search_idx': 14}, result: {'Shear Accuracy': 0.27}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2832; ce: 2.4878793;\nFastEstimator-Search: Evaluated {'rotate': 140, 'search_idx': 15}, result: {'Shear Accuracy': 0.2832}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2948; ce: 2.3791826;\nFastEstimator-Search: Evaluated {'rotate': 150, 'search_idx': 16}, result: {'Shear Accuracy': 0.2948}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3062; ce: 2.2879615;\nFastEstimator-Search: Evaluated {'rotate': 160, 'search_idx': 17}, result: {'Shear Accuracy': 0.3062}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3092; ce: 2.2508817;\nFastEstimator-Search: Evaluated {'rotate': 170, 'search_idx': 18}, result: {'Shear Accuracy': 0.3092}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3184; ce: 2.1892116;\nFastEstimator-Search: Evaluated {'rotate': 180, 'search_idx': 19}, result: {'Shear Accuracy': 0.3184}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3072; ce: 2.2258189;\nFastEstimator-Search: Evaluated {'rotate': 190, 'search_idx': 20}, result: {'Shear Accuracy': 0.3072}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3; ce: 2.2513013;\nFastEstimator-Search: Evaluated {'rotate': 200, 'search_idx': 21}, result: {'Shear Accuracy': 0.3}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2894; ce: 2.3335693;\nFastEstimator-Search: Evaluated {'rotate': 210, 'search_idx': 22}, result: {'Shear Accuracy': 0.2894}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2692; ce: 2.463834;\nFastEstimator-Search: Evaluated {'rotate': 220, 'search_idx': 23}, result: {'Shear Accuracy': 0.2692}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2524; ce: 2.5842586;\nFastEstimator-Search: Evaluated {'rotate': 230, 'search_idx': 24}, result: {'Shear Accuracy': 0.2524}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2506; ce: 2.62961;\nFastEstimator-Search: Evaluated {'rotate': 240, 'search_idx': 25}, result: {'Shear Accuracy': 0.2506}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2468; ce: 2.6211019;\nFastEstimator-Search: Evaluated {'rotate': 250, 'search_idx': 26}, result: {'Shear Accuracy': 0.2468}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2494; ce: 2.5996192;\nFastEstimator-Search: Evaluated {'rotate': 260, 'search_idx': 27}, result: {'Shear Accuracy': 0.2494}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2814; ce: 2.3607845;\nFastEstimator-Search: Evaluated {'rotate': 270, 'search_idx': 28}, result: {'Shear Accuracy': 0.2814}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.2798; ce: 2.3904302;\nFastEstimator-Search: Evaluated {'rotate': 280, 'search_idx': 29}, result: {'Shear Accuracy': 0.2798}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3108; ce: 2.2020493;\nFastEstimator-Search: Evaluated {'rotate': 290, 'search_idx': 30}, result: {'Shear Accuracy': 0.3108}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3652; ce: 1.9952031;\nFastEstimator-Search: Evaluated {'rotate': 300, 'search_idx': 31}, result: {'Shear Accuracy': 0.3652}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4182; ce: 1.7621305;\nFastEstimator-Search: Evaluated {'rotate': 310, 'search_idx': 32}, result: {'Shear Accuracy': 0.4182}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4808; ce: 1.5279862;\nFastEstimator-Search: Evaluated {'rotate': 320, 'search_idx': 33}, result: {'Shear Accuracy': 0.4808}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.538; ce: 1.3273461;\nFastEstimator-Search: Evaluated {'rotate': 330, 'search_idx': 34}, result: {'Shear Accuracy': 0.538}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5756; ce: 1.2092421;\nFastEstimator-Search: Evaluated {'rotate': 340, 'search_idx': 35}, result: {'Shear Accuracy': 0.5756}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5936; ce: 1.1571609;\nFastEstimator-Search: Evaluated {'rotate': 350, 'search_idx': 36}, result: {'Shear Accuracy': 0.5936}\n</pre> In\u00a0[10]: Copied! <pre>visualize_search(search=shear_grid_search, title=\"Model Robustness With Shear\")\n</pre> visualize_search(search=shear_grid_search, title=\"Model Robustness With Shear\") <p></p> In\u00a0[11]: Copied! <pre>est,rotate_pipe = get_estimator(save_dir, weight_path=None, model_name='Rotation', train_rotate=90, epochs=3, visualize=True)\n</pre> est,rotate_pipe = get_estimator(save_dir, weight_path=None, model_name='Rotation', train_rotate=90, epochs=3, visualize=True) In\u00a0[12]: Copied! <pre>rotate_results = rotate_pipe.get_results()\nno_aug_results = no_aug_pipe.get_results()\n\nsample_num = 3\n\nfig = GridDisplay([\n    BatchDisplay(image=no_aug_results['x'][0:sample_num], title=\"Pipeline Input\"),\n    BatchDisplay(image=rotate_results['x'][0:sample_num], title=\"Pipeline Output\")\n])\nfig.show()\n</pre> rotate_results = rotate_pipe.get_results() no_aug_results = no_aug_pipe.get_results()  sample_num = 3  fig = GridDisplay([     BatchDisplay(image=no_aug_results['x'][0:sample_num], title=\"Pipeline Input\"),     BatchDisplay(image=rotate_results['x'][0:sample_num], title=\"Pipeline Output\") ]) fig.show()  In\u00a0[13]: Copied! <pre>est.fit()\n</pre> est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.299035;\nFastEstimator-Train: step: 100; ce: 2.075551; steps/sec: 6.59;\nFastEstimator-Train: step: 200; ce: 1.96548; steps/sec: 14.31;\nFastEstimator-Train: step: 300; ce: 1.6383593; steps/sec: 7.94;\nFastEstimator-Train: step: 400; ce: 1.8449334; steps/sec: 8.19;\nFastEstimator-Train: step: 500; ce: 1.7785522; steps/sec: 11.14;\nFastEstimator-Train: step: 600; ce: 1.8474475; steps/sec: 10.75;\nFastEstimator-Train: step: 700; ce: 1.697898; steps/sec: 8.71;\nFastEstimator-Train: step: 800; ce: 1.5075471; steps/sec: 4.63;\nFastEstimator-Train: step: 900; ce: 1.6469618; steps/sec: 6.64;\nFastEstimator-Train: step: 1000; ce: 1.5503968; steps/sec: 9.44;\nFastEstimator-Train: step: 1100; ce: 1.711024; steps/sec: 9.04;\nFastEstimator-Train: step: 1200; ce: 1.5141683; steps/sec: 12.01;\nFastEstimator-Train: step: 1300; ce: 1.781623; steps/sec: 13.39;\nFastEstimator-Train: step: 1400; ce: 1.5333574; steps/sec: 10.26;\nFastEstimator-Train: step: 1500; ce: 1.853748; steps/sec: 11.98;\nFastEstimator-Train: step: 1563; epoch: 1; epoch_time(sec): 179.68;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 31.86;\nEval Progress: 104/157; steps/sec: 61.86;\nEval Progress: 157/157; steps/sec: 70.95;\nFastEstimator-Eval: step: 1563; epoch: 1; accuracy: 0.4368; ce: 1.5425291;\nFastEstimator-Train: step: 1600; ce: 1.4006021; steps/sec: 7.67;\nFastEstimator-Train: step: 1700; ce: 1.3395236; steps/sec: 10.88;\nFastEstimator-Train: step: 1800; ce: 1.498162; steps/sec: 15.62;\nFastEstimator-Train: step: 1900; ce: 1.5839293; steps/sec: 10.67;\nFastEstimator-Train: step: 2000; ce: 1.149652; steps/sec: 7.71;\nFastEstimator-Train: step: 2100; ce: 1.7589302; steps/sec: 10.33;\nFastEstimator-Train: step: 2200; ce: 1.4050286; steps/sec: 7.69;\nFastEstimator-Train: step: 2300; ce: 1.7004766; steps/sec: 13.79;\nFastEstimator-Train: step: 2400; ce: 1.3629683; steps/sec: 13.57;\nFastEstimator-Train: step: 2500; ce: 1.5042443; steps/sec: 7.83;\nFastEstimator-Train: step: 2600; ce: 1.4782196; steps/sec: 10.63;\nFastEstimator-Train: step: 2700; ce: 1.9164765; steps/sec: 9.35;\nFastEstimator-Train: step: 2800; ce: 1.4845995; steps/sec: 8.12;\nFastEstimator-Train: step: 2900; ce: 1.1254141; steps/sec: 7.15;\nFastEstimator-Train: step: 3000; ce: 1.6342312; steps/sec: 8.51;\nFastEstimator-Train: step: 3100; ce: 1.0823914; steps/sec: 10.25;\nFastEstimator-Train: step: 3126; epoch: 2; epoch_time(sec): 161.58;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 30.03;\nEval Progress: 104/157; steps/sec: 46.97;\nEval Progress: 157/157; steps/sec: 35.09;\nFastEstimator-Eval: step: 3126; epoch: 2; accuracy: 0.493; ce: 1.4054103;\nFastEstimator-Train: step: 3200; ce: 1.474694; steps/sec: 8.11;\nFastEstimator-Train: step: 3300; ce: 1.22069; steps/sec: 3.53;\nFastEstimator-Train: step: 3400; ce: 1.3306724; steps/sec: 3.82;\nFastEstimator-Train: step: 3500; ce: 1.6650531; steps/sec: 13.52;\nFastEstimator-Train: step: 3600; ce: 1.6610831; steps/sec: 13.37;\nFastEstimator-Train: step: 3700; ce: 1.4613279; steps/sec: 8.74;\nFastEstimator-Train: step: 3800; ce: 1.1969187; steps/sec: 12.35;\nFastEstimator-Train: step: 3900; ce: 1.2569786; steps/sec: 7.64;\nFastEstimator-Train: step: 4000; ce: 1.191232; steps/sec: 3.82;\nFastEstimator-Train: step: 4100; ce: 1.3513925; steps/sec: 10.37;\nFastEstimator-Train: step: 4200; ce: 1.3323293; steps/sec: 10.39;\nFastEstimator-Train: step: 4300; ce: 1.3886821; steps/sec: 13.58;\nFastEstimator-Train: step: 4400; ce: 0.8490051; steps/sec: 8.1;\nFastEstimator-Train: step: 4500; ce: 1.0546207; steps/sec: 13.12;\nFastEstimator-Train: step: 4600; ce: 1.5466611; steps/sec: 13.66;\nFastEstimator-ModelSaver: Saved model to /tmp/tmp26og0a4e/Rotation_epoch_3.pt\nFastEstimator-Train: step: 4689; epoch: 3; epoch_time(sec): 199.32;\nEval Progress: 1/157;\nEval Progress: 52/157; steps/sec: 33.79;\nEval Progress: 104/157; steps/sec: 95.27;\nEval Progress: 157/157; steps/sec: 89.83;\nFastEstimator-Eval: step: 4689; epoch: 3; accuracy: 0.5168; ce: 1.3441926;\nFastEstimator-Finish: step: 4689; Rotation_lr: 0.001; total_time(sec): 553.19;\n</pre> <p>Let us use Grid Search again to test the performance of the model while the input images are rotated in range 0 to 360 degrees(at an interval of 10)</p> In\u00a0[14]: Copied! <pre>weight_path = os.path.join(save_dir, 'Rotation_epoch_3.pt')\n\nrot = list(range(0, 360, 10))\n\nrotation_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Rotation Accuracy\"), params={\"rotate\": rot})\n\nrotation_grid_search.fit()\n</pre> weight_path = os.path.join(save_dir, 'Rotation_epoch_3.pt')  rot = list(range(0, 360, 10))  rotation_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: score_fn(search_idx, rotate, weight_path, save_dir, field_name=\"Rotation Accuracy\"), params={\"rotate\": rot})  rotation_grid_search.fit() <pre>FastEstimator-Test: step: None; epoch: 3; accuracy: 0.526; ce: 1.3442501;\nFastEstimator-Search: Evaluated {'rotate': 0, 'search_idx': 1}, result: {'Rotation Accuracy': 0.526}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.529; ce: 1.3476864;\nFastEstimator-Search: Evaluated {'rotate': 10, 'search_idx': 2}, result: {'Rotation Accuracy': 0.529}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5274; ce: 1.3577137;\nFastEstimator-Search: Evaluated {'rotate': 20, 'search_idx': 3}, result: {'Rotation Accuracy': 0.5274}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5238; ce: 1.3682574;\nFastEstimator-Search: Evaluated {'rotate': 30, 'search_idx': 4}, result: {'Rotation Accuracy': 0.5238}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5208; ce: 1.3765999;\nFastEstimator-Search: Evaluated {'rotate': 40, 'search_idx': 5}, result: {'Rotation Accuracy': 0.5208}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.52; ce: 1.3815999;\nFastEstimator-Search: Evaluated {'rotate': 50, 'search_idx': 6}, result: {'Rotation Accuracy': 0.52}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5024; ce: 1.3979217;\nFastEstimator-Search: Evaluated {'rotate': 60, 'search_idx': 7}, result: {'Rotation Accuracy': 0.5024}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4924; ce: 1.4351144;\nFastEstimator-Search: Evaluated {'rotate': 70, 'search_idx': 8}, result: {'Rotation Accuracy': 0.4924}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4728; ce: 1.4845449;\nFastEstimator-Search: Evaluated {'rotate': 80, 'search_idx': 9}, result: {'Rotation Accuracy': 0.4728}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4506; ce: 1.5334638;\nFastEstimator-Search: Evaluated {'rotate': 90, 'search_idx': 10}, result: {'Rotation Accuracy': 0.4506}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.413; ce: 1.6307365;\nFastEstimator-Search: Evaluated {'rotate': 100, 'search_idx': 11}, result: {'Rotation Accuracy': 0.413}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.381; ce: 1.7120235;\nFastEstimator-Search: Evaluated {'rotate': 110, 'search_idx': 12}, result: {'Rotation Accuracy': 0.381}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3628; ce: 1.7878714;\nFastEstimator-Search: Evaluated {'rotate': 120, 'search_idx': 13}, result: {'Rotation Accuracy': 0.3628}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3404; ce: 1.8583707;\nFastEstimator-Search: Evaluated {'rotate': 130, 'search_idx': 14}, result: {'Rotation Accuracy': 0.3404}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3232; ce: 1.9118625;\nFastEstimator-Search: Evaluated {'rotate': 140, 'search_idx': 15}, result: {'Rotation Accuracy': 0.3232}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3132; ce: 1.9561087;\nFastEstimator-Search: Evaluated {'rotate': 150, 'search_idx': 16}, result: {'Rotation Accuracy': 0.3132}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3096; ce: 1.982389;\nFastEstimator-Search: Evaluated {'rotate': 160, 'search_idx': 17}, result: {'Rotation Accuracy': 0.3096}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3082; ce: 2.002314;\nFastEstimator-Search: Evaluated {'rotate': 170, 'search_idx': 18}, result: {'Rotation Accuracy': 0.3082}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.31; ce: 1.9804295;\nFastEstimator-Search: Evaluated {'rotate': 180, 'search_idx': 19}, result: {'Rotation Accuracy': 0.31}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3014; ce: 2.0179684;\nFastEstimator-Search: Evaluated {'rotate': 190, 'search_idx': 20}, result: {'Rotation Accuracy': 0.3014}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3058; ce: 2.0084887;\nFastEstimator-Search: Evaluated {'rotate': 200, 'search_idx': 21}, result: {'Rotation Accuracy': 0.3058}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3106; ce: 1.9818201;\nFastEstimator-Search: Evaluated {'rotate': 210, 'search_idx': 22}, result: {'Rotation Accuracy': 0.3106}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.322; ce: 1.9469423;\nFastEstimator-Search: Evaluated {'rotate': 220, 'search_idx': 23}, result: {'Rotation Accuracy': 0.322}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3334; ce: 1.891612;\nFastEstimator-Search: Evaluated {'rotate': 230, 'search_idx': 24}, result: {'Rotation Accuracy': 0.3334}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.35; ce: 1.8142512;\nFastEstimator-Search: Evaluated {'rotate': 240, 'search_idx': 25}, result: {'Rotation Accuracy': 0.35}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.3808; ce: 1.7215782;\nFastEstimator-Search: Evaluated {'rotate': 250, 'search_idx': 26}, result: {'Rotation Accuracy': 0.3808}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4116; ce: 1.6249053;\nFastEstimator-Search: Evaluated {'rotate': 260, 'search_idx': 27}, result: {'Rotation Accuracy': 0.4116}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.444; ce: 1.5260195;\nFastEstimator-Search: Evaluated {'rotate': 270, 'search_idx': 28}, result: {'Rotation Accuracy': 0.444}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4716; ce: 1.4676648;\nFastEstimator-Search: Evaluated {'rotate': 280, 'search_idx': 29}, result: {'Rotation Accuracy': 0.4716}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.4906; ce: 1.413264;\nFastEstimator-Search: Evaluated {'rotate': 290, 'search_idx': 30}, result: {'Rotation Accuracy': 0.4906}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5042; ce: 1.3752073;\nFastEstimator-Search: Evaluated {'rotate': 300, 'search_idx': 31}, result: {'Rotation Accuracy': 0.5042}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5196; ce: 1.3499327;\nFastEstimator-Search: Evaluated {'rotate': 310, 'search_idx': 32}, result: {'Rotation Accuracy': 0.5196}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.52; ce: 1.3427712;\nFastEstimator-Search: Evaluated {'rotate': 320, 'search_idx': 33}, result: {'Rotation Accuracy': 0.52}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5126; ce: 1.3381072;\nFastEstimator-Search: Evaluated {'rotate': 330, 'search_idx': 34}, result: {'Rotation Accuracy': 0.5126}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.5194; ce: 1.3385159;\nFastEstimator-Search: Evaluated {'rotate': 340, 'search_idx': 35}, result: {'Rotation Accuracy': 0.5194}\nFastEstimator-Test: step: None; epoch: 3; accuracy: 0.522; ce: 1.3364942;\nFastEstimator-Search: Evaluated {'rotate': 350, 'search_idx': 36}, result: {'Rotation Accuracy': 0.522}\n</pre> In\u00a0[15]: Copied! <pre>visualize_search(search=rotation_grid_search, title=\"Model Robustness With Rotation\")\n</pre> visualize_search(search=rotation_grid_search, title=\"Model Robustness With Rotation\") <p>The above plot proves that if we introduce random rotation while training, we can make model more robust to rotation. The model is still not robust to the range [90, 270] while testing since it was not trained on the same.</p> <p></p> In\u00a0[16]: Copied! <pre>def comparison_fn(search_idx, rotate, no_aug_results, shear_results, rotation_results):\n    acc = no_aug_results[rotate//10]['result']\n    shear_acc = shear_results[rotate//10]['result']\n    rotation_acc = rotation_results[rotate//10]['result']\n    \n    \n    return {**acc, **shear_acc, **rotation_acc}\n\nno_aug_results = no_aug_grid_search.get_search_summary()\nshear_results = shear_grid_search.get_search_summary()\nrotation_results = rotation_grid_search.get_search_summary()\n\nrot = list(range(0, 360, 10))\n\nmodel_comparison_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: comparison_fn(search_idx, rotate, no_aug_results, shear_results, rotation_results), params={\"rotate\": rot})\n\nmodel_comparison_grid_search.fit()\n</pre> def comparison_fn(search_idx, rotate, no_aug_results, shear_results, rotation_results):     acc = no_aug_results[rotate//10]['result']     shear_acc = shear_results[rotate//10]['result']     rotation_acc = rotation_results[rotate//10]['result']               return {**acc, **shear_acc, **rotation_acc}  no_aug_results = no_aug_grid_search.get_search_summary() shear_results = shear_grid_search.get_search_summary() rotation_results = rotation_grid_search.get_search_summary()  rot = list(range(0, 360, 10))  model_comparison_grid_search = GridSearch(eval_fn=lambda search_idx, rotate: comparison_fn(search_idx, rotate, no_aug_results, shear_results, rotation_results), params={\"rotate\": rot})  model_comparison_grid_search.fit() <pre>FastEstimator-Search: Evaluated {'rotate': 0, 'search_idx': 1}, result: {'Accuracy without Augmentation': 0.6664, 'Shear Accuracy': 0.5988, 'Rotation Accuracy': 0.526}\nFastEstimator-Search: Evaluated {'rotate': 10, 'search_idx': 2}, result: {'Accuracy without Augmentation': 0.6422, 'Shear Accuracy': 0.5908, 'Rotation Accuracy': 0.529}\nFastEstimator-Search: Evaluated {'rotate': 20, 'search_idx': 3}, result: {'Accuracy without Augmentation': 0.5804, 'Shear Accuracy': 0.5784, 'Rotation Accuracy': 0.5274}\nFastEstimator-Search: Evaluated {'rotate': 30, 'search_idx': 4}, result: {'Accuracy without Augmentation': 0.4862, 'Shear Accuracy': 0.5358, 'Rotation Accuracy': 0.5238}\nFastEstimator-Search: Evaluated {'rotate': 40, 'search_idx': 5}, result: {'Accuracy without Augmentation': 0.3996, 'Shear Accuracy': 0.485, 'Rotation Accuracy': 0.5208}\nFastEstimator-Search: Evaluated {'rotate': 50, 'search_idx': 6}, result: {'Accuracy without Augmentation': 0.3414, 'Shear Accuracy': 0.427, 'Rotation Accuracy': 0.52}\nFastEstimator-Search: Evaluated {'rotate': 60, 'search_idx': 7}, result: {'Accuracy without Augmentation': 0.3072, 'Shear Accuracy': 0.3684, 'Rotation Accuracy': 0.5024}\nFastEstimator-Search: Evaluated {'rotate': 70, 'search_idx': 8}, result: {'Accuracy without Augmentation': 0.2864, 'Shear Accuracy': 0.3346, 'Rotation Accuracy': 0.4924}\nFastEstimator-Search: Evaluated {'rotate': 80, 'search_idx': 9}, result: {'Accuracy without Augmentation': 0.2818, 'Shear Accuracy': 0.2974, 'Rotation Accuracy': 0.4728}\nFastEstimator-Search: Evaluated {'rotate': 90, 'search_idx': 10}, result: {'Accuracy without Augmentation': 0.2932, 'Shear Accuracy': 0.2952, 'Rotation Accuracy': 0.4506}\nFastEstimator-Search: Evaluated {'rotate': 100, 'search_idx': 11}, result: {'Accuracy without Augmentation': 0.256, 'Shear Accuracy': 0.2602, 'Rotation Accuracy': 0.413}\nFastEstimator-Search: Evaluated {'rotate': 110, 'search_idx': 12}, result: {'Accuracy without Augmentation': 0.2414, 'Shear Accuracy': 0.2584, 'Rotation Accuracy': 0.381}\nFastEstimator-Search: Evaluated {'rotate': 120, 'search_idx': 13}, result: {'Accuracy without Augmentation': 0.2352, 'Shear Accuracy': 0.2612, 'Rotation Accuracy': 0.3628}\nFastEstimator-Search: Evaluated {'rotate': 130, 'search_idx': 14}, result: {'Accuracy without Augmentation': 0.2346, 'Shear Accuracy': 0.27, 'Rotation Accuracy': 0.3404}\nFastEstimator-Search: Evaluated {'rotate': 140, 'search_idx': 15}, result: {'Accuracy without Augmentation': 0.2496, 'Shear Accuracy': 0.2832, 'Rotation Accuracy': 0.3232}\nFastEstimator-Search: Evaluated {'rotate': 150, 'search_idx': 16}, result: {'Accuracy without Augmentation': 0.2694, 'Shear Accuracy': 0.2948, 'Rotation Accuracy': 0.3132}\nFastEstimator-Search: Evaluated {'rotate': 160, 'search_idx': 17}, result: {'Accuracy without Augmentation': 0.301, 'Shear Accuracy': 0.3062, 'Rotation Accuracy': 0.3096}\nFastEstimator-Search: Evaluated {'rotate': 170, 'search_idx': 18}, result: {'Accuracy without Augmentation': 0.316, 'Shear Accuracy': 0.3092, 'Rotation Accuracy': 0.3082}\nFastEstimator-Search: Evaluated {'rotate': 180, 'search_idx': 19}, result: {'Accuracy without Augmentation': 0.3428, 'Shear Accuracy': 0.3184, 'Rotation Accuracy': 0.31}\nFastEstimator-Search: Evaluated {'rotate': 190, 'search_idx': 20}, result: {'Accuracy without Augmentation': 0.3158, 'Shear Accuracy': 0.3072, 'Rotation Accuracy': 0.3014}\nFastEstimator-Search: Evaluated {'rotate': 200, 'search_idx': 21}, result: {'Accuracy without Augmentation': 0.3002, 'Shear Accuracy': 0.3, 'Rotation Accuracy': 0.3058}\nFastEstimator-Search: Evaluated {'rotate': 210, 'search_idx': 22}, result: {'Accuracy without Augmentation': 0.2768, 'Shear Accuracy': 0.2894, 'Rotation Accuracy': 0.3106}\nFastEstimator-Search: Evaluated {'rotate': 220, 'search_idx': 23}, result: {'Accuracy without Augmentation': 0.2486, 'Shear Accuracy': 0.2692, 'Rotation Accuracy': 0.322}\nFastEstimator-Search: Evaluated {'rotate': 230, 'search_idx': 24}, result: {'Accuracy without Augmentation': 0.2346, 'Shear Accuracy': 0.2524, 'Rotation Accuracy': 0.3334}\nFastEstimator-Search: Evaluated {'rotate': 240, 'search_idx': 25}, result: {'Accuracy without Augmentation': 0.2364, 'Shear Accuracy': 0.2506, 'Rotation Accuracy': 0.35}\nFastEstimator-Search: Evaluated {'rotate': 250, 'search_idx': 26}, result: {'Accuracy without Augmentation': 0.2488, 'Shear Accuracy': 0.2468, 'Rotation Accuracy': 0.3808}\nFastEstimator-Search: Evaluated {'rotate': 260, 'search_idx': 27}, result: {'Accuracy without Augmentation': 0.2634, 'Shear Accuracy': 0.2494, 'Rotation Accuracy': 0.4116}\nFastEstimator-Search: Evaluated {'rotate': 270, 'search_idx': 28}, result: {'Accuracy without Augmentation': 0.2904, 'Shear Accuracy': 0.2814, 'Rotation Accuracy': 0.444}\nFastEstimator-Search: Evaluated {'rotate': 280, 'search_idx': 29}, result: {'Accuracy without Augmentation': 0.2768, 'Shear Accuracy': 0.2798, 'Rotation Accuracy': 0.4716}\nFastEstimator-Search: Evaluated {'rotate': 290, 'search_idx': 30}, result: {'Accuracy without Augmentation': 0.277, 'Shear Accuracy': 0.3108, 'Rotation Accuracy': 0.4906}\nFastEstimator-Search: Evaluated {'rotate': 300, 'search_idx': 31}, result: {'Accuracy without Augmentation': 0.2798, 'Shear Accuracy': 0.3652, 'Rotation Accuracy': 0.5042}\nFastEstimator-Search: Evaluated {'rotate': 310, 'search_idx': 32}, result: {'Accuracy without Augmentation': 0.3082, 'Shear Accuracy': 0.4182, 'Rotation Accuracy': 0.5196}\nFastEstimator-Search: Evaluated {'rotate': 320, 'search_idx': 33}, result: {'Accuracy without Augmentation': 0.3678, 'Shear Accuracy': 0.4808, 'Rotation Accuracy': 0.52}\nFastEstimator-Search: Evaluated {'rotate': 330, 'search_idx': 34}, result: {'Accuracy without Augmentation': 0.478, 'Shear Accuracy': 0.538, 'Rotation Accuracy': 0.5126}\nFastEstimator-Search: Evaluated {'rotate': 340, 'search_idx': 35}, result: {'Accuracy without Augmentation': 0.5888, 'Shear Accuracy': 0.5756, 'Rotation Accuracy': 0.5194}\nFastEstimator-Search: Evaluated {'rotate': 350, 'search_idx': 36}, result: {'Accuracy without Augmentation': 0.646, 'Shear Accuracy': 0.5936, 'Rotation Accuracy': 0.522}\n</pre> In\u00a0[17]: Copied! <pre>visualize_search(search=model_comparison_grid_search, title=\"Model Robustness\",groups=[['Accuracy without Augmentation','Shear Accuracy','Rotation Accuracy']])\n</pre> visualize_search(search=model_comparison_grid_search, title=\"Model Robustness\",groups=[['Accuracy without Augmentation','Shear Accuracy','Rotation Accuracy']]) <p>It can be seen that by randomly introducing shear and rotation to the input images while training makes model more robust to rotation. So with augmentation we can make the model aware of rotation and increase robustness of the model.</p> <p><code>NOTE : All the three models compared above were trained only for 3 epochs on LeNet. One can achieve a much better performance by training long enough and choosing a different architecture.</code></p>"}, {"location": "tutorial/advanced/t16_robustness.html#advanced-tutorial-16-model-robustness", "title": "Advanced Tutorial 16: Model Robustness\u00b6", "text": ""}, {"location": "tutorial/advanced/t16_robustness.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>How robust a model is at testing?</li> <li>How Data augmentation can help improve rotation robustness?<ul> <li>The effect of applying shear augmentation</li> <li>The effect of applying rotation augmentation</li> <li>Model Comparison</li> </ul> </li> </ul>"}, {"location": "tutorial/advanced/t16_robustness.html#how-robust-a-model-is-at-testing", "title": "How robust a model is at testing?\u00b6", "text": "<p>Robustness is the ability of a model to estimate reliably when inputs are influenced by different conditions or when model\u2019s assumptions are not fully satisfied. In this tutorial we are going to introduce model robustness and how we can use FastEstimator Search API and Visualization API to check model robustness.</p> <p>In this tutorial, we will test model\u2019s capability at handling rotation varieties. First, let\u2019s design a generic <code>get_estimator</code> function to be used in all our experiments in this tutorial and ensure that we use same test and train set for each experiment.</p>"}, {"location": "tutorial/advanced/t16_robustness.html#how-data-augmentation-can-help-improve-rotation-robustness", "title": "How Data augmentation can help improve rotation robustness?\u00b6", "text": "<p>It is clear from last experiment that as the angle of rotation increases the performance drops. This shows that the trained model is not robust to rotation while testing. Can we improve rotation robustness by augmentating training images?</p> <p></p>"}, {"location": "tutorial/advanced/t16_robustness.html#the-effect-of-applying-shear-augmentation", "title": "The effect of applying shear augmentation\u00b6", "text": "<p>Shear is augmentation to move one or two sides of the image, turning a square or rectangle image into a trapezoidal image. Would training a model with Shear augmentation while training make it robust to rotation? Let's visualize the shear operation first.</p>"}, {"location": "tutorial/advanced/t16_robustness.html#the-effect-of-applying-rotation-augmentation", "title": "The effect of applying rotation augmentation\u00b6", "text": "<p>To test this out let's train the model while randomly rotating images in range[-90, 90]. First, let's visalize how the rotation operation looks.</p>"}, {"location": "tutorial/advanced/t16_robustness.html#model-comparison", "title": "Model Comparison\u00b6", "text": "<p>Let's compare all the trained model to compare model robustness to rotation</p>"}, {"location": "tutorial/beginner/t01_getting_started.html", "title": "Tutorial 1: Getting Started", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\ntrain_data, eval_data = mnist.load_data()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  train_data, eval_data = mnist.load_data()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[2]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\") \n    ])\n</pre> from fastestimator.architecture.tensorflow import LeNet # from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model  from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")      ]) In\u00a0[3]: Copied! <pre>from fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nimport tempfile\n\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=traces)\n</pre> from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver import tempfile  traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),           BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=traces) In\u00a0[4]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.308363;\nFastEstimator-Train: step: 100; ce: 0.32775605; steps/sec: 76.05;\nFastEstimator-Train: step: 200; ce: 0.09257892; steps/sec: 73.5;\nFastEstimator-Train: step: 300; ce: 0.34083164; steps/sec: 76.4;\nFastEstimator-Train: step: 400; ce: 0.1040692; steps/sec: 74.74;\nFastEstimator-Train: step: 500; ce: 0.021515703; steps/sec: 77.2;\nFastEstimator-Train: step: 600; ce: 0.06389343; steps/sec: 76.94;\nFastEstimator-Train: step: 700; ce: 0.081933156; steps/sec: 76.56;\nFastEstimator-Train: step: 800; ce: 0.04755495; steps/sec: 75.82;\nFastEstimator-Train: step: 900; ce: 0.09036083; steps/sec: 76.68;\nFastEstimator-Train: step: 1000; ce: 0.076977566; steps/sec: 73.93;\nFastEstimator-Train: step: 1100; ce: 0.006916199; steps/sec: 74.92;\nFastEstimator-Train: step: 1200; ce: 0.116034895; steps/sec: 72.66;\nFastEstimator-Train: step: 1300; ce: 0.0036065953; steps/sec: 73.45;\nFastEstimator-Train: step: 1400; ce: 0.1516164; steps/sec: 73.41;\nFastEstimator-Train: step: 1500; ce: 0.066313066; steps/sec: 73.17;\nFastEstimator-Train: step: 1600; ce: 0.1330988; steps/sec: 71.1;\nFastEstimator-Train: step: 1700; ce: 0.08547261; steps/sec: 70.95;\nFastEstimator-Train: step: 1800; ce: 0.024575546; steps/sec: 72.46;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 26.76 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 222.3;\nEval Progress: 208/312; steps/sec: 213.34;\nEval Progress: 312/312; steps/sec: 210.13;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpz8b4p4k4/model_best_accuracy.h5\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.9874; ce: 0.042314235; max_accuracy: 0.9874; since_best_accuracy: 0;\nFastEstimator-Train: step: 1900; ce: 0.018547835; steps/sec: 51.8;\nFastEstimator-Train: step: 2000; ce: 0.02083261; steps/sec: 72.46;\nFastEstimator-Train: step: 2100; ce: 0.0013426461; steps/sec: 72.69;\nFastEstimator-Train: step: 2200; ce: 0.03557183; steps/sec: 72.89;\nFastEstimator-Train: step: 2300; ce: 0.016057294; steps/sec: 71.61;\nFastEstimator-Train: step: 2400; ce: 0.019565197; steps/sec: 72.94;\nFastEstimator-Train: step: 2500; ce: 0.008084664; steps/sec: 70.73;\nFastEstimator-Train: step: 2600; ce: 0.047113482; steps/sec: 71.11;\nFastEstimator-Train: step: 2700; ce: 0.01939332; steps/sec: 73.15;\nFastEstimator-Train: step: 2800; ce: 0.0058479137; steps/sec: 70.8;\nFastEstimator-Train: step: 2900; ce: 0.0133756; steps/sec: 70.4;\nFastEstimator-Train: step: 3000; ce: 0.00542433; steps/sec: 66.53;\nFastEstimator-Train: step: 3100; ce: 0.0123183625; steps/sec: 66.0;\nFastEstimator-Train: step: 3200; ce: 0.0035992165; steps/sec: 67.31;\nFastEstimator-Train: step: 3300; ce: 0.029546442; steps/sec: 68.5;\nFastEstimator-Train: step: 3400; ce: 0.07383997; steps/sec: 71.68;\nFastEstimator-Train: step: 3500; ce: 0.0014300543; steps/sec: 71.02;\nFastEstimator-Train: step: 3600; ce: 0.00060256035; steps/sec: 69.73;\nFastEstimator-Train: step: 3700; ce: 0.034662727; steps/sec: 68.65;\nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 27.18 sec;\nEval Progress: 1/312;\nEval Progress: 104/312; steps/sec: 203.92;\nEval Progress: 208/312; steps/sec: 202.22;\nEval Progress: 312/312; steps/sec: 208.36;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpz8b4p4k4/model_best_accuracy.h5\nFastEstimator-Eval: step: 3750; epoch: 2; accuracy: 0.9896; ce: 0.03458583; max_accuracy: 0.9896; since_best_accuracy: 0;\nFastEstimator-Finish: step: 3750; model_lr: 0.001; total_time: 57.84 sec;\n</pre> In\u00a0[5]: Copied! <pre>import numpy as np\n\ndata = eval_data[0]\ndata = pipeline.transform(data, mode=\"eval\")\ndata = network.transform(data, mode=\"eval\")\n\nprint(\"Ground truth class is {}\".format(data[\"y\"][0]))\nprint(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\nimg = fe.util.BatchDisplay(title=\"x\", image=data[\"x\"])\nimg.show()\n</pre> import numpy as np  data = eval_data[0] data = pipeline.transform(data, mode=\"eval\") data = network.transform(data, mode=\"eval\")  print(\"Ground truth class is {}\".format(data[\"y\"][0])) print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"]))) img = fe.util.BatchDisplay(title=\"x\", image=data[\"x\"]) img.show() <pre>Ground truth class is 7\nPredicted class is 7\n</pre>"}, {"location": "tutorial/beginner/t01_getting_started.html#tutorial-1-getting-started", "title": "Tutorial 1: Getting Started\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#overview", "title": "Overview\u00b6", "text": "<p>Welcome to FastEstimator! In this tutorial we are going to cover:</p> <ul> <li>The three main APIs of FastEstimator: <code>Pipeline</code>, <code>Network</code>, <code>Estimator</code></li> <li>An image classification example<ul> <li>Pipeline</li> <li>Network</li> <li>Estimator</li> <li>Training</li> <li>Inferencing</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t01_getting_started.html#three-main-apis", "title": "Three main APIs\u00b6", "text": "<p>All deep learning training work\ufb02ows involve the following three essential components, each mapping to a critical API in FastEstimator.</p> <ul> <li><p>Data pipeline: extracts data from disk/RAM, performs transformations. -&gt;  <code>fe.Pipeline</code></p> </li> <li><p>Network: performs trainable and differentiable operations. -&gt;  <code>fe.Network</code></p> </li> <li><p>Training loop: combines the data pipeline and network in an iterative process. -&gt;  <code>fe.Estimator</code></p> </li> </ul>  Any deep learning task can be constructed by following the 3 main steps:"}, {"location": "tutorial/beginner/t01_getting_started.html#image-classification-example", "title": "Image Classification Example\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#step-1-pipeline", "title": "Step 1 - Pipeline\u00b6", "text": "<p>We use FastEstimator dataset API to load the MNIST dataset. Please check out Tutorial 2 for more details about the dataset API. In this case our data preprocessing involves:</p> <ol> <li>Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.</li> <li>Rescale pixel values from [0, 255] to [0, 1].</li> </ol> <p>Please check out Tutorial 3 for details about <code>Operator</code> and Tutorial 4 for <code>Pipeline</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-2-network", "title": "Step 2 - Network\u00b6", "text": "<p>The model definition can be either from <code>tf.keras.Model</code> or <code>torch.nn.Module</code>, for more info about network definitions, check out Tutorial 5. The differentiable operations during training are listed as follows:</p> <ol> <li>Feed the preprocessed images to the network and get prediction scores.</li> <li>Calculate <code>CrossEntropy</code> (loss) between prediction scores and ground truth.</li> <li>Update the model by minimizing <code>CrossEntropy</code>.</li> </ol> <p>For more info about <code>Network</code> and its operators, check out Tutorial 6.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-3-estimator", "title": "Step 3 - Estimator\u00b6", "text": "<p>We define the <code>Estimator</code> to connect the <code>Network</code> to the <code>Pipeline</code>, and compute accuracy as a validation metric. Please see Tutorial 7 for more about <code>Estimator</code> and <code>Traces</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>After training, we can do inferencing on new data with <code>Pipeline.transform</code> and <code>Netowork.transform</code>. Please checkout Tutorial 8 for more details. \\</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>DNN</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html", "title": "Tutorial 2: Creating a FastEstimator dataset", "text": "<p>A Dataset in FastEstimator is a class that wraps raw input data and makes it easier to ingest into your model(s). In this tutorial we will learn about the different ways we can create these Datasets.</p> <p>The FastEstimator Dataset class inherits from the PyTorch Dataset class which provides a clean and efficient interface to load raw data. Thus, any code that you have written for PyTorch will continue to work in FastEstimator too. For a refresher on PyTorch Datasets you can go here.</p> <p>In this tutorial we will focus on two key functionalities that we need to provide for the Dataset class. The first one is the ability to get an individual data entry from the Dataset and the second one is the ability to get the length of the Dataset. This is done as follows:</p> <ul> <li>len(dataset) should return the size (number of samples) of the dataset.</li> <li>dataset[i] should return the i-th sample in the dataset. The return value should be a dictionary with data values keyed by strings.</li> </ul> <p>Let's create a simple PyTorch Dataset which shows this functionality.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom torch.utils.data import Dataset\n\nclass mydataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data['x'].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n\na = {'x': np.random.rand(100,5), 'y': np.random.rand(100)}\nds = mydataset(a)\nprint(ds[0])\nprint(len(ds))\n</pre> import numpy as np from torch.utils.data import Dataset  class mydataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data['x'].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data}  a = {'x': np.random.rand(100,5), 'y': np.random.rand(100)} ds = mydataset(a) print(ds[0]) print(len(ds)) <pre>{'x': array([0.77730671, 0.99536305, 0.30362685, 0.82398129, 0.87116199]), 'y': 0.9211995152006527}\n100\n</pre> <p></p> <p>In this section we will showcase how a Dataset can be created using FastEstimator. This tutorial shows three ways to create Datasets. The first uses data from disk, the second uses data already in memory, and the third uses a generator to create a Dataset.</p> <p></p> <p>In this tutorial we will showcase two ways to create a Dataset from disk:</p> <p></p> <p>To showcase this we will first have to create a dummy directory structure representing the two classes. Then we create a few files in each of the directories. The following image shows the hierarchy of our temporary data directory:</p> <p></p> <p>Let's prepare the data according to the directory structure:</p> In\u00a0[2]: Copied! <pre>import os\nimport tempfile\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\na_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\nb_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n\na1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\")\na2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")\n\nb1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\")\nb2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\")\n</pre> import os import tempfile  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  a_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname) b_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)  a1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\") a2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")  b1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\") b2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\") <p>Once that is done, all you have to do is create a Dataset by passing the dummy directory to the <code>LabeledDirDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[3]: Copied! <pre>dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)  print(dataset[0]) print(len(dataset)) <pre>{'x': '/tmp/tmp4_th3s9a/tmphe1zvp3u/a2.txt', 'y': 1}\n4\n</pre> <p></p> <p>To showcase creating a Dataset based on a CSV file, we now create a dummy CSV file representing information for the two classes. First, let's create the data to be used as input as follows:</p> In\u00a0[4]: Copied! <pre>import os\nimport tempfile\nimport pandas as pd\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\ndata = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]}\ndf = pd.DataFrame(data=data)\ndf.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False)\n</pre> import os import tempfile import pandas as pd  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  data = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]} df = pd.DataFrame(data=data) df.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False) <p>Once that is done you can create a Dataset by passing the CSV to the <code>CSVDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[5]: Copied! <pre>dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))  print(dataset[0]) print(len(dataset)) <pre>{'x': 'a1.txt', 'y': 0}\n4\n</pre> <p></p> <p>It is also possible to create a Dataset from data stored in memory. This may be useful for smaller datasets.</p> <p></p> <p>If you already have data in memory in the form of a Numpy array, it is easy to convert this data into a FastEstimator Dataset. To accomplish this, simply pass your data dictionary into the <code>NumpyDataset</code> class constructor. The following code snippet demonstrates this:</p> In\u00a0[6]: Copied! <pre>import numpy as np\nimport tensorflow as tf\n\nimport fastestimator as fe\n\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n\nprint (train_data[0]['y'])\nprint (len(train_data))\n</pre> import numpy as np import tensorflow as tf  import fastestimator as fe  (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data() train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train}) eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})  print (train_data[0]['y']) print (len(train_data)) <pre>5\n60000\n</pre> <p></p> <p>It is also possible to create a Dataset using generators. As an example, we will first create a generator which will generate random input data for us.</p> In\u00a0[7]: Copied! <pre>import numpy as np\n\ndef inputs():\n    while True:\n        yield {'x': np.random.rand(4), 'y':np.random.randint(2)}\n</pre> import numpy as np  def inputs():     while True:         yield {'x': np.random.rand(4), 'y':np.random.randint(2)} <p>We then pass the generator as an argument to the <code>GeneratorDataset</code> class:</p> In\u00a0[8]: Copied! <pre>from fastestimator.dataset import GeneratorDataset\n\ndataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10)\nprint(dataset[0])\nprint(len(dataset))\n</pre> from fastestimator.dataset import GeneratorDataset  dataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10) print(dataset[0]) print(len(dataset)) <pre>{'x': array([0.15550239, 0.0600738 , 0.29110195, 0.09245787]), 'y': 1}\n10\n</pre> <p></p>"}, {"location": "tutorial/beginner/t02_dataset.html#tutorial-2-creating-a-fastestimator-dataset", "title": "Tutorial 2: Creating a FastEstimator dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover three different ways to create a Dataset using FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Torch Dataset Recap</li> <li>FastEstimator Dataset<ul> <li>Dataset from disk<ul> <li>LabeledDirDataset</li> <li>CSVDataset</li> </ul> </li> <li>Dataset from memory<ul> <li>NumpyDataset</li> </ul> </li> <li>Dataset from generator</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html#torch-dataset-recap", "title": "Torch Dataset Recap\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#fastestimator-dataset", "title": "FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#1-dataset-from-disk", "title": "1. Dataset from disk\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#11-labeleddirdataset", "title": "1.1 LabeledDirDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#12-csvdataset", "title": "1.2 CSVDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#2-dataset-from-memory", "title": "2. Dataset from memory\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#21-numpydataset", "title": "2.1 NumpyDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#3-dataset-from-generator", "title": "3. Dataset from Generator\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNET</li> <li>DCGAN</li> <li>Siamese Networks</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html", "title": "Tutorial 3: Operator", "text": "In\u00a0[1]: Copied! <pre>class Op:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n    \n    def forward(self, data, state):\n        return data\n</pre> class Op:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode          def forward(self, data, state):         return data"}, {"location": "tutorial/beginner/t03_operator.html#tutorial-3-operator", "title": "Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/beginner/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will introduce the <code>Operator</code> - a fundamental building block within FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Operator Definition</li> <li>Operator Structure</li> <li>Operator Expression</li> <li>Deep Learning Examples using Operators</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html#operator-definition", "title": "Operator Definition\u00b6", "text": "<p>From Tutorial 1, we know that the preprocessing in <code>Pipeline</code> and the training in <code>Network</code> can be divided into several sub-tasks:</p> <ul> <li>Pipeline: <code>Expand_dim</code> -&gt; <code>Minmax</code></li> <li>Network: <code>ModelOp</code> -&gt; <code>CrossEntropy</code> -&gt; <code>UpdateOp</code></li> </ul> <p>Each sub-task is a modular unit that takes inputs, performs an operation, and then produces outputs. We therefore call these sub-tasks <code>Operator</code>s, and they form the building blocks of the FastEstimator <code>Pipeline</code> and <code>Network</code> APIs.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-structure", "title": "Operator Structure\u00b6", "text": "<p>An Operator has 3 main components:</p> <ul> <li>inputs: the key(s) of input data</li> <li>outputs: the key(s) of output data</li> <li>forward function: the transformation to be applied</li> </ul> <p>The base class constructor also takes a <code>mode</code> argument, but for now we will ignore it since <code>mode</code> will be discussed extensively in Tutorial 9.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-expression", "title": "Operator Expression\u00b6", "text": "<p>In this section, we will demonstrate how different tasks can be concisely expressed in operators.</p>"}, {"location": "tutorial/beginner/t03_operator.html#single-operator", "title": "Single Operator\u00b6", "text": "<p>If the task only requires taking one feature as input and transforming it to overwrite the old feature (e.g, <code>Minmax</code>), it can be expressed as:</p> <p></p> <p>If the task involves taking multiple features and overwriting them respectively (e.g, rotation of both an image and its mask), it can be expressed as:</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#multiple-operators", "title": "Multiple Operators\u00b6", "text": "<p>If there are two <code>Operator</code>s executing in a sequential manner (e.g, <code>Minmax</code> followed by <code>Transpose</code>), it can be expressed as:</p> <p></p> <p><code>Operator</code>s can also easily handle more complicated data flows:</p> <p></p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#deep-learning-examples-using-operators", "title": "Deep Learning Examples using Operators\u00b6", "text": "<p>In this section, we will show you how deep learning tasks can be modularized into combinations of <code>Operator</code>s. Please note that the <code>Operator</code> expressions we provide in this section are essentially pseudo-code. Links to full python examples are also provided.</p>"}, {"location": "tutorial/beginner/t03_operator.html#image-classification", "title": "Image Classification:\u00b6", "text": "<p>MNIST</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#dc-gan", "title": "DC-GAN:\u00b6", "text": "<p>DC-GAN</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#adversarial-hardening", "title": "Adversarial Hardening:\u00b6", "text": "<p>FGSM</p> <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html", "title": "Tutorial 4: Pipeline", "text": "<p>In Tutorial 2 we demonstrated different ways to construct FastEstimator datasets. Here we will see how datasets can be loaded in the <code>Pipeline</code> and how various operations can then be applied to the data. <code>fe.Pipeline</code> handles three different types of datasets:</p> <ul> <li>tf.data.Dataset</li> <li>torch.data.Dataloader</li> <li>fe.dataset</li> </ul> <p>Let's create an example <code>tf.data.Dataset</code> and <code>torch.data.Dataloader</code> from numpy arrays and we will load them into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Make some random data to serve as the source for our datasets\nx_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np  # Make some random data to serve as the source for our datasets x_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1))) train_data = {\"x\": x_train, \"y\": y_train} <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nimport tensorflow as tf\n\n# Create a tf.data.Dataset from sample data\ndataset_tf = tf.data.Dataset.from_tensor_slices(train_data)\ndataset_tf = dataset_tf.batch(4)\n\n# Load data into the pipeline\npipeline_tf = fe.Pipeline(dataset_tf)\n</pre> import fastestimator as fe import tensorflow as tf  # Create a tf.data.Dataset from sample data dataset_tf = tf.data.Dataset.from_tensor_slices(train_data) dataset_tf = dataset_tf.batch(4)  # Load data into the pipeline pipeline_tf = fe.Pipeline(dataset_tf) <p></p> <p>We will create a custom dataset class to load our train data into a PyTorch DataLoader.</p> In\u00a0[3]: Copied! <pre>from torch.utils.data import Dataset\n\nclass TorchCustomDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data[\"x\"].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n</pre> from torch.utils.data import Dataset  class TorchCustomDataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data[\"x\"].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data} In\u00a0[4]: Copied! <pre>import torch\nfrom torch.utils import data\n\n# Create a torch.data.Dataloader from sample data\ndataset_torch = TorchCustomDataset(train_data)\ndataloader_torch = data.DataLoader(dataset_torch, batch_size=4)\n\n# Load data into the pipeline\npipeline_torch = fe.Pipeline(dataloader_torch)\n</pre> import torch from torch.utils import data  # Create a torch.data.Dataloader from sample data dataset_torch = TorchCustomDataset(train_data) dataloader_torch = data.DataLoader(dataset_torch, batch_size=4)  # Load data into the pipeline pipeline_torch = fe.Pipeline(dataloader_torch) <p></p> <p>Next, we will see how to use one of the Fastestimator Datasets in the <code>Pipeline</code>. We will create <code>fe.dataset.NumpyDataset</code> and load it into our pipeline. As we saw in Tutorial 2, <code>NumpyDataset</code> takes a dictionary with keys for the input data and ground truth labels.</p> In\u00a0[5]: Copied! <pre>from fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# Create a NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1)\n</pre> from fastestimator.dataset.numpy_dataset import NumpyDataset  # Create a NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1) <p></p> <p>After loading the data or performing preprocessing tasks, you might want to inspect the data in the <code>Pipeline</code> and ensure the output of the <code>Pipeline</code> is as you expected. <code>fe.Pipeline.get_results</code> provides this feature:</p> In\u00a0[6]: Copied! <pre>pipeline_tf.get_results(num_steps=1)\n</pre> pipeline_tf.get_results(num_steps=1) Out[6]: <pre>{'x': &lt;tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n array([[0.75018338, 0.96604765],\n        [0.08914385, 0.66352997],\n        [0.66633148, 0.01311595],\n        [0.27073451, 0.22176897]])&gt;,\n 'y': &lt;tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n array([[0.30493647],\n        [0.73671124],\n        [0.0560095 ],\n        [0.14244441]])&gt;}</pre> <p></p> <p>In Tutorial 3, we learned about <code>Operators</code> and their structure. They are used in FastEstimator for constructing workflow graphs. Here we will talk specifically about Numpy Operators (<code>NumpyOp</code>s) and how to use them in <code>Pipeline</code>.</p> <p><code>NumpyOp</code>s form the foundation of FastEstimator data augmentation within the <code>Pipeline</code>, and inherit from the <code>Op</code> base class. They perform preprocessing and augmentation tasks on non-Tensor data. With a list of <code>NumpyOp</code>s, even complicated preprocessing tasks can be implemented in only a few lines of code. Many of the augmentation operations in FastEstimator leverage the image augmentation library albumentations.</p> <p><code>NumpyOp</code> can be further subdivided into three main categories:</p> <ul> <li>Univariate <code>NumpyOp</code>s</li> <li>Multivariate <code>NumpyOp</code>s</li> <li>Meta <code>NumpyOp</code>s</li> </ul> <p>In addition to the pre-built offerings, we can customize the <code>NumpyOp</code> to perform our own operations on the data. By inheriting <code>fe.op.numpyop</code> we can create custom <code>NumpyOp</code>s and use them in our <code>Pipeline</code>. In this tutorial, we will learn about Univariate, Multivariate and Custom Numpy Operators. We will discuss Meta NumpyOp's an advanced tutorial.</p> <p>To demonstrate use of operators, we will first load the Fashion MNIST dataset in our Pipeline and then will define list of Numpy Operators for preprocessing data. We will then visualize the <code>Pipeline</code>s inputs and outputs.</p> In\u00a0[7]: Copied! <pre>from fastestimator.dataset.data import mnist\n\nmnist_train, mnist_eval = mnist.load_data()\n</pre> from fastestimator.dataset.data import mnist  mnist_train, mnist_eval = mnist.load_data() <p></p> <p>Univariate Numpy Operators perform the same operation for all input features. They take one or more input(s) and return an equal number of outputs, applying the same transformation to each input/output pair. For example, <code>Minmax</code> is an univariate Numpy Operator. No matter what feature it is given, it will perform:</p> <p>data = (data - min) / (max - min)</p> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax\n\nminmax_op = Minmax(inputs=\"x\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop.univariate import Minmax  minmax_op = Minmax(inputs=\"x\", outputs=\"x_out\") <p></p> <p>Multivariate Numpy Operators perform different operations based on the nature of the input features. For example, if you have an image with an associated mask as well as bounding boxes, rotating all three of these objects together requires the backend code to know which of the inputs is an image and which is a bounding box. Here we will demonstrate the <code>Rotate</code> Numpy Operator which will rotate images randomly by some angle in the range (-60, 60) degrees.</p> In\u00a0[9]: Copied! <pre>from fastestimator.op.numpyop.multivariate import Rotate\n\nrotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60)\n</pre> from fastestimator.op.numpyop.multivariate import Rotate  rotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60) <p></p> <p>Let's create custom Numpy Operator that adds random noise to the input images.</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddRandomNoise(NumpyOp):\n    def forward(self, data, state):\n        # generate noise array with 0 mean and 0.1 standard deviation\n        noise = np.random.normal(0, 0.1, data.shape)\n        data = data + noise\n        return data\n    \nrandom_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddRandomNoise(NumpyOp):     def forward(self, data, state):         # generate noise array with 0 mean and 0.1 standard deviation         noise = np.random.normal(0, 0.1, data.shape)         data = data + noise         return data      random_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\") <p></p> <p>Now, let's add our <code>NumpyOp</code>s into the <code>Pipeline</code> and visualize the results.</p> In\u00a0[11]: Copied! <pre>from fastestimator.util import BatchDisplay, GridDisplay\n\npipeline = fe.Pipeline(train_data=mnist_train,\n                       eval_data=mnist_eval,\n                       ops=[minmax_op, rotation_op, random_noise_op],\n                       batch_size=3)\n\ndata = pipeline.get_results()\nimg = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"Original Image\"), BatchDisplay(image=data[\"x_out\"], title=\"Pipeline Output\")])\nimg.show()\n</pre> from fastestimator.util import BatchDisplay, GridDisplay  pipeline = fe.Pipeline(train_data=mnist_train,                        eval_data=mnist_eval,                        ops=[minmax_op, rotation_op, random_noise_op],                        batch_size=3)  data = pipeline.get_results() img = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"Original Image\"), BatchDisplay(image=data[\"x_out\"], title=\"Pipeline Output\")]) img.show() <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html#tutorial-4-pipeline", "title": "Tutorial 4: Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following:</p> <ul> <li>Loading data into a <code>Pipeline</code><ul> <li>Using tf.data.Dataset</li> <li>Using torch.Dataloader</li> <li>Using FastEstimator Datasets</li> </ul> </li> <li>Getting results from a <code>Pipeline</code></li> <li>How to use Numpy Operators in a <code>Pipeline</code><ul> <li>Univariate Numpy Operators</li> <li>Multivariate Numpy Operators</li> <li>Customized Numpy Operators</li> <li>Visualizing <code>Pipeline</code> Output</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>In deep learning, data preprocessing is a way of converting data from its raw form to a more usable or desired representation. It is one crucial step in model training as it directly impacts the ability of model to learn. In FastEstimator, the <code>Pipeline</code> API enables such preprocessing tasks in an efficient manner. The <code>Pipeline</code> manages everything from  extracting data from the disk up until it is fed into the model. <code>Pipeline</code> operations usually happen on the CPU.</p>"}, {"location": "tutorial/beginner/t04_pipeline.html#loading-data-into-a-pipeline", "title": "Loading data into a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-tfdatadataset", "title": "Using tf.data.Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-torchdatadataloader", "title": "Using torch.data.Dataloader\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-a-fastestimator-dataset", "title": "Using a FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#getting-results-from-a-pipeline", "title": "Getting results from a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-numpy-operators-in-pipeline", "title": "Using Numpy Operators in Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#univariate-numpyop", "title": "Univariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#multivariate-numpyop", "title": "Multivariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#custom-numpyop", "title": "Custom NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#visualizing-pipeline-outputs", "title": "Visualizing Pipeline Outputs\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> <li>Bert</li> <li>FGSM</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html", "title": "Tutorial 5: Model", "text": "In\u00a0[1]: Copied! <pre># Some preliminary imports\n\nimport tensorflow as tf\n\n# Since we will be mixing TF and Torch in the tutorial, we need to stop TF from taking all of the GPU memory.\n# Normally you would pick either TF or Torch, so you don't need to worry about this.\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor device in physical_devices:\n    try:\n        tf.config.experimental.set_memory_growth(device, True)\n    except:\n        pass\n\nimport torch\nimport torch.nn as nn\n\nimport fastestimator as fe\n</pre> # Some preliminary imports  import tensorflow as tf  # Since we will be mixing TF and Torch in the tutorial, we need to stop TF from taking all of the GPU memory. # Normally you would pick either TF or Torch, so you don't need to worry about this. physical_devices = tf.config.list_physical_devices('GPU') for device in physical_devices:     try:         tf.config.experimental.set_memory_growth(device, True)     except:         pass  import torch import torch.nn as nn  import fastestimator as fe In\u00a0[2]: Copied! <pre>def my_model_tf(input_shape=(30, ), num_classes=2):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n    return model\n\nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\")\n</pre> def my_model_tf(input_shape=(30, ), num_classes=2):     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))     return model  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\") <pre>2022-04-28 17:35:58.413575: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-28 17:35:58.978109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34777 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:bd:00.0, compute capability: 8.0\n</pre> In\u00a0[3]: Copied! <pre>class my_model_torch(nn.Module):\n    def __init__(self, num_inputs=30, num_classes=2):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(num_inputs, 32), \n                                    nn.ReLU(inplace=True), \n                                    nn.Linear(32, 8), \n                                    nn.ReLU(inplace=True),\n                                    nn.Linear(8, num_classes))\n\n    def forward(self, x):\n        x = self.layers(x)\n        x_label = torch.softmax(x, dim=-1)\n        return x_label\n\n    \nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\")\n</pre> class my_model_torch(nn.Module):     def __init__(self, num_inputs=30, num_classes=2):         super().__init__()         self.layers = nn.Sequential(nn.Linear(num_inputs, 32),                                      nn.ReLU(inplace=True),                                      nn.Linear(32, 8),                                      nn.ReLU(inplace=True),                                     nn.Linear(8, num_classes))      def forward(self, x):         x = self.layers(x)         x_label = torch.softmax(x, dim=-1)         return x_label       model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\") In\u00a0[4]: Copied! <pre>from fastestimator.architecture.pytorch import LeNet\n# from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.pytorch import LeNet # from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[5]: Copied! <pre>resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\")\n</pre> resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\") In\u00a0[6]: Copied! <pre>from torchvision import models\n\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\")\n</pre> from torchvision import models  resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\") In\u00a0[7]: Copied! <pre># TensorFlow \nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n\n# PyTorch\nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4))\n</pre> # TensorFlow  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))  # PyTorch model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4)) <p>If a model function returns multiple models, a list of optimizers can be provided. See the pggan apphub for an example with multiple models and optimizers.</p> <p></p> In\u00a0[8]: Copied! <pre>import os\nimport tempfile\n\nmodel_dir = tempfile.mkdtemp()\n\n# TensorFlow\nfe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")\n\n# PyTorch\nfe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\")\n</pre> import os import tempfile  model_dir = tempfile.mkdtemp()  # TensorFlow fe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")  # PyTorch fe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\") Out[8]: <pre>'/tmp/tmpfnjigvpx/resnet50_torch.pt'</pre> In\u00a0[9]: Copied! <pre># TensorFlow\nresnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None), \n                       optimizer_fn=\"adam\", \n                       weights_path=os.path.join(model_dir, \"resnet50_tf.h5\"))\n</pre> # TensorFlow resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None),                         optimizer_fn=\"adam\",                         weights_path=os.path.join(model_dir, \"resnet50_tf.h5\")) In\u00a0[10]: Copied! <pre># PyTorch\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False), \n                          optimizer_fn=\"adam\", \n                          weights_path=os.path.join(model_dir, \"resnet50_torch.pt\"))\n</pre> # PyTorch resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False),                            optimizer_fn=\"adam\",                            weights_path=os.path.join(model_dir, \"resnet50_torch.pt\")) <p></p> In\u00a0[11]: Copied! <pre>model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\nprint(\"Model Name: \", model.model_name)\n</pre> model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\") print(\"Model Name: \", model.model_name) <pre>Model Name:  LeNet\n</pre> <p>If a model function returns multiple models, a list of model_names can be given. See the pggan apphub for an illustration with multiple models and model names.</p> <p></p>"}, {"location": "tutorial/beginner/t05_model.html#tutorial-5-model", "title": "Tutorial 5: Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will cover:</p> <ul> <li>Instantiating and Compiling a Model</li> <li>The Model Function<ul> <li>Custom Models</li> <li>FastEstimator Models</li> <li>Pre-Trained Models</li> </ul> </li> <li>The Optimizer Function</li> <li>Loading Model Weights</li> <li>Specifying a Model Name</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#instantiating-and-compiling-a-model", "title": "Instantiating and Compiling a model\u00b6", "text": "<p>We need to specify two things to instantiate and compile a model:</p> <ul> <li>model_fn</li> <li>optimizer_fn</li> </ul> <p>Model definitions can be implemented in Tensorflow or Pytorch and instantiated by calling <code>fe.build</code> which constructs a model instance and associates it with the specified optimizer.</p>"}, {"location": "tutorial/beginner/t05_model.html#model-function", "title": "Model Function\u00b6", "text": "<p><code>model_fn</code> should be a function/lambda function which returns either a <code>tf.keras.Model</code> or <code>torch.nn.Module</code>. FastEstimator provides several ways to specify the model architecture:</p> <ul> <li>Custom model architecture</li> <li>Importing a pre-built model architecture from FastEstimator</li> <li>Importing pre-trained models/architectures from PyTorch or TensorFlow</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#custom-model-architecture", "title": "Custom model architecture\u00b6", "text": "<p>Let's create a custom model in TensorFlow and PyTorch for demonstration.</p>"}, {"location": "tutorial/beginner/t05_model.html#tfkerasmodel", "title": "tf.keras.Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#torchnnmodule", "title": "torch.nn.Module\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#importing-model-architecture-from-fastestimator", "title": "Importing model architecture from FastEstimator\u00b6", "text": "<p>Below we import a PyTorch LeNet architecture from FastEstimator. See our Architectures folder for a full list of the architectures provided by FastEstimator.</p>"}, {"location": "tutorial/beginner/t05_model.html#importing-pre-trained-modelsarchitectures-from-pytorch-or-tensorflow", "title": "Importing pre-trained models/architectures from PyTorch or TensorFlow\u00b6", "text": "<p>Below we show how to define a model function using a pre-trained resnet model provided by TensorFlow and PyTorch respectively. We load the pre-trained models using a lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-tfkerasapplications", "title": "Pre-trained model from tf.keras.applications\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-torchvision", "title": "Pre-trained model from torchvision\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#optimizer-function", "title": "Optimizer function\u00b6", "text": "<p><code>optimizer_fn</code> can be a string or lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-string", "title": "Optimizer from String\u00b6", "text": "<p>Specifying a string for the <code>optimizer_fn</code> loads the optimizer with default parameters. The optimizer strings accepted by FastEstimator are as follows:</p> <ul> <li>Adadelta: 'adadelta'</li> <li>Adagrad: 'adagrad'</li> <li>Adam: 'adam'</li> <li>Adamax: 'adamax'</li> <li>RMSprop: 'rmsprop'</li> <li>SGD: 'sgd'</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-function", "title": "Optimizer from Function\u00b6", "text": "<p>To specify specific values for the optimizer learning rate or other parameters, we need to pass a lambda function to the <code>optimizer_fn</code>.</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-model-weights", "title": "Loading model weights\u00b6", "text": "<p>We often need to load the weights of a saved model. Model weights can be loaded by specifying the path of the saved weights using the <code>weights_path</code> parameter. Let's use the resnet models created earlier to showcase this.</p>"}, {"location": "tutorial/beginner/t05_model.html#saving-model-weights", "title": "Saving model weights\u00b6", "text": "<p>Here, we create a temporary directory and use FastEstimator backend to save the weights of our previously created resnet50 models:</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-weights-for-tensorflow-and-pytorch-models", "title": "Loading weights for TensorFlow and PyTorch models\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#specifying-a-model-name", "title": "Specifying a Model Name\u00b6", "text": "<p>The name of a model can be specified using the <code>model_name</code> parameter. The name of the model is helpful in distinguishing models when multiple are present.</p>"}, {"location": "tutorial/beginner/t05_model.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PG-GAN</li> <li>Uncertainty Weighted Loss</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html", "title": "Tutorial 6: Network", "text": "<p>As the figure shows, models (orange) are only piece of a <code>Network</code>. It also includes other operations such as loss computation (blue) and update rules (green) that will be used during the training process.</p> <p></p> <p>A <code>Network</code> is composed of basic units called <code>TensorOps</code>. All of the building blocks inside a <code>Network</code> should derive from the <code>TensorOp</code> base class. A <code>TensorOp</code> is a kind of <code>Op</code> and therefore follows the same rules described in Tutorial 3.</p> <p></p> <p>There are some common <code>TensorOp</code> classes we would like to specially mention because of their prevalence:</p> <p></p> <p></p> In\u00a0[1]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return tf.reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class ReduceMean(TensorOp):     def forward(self, data, state):         return tf.reduce_mean(data) <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport torch\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return torch.mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import torch  class ReduceMean(TensorOp):     def forward(self, data, state):         return torch.mean(data) <p></p> In\u00a0[3]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.backend import reduce_mean\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.backend import reduce_mean  class ReduceMean(TensorOp):     def forward(self, data, state):         return reduce_mean(data) <p></p>"}, {"location": "tutorial/beginner/t06_network.html#tutorial-6-network", "title": "Tutorial 6: Network\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li><code>Network</code> Scope</li> <li><code>TensorOp</code> and its Children</li> <li>How to Customize a <code>TensorOp</code><ul> <li>TensorFlow</li> <li>PyTorch</li> <li>fe.backend</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html#network-scope", "title": "Network Scope\u00b6", "text": "<p><code>Network</code> is one of the three main FastestEstimator APIs that defines not only a neural network model but also all of the operations to be performed on it. This can include the deep-learning model itself, loss calculations, model updating rules, and any other functionality that you wish to execute within a GPU.</p> <p>Here we show two <code>Network</code> example graphs to enhance the concept:</p>"}, {"location": "tutorial/beginner/t06_network.html#tensorop-and-its-children", "title": "TensorOp and its Children\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#modelop", "title": "ModelOp\u00b6", "text": "<p>Any model instance created from <code>fe.build</code> (see Tutorial 5) needs to be packaged as a <code>ModelOp</code> such that it can interact with other components inside the <code>Network</code> API. The orange blocks in the first figure are <code>ModelOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#updateop", "title": "UpdateOp\u00b6", "text": "<p>FastEstimator use <code>UpdateOp</code> to associate the model with its loss. Unlike other <code>Ops</code> that use <code>inputs</code> and <code>outputs</code> for expressing their connections, <code>UpdateOp</code> uses the arguments <code>loss</code>, and <code>model</code> instead. The green blocks in the first figure are <code>UpdateOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#others-loss-gradient-meta-etc", "title": "Others (loss, gradient, meta, etc.)\u00b6", "text": "<p>There are many ready-to-use <code>TensorOps</code> that users can directly import from <code>fe.op.tensorop</code>. Some examples include loss and gradient computation ops. There is also a category of <code>TensorOp</code> called <code>MetaOp</code>, which takes other Ops as input and generates more complex execution graphs (see Advanced Tutorial 9).</p> <p>For all available Ops please check out the FastEstimator API.</p>"}, {"location": "tutorial/beginner/t06_network.html#customize-a-tensorop", "title": "Customize a TensorOp\u00b6", "text": "<p>FastEstimator provides flexibility that allows users to customize their own <code>TensorOp</code>s by wrapping TensorFlow or PyTorch library calls, or by leveraging <code>fe.backend</code> API functions. Users only need to inherit the <code>TensorOp</code> class and overwrite its <code>forward</code> function.</p> <p>If you want to customize a <code>TensorOp</code> by directly leveraging API calls from TensorFlow or PyTorch, please make sure that all of the <code>TensorOp</code>s in the <code>Network</code> are backend-consistent. In other words, you cannot have <code>TensorOp</code>s built specifically for TensorFlow and PyTorch in the same <code>Network</code>. Note that the <code>ModelOp</code> backend is determined by which library the model function uses, and so must be consistent with any custom <code>TensorOp</code> that you write.</p> <p>Here we are going to demonstrate how to build a <code>TensorOp</code> that takes high dimensional inputs and returns an average scalar value. For more advanced tutorial of customizing a <code>TensorOp</code> please check out Advanced Tutorial 3.</p>"}, {"location": "tutorial/beginner/t06_network.html#example-using-tensorflow", "title": "Example Using TensorFlow\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-pytorch", "title": "Example Using PyTorch\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-febackend", "title": "Example Using <code>fe.backend</code>\u00b6", "text": "<p>You don't need to worry about backend consistency if you import a FastEstimator-provided <code>TensorOp</code>, or customize your <code>TensorOp</code> using the <code>fe.backend</code> API. FastEstimator auto-magically handles everything for you.</p>"}, {"location": "tutorial/beginner/t06_network.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>DC-GAN</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html", "title": "Tutorial 7: Estimator", "text": "<p><code>Estimator</code> is the API that manages everything related to the training loop. It combines <code>Pipeline</code> and <code>Network</code> together and provides users with fine-grain control over the training loop. Before we demonstrate different ways to control the training loop let's define a template similar to tutorial 1, but this time we will use a PyTorch model.</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.pytorch import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nimport tempfile\n\ndef get_estimator(log_steps=100, monitor_names=None, use_trace=False, train_steps_per_epoch=None, epochs=2):\n    # step 1\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])\n    # step 2\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    # step 3\n    traces = None\n    if use_trace:\n        traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), \n                  BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             train_steps_per_epoch=train_steps_per_epoch,\n                             log_steps=log_steps,\n                             monitor_names=monitor_names)\n    return estimator\n</pre> import fastestimator as fe from fastestimator.architecture.pytorch import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp import tempfile  def get_estimator(log_steps=100, monitor_names=None, use_trace=False, train_steps_per_epoch=None, epochs=2):     # step 1     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])     # step 2     model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     # step 3     traces = None     if use_trace:         traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                    BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              train_steps_per_epoch=train_steps_per_epoch,                              log_steps=log_steps,                              monitor_names=monitor_names)     return estimator <p>Let's train our model using the default <code>Estimator</code> arguments:</p> In\u00a0[2]: Copied! <pre>est = get_estimator()\nest.fit()\n</pre> est = get_estimator() est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.295395; \nFastEstimator-Train: step: 100; ce: 0.8820845; steps/sec: 320.73; \nFastEstimator-Train: step: 200; ce: 0.37291068; steps/sec: 319.45; \nFastEstimator-Train: step: 300; ce: 0.06651708; steps/sec: 309.93; \nFastEstimator-Train: step: 400; ce: 0.21876352; steps/sec: 309.78; \nFastEstimator-Train: step: 500; ce: 0.08403016; steps/sec: 309.19; \nFastEstimator-Train: step: 600; ce: 0.35541984; steps/sec: 308.78; \nFastEstimator-Train: step: 700; ce: 0.06964149; steps/sec: 300.41; \nFastEstimator-Train: step: 800; ce: 0.13983297; steps/sec: 309.22; \nFastEstimator-Train: step: 900; ce: 0.037845124; steps/sec: 312.94; \nFastEstimator-Train: step: 1000; ce: 0.13029681; steps/sec: 316.27; \nFastEstimator-Train: step: 1100; ce: 0.022184685; steps/sec: 312.62; \nFastEstimator-Train: step: 1200; ce: 0.039918672; steps/sec: 315.24; \nFastEstimator-Train: step: 1300; ce: 0.05553157; steps/sec: 313.87; \nFastEstimator-Train: step: 1400; ce: 0.0021400168; steps/sec: 343.22; \nFastEstimator-Train: step: 1500; ce: 0.07833527; steps/sec: 336.56; \nFastEstimator-Train: step: 1600; ce: 0.09543828; steps/sec: 324.81; \nFastEstimator-Train: step: 1700; ce: 0.14825855; steps/sec: 318.52; \nFastEstimator-Train: step: 1800; ce: 0.01032154; steps/sec: 322.95; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 5.99 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.06244616; \nFastEstimator-Train: step: 1900; ce: 0.015050106; steps/sec: 263.35; \nFastEstimator-Train: step: 2000; ce: 0.003486173; steps/sec: 295.2; \nFastEstimator-Train: step: 2100; ce: 0.06401425; steps/sec: 310.64; \nFastEstimator-Train: step: 2200; ce: 0.008118075; steps/sec: 297.08; \nFastEstimator-Train: step: 2300; ce: 0.05136842; steps/sec: 289.31; \nFastEstimator-Train: step: 2400; ce: 0.10011706; steps/sec: 290.44; \nFastEstimator-Train: step: 2500; ce: 0.007041894; steps/sec: 287.94; \nFastEstimator-Train: step: 2600; ce: 0.041005336; steps/sec: 301.21; \nFastEstimator-Train: step: 2700; ce: 0.0023359149; steps/sec: 311.66; \nFastEstimator-Train: step: 2800; ce: 0.034970395; steps/sec: 278.47; \nFastEstimator-Train: step: 2900; ce: 0.024958389; steps/sec: 294.08; \nFastEstimator-Train: step: 3000; ce: 0.0038549905; steps/sec: 291.1; \nFastEstimator-Train: step: 3100; ce: 0.14712071; steps/sec: 311.67; \nFastEstimator-Train: step: 3200; ce: 0.14290668; steps/sec: 316.4; \nFastEstimator-Train: step: 3300; ce: 0.34252185; steps/sec: 304.94; \nFastEstimator-Train: step: 3400; ce: 0.0059393854; steps/sec: 297.43; \nFastEstimator-Train: step: 3500; ce: 0.2493474; steps/sec: 323.9; \nFastEstimator-Train: step: 3600; ce: 0.004362625; steps/sec: 322.78; \nFastEstimator-Train: step: 3700; ce: 0.0058870725; steps/sec: 296.6; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 6.31 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.053535815; \nFastEstimator-Finish: step: 3750; total_time: 14.81 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[3]: Copied! <pre>est = get_estimator(train_steps_per_epoch=300, epochs=4)\nest.fit()\n</pre> est = get_estimator(train_steps_per_epoch=300, epochs=4) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.311253; \nFastEstimator-Train: step: 100; ce: 0.66614795; steps/sec: 274.95; \nFastEstimator-Train: step: 200; ce: 0.46526748; steps/sec: 309.99; \nFastEstimator-Train: step: 300; ce: 0.11188476; steps/sec: 336.57; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 0.99 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.17669827; \nFastEstimator-Train: step: 400; ce: 0.2917202; steps/sec: 332.81; \nFastEstimator-Train: step: 500; ce: 0.047290877; steps/sec: 323.56; \nFastEstimator-Train: step: 600; ce: 0.053344093; steps/sec: 315.52; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 0.93 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.11926653; \nFastEstimator-Train: step: 700; ce: 0.06439964; steps/sec: 300.28; \nFastEstimator-Train: step: 800; ce: 0.026502458; steps/sec: 300.13; \nFastEstimator-Train: step: 900; ce: 0.34012184; steps/sec: 303.24; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 1.0 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.075678065; \nFastEstimator-Train: step: 1000; ce: 0.044892587; steps/sec: 285.03; \nFastEstimator-Train: step: 1100; ce: 0.037321247; steps/sec: 293.51; \nFastEstimator-Train: step: 1200; ce: 0.011022182; steps/sec: 294.91; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 1.03 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.06031439; \nFastEstimator-Finish: step: 1200; total_time: 9.29 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>est = get_estimator(train_steps_per_epoch=300, epochs=4, log_steps=0)\nest.fit()\n</pre> est = get_estimator(train_steps_per_epoch=300, epochs=4, log_steps=0) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 0; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.16322972; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.10085282; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.08177921; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.0629242; \nFastEstimator-Finish: step: 1200; total_time: 9.14 sec; LeNet_lr: 0.001; \n</pre> <p></p> In\u00a0[5]: Copied! <pre>est = get_estimator(train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\")\nest.fit()\n</pre> est = get_estimator(train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\") est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 150; \nFastEstimator-Train: step: 1; ce: 2.2930875; ce1: 2.2930875; \nFastEstimator-Train: step: 150; ce: 0.29343712; ce1: 0.29343712; steps/sec: 292.71; \nFastEstimator-Train: step: 300; ce: 0.37277684; ce1: 0.37277684; steps/sec: 284.64; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 1.05 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.21327984; ce1: 0.21327984; \nFastEstimator-Train: step: 450; ce: 0.3631664; ce1: 0.3631664; steps/sec: 277.43; \nFastEstimator-Train: step: 600; ce: 0.2957161; ce1: 0.2957161; steps/sec: 304.11; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 1.03 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.10858435; ce1: 0.10858435; \nFastEstimator-Train: step: 750; ce: 0.1193773; ce1: 0.1193773; steps/sec: 301.03; \nFastEstimator-Train: step: 900; ce: 0.05718822; ce1: 0.05718822; steps/sec: 294.92; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 1.01 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.093043245; ce1: 0.093043245; \nFastEstimator-Train: step: 1050; ce: 0.102503434; ce1: 0.102503434; steps/sec: 297.27; \nFastEstimator-Train: step: 1200; ce: 0.011180073; ce1: 0.011180073; steps/sec: 296.55; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 1.01 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.082674295; ce1: 0.082674295; \nFastEstimator-Finish: step: 1200; total_time: 9.62 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see, both <code>ce</code> and <code>ce1</code> showed up in the log above. Unsurprisingly, their values are identical because because they have the same inputs and forward function.</p> <p></p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class Trace:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n\n    def on_begin(self, data):\n\"\"\"Runs once at the beginning of training\"\"\"\n\n    def on_epoch_begin(self, data):\n\"\"\"Runs at the beginning of each epoch\"\"\"\n\n    def on_batch_begin(self, data):\n\"\"\"Runs at the beginning of each batch\"\"\"\n\n    def on_batch_end(self, data):\n\"\"\"Runs at the end of each batch\"\"\"\n\n    def on_epoch_end(self, data):\n\"\"\"Runs at the end of each epoch\"\"\"\n\n    def on_end(self, data):\n\"\"\"Runs once at the end training\"\"\"\n</pre> class Trace:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode      def on_begin(self, data):         \"\"\"Runs once at the beginning of training\"\"\"      def on_epoch_begin(self, data):         \"\"\"Runs at the beginning of each epoch\"\"\"      def on_batch_begin(self, data):         \"\"\"Runs at the beginning of each batch\"\"\"      def on_batch_end(self, data):         \"\"\"Runs at the end of each batch\"\"\"      def on_epoch_end(self, data):         \"\"\"Runs at the end of each epoch\"\"\"      def on_end(self, data):         \"\"\"Runs once at the end training\"\"\" <p>Given the structure, users can customize their own functions at different stages and insert them into the training loop. We will leave the customization of <code>Traces</code> to the advanced tutorial. For now, let's use some pre-built <code>Traces</code> from FastEstimator.</p> <p>During the training loop in our earlier example, we want 2 things to happen:</p> <ol> <li>Save the model weights if the evaluation loss is the best we have seen so far</li> <li>Calculate the model accuracy during evaluation</li> </ol> <p></p> In\u00a0[7]: Copied! <pre>from fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\nest = get_estimator(use_trace=True)\nest.fit()\n</pre> from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  est = get_estimator(use_trace=True) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 1; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.303516; \nFastEstimator-Train: step: 100; ce: 1.004676; steps/sec: 286.36; \nFastEstimator-Train: step: 200; ce: 0.49630624; steps/sec: 286.83; \nFastEstimator-Train: step: 300; ce: 0.12231735; steps/sec: 291.9; \nFastEstimator-Train: step: 400; ce: 0.14592598; steps/sec: 315.7; \nFastEstimator-Train: step: 500; ce: 0.25857; steps/sec: 326.27; \nFastEstimator-Train: step: 600; ce: 0.13771628; steps/sec: 331.77; \nFastEstimator-Train: step: 700; ce: 0.38054478; steps/sec: 301.89; \nFastEstimator-Train: step: 800; ce: 0.07086247; steps/sec: 291.58; \nFastEstimator-Train: step: 900; ce: 0.16959156; steps/sec: 308.7; \nFastEstimator-Train: step: 1000; ce: 0.021332668; steps/sec: 324.17; \nFastEstimator-Train: step: 1100; ce: 0.055990797; steps/sec: 287.57; \nFastEstimator-Train: step: 1200; ce: 0.2849428; steps/sec: 292.77; \nFastEstimator-Train: step: 1300; ce: 0.20509654; steps/sec: 288.14; \nFastEstimator-Train: step: 1400; ce: 0.08241908; steps/sec: 321.32; \nFastEstimator-Train: step: 1500; ce: 0.024668839; steps/sec: 320.73; \nFastEstimator-Train: step: 1600; ce: 0.01093893; steps/sec: 325.12; \nFastEstimator-Train: step: 1700; ce: 0.012216274; steps/sec: 330.77; \nFastEstimator-Train: step: 1800; ce: 0.01524183; steps/sec: 328.2; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 6.15 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmplhuyv721/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.048887283; accuracy: 0.9814; since_best_accuracy: 0; max_accuracy: 0.9814; \nFastEstimator-Train: step: 1900; ce: 0.0056912354; steps/sec: 267.68; \nFastEstimator-Train: step: 2000; ce: 0.06863687; steps/sec: 312.62; \nFastEstimator-Train: step: 2100; ce: 0.071683794; steps/sec: 319.51; \nFastEstimator-Train: step: 2200; ce: 0.023103738; steps/sec: 313.75; \nFastEstimator-Train: step: 2300; ce: 0.011231604; steps/sec: 315.5; \nFastEstimator-Train: step: 2400; ce: 0.17630634; steps/sec: 310.87; \nFastEstimator-Train: step: 2500; ce: 0.01526911; steps/sec: 315.78; \nFastEstimator-Train: step: 2600; ce: 0.06935612; steps/sec: 310.69; \nFastEstimator-Train: step: 2700; ce: 0.14090665; steps/sec: 308.39; \nFastEstimator-Train: step: 2800; ce: 0.0023762842; steps/sec: 309.23; \nFastEstimator-Train: step: 2900; ce: 0.025511805; steps/sec: 309.84; \nFastEstimator-Train: step: 3000; ce: 0.094952986; steps/sec: 318.57; \nFastEstimator-Train: step: 3100; ce: 0.011754904; steps/sec: 299.48; \nFastEstimator-Train: step: 3200; ce: 0.033963054; steps/sec: 303.24; \nFastEstimator-Train: step: 3300; ce: 0.013373202; steps/sec: 317.35; \nFastEstimator-Train: step: 3400; ce: 0.064900294; steps/sec: 295.58; \nFastEstimator-Train: step: 3500; ce: 0.29719537; steps/sec: 307.13; \nFastEstimator-Train: step: 3600; ce: 0.185368; steps/sec: 307.28; \nFastEstimator-Train: step: 3700; ce: 0.005988597; steps/sec: 278.04; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 6.19 sec; \nFastEstimator-BestModelSaver: Saved model to /tmp/tmplhuyv721/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.03341377; accuracy: 0.9896; since_best_accuracy: 0; max_accuracy: 0.9896; \nFastEstimator-Finish: step: 3750; total_time: 14.96 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see from the log, the model is saved in a predefined location and the accuracy is displayed during evaluation.</p> <p></p> In\u00a0[8]: Copied! <pre>est.test()\n</pre> est.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.9894; \n</pre> <p>This will feed all of your test dataset through the <code>Pipeline</code> and <code>Network</code>, and finally execute the traces (in our case, compute accuracy) just like during the training.</p> <p></p>"}, {"location": "tutorial/beginner/t07_estimator.html#tutorial-7-estimator", "title": "Tutorial 7: Estimator\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Estimator API<ul> <li>Reducing the number of training steps per epoch</li> <li>Reducing the number of evaluation steps per epoch</li> <li>Changing logging behavior</li> <li>Monitoring intermediate results during training</li> </ul> </li> <li>Trace<ul> <li>Concept</li> <li>Structure</li> <li>Usage</li> </ul> </li> <li>Model Testing</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html#estimator-api", "title": "Estimator API\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-training-steps-per-epoch", "title": "Reduce the number of training steps per epoch\u00b6", "text": "<p>In general, one epoch of training means that every element in the training dataset will be visited exactly one time. If evaluation data is available, evaluation happens after every epoch by default. Consider the following two scenarios:</p> <ul> <li>The training dataset is very large such that evaluation needs to happen multiple times during one epoch.</li> <li>Different training datasets are being used for different epochs, but the number of training steps should be consistent between each epoch.</li> </ul> <p>One easy solution to the above scenarios is to limit the number of training steps per epoch. For example, if we want to train for only 300 steps per epoch, with training lasting for 4 epochs (1200 steps total), we would do the following:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-evaluation-steps-per-epoch", "title": "Reduce the number of evaluation steps per epoch\u00b6", "text": "<p>One may need to reduce the number of evaluation steps for debugging purpose. This can be easily done by setting the <code>eval_steps_per_epoch</code> argument in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#change-logging-behavior", "title": "Change logging behavior\u00b6", "text": "<p>When the number of training epochs is large, the log can become verbose. You can change the logging behavior by choosing one of following options:</p> <ul> <li>set <code>log_steps</code> to <code>None</code> if you do not want to see any training logs printed.</li> <li>set <code>log_steps</code> to 0 if you only wish to see the evaluation logs.</li> <li>set <code>log_steps</code> to some integer 'x' if you want training logs to be printed every 'x' steps.</li> </ul> <p>Let's set the <code>log_steps</code> to 0:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#monitor-intermediate-results", "title": "Monitor intermediate results\u00b6", "text": "<p>You might have noticed that in our example <code>Network</code> there is an op: <code>CrossEntropy(inputs=(\"y_pred\", \"y\") outputs=\"ce1\")</code>. However, the <code>ce1</code> never shows up in the training log above. This is because FastEstimator identifies and filters out unused variables to reduce unnecessary communication between the GPU and CPU. On the contrary, <code>ce</code> shows up in the log because by default we log all loss values that are used to update models.</p> <p>But what if we want to see the value of <code>ce1</code> throughout training?</p> <p>Easy: just add <code>ce1</code> to <code>monitor_names</code> in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#trace", "title": "Trace\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#concept", "title": "Concept\u00b6", "text": "<p>Now you might be thinking: 'changing logging behavior and monitoring extra keys is cool, but where is the fine-grained access to the training loop?'</p> <p>The answer is <code>Trace</code>. <code>Trace</code> is a module that can offer you access to different training stages and allow you \"do stuff\" with them. Here are some examples of what a <code>Trace</code> can do:</p> <ul> <li>print any training data at any training step</li> <li>write results to a file during training</li> <li>change learning rate based on some loss conditions</li> <li>calculate any metrics</li> <li>order you a pizza after training ends</li> <li>...</li> </ul> <p>So what are the different training stages? They are:</p> <ul> <li>Beginning of training</li> <li>Beginning of epoch</li> <li>Beginning of batch</li> <li>End of batch</li> <li>End of epoch</li> <li>End of training</li> </ul> <p></p> <p>As we can see from the illustration above, the training process is essentially a nested combination of batch loops and epoch loops. Over the course of training, <code>Trace</code> places 6 different \"road blocks\" for you to leverage.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#structure", "title": "Structure\u00b6", "text": "<p>If you are familiar with Keras, you will notice that the structure of <code>Trace</code> is very similar to the <code>Callback</code> in keras.  Despite the structural similarity, <code>Trace</code> gives you a lot more flexibility which we will talk about in depth in advanced tutorial 4. Implementation-wise, <code>Trace</code> is a python class with the following structure:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Sometimes you have a separate testing dataset other than training and evaluation data. If you want to evalate the model metrics on test data, you can simply call:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNet</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html", "title": "Tutorial 8: Mode", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]"}, {"location": "tutorial/beginner/t08_mode.html#tutorial-8-mode", "title": "Tutorial 8: Mode\u00b6", "text": ""}, {"location": "tutorial/beginner/t08_mode.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Modes</li> <li>When Modes are Activated</li> <li>How to Set Modes</li> <li>A Code Example</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#modes", "title": "Modes\u00b6", "text": "<p>The development cycle of a deep learning application can usually be broken into 4 phases: training, evaluation, testing, and inference. FastEstimator provides 4 corresponding modes: <code>train</code>, <code>eval</code>, <code>test</code>, and <code>infer</code> that allow users to manage each phase independently. Users have the flexibility to construct the <code>Network</code> and <code>Pipeline</code> in different ways for each of those modes. Only a single mode can ever be active at a time, and for each given mode the corresponding graph topology will be computed and executed.</p>"}, {"location": "tutorial/beginner/t08_mode.html#when-modes-are-activated", "title": "When Modes are Activated\u00b6", "text": "<ul> <li>train: <code>estimator.fit()</code> being called, during training cycle</li> <li>eval: <code>estimator.fit()</code> being called, during evaluation cycle</li> <li>test: <code>estimator.test()</code> being called</li> <li>infer: <code>pipeline.transform(mode=\"infer\")</code> or <code>network.transform(mode=\"infer\")</code> being called (inference will be covered in Tutorial 9)</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#how-to-set-modes", "title": "How to Set Modes\u00b6", "text": "<p>From the previous tutorials we already know that <code>Ops</code> define the workflow of <code>Networks</code> and <code>Pipelines</code>, whereas <code>Traces</code> control the training process. All <code>Ops</code> and <code>Traces</code> can be specified to run in one or more modes. Here are all 5 ways to set the modes:</p> <ol> <li><p>Setting a single mode Specify the desired mode as string. Ex: Op(mode=\"train\")</p> </li> <li><p>Setting multiple modes Put all desired modes in a tuple or list as an argument. Ex: Trace(mode=[\"train\", \"test\"]) </p> </li> <li><p>Setting an exception mode Prefix a \"!\" on a mode, and then the object will execute during all modes that are NOT the specified one. Ex: Op(mode=\"!train\") </p> </li> <li><p>Setting all modes Set the mode argument equal to None. Ex: Trace(mode=None) </p> </li> <li><p>Using the default mode setting Don't specify anything in mode argument. Different <code>Ops</code> and <code>Traces</code> have different default mode settings. Ex: <code>UpdateOp</code> -&gt; default mode: train  Ex: <code>Accuracy</code> trace -&gt; default mode: eval, test</p> </li> </ol>"}, {"location": "tutorial/beginner/t08_mode.html#code-example", "title": "Code Example\u00b6", "text": "<p>Let's see come example code and visualize the topology of the corresponding execution graphs for each mode:</p>"}, {"location": "tutorial/beginner/t08_mode.html#train-mode", "title": "Train Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"train\" mode. It has a complete data pipeline including the <code>CoarseDropout</code> data augmentation Op. The data source of the pipeline is \"train_data\". The <code>Accuracy</code> Trace will not exist in this mode because the default mode of that trace is \"eval\" and \"test\".</p>"}, {"location": "tutorial/beginner/t08_mode.html#eval-mode", "title": "Eval Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"eval\" mode. The data augmentation block is missing and the pipeline data source is \"eval_data\". The <code>Accuracy</code> block exist in this mode because of its default trace setting.</p>"}, {"location": "tutorial/beginner/t08_mode.html#test-mode", "title": "Test Mode\u00b6", "text": "<p>Everything in the \"test\" mode is the same as the \"eval\" mode, except that the data source of pipeline has switched to \"test_data\":</p>"}, {"location": "tutorial/beginner/t08_mode.html#infer-mode", "title": "Infer Mode\u00b6", "text": "<p>\"Infer\" mode only has the minimum operations that model inference requires. The data source is not defined yet because input data will not be passed until the inference function is invoked. See Tutorial 9 for more details.</p>"}, {"location": "tutorial/beginner/t08_mode.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html", "title": "Tutorial 9: Inference", "text": "<p>Running inference means using a trained deep learning model to get a prediction from some input data. Users can use <code>pipeline.transform</code> and <code>network.transform</code> to feed the data forward and get the computed result in any operation mode. Here we are going to use an end-to-end example (the same example code from Tutorial 8) on MNIST image classification to demonstrate how to run inference.</p> <p>We first train a deep leaning model with the following code:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=1,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\nestimator.fit()\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=1,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test] estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Warn: the key 'x' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2991223;\nFastEstimator-Train: step: 100; ce: 1.9097762; steps/sec: 71.55;\nFastEstimator-Train: step: 200; ce: 1.5507967; steps/sec: 71.29;\nFastEstimator-Train: step: 300; ce: 1.1750956; steps/sec: 71.01;\nFastEstimator-Train: step: 400; ce: 0.8331691; steps/sec: 52.24;\nFastEstimator-Train: step: 500; ce: 1.0825467; steps/sec: 49.03;\nFastEstimator-Train: step: 600; ce: 0.96594095; steps/sec: 54.5;\nFastEstimator-Train: step: 700; ce: 0.7712606; steps/sec: 59.52;\nFastEstimator-Train: step: 800; ce: 0.92222476; steps/sec: 64.22;\nFastEstimator-Train: step: 900; ce: 0.6225736; steps/sec: 66.74;\nFastEstimator-Train: step: 1000; ce: 1.065671; steps/sec: 66.21;\nFastEstimator-Train: step: 1100; ce: 0.75097674; steps/sec: 66.05;\nFastEstimator-Train: step: 1200; ce: 0.87365466; steps/sec: 66.14;\nFastEstimator-Train: step: 1300; ce: 0.63997686; steps/sec: 44.37;\nFastEstimator-Train: step: 1400; ce: 0.8474549; steps/sec: 47.43;\nFastEstimator-Train: step: 1500; ce: 0.81277585; steps/sec: 39.03;\nFastEstimator-Train: step: 1600; ce: 0.8506465; steps/sec: 49.19;\nFastEstimator-Train: step: 1700; ce: 0.76338726; steps/sec: 61.44;\nFastEstimator-Train: step: 1800; ce: 0.6283993; steps/sec: 65.14;\nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 34.18 sec;\nEval Progress: 1/156;\nEval Progress: 52/156; steps/sec: 197.95;\nEval Progress: 104/156; steps/sec: 216.01;\nEval Progress: 156/156; steps/sec: 206.73;\nFastEstimator-Eval: step: 1875; epoch: 1; accuracy: 0.951; ce: 0.16610472;\nFastEstimator-Finish: step: 1875; model_lr: 0.001; total_time: 35.46 sec;\n</pre> <p>Let's create a customized print function to showcase our inferencing easier:</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nimport torch\n\ndef print_dict_but_value(data):\n    for key, value in data.items():\n        if isinstance(value, np.ndarray):\n            print(\"{}: ndarray with shape {}\".format(key, value.shape))\n        \n        elif isinstance(value, tf.Tensor):\n            print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))\n        \n        elif isinstance(value, torch.Tensor):\n            print(\"{}: torch Tensor with shape {}\".format(key, value.shape))\n        \n        else:\n            print(\"{}: {}\".format(key, value))\n</pre> import numpy as np import tensorflow as tf import torch  def print_dict_but_value(data):     for key, value in data.items():         if isinstance(value, np.ndarray):             print(\"{}: ndarray with shape {}\".format(key, value.shape))                  elif isinstance(value, tf.Tensor):             print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))                  elif isinstance(value, torch.Tensor):             print(\"{}: torch Tensor with shape {}\".format(key, value.shape))                  else:             print(\"{}: {}\".format(key, value)) <p>The following figure shows the complete execution graph (consisting <code>Pipeline</code> and <code>Network</code>) for the \"infer\" mode:</p> <p></p> <p>Our goal is to provide an input image \"x\" and get the prediction result \"y_pred\".</p> <p></p> In\u00a0[3]: Copied! <pre>import copy \n\ninfer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])}\nprint_dict_but_value(infer_data)\n</pre> import copy   infer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])} print_dict_but_value(infer_data) <pre>x: ndarray with shape (28, 28)\n</pre> In\u00a0[4]: Copied! <pre>infer_data = pipeline.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = pipeline.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: ndarray with shape (1, 28, 28, 1)\nx_out: ndarray with shape (1, 28, 28, 1)\n</pre> <p></p> In\u00a0[5]: Copied! <pre>infer_data = network.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = network.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: tf.Tensor with shape (1, 28, 28, 1)\nx_out: tf.Tensor with shape (1, 28, 28, 1)\ny_pred: tf.Tensor with shape (1, 10)\n</pre> <p>Now we can visualize the input image and compare with its prediction class.</p> In\u00a0[6]: Copied! <pre>print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"])))\nimg = fe.util.BatchDisplay(image=infer_data[\"x\"], title=\"x\")\nimg.show()\n</pre> print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"]))) img = fe.util.BatchDisplay(image=infer_data[\"x\"], title=\"x\") img.show() <pre>Predicted class is 1\n</pre> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#tutorial-9-inference", "title": "Tutorial 9: Inference\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Running inference with the transform method<ul> <li>Pipeline.transform</li> <li>Network.transform</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html#running-inference-with-transform-method", "title": "Running inference with transform method\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#pipelinetransform", "title": "Pipeline.transform\u00b6", "text": "<p>The <code>Pipeline</code> object has a <code>transform</code> method that runs the pipeline graph (\"x\" to \"x_out\") when inference data (a dictionary of keys and values like {\"x\":image}), is inserted. The returned output will be the dictionary of computed results after applying all <code>Pipeline</code> Ops, where the dictionary values will be Numpy arrays.</p> <p></p> <p>Here we take eval_data's first image, package it into a dictionary, and then call <code>pipeline.transform</code>:</p>"}, {"location": "tutorial/beginner/t09_inference.html#networktransform", "title": "Network.transform\u00b6", "text": "<p>We then use the network object to call the <code>transform</code> method that runs the network graph (\"x_out\" to \"y_pred\"). Much like with <code>pipeline.transform</code>, it will return it's Op outputs, though this time in the form of a dictionary of Tensors. The data type of the returned values depends on the backend of the network. It is <code>tf.Tensor</code> when using the TensorFlow backend and <code>torch.Tensor</code> with PyTorch. Please check out Tutorial 6 for more details about <code>Network</code> backends).</p> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>IMDB</li> </ul>"}, {"location": "tutorial/beginner/t10_cli.html", "title": "Tutorial 10: How to use FastEstimator Command Line Interface (CLI)", "text": ""}, {"location": "tutorial/beginner/t10_cli.html#overview", "title": "Overview", "text": "<p>FastEstimator comes with a set of CLI commands that can help users train and test their models quickly. In this tutorial, we will go through the CLI usage and the arguments these CLI commands take. This tutorial is divided into the following sections:</p> <ul> <li>How Does the CLI Work</li> <li>CLI Usage</li> <li>Sending Input Args to <code>get_estimator</code></li> <li>Using --arg</li> <li>Using a JSON file</li> <li>System argument</li> </ul> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#how_does_the_cli_work", "title": "How Does the CLI Work", "text": "<ul> <li><code>fastestimator train</code>: the command will look for a <code>get_estimator</code> function, invoke it, and then call the <code>fit()</code> method on the returned estimator instance to start the training.</li> <li><code>fastestimator test</code>: the command will look for a <code>get_estimator</code> function, invoke it, and then call the <code>test()</code> method on the returned estimator instance to run testing.</li> <li><code>fastestimator run</code>: the command will look for a <code>fastestimator_run</code> function and invoke it. If <code>fastestimator_run</code> is not available, it will instead look for <code>get_estimator</code>, invoke it, and then call <code>fit()</code> and/ or <code>test</code> depending on what data is available within the estimator's Pipeline.</li> </ul>"}, {"location": "tutorial/beginner/t10_cli.html#_1", "title": "Tutorial 10: How to use CLI", "text": ""}, {"location": "tutorial/beginner/t10_cli.html#cli_usage", "title": "CLI Usage", "text": "<p>In this section we will show the actual commands that we can use to train and test our models. We will use mnist_tf.py for illustration.</p> <p>To call <code>estimator.fit()</code> and start the training on terminal:</p> <pre><code>$ fastestimator train mnist_tf.py\n</code></pre> <p>To call <code>estimator.test()</code> and start testing on terminal:</p> <pre><code>$ fastestimator test mnist_tf.py\n</code></pre> <p>To first call <code>estimator.fit()</code> then <code>estimator.test()</code>, you can use: <pre><code>$ fastestimator run mnist_tf.py\n</code></pre></p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#sending_input_args_to_get_estimator_or_fastestimator_run", "title": "Sending Input Args to <code>get_estimator</code> or <code>fastestimator_run</code>", "text": "<p>We can also pass arguments to the <code>get_estimator</code> or <code>fastestimator_run</code> functions from the CLI. The following code snippet shows the <code>get_estimator</code> method for our MNIST example: <pre><code>def get_estimator(epochs=2, batch_size=32, ...):\n...\n</code></pre></p> <p>Next, we try to change these arguments in two ways:</p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_--arg", "title": "Using --arg", "text": "<p>To pass the arguments directly from the CLI we can use the <code>--arg</code> format. The following shows an example of how we can set the number of epochs to 3 and batch_size to 64:</p> <pre><code>$ fastestimator train mnist_tf.py --epochs 3 --batch_size 64\n</code></pre> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_a_json_file", "title": "Using a JSON file", "text": "<p>The other way we can send arguments is by using the <code>--hyperparameters</code> argument and passing it a json file containing all the training hyperparameters like epochs, batch_size, optimizer, etc. This option is really useful when you want to repeat the training job more than once and/or the list of the hyperparameter is getting really long. The following shows an example JSON file and how it could be used for our MNIST example: <pre><code>{\n\"epochs\": 1,\n\"batch_size\": 64\n}\n</code></pre> <pre><code>$ fastestimator train mnist_tf.py --hyperparameters hp.json\n</code></pre> </p>"}, {"location": "tutorial/beginner/t10_cli.html#system_argument", "title": "System argument", "text": "<p>There are some default system arguments in the <code>fastestimator train</code> and <code>fastestimator test</code> commands. Here are a list of them: * <code>warmup</code>:  Only available in <code>fastestimator train</code>, it controls whether to perform warmup checking before the actual training starts. Default is True. Users can disable warmup before training by <code>--warmup False</code>. * <code>summary</code>: Available in both <code>fastestimator train</code> and <code>fastestimator test</code>, this is the same argument used in <code>estimator.fit()</code> or <code>estimator.test()</code>. It allows users to specify experiment name when generating reports. For example, Users can set experiment name by <code>--summary exp_name</code>.</p>"}, {"location": "tutorial/beginner/t11_debugging.html", "title": "Tutorial 11: Debugging", "text": "<ul> <li>Pipeline Debugging<ul> <li>Debugging a Single NumpyOp</li> <li>Debugging and Verifying the Pipeline Results</li> </ul> </li> <li>Network Debugging<ul> <li>Debugging a Single TensorOp</li> <li>Debugging and Verifying the Network Results</li> </ul> </li> <li>Trace Debugging<ul> <li>Conditional Debugging During Training</li> <li>Debugging Pipeline and Network from a Trace</li> </ul> </li> </ul> <p>In Tutorial 4 we demonstrated what the Pipeline is and how it is created to handle different preprocessing tasks using <code>NumpyOp</code>s. Since the pipeline consists of a series of NumpyOps, it's a vital to know how to debug those <code>NumpyOp</code>s. It is also a good practice to inspect the results of the pipeline and ensure that the output is as you expected.</p> <p>There are two ways we can debug the <code>Pipeline</code>,</p> <ol> <li>Debug a single <code>NumpyOp</code></li> <li>Debug and verify the results of <code>Pipeline</code></li> </ol> <p>We will first create a simple <code>Pipeline</code> with a few <code>NumpyOps</code> that add random noise and rotate the image.</p> <p>Now, if we want to debug the variable values in the <code>AddNoise</code> op we will do the following,</p> <ol> <li>Set the num_process=0 to disable the multiprocessing</li> <li>Add your choice of a debugger such as the python debugger (PDB), an IDE specific debugger, or print statement in the <code>NumpyOp</code></li> </ol> In\u00a0[1]: Copied! <pre>import numpy as np\nimport fastestimator as fe\n\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.multivariate import Rotate\n\nfrom fastestimator.architecture.tensorflow import LeNet\n</pre> import numpy as np import fastestimator as fe  from fastestimator.dataset.data import mnist from fastestimator.op.numpyop import NumpyOp from fastestimator.op.numpyop.multivariate import Rotate  from fastestimator.architecture.tensorflow import LeNet In\u00a0[2]: Copied! <pre>train_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") <p>Single <code>NumpyOp</code> can be debugged in two ways,</p> <ul> <li>Using <code>Pipeline.transform</code></li> <li>Running the training loop</li> </ul> In\u00a0[3]: Copied! <pre>class AddNoiseDebug(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        noise = np.random.normal(0, 1, size=data.shape)\n        print('Noise shape ', noise.shape)\n        print('data shape ', data.shape) # add print statement to check the data and noise\n        data = data + noise\n        return data\n</pre> class AddNoiseDebug(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         noise = np.random.normal(0, 1, size=data.shape)         print('Noise shape ', noise.shape)         print('data shape ', data.shape) # add print statement to check the data and noise         data = data + noise         return data In\u00a0[4]: Copied! <pre>debug_pipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=3,\n                       ops=[AddNoiseDebug(inputs='x', outputs='x_out'),\n                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60)], \n                             num_process=0)\n</pre> debug_pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=3,                        ops=[AddNoiseDebug(inputs='x', outputs='x_out'),                             Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=60)],                               num_process=0) In\u00a0[5]: Copied! <pre>results = debug_pipeline.transform(train_data[0], mode='train')\n</pre> results = debug_pipeline.transform(train_data[0], mode='train') <pre>Noise shape  (28, 28)\ndata shape  (28, 28)\n</pre> In\u00a0[6]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp In\u00a0[7]: Copied! <pre>network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n</pre> network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")]) <p>NOTE: The training logs will print extra debug messages if warmup is not set to False in <code>Estimator.fit</code>. When warmup is enabled it will perform a test run on both <code>Pipeline</code> and <code>Network</code> to make sure that training will not fail in later stages.</p> In\u00a0[8]: Copied! <pre>estimator = fe.Estimator(pipeline=debug_pipeline,\n                         network=network,\n                         epochs=1,\n                         train_steps_per_epoch=1, \n                         eval_steps_per_epoch=1)\nestimator.fit(warmup=False)\n</pre> estimator = fe.Estimator(pipeline=debug_pipeline,                          network=network,                          epochs=1,                          train_steps_per_epoch=1,                           eval_steps_per_epoch=1) estimator.fit(warmup=False) <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nNoise shape  (28, 28)\ndata shape  (28, 28)\nFastEstimator-Warn: the key 'x' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\nFastEstimator-Train: step: 1; ce: 13.519921;\nFastEstimator-Train: step: 1; epoch: 1; epoch_time: 3.26 sec;\nNoise shape  (28, 28)\ndata shape  (28, 28)\nEval Progress: 1/1;\nFastEstimator-Eval: step: 1; epoch: 1; ce: 29.12915;\nFastEstimator-Finish: step: 1; model_lr: 0.001; total_time: 3.55 sec;\n</pre> <p>In order to debug and verify the pipeline results, we will use the <code>pipeline.get_results()</code>. You can also visualize the results using the utility functions.</p> In\u00a0[9]: Copied! <pre>from fastestimator.util import BatchDisplay, GridDisplay\n\ndata = debug_pipeline.get_results()\nimg = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"original image\"), BatchDisplay(image=data[\"x_out\"], title=\"pipeline output\")])\nimg.show()\n</pre> from fastestimator.util import BatchDisplay, GridDisplay  data = debug_pipeline.get_results() img = GridDisplay([BatchDisplay(image=data[\"x\"], title=\"original image\"), BatchDisplay(image=data[\"x_out\"], title=\"pipeline output\")]) img.show() <pre>Noise shape  (28, 28)\ndata shape  (28, 28)\nNoise shape  (28, 28)\ndata shape  (28, 28)\nNoise shape  (28, 28)\ndata shape  (28, 28)\n</pre> <p><code>Network</code> defines the model and the operations that need to be performed on it. It is composed of series of TensorOps and can be debugged in similar fashion as the <code>Pipeline</code>.</p> <ol> <li>Debugging a single <code>TensorOp</code></li> <li>Verifying and debugging the network results</li> </ol> <p>A TensorOp can be debugged in two ways,</p> <ul> <li>Using <code>network.transform</code></li> <li>Running the training loop</li> </ul> <p>We will add a Custom <code>TensorOp</code> that will print the value of prediction and run the forward step through the network using <code>network.transform</code>.</p> In\u00a0[10]: Copied! <pre>import tensorflow as tf\nfrom fastestimator.op.tensorop import TensorOp\n</pre> import tensorflow as tf from fastestimator.op.tensorop import TensorOp In\u00a0[11]: Copied! <pre>class CustomTensorOp(TensorOp):\n    def forward(self, data, state):\n        pred = data[0]\n        labels = data[1]\n        print('Predictions:\\n ', pred)\n</pre> class CustomTensorOp(TensorOp):     def forward(self, data, state):         pred = data[0]         labels = data[1]         print('Predictions:\\n ', pred) In\u00a0[12]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=3,\n                       ops=[Rotate(image_in=\"x\", image_out=\"x_out\", limit=60)])\n</pre> pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=3,                        ops=[Rotate(image_in=\"x\", image_out=\"x_out\", limit=60)]) In\u00a0[13]: Copied! <pre>network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n                          CustomTensorOp(inputs=(\"y_pred\", \"y\")),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n</pre> network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),                           CustomTensorOp(inputs=(\"y_pred\", \"y\")),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")]) In\u00a0[14]: Copied! <pre>test_data = pipeline.get_results(mode=\"test\")\ntest_data = network.transform(test_data, mode=\"test\")\n</pre> test_data = pipeline.get_results(mode=\"test\") test_data = network.transform(test_data, mode=\"test\") <pre>Predictions:\n  tf.Tensor(\n[[9.99892116e-01 2.13048708e-07 2.33150109e-12 1.14148029e-17\n  5.93473363e-14 9.23155219e-10 1.07507796e-04 6.68245841e-14\n  3.31131176e-08 8.69427765e-08]\n [1.93564352e-02 1.47529849e-10 4.48650781e-06 1.32073909e-15\n  5.44973355e-12 2.95591729e-09 1.31416485e-01 1.68490661e-08\n  8.49220872e-01 1.71936131e-06]\n [1.00000000e+00 2.02585757e-16 3.05128769e-20 5.85469986e-33\n  8.33811057e-33 6.08177679e-13 1.62758016e-16 3.02257723e-22\n  4.62641339e-08 5.39494084e-25]], shape=(3, 10), dtype=float32)\n</pre> <p>As you can see, the predictions are getting displayed from the print statement in the <code>CustomTensorOp</code></p> In\u00a0[15]: Copied! <pre>class CustomTensorOp(TensorOp):\n    def forward(self, data, state):\n        pred = data[0]\n        labels = data[1]\n        tf.print(pred.shape) # tf.print will work in both graph mode and eager mode\n        print(labels.shape) # print only work in eager mode for tensorflow\n</pre> class CustomTensorOp(TensorOp):     def forward(self, data, state):         pred = data[0]         labels = data[1]         tf.print(pred.shape) # tf.print will work in both graph mode and eager mode         print(labels.shape) # print only work in eager mode for tensorflow In\u00a0[16]: Copied! <pre>network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n                          CustomTensorOp(inputs=(\"y_pred\", \"y\")),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n</pre> network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),                           CustomTensorOp(inputs=(\"y_pred\", \"y\")),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")]) <p>If you are using the Tensorflow backend it's important to set <code>eager=True</code> in the <code>Estimator.fit()</code> or can use the <code>tf.print</code> function to print in graph mode. For the PyTorch backend, you can use any of your favorite debuggers and no graph mode settings are required.</p> In\u00a0[17]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         train_steps_per_epoch=2, \n                         eval_steps_per_epoch=1,\n                         log_steps=None)\nestimator.fit(eager=True)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          train_steps_per_epoch=2,                           eval_steps_per_epoch=1,                          log_steps=None) estimator.fit(eager=True) <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\nTensorShape([3, 10])\n(3,)\n</pre> <p>Now let's look at how to verify the output of the network using <code>network.transform</code></p> In\u00a0[18]: Copied! <pre>network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n</pre> network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")]) <p>We will take the output of the pipeline and feed it into the network to verify that the network is giving us the output as was expected</p> In\u00a0[19]: Copied! <pre>test_data = pipeline.get_results(mode=\"test\")\ntest_data = network.transform(test_data, mode=\"test\")\nprint('Labels: ', test_data['y'])\nprint('Predictions: ', np.argmax(test_data['y_pred'].numpy(), axis=1))\n</pre> test_data = pipeline.get_results(mode=\"test\") test_data = network.transform(test_data, mode=\"test\") print('Labels: ', test_data['y']) print('Predictions: ', np.argmax(test_data['y_pred'].numpy(), axis=1)) <pre>Labels:  tf.Tensor([4 3 3], shape=(3,), dtype=uint8)\nPredictions:  [9 9 9]\n</pre> <p>What if our training is giving some weird result for the specific sample after a certain number of training epochs? How do you debug such sample data based on specific conditions?</p> <p>In Tutorial 7 we introduced the <code>Trace</code> and it's various use cases during the training. Since the trace allows us to control the training loop it is possible to add conditions that suit your needs to debug the training code.</p> <p>Let's write a trace that prints the predictions and label values for second batch during the training. To access the training information inside the <code>Trace</code>, you can use the <code>System</code> instance as explained in Advanced Tutorial 4.</p> In\u00a0[20]: Copied! <pre>from fastestimator.trace import Trace\nfrom fastestimator.trace.metric import Accuracy\n</pre> from fastestimator.trace import Trace from fastestimator.trace.metric import Accuracy In\u00a0[21]: Copied! <pre>class MonitorResult(Trace):\n        \n    def on_batch_end(self, data):\n        if self.system.batch_idx == 2:\n            predictions = np.argmax(data[self.inputs[1]].numpy(), axis=1)\n            print('Current global step: ', self.system.global_step)\n            print(\"Batch true labels: \", data[self.inputs[0]])\n            print(\"Batch predictictions: \", predictions)\n</pre> class MonitorResult(Trace):              def on_batch_end(self, data):         if self.system.batch_idx == 2:             predictions = np.argmax(data[self.inputs[1]].numpy(), axis=1)             print('Current global step: ', self.system.global_step)             print(\"Batch true labels: \", data[self.inputs[0]])             print(\"Batch predictictions: \", predictions) In\u00a0[22]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    MonitorResult(inputs=(\"y\", \"y_pred\"), mode='train')\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=3,\n                         traces=traces,\n                         train_steps_per_epoch=15, \n                         log_steps=None)\nestimator.fit()\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     MonitorResult(inputs=(\"y\", \"y_pred\"), mode='train') ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=3,                          traces=traces,                          train_steps_per_epoch=15,                           log_steps=None) estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nWARNING:tensorflow:5 out of the last 5 calls to &lt;function TFNetwork._forward_step_static at 0x17a441dc0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nCurrent global step:  2\nBatch true labels:  tf.Tensor([6 8 7], shape=(3,), dtype=uint8)\nBatch predictictions:  [2 9 7]\nCurrent global step:  17\nBatch true labels:  tf.Tensor([1 4 5], shape=(3,), dtype=uint8)\nBatch predictictions:  [4 9 9]\nCurrent global step:  32\nBatch true labels:  tf.Tensor([5 8 1], shape=(3,), dtype=uint8)\nBatch predictictions:  [5 8 1]\n</pre> <p>Trace can also be used to debug the results of <code>Pipeline</code> and <code>Network</code>. In the previous example, we print the predictions and labels for epoch 2 and 3. But what if we want to debug the actual pipeline data used in the training.</p> <p>We can use the same conditional debugging as before, only this time printing the results of the <code>Pipeline</code>. Let's write a trace that prints the pipeline output when the loss value crosses 2.</p> In\u00a0[23]: Copied! <pre>model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[24]: Copied! <pre>class MonitorPipelineResults(Trace):\n    def __init__(self, true_key, pred_key, mode=\"train\"):\n        super().__init__(inputs=(true_key, pred_key), mode=mode)\n        \n    def on_batch_end(self, data):\n        if data['ce'] &gt; 3 and self.system.epoch_idx &gt;= 2:\n            print('\\nLoss is above 3. Check the pipeline results!')\n            print(data['x_out'])\n</pre> class MonitorPipelineResults(Trace):     def __init__(self, true_key, pred_key, mode=\"train\"):         super().__init__(inputs=(true_key, pred_key), mode=mode)              def on_batch_end(self, data):         if data['ce'] &gt; 3 and self.system.epoch_idx &gt;= 2:             print('\\nLoss is above 3. Check the pipeline results!')             print(data['x_out']) In\u00a0[25]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    MonitorPipelineResults(true_key=\"y\", pred_key=\"y_pred\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=3,\n                         traces=traces,\n                         train_steps_per_epoch=2)\nestimator.fit()\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     MonitorPipelineResults(true_key=\"y\", pred_key=\"y_pred\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=3,                          traces=traces,                          train_steps_per_epoch=2) estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; logging_interval: 100; num_device: 0;\nFastEstimator-Train: step: 1; ce: 1.1959546;\nFastEstimator-Train: step: 2; epoch: 1; epoch_time: 0.46 sec;\nEval Progress: 1/1666;\nEval Progress: 555/1666; steps/sec: 384.56;\nEval Progress: 1110/1666; steps/sec: 407.87;\nEval Progress: 1666/1666; steps/sec: 416.24;\nFastEstimator-Eval: step: 2; epoch: 1; accuracy: 0.2366; ce: 2.8156953;\n\nLoss is above 3. Check the pipeline results!\ntf.Tensor(\n[[[214  66   0 ...   0   4   0]\n  [190   0   0 ...   0   0   0]\n  [ 55   0   0 ...   0   0   0]\n  ...\n  [  0   0   0 ...   9 151 243]\n  [  0   0   0 ...   0  95 230]\n  [ 64   0   0 ...   0  43 216]]\n\n [[  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  ...\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]\n  [  0   0   0 ...   0   0   0]]\n\n [[105  35   0 ...   0   1   0]\n  [123   0   0 ...   0   0   0]\n  [ 37   0   0 ...   0   0   0]\n  ...\n  [  0   0   0 ...  11 108 191]\n  [  0   0   0 ...  15 141 154]\n  [  0   0   0 ...  14 136 189]]], shape=(3, 28, 28), dtype=uint8)\nFastEstimator-Train: step: 4; epoch: 2; epoch_time: 0.47 sec;\nEval Progress: 1/1666;\nEval Progress: 555/1666; steps/sec: 402.08;\nEval Progress: 1110/1666; steps/sec: 418.11;\nEval Progress: 1666/1666; steps/sec: 413.43;\nFastEstimator-Eval: step: 4; epoch: 2; accuracy: 0.258; ce: 2.7076159;\nFastEstimator-Train: step: 6; epoch: 3; epoch_time: 0.45 sec;\nEval Progress: 1/1666;\nEval Progress: 555/1666; steps/sec: 395.87;\nEval Progress: 1110/1666; steps/sec: 404.88;\nEval Progress: 1666/1666; steps/sec: 425.2;\nFastEstimator-Eval: step: 6; epoch: 3; accuracy: 0.2732; ce: 2.6286228;\nFastEstimator-Finish: step: 6; model_lr: 0.001; total_time: 14.83 sec;\n</pre>"}, {"location": "tutorial/beginner/t11_debugging.html#tutorial-11-debugging", "title": "Tutorial 11: Debugging\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p>"}, {"location": "tutorial/beginner/t11_debugging.html#pipeline-debugging", "title": "Pipeline Debugging\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#debugging-a-single-numpyop", "title": "Debugging a Single NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#using-pipelinetransform", "title": "Using Pipeline.transform\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#running-the-training-loop", "title": "Running the Training Loop\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#debugging-and-verifying-the-pipeline-results", "title": "Debugging and Verifying the Pipeline Results\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#network-debugging", "title": "Network Debugging\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#debugging-a-single-tensorop", "title": "Debugging a Single TensorOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#using-networktransform", "title": "Using Network.transform\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#running-the-training-loop", "title": "Running the Training Loop\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#debugging-and-verifying-the-network-results", "title": "Debugging and Verifying the Network Results\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#trace-debugging", "title": "Trace Debugging\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#conditional-debugging-during-training", "title": "Conditional Debugging During Training\u00b6", "text": ""}, {"location": "tutorial/beginner/t11_debugging.html#debugging-pipeline-and-network-from-a-trace", "title": "Debugging Pipeline and Network From a Trace\u00b6", "text": ""}, {"location": "tutorial/xai/t01_label_tracking.html", "title": "XAI Tutorial 1: Label Tracking", "text": "<p>We'll start by getting the imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop import LambdaOp\nfrom fastestimator.trace.io import BestModelSaver, ImageViewer\nfrom fastestimator.trace.metric import MCC\nfrom fastestimator.trace.xai import LabelTracker\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n</pre> import tempfile  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import reduce_mean from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop import LambdaOp from fastestimator.trace.io import BestModelSaver, ImageViewer from fastestimator.trace.metric import MCC from fastestimator.trace.xai import LabelTracker  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 } <p></p> In\u00a0[2]: Copied! <pre>batch_size=128\nsave_dir = tempfile.mkdtemp()\n\ntrain_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(range(len(eval_data) // 2))\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n         ],\n    num_process=0)\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"train\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"sample_ce\", mode=(\"eval\", \"test\"), average_loss=False),\n    LambdaOp(inputs=\"sample_ce\", outputs=\"ce\", mode=(\"eval\", \"test\"), fn=lambda x: reduce_mean(x)),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    MCC(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),\n    LabelTracker(label=\"y\", metric=\"sample_ce\", label_mapping=label_mapping, outputs=\"ce_vs_y\", bounds=None, mode=[\"eval\", \"test\"]),\n    ImageViewer(inputs=\"ce_vs_y\", mode=[\"eval\", \"test\"])\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=10,\n                         traces=traces,\n                         log_steps=300)\n</pre> batch_size=128 save_dir = tempfile.mkdtemp()  train_data, eval_data = cifair10.load_data() test_data = eval_data.split(range(len(eval_data) // 2)) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),          PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),          RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),          Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),          CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),          ],     num_process=0)  model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"train\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"sample_ce\", mode=(\"eval\", \"test\"), average_loss=False),     LambdaOp(inputs=\"sample_ce\", outputs=\"ce\", mode=(\"eval\", \"test\"), fn=lambda x: reduce_mean(x)),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     MCC(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),     LabelTracker(label=\"y\", metric=\"sample_ce\", label_mapping=label_mapping, outputs=\"ce_vs_y\", bounds=None, mode=[\"eval\", \"test\"]),     ImageViewer(inputs=\"ce_vs_y\", mode=[\"eval\", \"test\"]) ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=10,                          traces=traces,                          log_steps=300) <pre>2022-04-13 16:01:46.649443: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-04-13 16:01:46.739177: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n</pre> In\u00a0[3]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.3176317;\nFastEstimator-Train: step: 300; ce: 1.5395001; steps/sec: 14.3;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 29.12 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 391; epoch: 1; ce: 1.2877194; max_mcc: 0.4842492824031186; mcc: 0.4842492824031186; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 1.2664933; steps/sec: 13.28;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 29.89 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; ce: 1.1504955; max_mcc: 0.5389898387019242; mcc: 0.5389898387019242; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 1.2066176; steps/sec: 12.84;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 28.73 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; ce: 1.0454248; max_mcc: 0.5840932018667955; mcc: 0.5840932018667955; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 1.1817951; steps/sec: 13.8;\nFastEstimator-Train: step: 1500; ce: 1.1124868; steps/sec: 10.77;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 35.84 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; ce: 0.9683806; max_mcc: 0.6100634215605975; mcc: 0.6100634215605975; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 1.2120445; steps/sec: 12.0;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 32.19 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; ce: 0.89093876; max_mcc: 0.6506148663922158; mcc: 0.6506148663922158; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 0.7867976; steps/sec: 11.92;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 30.14 sec;\nFastEstimator-Eval: step: 2346; epoch: 6; ce: 0.88949853; max_mcc: 0.6506148663922158; mcc: 0.6494923768152842; since_best_mcc: 1;\nFastEstimator-Train: step: 2400; ce: 0.8040351; steps/sec: 13.66;\nFastEstimator-Train: step: 2700; ce: 1.0257889; steps/sec: 13.92;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 28.51 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; ce: 0.8607704; max_mcc: 0.671126859105468; mcc: 0.671126859105468; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 0.9415474; steps/sec: 13.78;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 27.86 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 3128; epoch: 8; ce: 0.79306936; max_mcc: 0.6862720645156658; mcc: 0.6862720645156658; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: 0.8957753; steps/sec: 14.19;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 27.66 sec;\nFastEstimator-Eval: step: 3519; epoch: 9; ce: 0.78325427; max_mcc: 0.6862720645156658; mcc: 0.6832005047098153; since_best_mcc: 1;\nFastEstimator-Train: step: 3600; ce: 0.8727247; steps/sec: 13.87;\nFastEstimator-Train: step: 3900; ce: 0.89711684; steps/sec: 11.83;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 32.38 sec;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\nFastEstimator-Eval: step: 3910; epoch: 10; ce: 0.79790723; max_mcc: 0.6892349113253013; mcc: 0.6892349113253013; since_best_mcc: 0;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmph1ky2kgz/model_best_mcc.h5\n</pre> <pre>FastEstimator-Finish: step: 3910; model_lr: 0.001; total_time: 317.37 sec;\n</pre> <p>From the graph above it seems that cats are relatively difficult for the network to learn well, whereas automobiles are pretty easy.</p>"}, {"location": "tutorial/xai/t01_label_tracking.html#xai-tutorial-1-label-tracking", "title": "XAI Tutorial 1: Label Tracking\u00b6", "text": ""}, {"location": "tutorial/xai/t01_label_tracking.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Label Tracking</li> </ul>"}, {"location": "tutorial/xai/t01_label_tracking.html#label-tracking", "title": "Label Tracking\u00b6", "text": "<p>Suppose you are doing some training, and you want to know whether a particular class is easier or harder than other classes for your network to learn. One way to investigate this is with the <code>LabelTracker</code> <code>Trace</code>. It takes as input any per-element metric (such as sample-wise loss), as well as any label vector (usually class labels, but it could be any grouping) and produces a visualization at the end of training:</p>"}, {"location": "tutorial/xai/t02_instance_tracking.html", "title": "XAI Tutorial 2: Instance Tracking", "text": "<p>We'll start by getting the imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop import LambdaOp\nfrom fastestimator.trace.io import BestModelSaver, ImageViewer\nfrom fastestimator.trace.metric import MCC\nfrom fastestimator.trace.xai import InstanceTracker\nfrom fastestimator.util import to_number, GridDisplay, ImageDisplay\n\nimport numpy as np\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n</pre> import tempfile  import tensorflow as tf  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import reduce_mean from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop import LambdaOp from fastestimator.trace.io import BestModelSaver, ImageViewer from fastestimator.trace.metric import MCC from fastestimator.trace.xai import InstanceTracker from fastestimator.util import to_number, GridDisplay, ImageDisplay  import numpy as np  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 } <p></p> In\u00a0[2]: Copied! <pre>batch_size=128\nsave_dir = tempfile.mkdtemp()\n\ntrain_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(range(len(eval_data) // 2))\n\ntrain_data['index'] = np.array([i for i in range(len(train_data))], dtype=np.int).reshape((len(train_data), 1))\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n         ],\n    num_process=0)\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"sample_ce\", average_loss=False),\n    LambdaOp(inputs=\"sample_ce\", outputs=\"ce\", fn=lambda x: reduce_mean(x)),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    MCC(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),\n    InstanceTracker(index=\"index\", metric=\"sample_ce\", n_max_to_keep=4, n_min_to_keep=0, list_to_keep=[10, 18, 10380], outputs=\"ce_vs_idx\", mode=\"train\"),\n    ImageViewer(inputs=\"ce_vs_idx\", mode=\"train\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=10,\n                         traces=traces,\n                         log_steps=300)\n</pre> batch_size=128 save_dir = tempfile.mkdtemp()  train_data, eval_data = cifair10.load_data() test_data = eval_data.split(range(len(eval_data) // 2))  train_data['index'] = np.array([i for i in range(len(train_data))], dtype=np.int).reshape((len(train_data), 1))  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),          PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),          RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),          Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),          CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),          ],     num_process=0)  model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"sample_ce\", average_loss=False),     LambdaOp(inputs=\"sample_ce\", outputs=\"ce\", fn=lambda x: reduce_mean(x)),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     MCC(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),     InstanceTracker(index=\"index\", metric=\"sample_ce\", n_max_to_keep=4, n_min_to_keep=0, list_to_keep=[10, 18, 10380], outputs=\"ce_vs_idx\", mode=\"train\"),     ImageViewer(inputs=\"ce_vs_idx\", mode=\"train\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=10,                          traces=traces,                          log_steps=300) In\u00a0[3]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.2912202;\nFastEstimator-Train: step: 300; ce: 1.5846689; steps/sec: 13.43;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 31.35 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 30.96;\nEval Progress: 26/39; steps/sec: 33.3;\nEval Progress: 39/39; steps/sec: 29.91;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 391; epoch: 1; ce: 1.3090442; max_mcc: 0.47626476759536035; mcc: 0.47626476759536035; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 1.2921344; steps/sec: 11.57;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 36.13 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.74;\nEval Progress: 26/39; steps/sec: 24.1;\nEval Progress: 39/39; steps/sec: 23.63;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; ce: 1.115688; max_mcc: 0.5713074370763542; mcc: 0.5713074370763542; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 1.114191; steps/sec: 10.21;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 37.56 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 23.43;\nEval Progress: 26/39; steps/sec: 22.79;\nEval Progress: 39/39; steps/sec: 24.15;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; ce: 1.0538898; max_mcc: 0.5788361476143156; mcc: 0.5788361476143156; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 1.3872262; steps/sec: 10.4;\nFastEstimator-Train: step: 1500; ce: 1.2397084; steps/sec: 9.96;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 39.27 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 23.4;\nEval Progress: 26/39; steps/sec: 25.77;\nEval Progress: 39/39; steps/sec: 24.88;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; ce: 0.9873681; max_mcc: 0.6094672773776373; mcc: 0.6094672773776373; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 1.0803001; steps/sec: 9.81;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 39.35 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.01;\nEval Progress: 26/39; steps/sec: 25.06;\nEval Progress: 39/39; steps/sec: 24.64;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; ce: 0.985664; max_mcc: 0.6117049529662967; mcc: 0.6117049529662967; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 1.0776561; steps/sec: 10.5;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 36.1 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 25.68;\nEval Progress: 26/39; steps/sec: 25.82;\nEval Progress: 39/39; steps/sec: 24.76;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 2346; epoch: 6; ce: 0.91793406; max_mcc: 0.6461908231587581; mcc: 0.6461908231587581; since_best_mcc: 0;\nFastEstimator-Train: step: 2400; ce: 1.0986989; steps/sec: 10.94;\nFastEstimator-Train: step: 2700; ce: 0.9455121; steps/sec: 11.03;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 35.52 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.21;\nEval Progress: 26/39; steps/sec: 25.1;\nEval Progress: 39/39; steps/sec: 23.46;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; ce: 0.85772955; max_mcc: 0.6636882958141539; mcc: 0.6636882958141539; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 0.9812018; steps/sec: 10.88;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 35.98 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.45;\nEval Progress: 26/39; steps/sec: 26.3;\nEval Progress: 39/39; steps/sec: 26.25;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 3128; epoch: 8; ce: 0.8402723; max_mcc: 0.6733243035167943; mcc: 0.6733243035167943; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: 1.0235794; steps/sec: 10.95;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 36.29 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.68;\nEval Progress: 26/39; steps/sec: 26.31;\nEval Progress: 39/39; steps/sec: 25.95;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\nFastEstimator-Eval: step: 3519; epoch: 9; ce: 0.8024753; max_mcc: 0.6875346994702772; mcc: 0.6875346994702772; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: 1.112258; steps/sec: 10.68;\nFastEstimator-Train: step: 3900; ce: 0.9471506; steps/sec: 10.55;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 36.79 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.96;\nEval Progress: 26/39; steps/sec: 25.01;\nEval Progress: 39/39; steps/sec: 25.77;\nFastEstimator-Eval: step: 3910; epoch: 10; ce: 0.82467973; max_mcc: 0.6875346994702772; mcc: 0.6747237819814983; since_best_mcc: 1;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpsp9wc_hw/model_best_mcc.h5\n</pre> <pre>FastEstimator-Finish: step: 3910; model_lr: 0.001; total_time: 385.42 sec;\n</pre> <p>From the graph above it looks like datapoint number 10 is pretty easy, whereas 18 is somewhat difficult. Performance on some of the hardest points actually seems to get worse over time, so perhaps it would be worth visualizing them to see if there's a reason the network is having a hard time. Let's take a look at 10380, for example:</p> In\u00a0[4]: Copied! <pre>data_idx = 10380\n\nclass_map = {v: k for k, v in label_mapping.items()}\ntrue_key = np.array([class_map[train_data[data_idx]['y'].item()]])\n\ndata = pipeline.transform(train_data[data_idx], mode='test', target_type='tf')\ny_pred = tf.argmax(model(data['x']), axis=-1).numpy().item()\npred_key = np.array([class_map[y_pred]])\n\nfig = GridDisplay([ImageDisplay(text=true_key, title=\"y\"),\n                   ImageDisplay(text=pred_key, title=\"y_pred\"),\n                   ImageDisplay(image=train_data[data_idx][\"x\"], title=\"x\")\n                  ])\nfig.show()\n</pre> data_idx = 10380  class_map = {v: k for k, v in label_mapping.items()} true_key = np.array([class_map[train_data[data_idx]['y'].item()]])  data = pipeline.transform(train_data[data_idx], mode='test', target_type='tf') y_pred = tf.argmax(model(data['x']), axis=-1).numpy().item() pred_key = np.array([class_map[y_pred]])  fig = GridDisplay([ImageDisplay(text=true_key, title=\"y\"),                    ImageDisplay(text=pred_key, title=\"y_pred\"),                    ImageDisplay(image=train_data[data_idx][\"x\"], title=\"x\")                   ]) fig.show() <p>So we've got a (sideways) image of a car, but the network is probably looking at blue/green tint of the image and deciding that the image is a ship. It might also be confused by the angle/rotation of the image. If you're trying to expand your dataset this could provide some useful information about what sort of images you might need to collect in order to get a more robust network. You could also try some hue-shift and rotation data augmentation to correct for this.</p>"}, {"location": "tutorial/xai/t02_instance_tracking.html#xai-tutorial-2-instance-tracking", "title": "XAI Tutorial 2: Instance Tracking\u00b6", "text": ""}, {"location": "tutorial/xai/t02_instance_tracking.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Instance Tracking</li> </ul>"}, {"location": "tutorial/xai/t02_instance_tracking.html#instance-tracking", "title": "Instance Tracking\u00b6", "text": "<p>Suppose you are doing some training, and you want to know which samples from your dataset are the most difficult to learn. Perhaps they were mislabeled, for example. Let's suppose you're also very curious about how well sample 10 and sample 18 from your training data do over time. One way to investigate this is with the <code>InstanceTracker</code> <code>Trace</code>. It takes as input any per-element metric (such as sample-wise loss), as well as an index vector and produces a visualization at the end of training:</p>"}, {"location": "tutorial/xai/t03_saliency.html", "title": "XAI Tutorial 3: Saliency Maps", "text": "<p>We'll start by getting the imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tempfile\nimport os\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import squeeze\nfrom fastestimator.dataset.data import cifair10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import RepeatScheduler\nfrom fastestimator.trace.io import BestModelSaver, ImageViewer\nfrom fastestimator.trace.metric import MCC\nfrom fastestimator.trace.xai import Saliency\nfrom fastestimator.util import to_number, BatchDisplay, GridDisplay\n\nimport numpy as np\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n</pre> import tempfile import os  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import squeeze from fastestimator.dataset.data import cifair10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import RepeatScheduler from fastestimator.trace.io import BestModelSaver, ImageViewer from fastestimator.trace.metric import MCC from fastestimator.trace.xai import Saliency from fastestimator.util import to_number, BatchDisplay, GridDisplay  import numpy as np  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 } <p></p> <p></p> In\u00a0[2]: Copied! <pre>batch_size=128\nsave_dir = tempfile.mkdtemp()\n\ntrain_data, eval_data = cifair10.load_data()\ntest_data = eval_data.split(range(len(eval_data) // 2))\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n         PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),\n         Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),\n         CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),\n         ],\n    num_process=0)\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    MCC(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),\n    RepeatScheduler([Saliency(model=model,\n                             model_inputs=\"x\",\n                             class_key=\"y\",\n                             model_outputs=\"y_pred\",\n                             samples=5,\n                             label_mapping=label_mapping),\n                     None, None, None, None]),  # Only compute Saliency every 5 epochs for cleaner logs\n    RepeatScheduler([ImageViewer(inputs=\"saliency\"), None, None, None, None])  # Only display the images every 5 epochs for cleaner logs\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=21,\n                         traces=traces,\n                         log_steps=300)\n</pre> batch_size=128 save_dir = tempfile.mkdtemp()  train_data, eval_data = cifair10.load_data() test_data = eval_data.split(range(len(eval_data) // 2)) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),          PadIfNeeded(min_height=40, min_width=40, image_in=\"x\", image_out=\"x\", mode=\"train\"),          RandomCrop(32, 32, image_in=\"x\", image_out=\"x\", mode=\"train\"),          Sometimes(HorizontalFlip(image_in=\"x\", image_out=\"x\", mode=\"train\")),          CoarseDropout(inputs=\"x\", outputs=\"x\", mode=\"train\", max_holes=1),          ],     num_process=0)  model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     MCC(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"mcc\", save_best_mode=\"max\", load_best_final=True),     RepeatScheduler([Saliency(model=model,                              model_inputs=\"x\",                              class_key=\"y\",                              model_outputs=\"y_pred\",                              samples=5,                              label_mapping=label_mapping),                      None, None, None, None]),  # Only compute Saliency every 5 epochs for cleaner logs     RepeatScheduler([ImageViewer(inputs=\"saliency\"), None, None, None, None])  # Only display the images every 5 epochs for cleaner logs ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=21,                          traces=traces,                          log_steps=300) <p>In this example we will be using the <code>ImageViewer</code> <code>Trace</code>, since it will allow us to visualize the outputs within this Notebook. If you wanted your images to appear in TensorBoard, simply construct a <code>TensorBoard</code> <code>Trace</code> with the \"write_images\" argument set to \"saliency\".</p> In\u00a0[3]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; logging_interval: 300; num_device: 0;\nFastEstimator-Train: step: 1; ce: 2.388823;\nFastEstimator-Train: step: 300; ce: 1.3864682; steps/sec: 14.24;\nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 30.73 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 38.24;\nEval Progress: 26/39; steps/sec: 36.03;\nEval Progress: 39/39; steps/sec: 33.58;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\n</pre> <pre>FastEstimator-Eval: step: 391; epoch: 1; ce: 1.3359833; max_mcc: 0.4555820671855027; mcc: 0.4555820671855027; since_best_mcc: 0;\nFastEstimator-Train: step: 600; ce: 1.3321579; steps/sec: 13.15;\nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 27.53 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 37.91;\nEval Progress: 26/39; steps/sec: 34.6;\nEval Progress: 39/39; steps/sec: 33.72;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 782; epoch: 2; ce: 1.1216551; max_mcc: 0.549275060620413; mcc: 0.549275060620413; since_best_mcc: 0;\nFastEstimator-Train: step: 900; ce: 1.1421483; steps/sec: 14.22;\nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 27.34 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 38.63;\nEval Progress: 26/39; steps/sec: 35.39;\nEval Progress: 39/39; steps/sec: 34.63;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 1173; epoch: 3; ce: 1.0429373; max_mcc: 0.5941480962039674; mcc: 0.5941480962039674; since_best_mcc: 0;\nFastEstimator-Train: step: 1200; ce: 1.0719717; steps/sec: 14.11;\nFastEstimator-Train: step: 1500; ce: 1.1470697; steps/sec: 11.58;\nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 35.64 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 21.38;\nEval Progress: 26/39; steps/sec: 19.8;\nEval Progress: 39/39; steps/sec: 21.4;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 1564; epoch: 4; ce: 0.94491786; max_mcc: 0.6282373140880735; mcc: 0.6282373140880735; since_best_mcc: 0;\nFastEstimator-Train: step: 1800; ce: 1.0663648; steps/sec: 8.0;\nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 49.97 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 22.19;\nEval Progress: 26/39; steps/sec: 19.47;\nEval Progress: 39/39; steps/sec: 20.88;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 1955; epoch: 5; ce: 0.93076646; max_mcc: 0.6366079684298591; mcc: 0.6366079684298591; since_best_mcc: 0;\nFastEstimator-Train: step: 2100; ce: 0.9752668; steps/sec: 7.75;\nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 45.59 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 19.19;\nEval Progress: 26/39; steps/sec: 20.76;\nEval Progress: 39/39; steps/sec: 18.41;\n</pre> <pre>FastEstimator-Eval: step: 2346; epoch: 6; ce: 0.94940007; max_mcc: 0.6366079684298591; mcc: 0.6282007111891783; since_best_mcc: 1;\nFastEstimator-Train: step: 2400; ce: 0.8638771; steps/sec: 9.27;\nFastEstimator-Train: step: 2700; ce: 0.9501883; steps/sec: 8.57;\nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 44.78 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 21.97;\nEval Progress: 26/39; steps/sec: 23.74;\nEval Progress: 39/39; steps/sec: 24.25;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 2737; epoch: 7; ce: 0.86018026; max_mcc: 0.6611946478733883; mcc: 0.6611946478733883; since_best_mcc: 0;\nFastEstimator-Train: step: 3000; ce: 1.0164135; steps/sec: 9.76;\nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 37.88 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 25.69;\nEval Progress: 26/39; steps/sec: 25.81;\nEval Progress: 39/39; steps/sec: 25.49;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 3128; epoch: 8; ce: 0.82946527; max_mcc: 0.6749589983887951; mcc: 0.6749589983887951; since_best_mcc: 0;\nFastEstimator-Train: step: 3300; ce: 1.04843; steps/sec: 11.26;\nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 37.24 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 22.83;\nEval Progress: 26/39; steps/sec: 23.87;\nEval Progress: 39/39; steps/sec: 24.11;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 3519; epoch: 9; ce: 0.8264248; max_mcc: 0.6798102916434279; mcc: 0.6798102916434279; since_best_mcc: 0;\nFastEstimator-Train: step: 3600; ce: 0.9739647; steps/sec: 10.03;\nFastEstimator-Train: step: 3900; ce: 0.8769522; steps/sec: 10.1;\nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 38.44 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.93;\nEval Progress: 26/39; steps/sec: 24.91;\nEval Progress: 39/39; steps/sec: 25.16;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 3910; epoch: 10; ce: 0.7981984; max_mcc: 0.6902392572292428; mcc: 0.6902392572292428; since_best_mcc: 0;\nFastEstimator-Train: step: 4200; ce: 0.918536; steps/sec: 10.6;\nFastEstimator-Train: step: 4301; epoch: 11; epoch_time: 36.43 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 24.89;\nEval Progress: 26/39; steps/sec: 25.18;\nEval Progress: 39/39; steps/sec: 26.18;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\n</pre> <pre>FastEstimator-Eval: step: 4301; epoch: 11; ce: 0.76242864; max_mcc: 0.6969021893369869; mcc: 0.6969021893369869; since_best_mcc: 0;\nFastEstimator-Train: step: 4500; ce: 0.8292513; steps/sec: 12.17;\nFastEstimator-Train: step: 4692; epoch: 12; epoch_time: 31.82 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 27.31;\nEval Progress: 26/39; steps/sec: 27.0;\nEval Progress: 39/39; steps/sec: 27.18;\nFastEstimator-Eval: step: 4692; epoch: 12; ce: 0.7939056; max_mcc: 0.6969021893369869; mcc: 0.6816917188407557; since_best_mcc: 1;\nFastEstimator-Train: step: 4800; ce: 0.932951; steps/sec: 12.07;\nFastEstimator-Train: step: 5083; epoch: 13; epoch_time: 34.38 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 27.32;\nEval Progress: 26/39; steps/sec: 27.66;\nEval Progress: 39/39; steps/sec: 28.06;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 5083; epoch: 13; ce: 0.7424109; max_mcc: 0.7060542503045709; mcc: 0.7060542503045709; since_best_mcc: 0;\nFastEstimator-Train: step: 5100; ce: 0.72088; steps/sec: 10.88;\nFastEstimator-Train: step: 5400; ce: 0.91354585; steps/sec: 10.09;\nFastEstimator-Train: step: 5474; epoch: 14; epoch_time: 39.62 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 20.92;\nEval Progress: 26/39; steps/sec: 24.26;\nEval Progress: 39/39; steps/sec: 21.41;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 5474; epoch: 14; ce: 0.7411622; max_mcc: 0.7093458570160348; mcc: 0.7093458570160348; since_best_mcc: 0;\nFastEstimator-Train: step: 5700; ce: 0.8136387; steps/sec: 9.16;\nFastEstimator-Train: step: 5865; epoch: 15; epoch_time: 42.75 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 19.89;\nEval Progress: 26/39; steps/sec: 21.78;\nEval Progress: 39/39; steps/sec: 21.96;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 5865; epoch: 15; ce: 0.7313029; max_mcc: 0.7149928925831751; mcc: 0.7149928925831751; since_best_mcc: 0;\nFastEstimator-Train: step: 6000; ce: 0.90510035; steps/sec: 8.93;\nFastEstimator-Train: step: 6256; epoch: 16; epoch_time: 44.4 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 25.51;\nEval Progress: 26/39; steps/sec: 25.13;\nEval Progress: 39/39; steps/sec: 25.51;\n</pre> <pre>FastEstimator-Eval: step: 6256; epoch: 16; ce: 0.73286784; max_mcc: 0.7149928925831751; mcc: 0.7099151095875645; since_best_mcc: 1;\nFastEstimator-Train: step: 6300; ce: 0.67238724; steps/sec: 9.33;\nFastEstimator-Train: step: 6600; ce: 0.8542966; steps/sec: 11.31;\nFastEstimator-Train: step: 6647; epoch: 17; epoch_time: 33.8 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 26.94;\nEval Progress: 26/39; steps/sec: 26.69;\nEval Progress: 39/39; steps/sec: 27.06;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 6647; epoch: 17; ce: 0.7084915; max_mcc: 0.7236824568143699; mcc: 0.7236824568143699; since_best_mcc: 0;\nFastEstimator-Train: step: 6900; ce: 0.6791389; steps/sec: 11.57;\nFastEstimator-Train: step: 7038; epoch: 18; epoch_time: 34.54 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 25.54;\nEval Progress: 26/39; steps/sec: 26.45;\nEval Progress: 39/39; steps/sec: 26.78;\nFastEstimator-BestModelSaver: Saved model to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Eval: step: 7038; epoch: 18; ce: 0.69131714; max_mcc: 0.7360980191985622; mcc: 0.7360980191985622; since_best_mcc: 0;\nFastEstimator-Train: step: 7200; ce: 0.9284673; steps/sec: 11.07;\nFastEstimator-Train: step: 7429; epoch: 19; epoch_time: 37.6 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 23.2;\nEval Progress: 26/39; steps/sec: 23.58;\nEval Progress: 39/39; steps/sec: 25.82;\nFastEstimator-Eval: step: 7429; epoch: 19; ce: 0.71362627; max_mcc: 0.7360980191985622; mcc: 0.7194020040317212; since_best_mcc: 1;\nFastEstimator-Train: step: 7500; ce: 0.72266275; steps/sec: 10.25;\nFastEstimator-Train: step: 7800; ce: 0.8315146; steps/sec: 9.93;\nFastEstimator-Train: step: 7820; epoch: 20; epoch_time: 38.39 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 23.6;\nEval Progress: 26/39; steps/sec: 23.85;\nEval Progress: 39/39; steps/sec: 21.65;\nFastEstimator-Eval: step: 7820; epoch: 20; ce: 0.71525294; max_mcc: 0.7360980191985622; mcc: 0.7239562712561208; since_best_mcc: 2;\nFastEstimator-Train: step: 8100; ce: 0.70906603; steps/sec: 9.13;\nFastEstimator-Train: step: 8211; epoch: 21; epoch_time: 40.53 sec;\nEval Progress: 1/39;\nEval Progress: 13/39; steps/sec: 26.01;\nEval Progress: 26/39; steps/sec: 27.51;\nEval Progress: 39/39; steps/sec: 28.27;\n</pre> <pre>FastEstimator-Eval: step: 8211; epoch: 21; ce: 0.7312853; max_mcc: 0.7360980191985622; mcc: 0.7147634884721064; since_best_mcc: 3;\nFastEstimator-BestModelSaver: Restoring model from /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmppqk4i0uq/model_best_mcc.h5\nFastEstimator-Finish: step: 8211; model_lr: 0.001; total_time: 910.36 sec;\n</pre> In\u00a0[4]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 8211; epoch: 21; ce: 0.7301065; mcc: 0.7327857485184789;\n</pre> <p>In the images above, the 'saliency' column corresponds to a raw saliency mask generated by back-propagating a model's output prediction onto the input image. 'Smoothed saliency' combines multiple saliency masks for each image 'x', where each mask is generated by slightly perturbing the input 'x' before running the forward and backward gradient passes. The number of samples to be combined is controlled by the \"smoothing\" argument in the <code>Saliency</code> <code>Trace</code> constructor. 'Integrated saliency' is a saliency mask generated by starting from a baseline noise image and linearly interpolating the image towards 'x' over a number of steps defined by the \"integrating\" argument in the Saliency constructor. The resulting masks are then combined together. The 'SmInt Saliency' (Smoothed-Integrated) column combines smoothing and integration together. SmInt is generally considered to give the most reliable indication of the important features in an image, but it also takes the longest to compute. It is possible to disable the more complex columns by setting the 'smoothing' and 'integrating' parameters to 0. The 'x saliency' column shows the input image overlaid with whatever saliency column is furthest to the right (SmInt, unless that has been disabled).</p> <p></p> In\u00a0[5]: Copied! <pre>pipeline.batch_size = 6\nbatch = pipeline.get_results(num_steps=2, mode='eval')[1] # Get some images we didn't see already above\nbatch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow\n\nsaliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\")\nimages = saliency_generator.get_smoothed_masks(batch=batch, nsamples=6, nintegration=100)\n\n# Let's convert 'y' and 'y_pred' from numeric values to strings for readability:\nval_to_label = {val: key for key, val in label_mapping.items()}\ny = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))])\ny_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])\n\n# Now simply load up a Display object and let it handle laying out the final result for you\nsave_dir = tempfile.mkdtemp()\nfig = GridDisplay([BatchDisplay(text=y, title='y'), \n                   BatchDisplay(text=y_pred, title='y_pred'),\n                   BatchDisplay(image=batch['x'], title='x'),\n                   BatchDisplay(image=images['saliency'], title='saliency', color_map='inferno')\n                 ])\nfig.show(save_path=os.path.join(save_dir, \"t08a_saliency.png\"))  # save_path is optional, but a useful feature to know about\nfig.show()\n</pre> pipeline.batch_size = 6 batch = pipeline.get_results(num_steps=2, mode='eval')[1] # Get some images we didn't see already above batch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow  saliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\") images = saliency_generator.get_smoothed_masks(batch=batch, nsamples=6, nintegration=100)  # Let's convert 'y' and 'y_pred' from numeric values to strings for readability: val_to_label = {val: key for key, val in label_mapping.items()} y = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))]) y_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])  # Now simply load up a Display object and let it handle laying out the final result for you save_dir = tempfile.mkdtemp() fig = GridDisplay([BatchDisplay(text=y, title='y'),                     BatchDisplay(text=y_pred, title='y_pred'),                    BatchDisplay(image=batch['x'], title='x'),                    BatchDisplay(image=images['saliency'], title='saliency', color_map='inferno')                  ]) fig.show(save_path=os.path.join(save_dir, \"t08a_saliency.png\"))  # save_path is optional, but a useful feature to know about fig.show() <pre>Saving to /var/folders/3r/h9kh47050gv6rbt_pgf8cl540000gn/T/tmpu8vwrtta/t08a_saliency.png\n</pre> <p>The <code>SaliencyNet</code> class also provides 'get_masks' and 'get_integrated_masks' methods for generating other versions of saliency masks. For a detailed overview of various saliency algorithms and their benefits / drawbacks, see https://distill.pub/2020/attribution-baselines/</p>"}, {"location": "tutorial/xai/t03_saliency.html#xai-tutorial-3-saliency-maps", "title": "XAI Tutorial 3: Saliency Maps\u00b6", "text": ""}, {"location": "tutorial/xai/t03_saliency.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Saliency Maps<ul> <li>With Traces</li> <li>Without Traces</li> </ul> </li> </ul>"}, {"location": "tutorial/xai/t03_saliency.html#saliency-maps", "title": "Saliency Maps\u00b6", "text": "<p>Suppose you have a neural network that is performing image classification. The network tells you that the image it is looking at is an airplane, but you want to know whether it is really detecting an airplane, or if it is 'cheating' by noticing the blue sky in the image background. To answer this question, all you need to do is add the <code>Saliency</code> <code>Trace</code> to your list of traces, and pass its output to one of either the <code>ImageSaver</code>, <code>ImageViewer</code>, or <code>TensorBoard</code> <code>Traces</code>.</p>"}, {"location": "tutorial/xai/t03_saliency.html#saliency-maps-without-traces", "title": "Saliency Maps without Traces\u00b6", "text": "<p>Suppose that you want to generate Saliency masks without using a <code>Trace</code>. This can be done through the fe.xai package:</p>"}, {"location": "tutorial/xai/t04_eigencam.html", "title": "XAI Tutorial 4: EigenCAM", "text": "<p>First let's get some imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tensorflow as tf\n\nimport fastestimator as fe\nfrom fastestimator import Network, Pipeline, Estimator\nfrom fastestimator.dataset.data import horse2zebra\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage\nfrom fastestimator.op.tensorop.model import ModelOp\nfrom fastestimator.trace.io import ImageViewer\nfrom fastestimator.trace.xai import EigenCAM\nfrom fastestimator.util import BatchDisplay\n</pre> import tensorflow as tf  import fastestimator as fe from fastestimator import Network, Pipeline, Estimator from fastestimator.dataset.data import horse2zebra from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import Normalize, ReadImage from fastestimator.op.tensorop.model import ModelOp from fastestimator.trace.io import ImageViewer from fastestimator.trace.xai import EigenCAM from fastestimator.util import BatchDisplay <p></p> In\u00a0[2]: Copied! <pre>train_data, eval_data = horse2zebra.load_data(batch_size=5)\ntest_data = eval_data.split(range(5))  # We will just use the first 5 images for our visualizations\npipeline = Pipeline(test_data=test_data, \n                    ops=[ReadImage(inputs=\"B\", outputs=\"B\"), \n                         Resize(image_in=\"B\", image_out=\"B\", height=224, width=224),\n                         Normalize(inputs=\"B\", outputs=\"B\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n                        ])\n</pre> train_data, eval_data = horse2zebra.load_data(batch_size=5) test_data = eval_data.split(range(5))  # We will just use the first 5 images for our visualizations pipeline = Pipeline(test_data=test_data,                      ops=[ReadImage(inputs=\"B\", outputs=\"B\"),                           Resize(image_in=\"B\", image_out=\"B\", height=224, width=224),                          Normalize(inputs=\"B\", outputs=\"B\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),                         ]) In\u00a0[3]: Copied! <pre>batch = pipeline.get_results(mode='test')\nfig = BatchDisplay(image=batch[\"B\"], title=\"Zebras\")\nfig.show()\n</pre> batch = pipeline.get_results(mode='test') fig = BatchDisplay(image=batch[\"B\"], title=\"Zebras\") fig.show() <p></p> In\u00a0[4]: Copied! <pre>model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=\"imagenet\"), optimizer_fn=\"adam\")\nprint(\"\\n\".join([f\"{idx}: {x.name}\" for idx, x in enumerate(model._flatten_layers(include_self=False, recursive=True))]))\n</pre> model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=\"imagenet\"), optimizer_fn=\"adam\") print(\"\\n\".join([f\"{idx}: {x.name}\" for idx, x in enumerate(model._flatten_layers(include_self=False, recursive=True))])) <pre>0: input_1\n1: conv1_pad\n2: conv1_conv\n3: pool1_pad\n4: pool1_pool\n5: conv2_block1_preact_bn\n6: conv2_block1_preact_relu\n7: conv2_block1_1_conv\n8: conv2_block1_1_bn\n9: conv2_block1_1_relu\n10: conv2_block1_2_pad\n11: conv2_block1_2_conv\n12: conv2_block1_2_bn\n13: conv2_block1_2_relu\n14: conv2_block1_0_conv\n15: conv2_block1_3_conv\n16: conv2_block1_out\n17: conv2_block2_preact_bn\n18: conv2_block2_preact_relu\n19: conv2_block2_1_conv\n20: conv2_block2_1_bn\n21: conv2_block2_1_relu\n22: conv2_block2_2_pad\n23: conv2_block2_2_conv\n24: conv2_block2_2_bn\n25: conv2_block2_2_relu\n26: conv2_block2_3_conv\n27: conv2_block2_out\n28: conv2_block3_preact_bn\n29: conv2_block3_preact_relu\n30: conv2_block3_1_conv\n31: conv2_block3_1_bn\n32: conv2_block3_1_relu\n33: conv2_block3_2_pad\n34: conv2_block3_2_conv\n35: conv2_block3_2_bn\n36: conv2_block3_2_relu\n37: max_pooling2d\n38: conv2_block3_3_conv\n39: conv2_block3_out\n40: conv3_block1_preact_bn\n41: conv3_block1_preact_relu\n42: conv3_block1_1_conv\n43: conv3_block1_1_bn\n44: conv3_block1_1_relu\n45: conv3_block1_2_pad\n46: conv3_block1_2_conv\n47: conv3_block1_2_bn\n48: conv3_block1_2_relu\n49: conv3_block1_0_conv\n50: conv3_block1_3_conv\n51: conv3_block1_out\n52: conv3_block2_preact_bn\n53: conv3_block2_preact_relu\n54: conv3_block2_1_conv\n55: conv3_block2_1_bn\n56: conv3_block2_1_relu\n57: conv3_block2_2_pad\n58: conv3_block2_2_conv\n59: conv3_block2_2_bn\n60: conv3_block2_2_relu\n61: conv3_block2_3_conv\n62: conv3_block2_out\n63: conv3_block3_preact_bn\n64: conv3_block3_preact_relu\n65: conv3_block3_1_conv\n66: conv3_block3_1_bn\n67: conv3_block3_1_relu\n68: conv3_block3_2_pad\n69: conv3_block3_2_conv\n70: conv3_block3_2_bn\n71: conv3_block3_2_relu\n72: conv3_block3_3_conv\n73: conv3_block3_out\n74: conv3_block4_preact_bn\n75: conv3_block4_preact_relu\n76: conv3_block4_1_conv\n77: conv3_block4_1_bn\n78: conv3_block4_1_relu\n79: conv3_block4_2_pad\n80: conv3_block4_2_conv\n81: conv3_block4_2_bn\n82: conv3_block4_2_relu\n83: conv3_block4_3_conv\n84: conv3_block4_out\n85: conv3_block5_preact_bn\n86: conv3_block5_preact_relu\n87: conv3_block5_1_conv\n88: conv3_block5_1_bn\n89: conv3_block5_1_relu\n90: conv3_block5_2_pad\n91: conv3_block5_2_conv\n92: conv3_block5_2_bn\n93: conv3_block5_2_relu\n94: conv3_block5_3_conv\n95: conv3_block5_out\n96: conv3_block6_preact_bn\n97: conv3_block6_preact_relu\n98: conv3_block6_1_conv\n99: conv3_block6_1_bn\n100: conv3_block6_1_relu\n101: conv3_block6_2_pad\n102: conv3_block6_2_conv\n103: conv3_block6_2_bn\n104: conv3_block6_2_relu\n105: conv3_block6_3_conv\n106: conv3_block6_out\n107: conv3_block7_preact_bn\n108: conv3_block7_preact_relu\n109: conv3_block7_1_conv\n110: conv3_block7_1_bn\n111: conv3_block7_1_relu\n112: conv3_block7_2_pad\n113: conv3_block7_2_conv\n114: conv3_block7_2_bn\n115: conv3_block7_2_relu\n116: conv3_block7_3_conv\n117: conv3_block7_out\n118: conv3_block8_preact_bn\n119: conv3_block8_preact_relu\n120: conv3_block8_1_conv\n121: conv3_block8_1_bn\n122: conv3_block8_1_relu\n123: conv3_block8_2_pad\n124: conv3_block8_2_conv\n125: conv3_block8_2_bn\n126: conv3_block8_2_relu\n127: max_pooling2d_1\n128: conv3_block8_3_conv\n129: conv3_block8_out\n130: conv4_block1_preact_bn\n131: conv4_block1_preact_relu\n132: conv4_block1_1_conv\n133: conv4_block1_1_bn\n134: conv4_block1_1_relu\n135: conv4_block1_2_pad\n136: conv4_block1_2_conv\n137: conv4_block1_2_bn\n138: conv4_block1_2_relu\n139: conv4_block1_0_conv\n140: conv4_block1_3_conv\n141: conv4_block1_out\n142: conv4_block2_preact_bn\n143: conv4_block2_preact_relu\n144: conv4_block2_1_conv\n145: conv4_block2_1_bn\n146: conv4_block2_1_relu\n147: conv4_block2_2_pad\n148: conv4_block2_2_conv\n149: conv4_block2_2_bn\n150: conv4_block2_2_relu\n151: conv4_block2_3_conv\n152: conv4_block2_out\n153: conv4_block3_preact_bn\n154: conv4_block3_preact_relu\n155: conv4_block3_1_conv\n156: conv4_block3_1_bn\n157: conv4_block3_1_relu\n158: conv4_block3_2_pad\n159: conv4_block3_2_conv\n160: conv4_block3_2_bn\n161: conv4_block3_2_relu\n162: conv4_block3_3_conv\n163: conv4_block3_out\n164: conv4_block4_preact_bn\n165: conv4_block4_preact_relu\n166: conv4_block4_1_conv\n167: conv4_block4_1_bn\n168: conv4_block4_1_relu\n169: conv4_block4_2_pad\n170: conv4_block4_2_conv\n171: conv4_block4_2_bn\n172: conv4_block4_2_relu\n173: conv4_block4_3_conv\n174: conv4_block4_out\n175: conv4_block5_preact_bn\n176: conv4_block5_preact_relu\n177: conv4_block5_1_conv\n178: conv4_block5_1_bn\n179: conv4_block5_1_relu\n180: conv4_block5_2_pad\n181: conv4_block5_2_conv\n182: conv4_block5_2_bn\n183: conv4_block5_2_relu\n184: conv4_block5_3_conv\n185: conv4_block5_out\n186: conv4_block6_preact_bn\n187: conv4_block6_preact_relu\n188: conv4_block6_1_conv\n189: conv4_block6_1_bn\n190: conv4_block6_1_relu\n191: conv4_block6_2_pad\n192: conv4_block6_2_conv\n193: conv4_block6_2_bn\n194: conv4_block6_2_relu\n195: conv4_block6_3_conv\n196: conv4_block6_out\n197: conv4_block7_preact_bn\n198: conv4_block7_preact_relu\n199: conv4_block7_1_conv\n200: conv4_block7_1_bn\n201: conv4_block7_1_relu\n202: conv4_block7_2_pad\n203: conv4_block7_2_conv\n204: conv4_block7_2_bn\n205: conv4_block7_2_relu\n206: conv4_block7_3_conv\n207: conv4_block7_out\n208: conv4_block8_preact_bn\n209: conv4_block8_preact_relu\n210: conv4_block8_1_conv\n211: conv4_block8_1_bn\n212: conv4_block8_1_relu\n213: conv4_block8_2_pad\n214: conv4_block8_2_conv\n215: conv4_block8_2_bn\n216: conv4_block8_2_relu\n217: conv4_block8_3_conv\n218: conv4_block8_out\n219: conv4_block9_preact_bn\n220: conv4_block9_preact_relu\n221: conv4_block9_1_conv\n222: conv4_block9_1_bn\n223: conv4_block9_1_relu\n224: conv4_block9_2_pad\n225: conv4_block9_2_conv\n226: conv4_block9_2_bn\n227: conv4_block9_2_relu\n228: conv4_block9_3_conv\n229: conv4_block9_out\n230: conv4_block10_preact_bn\n231: conv4_block10_preact_relu\n232: conv4_block10_1_conv\n233: conv4_block10_1_bn\n234: conv4_block10_1_relu\n235: conv4_block10_2_pad\n236: conv4_block10_2_conv\n237: conv4_block10_2_bn\n238: conv4_block10_2_relu\n239: conv4_block10_3_conv\n240: conv4_block10_out\n241: conv4_block11_preact_bn\n242: conv4_block11_preact_relu\n243: conv4_block11_1_conv\n244: conv4_block11_1_bn\n245: conv4_block11_1_relu\n246: conv4_block11_2_pad\n247: conv4_block11_2_conv\n248: conv4_block11_2_bn\n249: conv4_block11_2_relu\n250: conv4_block11_3_conv\n251: conv4_block11_out\n252: conv4_block12_preact_bn\n253: conv4_block12_preact_relu\n254: conv4_block12_1_conv\n255: conv4_block12_1_bn\n256: conv4_block12_1_relu\n257: conv4_block12_2_pad\n258: conv4_block12_2_conv\n259: conv4_block12_2_bn\n260: conv4_block12_2_relu\n261: conv4_block12_3_conv\n262: conv4_block12_out\n263: conv4_block13_preact_bn\n264: conv4_block13_preact_relu\n265: conv4_block13_1_conv\n266: conv4_block13_1_bn\n267: conv4_block13_1_relu\n268: conv4_block13_2_pad\n269: conv4_block13_2_conv\n270: conv4_block13_2_bn\n271: conv4_block13_2_relu\n272: conv4_block13_3_conv\n273: conv4_block13_out\n274: conv4_block14_preact_bn\n275: conv4_block14_preact_relu\n276: conv4_block14_1_conv\n277: conv4_block14_1_bn\n278: conv4_block14_1_relu\n279: conv4_block14_2_pad\n280: conv4_block14_2_conv\n281: conv4_block14_2_bn\n282: conv4_block14_2_relu\n283: conv4_block14_3_conv\n284: conv4_block14_out\n285: conv4_block15_preact_bn\n286: conv4_block15_preact_relu\n287: conv4_block15_1_conv\n288: conv4_block15_1_bn\n289: conv4_block15_1_relu\n290: conv4_block15_2_pad\n291: conv4_block15_2_conv\n292: conv4_block15_2_bn\n293: conv4_block15_2_relu\n294: conv4_block15_3_conv\n295: conv4_block15_out\n296: conv4_block16_preact_bn\n297: conv4_block16_preact_relu\n298: conv4_block16_1_conv\n299: conv4_block16_1_bn\n300: conv4_block16_1_relu\n301: conv4_block16_2_pad\n302: conv4_block16_2_conv\n303: conv4_block16_2_bn\n304: conv4_block16_2_relu\n305: conv4_block16_3_conv\n306: conv4_block16_out\n307: conv4_block17_preact_bn\n308: conv4_block17_preact_relu\n309: conv4_block17_1_conv\n310: conv4_block17_1_bn\n311: conv4_block17_1_relu\n312: conv4_block17_2_pad\n313: conv4_block17_2_conv\n314: conv4_block17_2_bn\n315: conv4_block17_2_relu\n316: conv4_block17_3_conv\n317: conv4_block17_out\n318: conv4_block18_preact_bn\n319: conv4_block18_preact_relu\n320: conv4_block18_1_conv\n321: conv4_block18_1_bn\n322: conv4_block18_1_relu\n323: conv4_block18_2_pad\n324: conv4_block18_2_conv\n325: conv4_block18_2_bn\n326: conv4_block18_2_relu\n327: conv4_block18_3_conv\n328: conv4_block18_out\n329: conv4_block19_preact_bn\n330: conv4_block19_preact_relu\n331: conv4_block19_1_conv\n332: conv4_block19_1_bn\n333: conv4_block19_1_relu\n334: conv4_block19_2_pad\n335: conv4_block19_2_conv\n336: conv4_block19_2_bn\n337: conv4_block19_2_relu\n338: conv4_block19_3_conv\n339: conv4_block19_out\n340: conv4_block20_preact_bn\n341: conv4_block20_preact_relu\n342: conv4_block20_1_conv\n343: conv4_block20_1_bn\n344: conv4_block20_1_relu\n345: conv4_block20_2_pad\n346: conv4_block20_2_conv\n347: conv4_block20_2_bn\n348: conv4_block20_2_relu\n349: conv4_block20_3_conv\n350: conv4_block20_out\n351: conv4_block21_preact_bn\n352: conv4_block21_preact_relu\n353: conv4_block21_1_conv\n354: conv4_block21_1_bn\n355: conv4_block21_1_relu\n356: conv4_block21_2_pad\n357: conv4_block21_2_conv\n358: conv4_block21_2_bn\n359: conv4_block21_2_relu\n360: conv4_block21_3_conv\n361: conv4_block21_out\n362: conv4_block22_preact_bn\n363: conv4_block22_preact_relu\n364: conv4_block22_1_conv\n365: conv4_block22_1_bn\n366: conv4_block22_1_relu\n367: conv4_block22_2_pad\n368: conv4_block22_2_conv\n369: conv4_block22_2_bn\n370: conv4_block22_2_relu\n371: conv4_block22_3_conv\n372: conv4_block22_out\n373: conv4_block23_preact_bn\n374: conv4_block23_preact_relu\n375: conv4_block23_1_conv\n376: conv4_block23_1_bn\n377: conv4_block23_1_relu\n378: conv4_block23_2_pad\n379: conv4_block23_2_conv\n380: conv4_block23_2_bn\n381: conv4_block23_2_relu\n382: conv4_block23_3_conv\n383: conv4_block23_out\n384: conv4_block24_preact_bn\n385: conv4_block24_preact_relu\n386: conv4_block24_1_conv\n387: conv4_block24_1_bn\n388: conv4_block24_1_relu\n389: conv4_block24_2_pad\n390: conv4_block24_2_conv\n391: conv4_block24_2_bn\n392: conv4_block24_2_relu\n393: conv4_block24_3_conv\n394: conv4_block24_out\n395: conv4_block25_preact_bn\n396: conv4_block25_preact_relu\n397: conv4_block25_1_conv\n398: conv4_block25_1_bn\n399: conv4_block25_1_relu\n400: conv4_block25_2_pad\n401: conv4_block25_2_conv\n402: conv4_block25_2_bn\n403: conv4_block25_2_relu\n404: conv4_block25_3_conv\n405: conv4_block25_out\n406: conv4_block26_preact_bn\n407: conv4_block26_preact_relu\n408: conv4_block26_1_conv\n409: conv4_block26_1_bn\n410: conv4_block26_1_relu\n411: conv4_block26_2_pad\n412: conv4_block26_2_conv\n413: conv4_block26_2_bn\n414: conv4_block26_2_relu\n415: conv4_block26_3_conv\n416: conv4_block26_out\n417: conv4_block27_preact_bn\n418: conv4_block27_preact_relu\n419: conv4_block27_1_conv\n420: conv4_block27_1_bn\n421: conv4_block27_1_relu\n422: conv4_block27_2_pad\n423: conv4_block27_2_conv\n424: conv4_block27_2_bn\n425: conv4_block27_2_relu\n426: conv4_block27_3_conv\n427: conv4_block27_out\n428: conv4_block28_preact_bn\n429: conv4_block28_preact_relu\n430: conv4_block28_1_conv\n431: conv4_block28_1_bn\n432: conv4_block28_1_relu\n433: conv4_block28_2_pad\n434: conv4_block28_2_conv\n435: conv4_block28_2_bn\n436: conv4_block28_2_relu\n437: conv4_block28_3_conv\n438: conv4_block28_out\n439: conv4_block29_preact_bn\n440: conv4_block29_preact_relu\n441: conv4_block29_1_conv\n442: conv4_block29_1_bn\n443: conv4_block29_1_relu\n444: conv4_block29_2_pad\n445: conv4_block29_2_conv\n446: conv4_block29_2_bn\n447: conv4_block29_2_relu\n448: conv4_block29_3_conv\n449: conv4_block29_out\n450: conv4_block30_preact_bn\n451: conv4_block30_preact_relu\n452: conv4_block30_1_conv\n453: conv4_block30_1_bn\n454: conv4_block30_1_relu\n455: conv4_block30_2_pad\n456: conv4_block30_2_conv\n457: conv4_block30_2_bn\n458: conv4_block30_2_relu\n459: conv4_block30_3_conv\n460: conv4_block30_out\n461: conv4_block31_preact_bn\n462: conv4_block31_preact_relu\n463: conv4_block31_1_conv\n464: conv4_block31_1_bn\n465: conv4_block31_1_relu\n466: conv4_block31_2_pad\n467: conv4_block31_2_conv\n468: conv4_block31_2_bn\n469: conv4_block31_2_relu\n470: conv4_block31_3_conv\n471: conv4_block31_out\n472: conv4_block32_preact_bn\n473: conv4_block32_preact_relu\n474: conv4_block32_1_conv\n475: conv4_block32_1_bn\n476: conv4_block32_1_relu\n477: conv4_block32_2_pad\n478: conv4_block32_2_conv\n479: conv4_block32_2_bn\n480: conv4_block32_2_relu\n481: conv4_block32_3_conv\n482: conv4_block32_out\n483: conv4_block33_preact_bn\n484: conv4_block33_preact_relu\n485: conv4_block33_1_conv\n486: conv4_block33_1_bn\n487: conv4_block33_1_relu\n488: conv4_block33_2_pad\n489: conv4_block33_2_conv\n490: conv4_block33_2_bn\n491: conv4_block33_2_relu\n492: conv4_block33_3_conv\n493: conv4_block33_out\n494: conv4_block34_preact_bn\n495: conv4_block34_preact_relu\n496: conv4_block34_1_conv\n497: conv4_block34_1_bn\n498: conv4_block34_1_relu\n499: conv4_block34_2_pad\n500: conv4_block34_2_conv\n501: conv4_block34_2_bn\n502: conv4_block34_2_relu\n503: conv4_block34_3_conv\n504: conv4_block34_out\n505: conv4_block35_preact_bn\n506: conv4_block35_preact_relu\n507: conv4_block35_1_conv\n508: conv4_block35_1_bn\n509: conv4_block35_1_relu\n510: conv4_block35_2_pad\n511: conv4_block35_2_conv\n512: conv4_block35_2_bn\n513: conv4_block35_2_relu\n514: conv4_block35_3_conv\n515: conv4_block35_out\n516: conv4_block36_preact_bn\n517: conv4_block36_preact_relu\n518: conv4_block36_1_conv\n519: conv4_block36_1_bn\n520: conv4_block36_1_relu\n521: conv4_block36_2_pad\n522: conv4_block36_2_conv\n523: conv4_block36_2_bn\n524: conv4_block36_2_relu\n525: max_pooling2d_2\n526: conv4_block36_3_conv\n527: conv4_block36_out\n528: conv5_block1_preact_bn\n529: conv5_block1_preact_relu\n530: conv5_block1_1_conv\n531: conv5_block1_1_bn\n532: conv5_block1_1_relu\n533: conv5_block1_2_pad\n534: conv5_block1_2_conv\n535: conv5_block1_2_bn\n536: conv5_block1_2_relu\n537: conv5_block1_0_conv\n538: conv5_block1_3_conv\n539: conv5_block1_out\n540: conv5_block2_preact_bn\n541: conv5_block2_preact_relu\n542: conv5_block2_1_conv\n543: conv5_block2_1_bn\n544: conv5_block2_1_relu\n545: conv5_block2_2_pad\n546: conv5_block2_2_conv\n547: conv5_block2_2_bn\n548: conv5_block2_2_relu\n549: conv5_block2_3_conv\n550: conv5_block2_out\n551: conv5_block3_preact_bn\n552: conv5_block3_preact_relu\n553: conv5_block3_1_conv\n554: conv5_block3_1_bn\n555: conv5_block3_1_relu\n556: conv5_block3_2_pad\n557: conv5_block3_2_conv\n558: conv5_block3_2_bn\n559: conv5_block3_2_relu\n560: conv5_block3_3_conv\n561: conv5_block3_out\n562: post_bn\n563: post_relu\n564: avg_pool\n565: predictions\n</pre> <p>This model has quite a few layers to choose from. We will try the avg_pool layer at the end</p> In\u00a0[5]: Copied! <pre>network = Network(ops=[\n    ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='post_relu'),\n])\n</pre> network = Network(ops=[     ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='post_relu'), ]) <p></p> In\u00a0[6]: Copied! <pre>traces = [\n        EigenCAM(images=\"B\", activations=\"embedding\", preds=\"y_pred\"),\n        ImageViewer(inputs=\"eigencam\", mode=\"test\")\n    ]\n\nestimator = Estimator(pipeline=pipeline,\n                      network=network,\n                      traces=traces,\n                      epochs=1,\n                     )\n</pre> traces = [         EigenCAM(images=\"B\", activations=\"embedding\", preds=\"y_pred\"),         ImageViewer(inputs=\"eigencam\", mode=\"test\")     ]  estimator = Estimator(pipeline=pipeline,                       network=network,                       traces=traces,                       epochs=1,                      ) In\u00a0[7]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Warn: the key 'A' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>FastEstimator-Test: step: None; epoch: 1;\n</pre> <p>ImageNet class 340 refers to zebras, so the model is correct for all of our data here. We can also see through the EigenCAM output that the model does seem to be focused on the zebra as the largest principal component of the feature output. It is interesting to note, though, that for several images the second or third principal components are focused more on background regions adjacent to zebras. Let's compare this with an untrained model:</p> In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=None), optimizer_fn=\"adam\")\nnetwork = Network(ops=[\n    ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='post_relu'),\n])\ntraces = [\n        EigenCAM(images=\"B\", activations=\"embedding\", preds=\"y_pred\"),\n        ImageViewer(inputs=\"eigencam\", mode=\"test\")\n    ]\n\nestimator = Estimator(pipeline=pipeline,\n                      network=network,\n                      traces=traces,\n                      epochs=1,\n                     )\nestimator.test()\n</pre> model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=None), optimizer_fn=\"adam\") network = Network(ops=[     ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='post_relu'), ]) traces = [         EigenCAM(images=\"B\", activations=\"embedding\", preds=\"y_pred\"),         ImageViewer(inputs=\"eigencam\", mode=\"test\")     ]  estimator = Estimator(pipeline=pipeline,                       network=network,                       traces=traces,                       epochs=1,                      ) estimator.test() <pre>FastEstimator-Test: step: None; epoch: 1;\n</pre> <p>As we can see from the images above, the untrained model does happen to focus on zebra-adjacent areas when the zebras take up a large percentage of the image (convolution layers are natural edge detectors), but in images where the zebras are further away / smaller the untrained network has no idea where to focus. It is good to keep in mind, however, that for some images even completely untrained models can appear to be looking in approximately the correct location.</p>"}, {"location": "tutorial/xai/t04_eigencam.html#xai-tutorial-4-eigencam", "title": "XAI Tutorial 4: EigenCAM\u00b6", "text": ""}, {"location": "tutorial/xai/t04_eigencam.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Basic Pipeline Visualization</li> <li>Extracting Intermediate Layer Outputs</li> <li>EigenCAM</li> </ul>"}, {"location": "tutorial/xai/t04_eigencam.html#example-data-and-pipeline", "title": "Example Data and Pipeline\u00b6", "text": "<p>For this tutorial we will use some pictures of zebras with minimal pre-processing. Let's visualize some of the images to see what we're working with.</p>"}, {"location": "tutorial/xai/t04_eigencam.html#extracting-intermediate-layer-outputs", "title": "Extracting Intermediate Layer Outputs\u00b6", "text": "<p>We will use a pre-trained ResNet151 model for this example, with standard ImageNet weights. We will inspect the model to decide which layer we want to analyze with EigenCAM.</p>"}, {"location": "tutorial/xai/t04_eigencam.html#eigencam", "title": "EigenCAM\u00b6", "text": "<p>Now it's time to generate some EiganCAM images. You can read more about this method here. In short, this is a CAM method that only considers the principal components of convolution layer feature representations rather than the actual outputs of a model. As such it does not require any gradients, and is relatively blind to adversarial perturbation.</p>"}, {"location": "tutorial/xai/t05_gradcam.html", "title": "XAI Tutorial 5: GradCAM", "text": "<p>First let's get some imports out of the way:</p> In\u00a0[1]: Copied! <pre>import tensorflow as tf\n\nimport fastestimator as fe\nfrom fastestimator import Network, Pipeline, Estimator\nfrom fastestimator.dataset.data import horse2zebra\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage\nfrom fastestimator.op.tensorop import LambdaOp\nfrom fastestimator.op.tensorop.gradient import GradientOp\nfrom fastestimator.op.tensorop.model import ModelOp\nfrom fastestimator.trace.io import ImageViewer\nfrom fastestimator.trace.xai import GradCAM\nfrom fastestimator.util import BatchDisplay\n</pre> import tensorflow as tf  import fastestimator as fe from fastestimator import Network, Pipeline, Estimator from fastestimator.dataset.data import horse2zebra from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import Normalize, ReadImage from fastestimator.op.tensorop import LambdaOp from fastestimator.op.tensorop.gradient import GradientOp from fastestimator.op.tensorop.model import ModelOp from fastestimator.trace.io import ImageViewer from fastestimator.trace.xai import GradCAM from fastestimator.util import BatchDisplay <p></p> In\u00a0[2]: Copied! <pre>train_data, eval_data = horse2zebra.load_data(batch_size=5)\ntest_data = eval_data.split(range(5))  # We will just use the first 5 images for our visualizations\npipeline = Pipeline(test_data=test_data, \n                    ops=[ReadImage(inputs=\"B\", outputs=\"B\"), \n                         Resize(image_in=\"B\", image_out=\"B\", height=224, width=224),\n                         Normalize(inputs=\"B\", outputs=\"B\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n                        ])\n</pre> train_data, eval_data = horse2zebra.load_data(batch_size=5) test_data = eval_data.split(range(5))  # We will just use the first 5 images for our visualizations pipeline = Pipeline(test_data=test_data,                      ops=[ReadImage(inputs=\"B\", outputs=\"B\"),                           Resize(image_in=\"B\", image_out=\"B\", height=224, width=224),                          Normalize(inputs=\"B\", outputs=\"B\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),                         ]) In\u00a0[3]: Copied! <pre>batch = pipeline.get_results(mode='test')\nfig = BatchDisplay(image=batch['B'], title=\"Zebras\")\nfig.show()\n</pre> batch = pipeline.get_results(mode='test') fig = BatchDisplay(image=batch['B'], title=\"Zebras\") fig.show() <p></p> In\u00a0[4]: Copied! <pre>model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=\"imagenet\"), optimizer_fn=\"adam\")\nprint(\"\\n\".join([f\"{idx}: {x.name}\" for idx, x in enumerate(model._flatten_layers(include_self=False, recursive=True))]))\n</pre> model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=\"imagenet\"), optimizer_fn=\"adam\") print(\"\\n\".join([f\"{idx}: {x.name}\" for idx, x in enumerate(model._flatten_layers(include_self=False, recursive=True))])) <pre>0: input_1\n1: conv1_pad\n2: conv1_conv\n3: pool1_pad\n4: pool1_pool\n5: conv2_block1_preact_bn\n6: conv2_block1_preact_relu\n7: conv2_block1_1_conv\n8: conv2_block1_1_bn\n9: conv2_block1_1_relu\n10: conv2_block1_2_pad\n11: conv2_block1_2_conv\n12: conv2_block1_2_bn\n13: conv2_block1_2_relu\n14: conv2_block1_0_conv\n15: conv2_block1_3_conv\n16: conv2_block1_out\n17: conv2_block2_preact_bn\n18: conv2_block2_preact_relu\n19: conv2_block2_1_conv\n20: conv2_block2_1_bn\n21: conv2_block2_1_relu\n22: conv2_block2_2_pad\n23: conv2_block2_2_conv\n24: conv2_block2_2_bn\n25: conv2_block2_2_relu\n26: conv2_block2_3_conv\n27: conv2_block2_out\n28: conv2_block3_preact_bn\n29: conv2_block3_preact_relu\n30: conv2_block3_1_conv\n31: conv2_block3_1_bn\n32: conv2_block3_1_relu\n33: conv2_block3_2_pad\n34: conv2_block3_2_conv\n35: conv2_block3_2_bn\n36: conv2_block3_2_relu\n37: max_pooling2d\n38: conv2_block3_3_conv\n39: conv2_block3_out\n40: conv3_block1_preact_bn\n41: conv3_block1_preact_relu\n42: conv3_block1_1_conv\n43: conv3_block1_1_bn\n44: conv3_block1_1_relu\n45: conv3_block1_2_pad\n46: conv3_block1_2_conv\n47: conv3_block1_2_bn\n48: conv3_block1_2_relu\n49: conv3_block1_0_conv\n50: conv3_block1_3_conv\n51: conv3_block1_out\n52: conv3_block2_preact_bn\n53: conv3_block2_preact_relu\n54: conv3_block2_1_conv\n55: conv3_block2_1_bn\n56: conv3_block2_1_relu\n57: conv3_block2_2_pad\n58: conv3_block2_2_conv\n59: conv3_block2_2_bn\n60: conv3_block2_2_relu\n61: conv3_block2_3_conv\n62: conv3_block2_out\n63: conv3_block3_preact_bn\n64: conv3_block3_preact_relu\n65: conv3_block3_1_conv\n66: conv3_block3_1_bn\n67: conv3_block3_1_relu\n68: conv3_block3_2_pad\n69: conv3_block3_2_conv\n70: conv3_block3_2_bn\n71: conv3_block3_2_relu\n72: conv3_block3_3_conv\n73: conv3_block3_out\n74: conv3_block4_preact_bn\n75: conv3_block4_preact_relu\n76: conv3_block4_1_conv\n77: conv3_block4_1_bn\n78: conv3_block4_1_relu\n79: conv3_block4_2_pad\n80: conv3_block4_2_conv\n81: conv3_block4_2_bn\n82: conv3_block4_2_relu\n83: conv3_block4_3_conv\n84: conv3_block4_out\n85: conv3_block5_preact_bn\n86: conv3_block5_preact_relu\n87: conv3_block5_1_conv\n88: conv3_block5_1_bn\n89: conv3_block5_1_relu\n90: conv3_block5_2_pad\n91: conv3_block5_2_conv\n92: conv3_block5_2_bn\n93: conv3_block5_2_relu\n94: conv3_block5_3_conv\n95: conv3_block5_out\n96: conv3_block6_preact_bn\n97: conv3_block6_preact_relu\n98: conv3_block6_1_conv\n99: conv3_block6_1_bn\n100: conv3_block6_1_relu\n101: conv3_block6_2_pad\n102: conv3_block6_2_conv\n103: conv3_block6_2_bn\n104: conv3_block6_2_relu\n105: conv3_block6_3_conv\n106: conv3_block6_out\n107: conv3_block7_preact_bn\n108: conv3_block7_preact_relu\n109: conv3_block7_1_conv\n110: conv3_block7_1_bn\n111: conv3_block7_1_relu\n112: conv3_block7_2_pad\n113: conv3_block7_2_conv\n114: conv3_block7_2_bn\n115: conv3_block7_2_relu\n116: conv3_block7_3_conv\n117: conv3_block7_out\n118: conv3_block8_preact_bn\n119: conv3_block8_preact_relu\n120: conv3_block8_1_conv\n121: conv3_block8_1_bn\n122: conv3_block8_1_relu\n123: conv3_block8_2_pad\n124: conv3_block8_2_conv\n125: conv3_block8_2_bn\n126: conv3_block8_2_relu\n127: max_pooling2d_1\n128: conv3_block8_3_conv\n129: conv3_block8_out\n130: conv4_block1_preact_bn\n131: conv4_block1_preact_relu\n132: conv4_block1_1_conv\n133: conv4_block1_1_bn\n134: conv4_block1_1_relu\n135: conv4_block1_2_pad\n136: conv4_block1_2_conv\n137: conv4_block1_2_bn\n138: conv4_block1_2_relu\n139: conv4_block1_0_conv\n140: conv4_block1_3_conv\n141: conv4_block1_out\n142: conv4_block2_preact_bn\n143: conv4_block2_preact_relu\n144: conv4_block2_1_conv\n145: conv4_block2_1_bn\n146: conv4_block2_1_relu\n147: conv4_block2_2_pad\n148: conv4_block2_2_conv\n149: conv4_block2_2_bn\n150: conv4_block2_2_relu\n151: conv4_block2_3_conv\n152: conv4_block2_out\n153: conv4_block3_preact_bn\n154: conv4_block3_preact_relu\n155: conv4_block3_1_conv\n156: conv4_block3_1_bn\n157: conv4_block3_1_relu\n158: conv4_block3_2_pad\n159: conv4_block3_2_conv\n160: conv4_block3_2_bn\n161: conv4_block3_2_relu\n162: conv4_block3_3_conv\n163: conv4_block3_out\n164: conv4_block4_preact_bn\n165: conv4_block4_preact_relu\n166: conv4_block4_1_conv\n167: conv4_block4_1_bn\n168: conv4_block4_1_relu\n169: conv4_block4_2_pad\n170: conv4_block4_2_conv\n171: conv4_block4_2_bn\n172: conv4_block4_2_relu\n173: conv4_block4_3_conv\n174: conv4_block4_out\n175: conv4_block5_preact_bn\n176: conv4_block5_preact_relu\n177: conv4_block5_1_conv\n178: conv4_block5_1_bn\n179: conv4_block5_1_relu\n180: conv4_block5_2_pad\n181: conv4_block5_2_conv\n182: conv4_block5_2_bn\n183: conv4_block5_2_relu\n184: conv4_block5_3_conv\n185: conv4_block5_out\n186: conv4_block6_preact_bn\n187: conv4_block6_preact_relu\n188: conv4_block6_1_conv\n189: conv4_block6_1_bn\n190: conv4_block6_1_relu\n191: conv4_block6_2_pad\n192: conv4_block6_2_conv\n193: conv4_block6_2_bn\n194: conv4_block6_2_relu\n195: conv4_block6_3_conv\n196: conv4_block6_out\n197: conv4_block7_preact_bn\n198: conv4_block7_preact_relu\n199: conv4_block7_1_conv\n200: conv4_block7_1_bn\n201: conv4_block7_1_relu\n202: conv4_block7_2_pad\n203: conv4_block7_2_conv\n204: conv4_block7_2_bn\n205: conv4_block7_2_relu\n206: conv4_block7_3_conv\n207: conv4_block7_out\n208: conv4_block8_preact_bn\n209: conv4_block8_preact_relu\n210: conv4_block8_1_conv\n211: conv4_block8_1_bn\n212: conv4_block8_1_relu\n213: conv4_block8_2_pad\n214: conv4_block8_2_conv\n215: conv4_block8_2_bn\n216: conv4_block8_2_relu\n217: conv4_block8_3_conv\n218: conv4_block8_out\n219: conv4_block9_preact_bn\n220: conv4_block9_preact_relu\n221: conv4_block9_1_conv\n222: conv4_block9_1_bn\n223: conv4_block9_1_relu\n224: conv4_block9_2_pad\n225: conv4_block9_2_conv\n226: conv4_block9_2_bn\n227: conv4_block9_2_relu\n228: conv4_block9_3_conv\n229: conv4_block9_out\n230: conv4_block10_preact_bn\n231: conv4_block10_preact_relu\n232: conv4_block10_1_conv\n233: conv4_block10_1_bn\n234: conv4_block10_1_relu\n235: conv4_block10_2_pad\n236: conv4_block10_2_conv\n237: conv4_block10_2_bn\n238: conv4_block10_2_relu\n239: conv4_block10_3_conv\n240: conv4_block10_out\n241: conv4_block11_preact_bn\n242: conv4_block11_preact_relu\n243: conv4_block11_1_conv\n244: conv4_block11_1_bn\n245: conv4_block11_1_relu\n246: conv4_block11_2_pad\n247: conv4_block11_2_conv\n248: conv4_block11_2_bn\n249: conv4_block11_2_relu\n250: conv4_block11_3_conv\n251: conv4_block11_out\n252: conv4_block12_preact_bn\n253: conv4_block12_preact_relu\n254: conv4_block12_1_conv\n255: conv4_block12_1_bn\n256: conv4_block12_1_relu\n257: conv4_block12_2_pad\n258: conv4_block12_2_conv\n259: conv4_block12_2_bn\n260: conv4_block12_2_relu\n261: conv4_block12_3_conv\n262: conv4_block12_out\n263: conv4_block13_preact_bn\n264: conv4_block13_preact_relu\n265: conv4_block13_1_conv\n266: conv4_block13_1_bn\n267: conv4_block13_1_relu\n268: conv4_block13_2_pad\n269: conv4_block13_2_conv\n270: conv4_block13_2_bn\n271: conv4_block13_2_relu\n272: conv4_block13_3_conv\n273: conv4_block13_out\n274: conv4_block14_preact_bn\n275: conv4_block14_preact_relu\n276: conv4_block14_1_conv\n277: conv4_block14_1_bn\n278: conv4_block14_1_relu\n279: conv4_block14_2_pad\n280: conv4_block14_2_conv\n281: conv4_block14_2_bn\n282: conv4_block14_2_relu\n283: conv4_block14_3_conv\n284: conv4_block14_out\n285: conv4_block15_preact_bn\n286: conv4_block15_preact_relu\n287: conv4_block15_1_conv\n288: conv4_block15_1_bn\n289: conv4_block15_1_relu\n290: conv4_block15_2_pad\n291: conv4_block15_2_conv\n292: conv4_block15_2_bn\n293: conv4_block15_2_relu\n294: conv4_block15_3_conv\n295: conv4_block15_out\n296: conv4_block16_preact_bn\n297: conv4_block16_preact_relu\n298: conv4_block16_1_conv\n299: conv4_block16_1_bn\n300: conv4_block16_1_relu\n301: conv4_block16_2_pad\n302: conv4_block16_2_conv\n303: conv4_block16_2_bn\n304: conv4_block16_2_relu\n305: conv4_block16_3_conv\n306: conv4_block16_out\n307: conv4_block17_preact_bn\n308: conv4_block17_preact_relu\n309: conv4_block17_1_conv\n310: conv4_block17_1_bn\n311: conv4_block17_1_relu\n312: conv4_block17_2_pad\n313: conv4_block17_2_conv\n314: conv4_block17_2_bn\n315: conv4_block17_2_relu\n316: conv4_block17_3_conv\n317: conv4_block17_out\n318: conv4_block18_preact_bn\n319: conv4_block18_preact_relu\n320: conv4_block18_1_conv\n321: conv4_block18_1_bn\n322: conv4_block18_1_relu\n323: conv4_block18_2_pad\n324: conv4_block18_2_conv\n325: conv4_block18_2_bn\n326: conv4_block18_2_relu\n327: conv4_block18_3_conv\n328: conv4_block18_out\n329: conv4_block19_preact_bn\n330: conv4_block19_preact_relu\n331: conv4_block19_1_conv\n332: conv4_block19_1_bn\n333: conv4_block19_1_relu\n334: conv4_block19_2_pad\n335: conv4_block19_2_conv\n336: conv4_block19_2_bn\n337: conv4_block19_2_relu\n338: conv4_block19_3_conv\n339: conv4_block19_out\n340: conv4_block20_preact_bn\n341: conv4_block20_preact_relu\n342: conv4_block20_1_conv\n343: conv4_block20_1_bn\n344: conv4_block20_1_relu\n345: conv4_block20_2_pad\n346: conv4_block20_2_conv\n347: conv4_block20_2_bn\n348: conv4_block20_2_relu\n349: conv4_block20_3_conv\n350: conv4_block20_out\n351: conv4_block21_preact_bn\n352: conv4_block21_preact_relu\n353: conv4_block21_1_conv\n354: conv4_block21_1_bn\n355: conv4_block21_1_relu\n356: conv4_block21_2_pad\n357: conv4_block21_2_conv\n358: conv4_block21_2_bn\n359: conv4_block21_2_relu\n360: conv4_block21_3_conv\n361: conv4_block21_out\n362: conv4_block22_preact_bn\n363: conv4_block22_preact_relu\n364: conv4_block22_1_conv\n365: conv4_block22_1_bn\n366: conv4_block22_1_relu\n367: conv4_block22_2_pad\n368: conv4_block22_2_conv\n369: conv4_block22_2_bn\n370: conv4_block22_2_relu\n371: conv4_block22_3_conv\n372: conv4_block22_out\n373: conv4_block23_preact_bn\n374: conv4_block23_preact_relu\n375: conv4_block23_1_conv\n376: conv4_block23_1_bn\n377: conv4_block23_1_relu\n378: conv4_block23_2_pad\n379: conv4_block23_2_conv\n380: conv4_block23_2_bn\n381: conv4_block23_2_relu\n382: conv4_block23_3_conv\n383: conv4_block23_out\n384: conv4_block24_preact_bn\n385: conv4_block24_preact_relu\n386: conv4_block24_1_conv\n387: conv4_block24_1_bn\n388: conv4_block24_1_relu\n389: conv4_block24_2_pad\n390: conv4_block24_2_conv\n391: conv4_block24_2_bn\n392: conv4_block24_2_relu\n393: conv4_block24_3_conv\n394: conv4_block24_out\n395: conv4_block25_preact_bn\n396: conv4_block25_preact_relu\n397: conv4_block25_1_conv\n398: conv4_block25_1_bn\n399: conv4_block25_1_relu\n400: conv4_block25_2_pad\n401: conv4_block25_2_conv\n402: conv4_block25_2_bn\n403: conv4_block25_2_relu\n404: conv4_block25_3_conv\n405: conv4_block25_out\n406: conv4_block26_preact_bn\n407: conv4_block26_preact_relu\n408: conv4_block26_1_conv\n409: conv4_block26_1_bn\n410: conv4_block26_1_relu\n411: conv4_block26_2_pad\n412: conv4_block26_2_conv\n413: conv4_block26_2_bn\n414: conv4_block26_2_relu\n415: conv4_block26_3_conv\n416: conv4_block26_out\n417: conv4_block27_preact_bn\n418: conv4_block27_preact_relu\n419: conv4_block27_1_conv\n420: conv4_block27_1_bn\n421: conv4_block27_1_relu\n422: conv4_block27_2_pad\n423: conv4_block27_2_conv\n424: conv4_block27_2_bn\n425: conv4_block27_2_relu\n426: conv4_block27_3_conv\n427: conv4_block27_out\n428: conv4_block28_preact_bn\n429: conv4_block28_preact_relu\n430: conv4_block28_1_conv\n431: conv4_block28_1_bn\n432: conv4_block28_1_relu\n433: conv4_block28_2_pad\n434: conv4_block28_2_conv\n435: conv4_block28_2_bn\n436: conv4_block28_2_relu\n437: conv4_block28_3_conv\n438: conv4_block28_out\n439: conv4_block29_preact_bn\n440: conv4_block29_preact_relu\n441: conv4_block29_1_conv\n442: conv4_block29_1_bn\n443: conv4_block29_1_relu\n444: conv4_block29_2_pad\n445: conv4_block29_2_conv\n446: conv4_block29_2_bn\n447: conv4_block29_2_relu\n448: conv4_block29_3_conv\n449: conv4_block29_out\n450: conv4_block30_preact_bn\n451: conv4_block30_preact_relu\n452: conv4_block30_1_conv\n453: conv4_block30_1_bn\n454: conv4_block30_1_relu\n455: conv4_block30_2_pad\n456: conv4_block30_2_conv\n457: conv4_block30_2_bn\n458: conv4_block30_2_relu\n459: conv4_block30_3_conv\n460: conv4_block30_out\n461: conv4_block31_preact_bn\n462: conv4_block31_preact_relu\n463: conv4_block31_1_conv\n464: conv4_block31_1_bn\n465: conv4_block31_1_relu\n466: conv4_block31_2_pad\n467: conv4_block31_2_conv\n468: conv4_block31_2_bn\n469: conv4_block31_2_relu\n470: conv4_block31_3_conv\n471: conv4_block31_out\n472: conv4_block32_preact_bn\n473: conv4_block32_preact_relu\n474: conv4_block32_1_conv\n475: conv4_block32_1_bn\n476: conv4_block32_1_relu\n477: conv4_block32_2_pad\n478: conv4_block32_2_conv\n479: conv4_block32_2_bn\n480: conv4_block32_2_relu\n481: conv4_block32_3_conv\n482: conv4_block32_out\n483: conv4_block33_preact_bn\n484: conv4_block33_preact_relu\n485: conv4_block33_1_conv\n486: conv4_block33_1_bn\n487: conv4_block33_1_relu\n488: conv4_block33_2_pad\n489: conv4_block33_2_conv\n490: conv4_block33_2_bn\n491: conv4_block33_2_relu\n492: conv4_block33_3_conv\n493: conv4_block33_out\n494: conv4_block34_preact_bn\n495: conv4_block34_preact_relu\n496: conv4_block34_1_conv\n497: conv4_block34_1_bn\n498: conv4_block34_1_relu\n499: conv4_block34_2_pad\n500: conv4_block34_2_conv\n501: conv4_block34_2_bn\n502: conv4_block34_2_relu\n503: conv4_block34_3_conv\n504: conv4_block34_out\n505: conv4_block35_preact_bn\n506: conv4_block35_preact_relu\n507: conv4_block35_1_conv\n508: conv4_block35_1_bn\n509: conv4_block35_1_relu\n510: conv4_block35_2_pad\n511: conv4_block35_2_conv\n512: conv4_block35_2_bn\n513: conv4_block35_2_relu\n514: conv4_block35_3_conv\n515: conv4_block35_out\n516: conv4_block36_preact_bn\n517: conv4_block36_preact_relu\n518: conv4_block36_1_conv\n519: conv4_block36_1_bn\n520: conv4_block36_1_relu\n521: conv4_block36_2_pad\n522: conv4_block36_2_conv\n523: conv4_block36_2_bn\n524: conv4_block36_2_relu\n525: max_pooling2d_2\n526: conv4_block36_3_conv\n527: conv4_block36_out\n528: conv5_block1_preact_bn\n529: conv5_block1_preact_relu\n530: conv5_block1_1_conv\n531: conv5_block1_1_bn\n532: conv5_block1_1_relu\n533: conv5_block1_2_pad\n534: conv5_block1_2_conv\n535: conv5_block1_2_bn\n536: conv5_block1_2_relu\n537: conv5_block1_0_conv\n538: conv5_block1_3_conv\n539: conv5_block1_out\n540: conv5_block2_preact_bn\n541: conv5_block2_preact_relu\n542: conv5_block2_1_conv\n543: conv5_block2_1_bn\n544: conv5_block2_1_relu\n545: conv5_block2_2_pad\n546: conv5_block2_2_conv\n547: conv5_block2_2_bn\n548: conv5_block2_2_relu\n549: conv5_block2_3_conv\n550: conv5_block2_out\n551: conv5_block3_preact_bn\n552: conv5_block3_preact_relu\n553: conv5_block3_1_conv\n554: conv5_block3_1_bn\n555: conv5_block3_1_relu\n556: conv5_block3_2_pad\n557: conv5_block3_2_conv\n558: conv5_block3_2_bn\n559: conv5_block3_2_relu\n560: conv5_block3_3_conv\n561: conv5_block3_out\n562: post_bn\n563: post_relu\n564: avg_pool\n565: predictions\n</pre> <p>This model has quite a few layers to choose from. We will try a convolution block near the end of the model. In order to get a GradCAM image we will need the gradients of the model prediction with respect to the intermediate layer outputs. This can be done using a combination of the ModelOp and GradientOp. We will also use a LambdaOp in order to get the gradients of only the model's most confident prediction.</p> In\u00a0[5]: Copied! <pre>network = Network(ops=[\n    ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='conv5_block1_out'),\n    LambdaOp(inputs=\"y_pred\", outputs=\"y_pred_max\", fn=lambda x: tf.reduce_max(x, axis=-1)),\n    GradientOp(finals=\"y_pred_max\", inputs=\"embedding\", outputs=\"grads\", mode=\"!train\")\n])\n</pre> network = Network(ops=[     ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='conv5_block1_out'),     LambdaOp(inputs=\"y_pred\", outputs=\"y_pred_max\", fn=lambda x: tf.reduce_max(x, axis=-1)),     GradientOp(finals=\"y_pred_max\", inputs=\"embedding\", outputs=\"grads\", mode=\"!train\") ]) <p></p> In\u00a0[6]: Copied! <pre>traces = [\n        GradCAM(images=\"B\", grads=\"grads\", preds=\"y_pred\"),\n        ImageViewer(inputs=\"gradcam\", mode=\"test\")\n    ]\n\nestimator = Estimator(pipeline=pipeline,\n                      network=network,\n                      traces=traces,\n                      epochs=1,\n                     )\n</pre> traces = [         GradCAM(images=\"B\", grads=\"grads\", preds=\"y_pred\"),         ImageViewer(inputs=\"gradcam\", mode=\"test\")     ]  estimator = Estimator(pipeline=pipeline,                       network=network,                       traces=traces,                       epochs=1,                      ) In\u00a0[7]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Warn: the key 'A' is being pruned since it is unused outside of the Pipeline. To prevent this, you can declare the key as an input of a Trace or TensorOp.\n</pre> <pre>FastEstimator-Test: step: None; epoch: 1;\n</pre> <p>ImageNet class 340 refers to zebras, so the model is correct for all of our data here. We can also see through the GradCAM output that the model sometimes seems to care more about the background of the image than the Zebras themselves. This might indicate that the model is using features that humans would deem non-robust in order to make its decisions. Let's compare this with an untrained model:</p> In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=None), optimizer_fn=\"adam\")\nnetwork = Network(ops=[\n    ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='conv5_block1_out'),\n    LambdaOp(inputs=\"y_pred\", outputs=\"y_pred_max\", fn=lambda x: tf.reduce_max(x, axis=-1)),\n    GradientOp(finals=\"y_pred_max\", inputs=\"embedding\", outputs=\"grads\", mode=\"!train\")\n])\ntraces = [\n        GradCAM(images=\"B\", grads=\"grads\", preds=\"y_pred\"),\n        ImageViewer(inputs=\"gradcam\", mode=\"test\")\n    ]\n\nestimator = Estimator(pipeline=pipeline,\n                      network=network,\n                      traces=traces,\n                      epochs=1,\n                     )\nestimator.test()\n</pre> model = fe.build(model_fn=lambda: tf.keras.applications.ResNet152V2(weights=None), optimizer_fn=\"adam\") network = Network(ops=[     ModelOp(model=model, inputs=\"B\", outputs=[\"y_pred\", \"embedding\"], intermediate_layers='conv5_block1_out'),     LambdaOp(inputs=\"y_pred\", outputs=\"y_pred_max\", fn=lambda x: tf.reduce_max(x, axis=-1)),     GradientOp(finals=\"y_pred_max\", inputs=\"embedding\", outputs=\"grads\", mode=\"!train\") ]) traces = [         GradCAM(images=\"B\", grads=\"grads\", preds=\"y_pred\"),         ImageViewer(inputs=\"gradcam\", mode=\"test\")     ]  estimator = Estimator(pipeline=pipeline,                       network=network,                       traces=traces,                       epochs=1,                      ) estimator.test() <pre>FastEstimator-Test: step: None; epoch: 1;\n</pre> <p>As we can see from the images above, the untrained model seems to 'focus' all over the images with no apparent correlation to the zebras whatsoever. The training process clearly helps the network to focus in on more specific parts of the images for its predictions.</p>"}, {"location": "tutorial/xai/t05_gradcam.html#xai-tutorial-5-gradcam", "title": "XAI Tutorial 5: GradCAM\u00b6", "text": ""}, {"location": "tutorial/xai/t05_gradcam.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Basic Pipeline Visualization</li> <li>Extracting Intermediate Layer Gradients</li> <li>GradCAM</li> </ul>"}, {"location": "tutorial/xai/t05_gradcam.html#example-data-and-pipeline", "title": "Example Data and Pipeline\u00b6", "text": "<p>For this tutorial we will use some pictures of zebras with minimal pre-processing. Let's visualize some of the images to see what we're working with.</p>"}, {"location": "tutorial/xai/t05_gradcam.html#extracting-intermediate-layer-gradients", "title": "Extracting Intermediate Layer Gradients\u00b6", "text": "<p>We will use a pre-trained ResNet151 model for this example, with standard ImageNet weights. We will inspect the model to decide which layer we want to analyze with GradCAM.</p>"}, {"location": "tutorial/xai/t05_gradcam.html#gradcam", "title": "GradCAM\u00b6", "text": "<p>Now it's time to generate some GradCAM images. You can read more about this method here.</p>"}]}