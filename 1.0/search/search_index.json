{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "installation.html", "title": "Install", "text": ""}, {"location": "installation.html#1_install_dependencies", "title": "1. Install Dependencies:", "text": "<ul> <li> <p>Windows (CPU):     <pre><code>$ pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n</code></pre></p> </li> <li> <p>Linux (CPU/GPU):     <pre><code>$ apt-get install libglib2.0-0 libsm6 libxrender1 libxext6\n</code></pre></p> </li> <li> <p>Mac (CPU):     <pre><code>$ echo No dependency needed \":)\"\n</code></pre></p> </li> </ul>"}, {"location": "installation.html#2_install_fastestimator", "title": "2. Install FastEstimator:", "text": "<ul> <li>Stable:     <pre><code>$ pip install fastestimator\n</code></pre></li> <li>Most Recent:     <pre><code>$ pip install fastestimator-nightly\n</code></pre></li> </ul>"}, {"location": "installation.html#docker_hub", "title": "Docker Hub", "text": "<p>Docker containers create isolated virtual environments that share resources with a host machine. Docker provides an easy way to set up a FastEstimator environment. You can simply pull our image from Docker Hub and get started:</p> <ul> <li>GPU:     <pre><code>docker pull fastestimator/fastestimator:latest-gpu\n</code></pre></li> <li>CPU:     <pre><code>docker pull fastestimator/fastestimator:latest-cpu\n</code></pre></li> </ul>"}, {"location": "apphub/index.html", "title": "FastEstimator Application Hub", "text": "<p>Welcome to the FastEstimator Application Hub! Here we showcase different end-to-end AI examples implemented in FastEstimator. We will keep implementing new AI ideas and making state-of-the-art solutions accessible to everyone.</p>"}, {"location": "apphub/index.html#purpose_of_application_hub", "title": "Purpose of Application Hub", "text": "<ul> <li>Provide a place to learn implementation details of state-of-the-art solutions</li> <li>Showcase FastEstimator functionalities in an end-to-end fashion</li> <li>Offer ready-made AI solutions for people to use in their own projects/products</li> </ul>"}, {"location": "apphub/index.html#why_not_just_learn_from_official_implementations", "title": "Why not just learn from official implementations?", "text": "<p>If you have ever spent time reading AI research papers, you will often find yourself asking: did I just spent 3 hours reading a paper where the underlying idea can be expressed in 3 minutes?</p> <p>Similarly, people may use 5000 lines of code to implement an idea which could have been expressed in 500 lines using a different AI framework. In FastEstimator, we strive to make things simpler and more intuitive while preserving flexibility. As a result, many state-of-the-art AI implementations can be simplified greatly such that the code directly reflects the key ideas. As an example, the official implementation of PGGAN includes 5000+ lines of code whereas our implementation requires less than 500.</p> <p>To summarize, we spent time learning from the official implementation, so you can save time by learning from us!</p>"}, {"location": "apphub/index.html#whats_included_in_each_example", "title": "What's included in each example?", "text": "<p>Each example contains three files:</p> <ol> <li>A TensorFlow python file (.py): The FastEstimator source code needed to run the example with TensorFlow.</li> <li>A PyTorch python file (.py): The FastEstimator source code needed to run the example with PyTorch.</li> <li>A jupyter notebook (.ipynb): A notebook that provides step-by-step instructions and explanations about the implementation.</li> </ol>"}, {"location": "apphub/index.html#how_do_i_run_each_example", "title": "How do I run each example", "text": "<p>One can simply execute the python file of any example: <pre><code>$ python mnist_tf.py\n</code></pre></p> <p>Or use our Command-Line Interface (CLI):</p> <pre><code>$ fastestimator train mnist_torch.py\n</code></pre> <p>One benefit of the CLI is that it allows users to configure the input args of <code>get_estimator</code>:</p> <pre><code>$ fastestimator train lenet_mnist.py --batch_size 64 --epochs 4\n</code></pre>"}, {"location": "apphub/NLP/imdb/imdb.html", "title": "Sentiment Prediction in IMDB Reviews using an LSTM", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nimport fastestimator as fe\nfrom fastestimator.dataset.data import imdb_review\nfrom fastestimator.op.numpyop.univariate.reshape import Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.backend import load_model\n</pre> import tempfile import os import numpy as np import torch import torch.nn as nn import torch.nn.functional as fn import fastestimator as fe from fastestimator.dataset.data import imdb_review from fastestimator.op.numpyop.univariate.reshape import Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.backend import load_model In\u00a0[2]: parameters Copied! <pre>MAX_WORDS = 10000\nMAX_LEN = 500\nbatch_size = 64\nepochs = 10\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\n</pre> MAX_WORDS = 10000 MAX_LEN = 500 batch_size = 64 epochs = 10 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None Building components <p>We are loading the dataset from tf.keras.datasets.imdb which contains movie reviews and sentiment scores. All the words have been replaced with the integers that specifies the popularity of the word in corpus. To ensure all the sequences are of same length we need to pad the input sequences before defining the <code>Pipeline</code>.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=batch_size,\n                       ops=Reshape(1, inputs=\"y\", outputs=\"y\"))\n</pre> train_data, eval_data = imdb_review.load_data(MAX_LEN, MAX_WORDS) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=batch_size,                        ops=Reshape(1, inputs=\"y\", outputs=\"y\")) <p>First, we have to define the neural network architecture, and then pass the definition, associated model name, and optimizer into fe.build:</p> In\u00a0[4]: Copied! <pre>class ReviewSentiment(nn.Module):\n    def __init__(self, embedding_size=64, hidden_units=64):\n        super().__init__()\n        self.embedding = nn.Embedding(MAX_WORDS, embedding_size)\n        self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n        self.maxpool1d = nn.MaxPool1d(kernel_size=4)\n        self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)\n        self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)\n        self.fc2 = nn.Linear(in_features=250, out_features=1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.permute((0, 2, 1))\n        x = self.conv1d(x)\n        x = fn.relu(x)\n        x = self.maxpool1d(x)\n        output, _ = self.lstm(x)\n        x = output[:, -1]  # sequence output of only last timestamp\n        x = fn.tanh(x)\n        x = self.fc1(x)\n        x = fn.relu(x)\n        x = self.fc2(x)\n        x = fn.sigmoid(x)\n        return x\n</pre> class ReviewSentiment(nn.Module):     def __init__(self, embedding_size=64, hidden_units=64):         super().__init__()         self.embedding = nn.Embedding(MAX_WORDS, embedding_size)         self.conv1d = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)         self.maxpool1d = nn.MaxPool1d(kernel_size=4)         self.lstm = nn.LSTM(input_size=125, hidden_size=hidden_units, num_layers=1)         self.fc1 = nn.Linear(in_features=hidden_units, out_features=250)         self.fc2 = nn.Linear(in_features=250, out_features=1)      def forward(self, x):         x = self.embedding(x)         x = x.permute((0, 2, 1))         x = self.conv1d(x)         x = fn.relu(x)         x = self.maxpool1d(x)         output, _ = self.lstm(x)         x = output[:, -1]  # sequence output of only last timestamp         x = fn.tanh(x)         x = self.fc1(x)         x = fn.relu(x)         x = self.fc2(x)         x = fn.sigmoid(x)         return x <p><code>Network</code> is the object that defines the whole training graph, including models, loss functions, optimizers etc. A <code>Network</code> can have several different models and loss functions (ex. GANs). <code>fe.Network</code> takes a series of operators, in this case just the basic <code>ModelOp</code>, loss op, and <code>UpdateOp</code> will suffice. It should be noted that \"y_pred\" is the key in the data dictionary which will store the predictions.</p> In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> model = fe.build(model_fn=lambda: ReviewSentiment(), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p><code>Estimator</code> is the API that wraps the <code>Pipeline</code>, <code>Network</code> and other training metadata together. <code>Estimator</code> also contains <code>Traces</code>, which are similar to the callbacks of Keras.</p> <p>In the training loop, we want to measure the validation loss and save the model that has the minimum loss. <code>BestModelSaver</code> is a convenient <code>Trace</code> to achieve this. Let's also measure accuracy over time using another <code>Trace</code>:</p> In\u00a0[6]: Copied! <pre>model_dir = tempfile.mkdtemp()\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> model_dir = tempfile.mkdtemp() traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=model_dir)] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) Training In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \n</pre> <pre>/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n/home/ubuntu/anaconda3/envs/fe_env/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n</pre> <pre>FastEstimator-Train: step: 1; loss: 0.6905144; \nFastEstimator-Train: step: 100; loss: 0.69094294; steps/sec: 46.99; \nFastEstimator-Train: step: 200; loss: 0.6621749; steps/sec: 48.85; \nFastEstimator-Train: step: 300; loss: 0.5835465; steps/sec: 54.12; \nFastEstimator-Train: step: 391; epoch: 1; epoch_time: 7.74 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 391; epoch: 1; loss: 0.5250161; min_loss: 0.5250161; since_best: 0; accuracy: 0.7377667446412374; \nFastEstimator-Train: step: 400; loss: 0.51533854; steps/sec: 55.54; \nFastEstimator-Train: step: 500; loss: 0.6381638; steps/sec: 60.01; \nFastEstimator-Train: step: 600; loss: 0.4390931; steps/sec: 58.14; \nFastEstimator-Train: step: 700; loss: 0.32808638; steps/sec: 59.57; \nFastEstimator-Train: step: 782; epoch: 2; epoch_time: 6.7 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 782; epoch: 2; loss: 0.41990075; min_loss: 0.41990075; since_best: 0; accuracy: 0.8072277653124552; \nFastEstimator-Train: step: 800; loss: 0.4495564; steps/sec: 56.01; \nFastEstimator-Train: step: 900; loss: 0.38001418; steps/sec: 60.14; \nFastEstimator-Train: step: 1000; loss: 0.28246647; steps/sec: 60.33; \nFastEstimator-Train: step: 1100; loss: 0.36126548; steps/sec: 60.51; \nFastEstimator-Train: step: 1173; epoch: 3; epoch_time: 6.63 sec; \nSaved model to /tmp/tmp69qyfzvm/model_best_loss.pt\nFastEstimator-Eval: step: 1173; epoch: 3; loss: 0.39232534; min_loss: 0.39232534; since_best: 0; accuracy: 0.8241752995655702; \nFastEstimator-Train: step: 1200; loss: 0.32620478; steps/sec: 55.57; \nFastEstimator-Train: step: 1300; loss: 0.33430642; steps/sec: 60.1; \nFastEstimator-Train: step: 1400; loss: 0.21134894; steps/sec: 62.23; \nFastEstimator-Train: step: 1500; loss: 0.34480703; steps/sec: 62.4; \nFastEstimator-Train: step: 1564; epoch: 4; epoch_time: 6.47 sec; \nFastEstimator-Eval: step: 1564; epoch: 4; loss: 0.3997118; min_loss: 0.39232534; since_best: 1; accuracy: 0.8274693273499785; \nFastEstimator-Train: step: 1600; loss: 0.14769143; steps/sec: 57.72; \nFastEstimator-Train: step: 1700; loss: 0.17477548; steps/sec: 60.4; \nFastEstimator-Train: step: 1800; loss: 0.34234992; steps/sec: 60.82; \nFastEstimator-Train: step: 1900; loss: 0.34789586; steps/sec: 61.12; \nFastEstimator-Train: step: 1955; epoch: 5; epoch_time: 6.55 sec; \nFastEstimator-Eval: step: 1955; epoch: 5; loss: 0.39978975; min_loss: 0.39232534; since_best: 2; accuracy: 0.8300950016708837; \nFastEstimator-Train: step: 2000; loss: 0.21192178; steps/sec: 56.29; \nFastEstimator-Train: step: 2100; loss: 0.24565384; steps/sec: 60.85; \nFastEstimator-Train: step: 2200; loss: 0.21373041; steps/sec: 60.08; \nFastEstimator-Train: step: 2300; loss: 0.24357724; steps/sec: 61.3; \nFastEstimator-Train: step: 2346; epoch: 6; epoch_time: 6.57 sec; \nFastEstimator-Eval: step: 2346; epoch: 6; loss: 0.39285892; min_loss: 0.39232534; since_best: 3; accuracy: 0.8357282665775528; \nFastEstimator-Train: step: 2400; loss: 0.08471558; steps/sec: 56.66; \nFastEstimator-Train: step: 2500; loss: 0.20877948; steps/sec: 60.78; \nFastEstimator-Train: step: 2600; loss: 0.09914401; steps/sec: 61.27; \nFastEstimator-Train: step: 2700; loss: 0.15458922; steps/sec: 60.94; \nFastEstimator-Train: step: 2737; epoch: 7; epoch_time: 6.53 sec; \nFastEstimator-Eval: step: 2737; epoch: 7; loss: 0.43396476; min_loss: 0.39232534; since_best: 4; accuracy: 0.8326729364586815; \nFastEstimator-Train: step: 2800; loss: 0.16790089; steps/sec: 56.49; \nFastEstimator-Train: step: 2900; loss: 0.09017545; steps/sec: 60.99; \nFastEstimator-Train: step: 3000; loss: 0.06604583; steps/sec: 61.62; \nFastEstimator-Train: step: 3100; loss: 0.2692815; steps/sec: 61.39; \nFastEstimator-Train: step: 3128; epoch: 8; epoch_time: 6.52 sec; \nFastEstimator-Eval: step: 3128; epoch: 8; loss: 0.47958812; min_loss: 0.39232534; since_best: 5; accuracy: 0.8260371413567575; \nFastEstimator-Train: step: 3200; loss: 0.12287539; steps/sec: 56.58; \nFastEstimator-Train: step: 3300; loss: 0.16652104; steps/sec: 62.04; \nFastEstimator-Train: step: 3400; loss: 0.08797251; steps/sec: 59.71; \nFastEstimator-Train: step: 3500; loss: 0.08476864; steps/sec: 60.8; \nFastEstimator-Train: step: 3519; epoch: 9; epoch_time: 6.56 sec; \nFastEstimator-Eval: step: 3519; epoch: 9; loss: 0.51876205; min_loss: 0.39232534; since_best: 6; accuracy: 0.824270778631785; \nFastEstimator-Train: step: 3600; loss: 0.14876236; steps/sec: 56.08; \nFastEstimator-Train: step: 3700; loss: 0.11151114; steps/sec: 60.72; \nFastEstimator-Train: step: 3800; loss: 0.05140955; steps/sec: 60.18; \nFastEstimator-Train: step: 3900; loss: 0.062084932; steps/sec: 59.42; \nFastEstimator-Train: step: 3910; epoch: 10; epoch_time: 6.62 sec; \nFastEstimator-Eval: step: 3910; epoch: 10; loss: 0.5560739; min_loss: 0.39232534; since_best: 7; accuracy: 0.831049792333031; \nFastEstimator-Finish: step: 3910; total_time: 87.54 sec; model_lr: 0.001; \n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We previously saved model weights corresponding to our minimum loss, and now we will load the weights using <code>load_model()</code>:</p> In\u00a0[8]: Copied! <pre>model_name = 'model_best_loss.pt'\nmodel_path = os.path.join(model_dir, model_name)\nload_model(model, model_path)\n</pre> model_name = 'model_best_loss.pt' model_path = os.path.join(model_dir, model_name) load_model(model, model_path) <pre>Loaded model weights from /tmp/tmp69qyfzvm/model_best_loss.pt\n</pre> <p>Let's get some random sequence and compare the prediction with the ground truth:</p> In\u00a0[9]: Copied! <pre>selected_idx = np.random.randint(10000)\nprint(\"Ground truth is: \",eval_data[selected_idx]['y'])\n</pre> selected_idx = np.random.randint(10000) print(\"Ground truth is: \",eval_data[selected_idx]['y']) <pre>Ground truth is:  0\n</pre> <p>Create data dictionary for the inference. The <code>Transform()</code> function in Pipeline and Network applies all the operations on the given data:</p> In\u00a0[10]: Copied! <pre>infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Finally, print the inferencing results.</p> In\u00a0[11]: Copied! <pre>print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0])\n</pre> print(\"Prediction for the input sequence: \", np.array(data[\"y_pred\"])[0][0]) <pre>Prediction for the input sequence:  0.30634004\n</pre>"}, {"location": "apphub/NLP/imdb/imdb.html#sentiment-prediction-in-imdb-reviews-using-an-lstm", "title": "Sentiment Prediction in IMDB Reviews using an LSTM\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/imdb/imdb.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html", "title": "Named Entity Recognition using BERT Fine-Tuning", "text": "<p>For downstream NLP tasks such as question answering, named entity recognition, and language inference, models built on pre-trained word representations tend to perform better. BERT, which fine tunes a deep bi-directional representation on a series of tasks, achieves state-of-the-art results. Unlike traditional transformers, BERT is trained on \"masked language modeling,\" which means that it is allowed to see the whole sentence and does not limit the context it can take into account.</p> <p>For this example, we are leveraging the transformers library to load a BERT model, along with some config files:</p> In\u00a0[2]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nfrom typing import Callable, Iterable, List, Union\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom transformers import BertTokenizer, TFBertModel\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import german_ner\nfrom fastestimator.op.numpyop.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId\nfrom fastestimator.op.tensorop import TensorOp, Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.backend import feed_forward\n</pre> import tempfile import os import numpy as np from typing import Callable, Iterable, List, Union  import tensorflow as tf from tensorflow.keras.layers import Dense, Input from tensorflow.keras.models import Model from transformers import BertTokenizer, TFBertModel  import fastestimator as fe from fastestimator.dataset.data import german_ner from fastestimator.op.numpyop.numpyop import NumpyOp from fastestimator.op.numpyop.univariate import PadSequence, Tokenize, WordtoId from fastestimator.op.tensorop import TensorOp, Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver from fastestimator.backend import feed_forward In\u00a0[3]: parameters Copied! <pre>max_len = 20\nbatch_size = 64\nepochs = 10\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> max_len = 20 batch_size = 64 epochs = 10 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We will need a custom <code>NumpyOp</code> that constructs attention masks for input sequences:</p> In\u00a0[4]: Copied! <pre>class AttentionMask(NumpyOp):\n    def forward(self, data, state):\n        masks = [float(i &gt; 0) for i in data]\n        return np.array(masks)\n</pre> class AttentionMask(NumpyOp):     def forward(self, data, state):         masks = [float(i &gt; 0) for i in data]         return np.array(masks) <p>Our <code>char2idx</code> function creates a look-up table to match ids and labels:</p> In\u00a0[5]: Copied! <pre>def char2idx(data):\n    tag2idx = {t: i for i, t in enumerate(data)}\n    return tag2idx\n</pre> def char2idx(data):     tag2idx = {t: i for i, t in enumerate(data)}     return tag2idx Building components <p>The NER dataset from GermEval contains sequences and entity tags from german wikipedia and news corpora. We are loading train and eval sequences as datasets, along with data and label vocabulary. For this example other nouns are omitted for the simplicity.</p> In\u00a0[6]: Copied! <pre>train_data, eval_data, data_vocab, label_vocab = german_ner.load_data(root_dir=data_dir)\n</pre> train_data, eval_data, data_vocab, label_vocab = german_ner.load_data(root_dir=data_dir) <p>Define a pipeline to tokenize and pad the input sequences and construct attention masks. Attention masks are used to avoid performing attention operations on padded tokens. We are using the BERT tokenizer for input sequence tokenization, and limiting our sequences to a max length of 50 for this example.</p> In\u00a0[7]: Copied! <pre>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntag2idx = char2idx(label_vocab)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    batch_size=batch_size,\n    ops=[\n        Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),\n        WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),\n        WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),\n        PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),\n        PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),\n        AttentionMask(inputs=\"x\", outputs=\"x_masks\")\n    ])\n</pre> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) tag2idx = char2idx(label_vocab) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     batch_size=batch_size,     ops=[         Tokenize(inputs=\"x\", outputs=\"x\", tokenize_fn=tokenizer.tokenize),         WordtoId(inputs=\"x\", outputs=\"x\", mapping=tokenizer.convert_tokens_to_ids),         WordtoId(inputs=\"y\", outputs=\"y\", mapping=tag2idx),         PadSequence(max_len=max_len, inputs=\"x\", outputs=\"x\"),         PadSequence(max_len=max_len, value=len(tag2idx), inputs=\"y\", outputs=\"y\"),         AttentionMask(inputs=\"x\", outputs=\"x_masks\")     ]) <p>Our neural network architecture leverages pre-trained weights as initialization for downstream tasks. The whole network is then trained during the fine-tuning.</p> In\u00a0[8]: Copied! <pre>def ner_model():\n    token_inputs = Input((max_len), dtype=tf.int32, name='input_words')\n    mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')\n    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    seq_output, _ = bert_model(token_inputs, attention_mask=mask_inputs)\n    output = Dense(24, activation='softmax')(seq_output)\n    model = Model([token_inputs, mask_inputs], output)\n    return model\n</pre> def ner_model():     token_inputs = Input((max_len), dtype=tf.int32, name='input_words')     mask_inputs = Input((max_len), dtype=tf.int32, name='input_masks')     bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")     seq_output, _ = bert_model(token_inputs, attention_mask=mask_inputs)     output = Dense(24, activation='softmax')(seq_output)     model = Model([token_inputs, mask_inputs], output)     return model <p>After defining the model, it is then instantiated by calling fe.build which also associates the model with a specific optimizer:</p> In\u00a0[9]: Copied! <pre>model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model = fe.build(model_fn=ner_model, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <p><code>fe.Network</code> takes a series of operators. In this case we use a <code>ModelOp</code> to run forward passes through the neural network. The <code>ReshapeOp</code> is then used to transform the prediction and ground truth to a two dimensional vector or scalar respectively before feeding them to the loss calculation.</p> In\u00a0[10]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),\n        Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),\n        Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, 24)),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),\n        UpdateOp(model=model, loss_name=\"loss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=model, inputs=[\"x\", \"x_masks\"], outputs=\"y_pred\"),         Reshape(inputs=\"y\", outputs=\"y\", shape=(-1, )),         Reshape(inputs=\"y_pred\", outputs=\"y_pred\", shape=(-1, 24)),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\"),         UpdateOp(model=model, loss_name=\"loss\")     ]) <p>The <code>Estimator</code> takes four important arguments: network, pipeline, epochs, and traces. During the training, we want to compute accuracy as well as to save the model with the minimum loss. This can be done using <code>Traces</code>.</p> In\u00a0[11]: Copied! <pre>traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)]\n</pre> traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), BestModelSaver(model=model, save_dir=save_dir)] In\u00a0[12]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) Training In\u00a0[13]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 1e-05; \nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nFastEstimator-Train: step: 1; loss: 3.8005962; \nFastEstimator-Train: step: 100; loss: 0.40420213; steps/sec: 2.05; \nFastEstimator-Train: step: 125; epoch: 1; epoch_time: 72.91 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 125; epoch: 1; loss: 0.30054897; min_loss: 0.30054897; since_best: 0; accuracy: 0.9269; \nFastEstimator-Train: step: 200; loss: 0.22695072; steps/sec: 2.0; \nFastEstimator-Train: step: 250; epoch: 2; epoch_time: 62.35 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 250; epoch: 2; loss: 0.1562941; min_loss: 0.1562941; since_best: 0; accuracy: 0.947175; \nFastEstimator-Train: step: 300; loss: 0.1829088; steps/sec: 1.99; \nFastEstimator-Train: step: 375; epoch: 3; epoch_time: 62.8 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 375; epoch: 3; loss: 0.12876438; min_loss: 0.12876438; since_best: 0; accuracy: 0.956675; \nFastEstimator-Train: step: 400; loss: 0.1481853; steps/sec: 2.0; \nFastEstimator-Train: step: 500; loss: 0.13613644; steps/sec: 2.01; \nFastEstimator-Train: step: 500; epoch: 4; epoch_time: 62.44 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 500; epoch: 4; loss: 0.10898326; min_loss: 0.10898326; since_best: 0; accuracy: 0.962875; \nFastEstimator-Train: step: 600; loss: 0.12551221; steps/sec: 2.0; \nFastEstimator-Train: step: 625; epoch: 5; epoch_time: 62.37 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 625; epoch: 5; loss: 0.097521596; min_loss: 0.097521596; since_best: 0; accuracy: 0.966675; \nFastEstimator-Train: step: 700; loss: 0.11037835; steps/sec: 2.0; \nFastEstimator-Train: step: 750; epoch: 6; epoch_time: 62.53 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 750; epoch: 6; loss: 0.0885827; min_loss: 0.0885827; since_best: 0; accuracy: 0.970525; \nFastEstimator-Train: step: 800; loss: 0.09738168; steps/sec: 2.0; \nFastEstimator-Train: step: 875; epoch: 7; epoch_time: 62.47 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 875; epoch: 7; loss: 0.08195208; min_loss: 0.08195208; since_best: 0; accuracy: 0.97235; \nFastEstimator-Train: step: 900; loss: 0.11297427; steps/sec: 2.0; \nFastEstimator-Train: step: 1000; loss: 0.08556217; steps/sec: 2.01; \nFastEstimator-Train: step: 1000; epoch: 8; epoch_time: 62.44 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 1000; epoch: 8; loss: 0.07855953; min_loss: 0.07855953; since_best: 0; accuracy: 0.974875; \nFastEstimator-Train: step: 1100; loss: 0.108659945; steps/sec: 2.01; \nFastEstimator-Train: step: 1125; epoch: 9; epoch_time: 62.28 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 1125; epoch: 9; loss: 0.074070126; min_loss: 0.074070126; since_best: 0; accuracy: 0.9765; \nFastEstimator-Train: step: 1200; loss: 0.071078494; steps/sec: 2.0; \nFastEstimator-Train: step: 1250; epoch: 10; epoch_time: 62.34 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpk1i5vjc2/model_best_loss.h5\nFastEstimator-Eval: step: 1250; epoch: 10; loss: 0.069807574; min_loss: 0.069807574; since_best: 0; accuracy: 0.977925; \nFastEstimator-Finish: step: 1250; total_time: 728.17 sec; model_lr: 1e-05; \n</pre> Inferencing <p>Load model weights using fe.build</p> In\u00a0[14]: Copied! <pre>model_name = 'model_best_loss.h5'\nmodel_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5))\n</pre> model_name = 'model_best_loss.h5' model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=ner_model, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-5)) <pre>Loaded model weights from /tmp/tmpk1i5vjc2/model_best_loss.h5\n</pre> In\u00a0[15]: Copied! <pre>selected_idx = np.random.randint(1000)\nprint(\"Ground truth is: \",eval_data[selected_idx]['y'])\n</pre> selected_idx = np.random.randint(1000) print(\"Ground truth is: \",eval_data[selected_idx]['y']) <pre>Ground truth is:  ['B-PER', 'I-PER', 'I-PER', 'I-PER']\n</pre> <p>Create a data dictionary for the inference. The <code>transform()</code> function in <code>Pipeline</code> and <code>Network</code> applies all their operations on the given data:</p> In\u00a0[16]: Copied! <pre>infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']}\ndata = pipeline.transform(infer_data, mode=\"infer\")\ndata = network.transform(data, mode=\"infer\")\n</pre> infer_data = {\"x\":eval_data[selected_idx]['x'], \"y\":eval_data[selected_idx]['y']} data = pipeline.transform(infer_data, mode=\"infer\") data = network.transform(data, mode=\"infer\") <p>Get the predictions using feed_forward</p> In\u00a0[17]: Copied! <pre>predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False)\npredictions = np.array(predictions).reshape(20,24)\npredictions = np.argmax(predictions, axis=-1)\n</pre> predictions = feed_forward(trained_model, [data[\"x\"],data[\"x_masks\"]], training=False) predictions = np.array(predictions).reshape(20,24) predictions = np.argmax(predictions, axis=-1) In\u00a0[18]: Copied! <pre>def get_key(val): \n    for key, value in tag2idx.items(): \n         if val == value: \n            return key\n</pre> def get_key(val):      for key, value in tag2idx.items():           if val == value:              return key  In\u00a0[19]: Copied! <pre>print(\"Predictions: \", [get_key(pred) for pred in predictions])\n</pre> print(\"Predictions: \", [get_key(pred) for pred in predictions]) <pre>Predictions:  ['B-PER', 'I-PER', 'I-PER', 'I-PER', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n</pre>"}, {"location": "apphub/NLP/named_entity_recognition/bert.html#named-entity-recognition-using-bert-fine-tuning", "title": "Named Entity Recognition using BERT Fine-Tuning\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-1-prepare-training-evaluation-data-and-define-a-pipeline", "title": "Step 1: Prepare training &amp; evaluation data and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-2-create-model-and-fastestimator-network", "title": "Step 2: Create <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/NLP/named_entity_recognition/bert.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\n\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import to_tensor, argmax, to_number\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip\nfrom fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.tensorop import Average\nfrom fastestimator.op.tensorop.gradient import Watch, FGSM\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import ImgData\n</pre> import tempfile import os  import numpy as np  import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import to_tensor, argmax, to_number from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip from fastestimator.op.numpyop.univariate import CoarseDropout, Normalize, Onehot from fastestimator.op.tensorop import Average from fastestimator.op.tensorop.gradient import Watch, FGSM from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import ImgData In\u00a0[2]: parameters Copied! <pre># training parameters\nepsilon=0.04  # The strength of the adversarial attack\nepochs=10\nbatch_size=50\nmax_train_steps_per_epoch=None\nmax_eval_steps_per_epoch=None\nsave_dir=tempfile.mkdtemp()\n</pre> # training parameters epsilon=0.04  # The strength of the adversarial attack epochs=10 batch_size=50 max_train_steps_per_epoch=None max_eval_steps_per_epoch=None save_dir=tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifar10  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(\n        train_data=train_data,\n        eval_data=eval_data,\n        test_data=test_data,\n        batch_size=batch_size,\n        ops=[\n            Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n        ])\n</pre> pipeline = fe.Pipeline(         train_data=train_data,         eval_data=eval_data,         test_data=test_data,         batch_size=batch_size,         ops=[             Normalize(inputs=\"x\", outputs=\"x\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))         ]) In\u00a0[5]: Copied! <pre>model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\")\n</pre> model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"adv_model\") In\u00a0[6]: Copied! <pre>network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),\n        ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),\n        CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),\n        Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),\n        UpdateOp(model=model, loss_name=\"avg_ce\")\n    ])\n</pre> network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon),         ModelOp(model=model, inputs=\"x_adverse\", outputs=\"y_pred_adv\"),         CrossEntropy(inputs=(\"y_pred_adv\", \"y\"), outputs=\"adv_ce\"),         Average(inputs=(\"base_ce\", \"adv_ce\"), outputs=\"avg_ce\"),         UpdateOp(model=model, loss_name=\"avg_ce\")     ]) In\u00a0[7]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         monitor_names=[\"base_ce\", \"adv_ce\"],\n                         log_steps=1000)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          monitor_names=[\"base_ce\", \"adv_ce\"],                          log_steps=1000) In\u00a0[8]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; avg_ce: 2.3945074; adv_ce: 2.4872663; base_ce: 2.3017485; \nFastEstimator-Train: step: 1000; avg_ce: 1.3094263; adv_ce: 1.4686574; base_ce: 1.1501954; steps/sec: 25.77; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 42.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; avg_ce: 1.4899004; adv_ce: 1.6584656; base_ce: 1.3213345; clean_accuracy: 0.5408; adversarial_accuracy: 0.3734; since_best_base_ce: 0; min_base_ce: 1.3213345; \nFastEstimator-Train: step: 2000; avg_ce: 1.143224; adv_ce: 1.3376933; base_ce: 0.9487548; steps/sec: 34.22; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 29.22 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; avg_ce: 1.3468871; adv_ce: 1.5618094; base_ce: 1.1319652; clean_accuracy: 0.5928; adversarial_accuracy: 0.412; since_best_base_ce: 0; min_base_ce: 1.1319652; \nFastEstimator-Train: step: 3000; avg_ce: 1.379614; adv_ce: 1.6073439; base_ce: 1.1518841; steps/sec: 36.24; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 27.6 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; avg_ce: 1.3020732; adv_ce: 1.5298728; base_ce: 1.074274; clean_accuracy: 0.622; adversarial_accuracy: 0.4274; since_best_base_ce: 0; min_base_ce: 1.074274; \nFastEstimator-Train: step: 4000; avg_ce: 1.2436087; adv_ce: 1.4758196; base_ce: 1.0113977; steps/sec: 33.11; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 30.2 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; avg_ce: 1.2154673; adv_ce: 1.4576334; base_ce: 0.9733012; clean_accuracy: 0.665; adversarial_accuracy: 0.4618; since_best_base_ce: 0; min_base_ce: 0.9733012; \nFastEstimator-Train: step: 5000; avg_ce: 1.154286; adv_ce: 1.3962423; base_ce: 0.9123298; steps/sec: 32.78; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 30.51 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; avg_ce: 1.2138638; adv_ce: 1.4704447; base_ce: 0.9572834; clean_accuracy: 0.6696; adversarial_accuracy: 0.4552; since_best_base_ce: 0; min_base_ce: 0.9572834; \nFastEstimator-Train: step: 6000; avg_ce: 1.1946353; adv_ce: 1.4845756; base_ce: 0.904695; steps/sec: 33.34; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 29.99 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 6000; epoch: 6; avg_ce: 1.1873512; adv_ce: 1.4471037; base_ce: 0.9275986; clean_accuracy: 0.6784; adversarial_accuracy: 0.4648; since_best_base_ce: 0; min_base_ce: 0.9275986; \nFastEstimator-Train: step: 7000; avg_ce: 1.3036005; adv_ce: 1.5895638; base_ce: 1.0176373; steps/sec: 32.39; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 30.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 7000; epoch: 7; avg_ce: 1.1750631; adv_ce: 1.4450079; base_ce: 0.9051186; clean_accuracy: 0.6934; adversarial_accuracy: 0.4764; since_best_base_ce: 0; min_base_ce: 0.9051186; \nFastEstimator-Train: step: 8000; avg_ce: 1.1723398; adv_ce: 1.4465908; base_ce: 0.89808875; steps/sec: 32.64; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 30.64 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 8000; epoch: 8; avg_ce: 1.1422093; adv_ce: 1.4145594; base_ce: 0.869859; clean_accuracy: 0.702; adversarial_accuracy: 0.477; since_best_base_ce: 0; min_base_ce: 0.869859; \nFastEstimator-Train: step: 9000; avg_ce: 1.0794711; adv_ce: 1.3604188; base_ce: 0.7985235; steps/sec: 32.8; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 30.49 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; avg_ce: 1.1585152; adv_ce: 1.4406301; base_ce: 0.8764003; clean_accuracy: 0.7014; adversarial_accuracy: 0.4746; since_best_base_ce: 1; min_base_ce: 0.869859; \nFastEstimator-Train: step: 10000; avg_ce: 1.2206926; adv_ce: 1.5394913; base_ce: 0.901894; steps/sec: 33.19; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 30.13 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/adv_model_best_base_ce.h5\nFastEstimator-Eval: step: 10000; epoch: 10; avg_ce: 1.1538234; adv_ce: 1.4526846; base_ce: 0.85496193; clean_accuracy: 0.7084; adversarial_accuracy: 0.4792; since_best_base_ce: 0; min_base_ce: 0.85496193; \nFastEstimator-Finish: step: 10000; total_time: 330.22 sec; adv_model_lr: 0.001; \n</pre> In\u00a0[9]: Copied! <pre>model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\"))\n</pre> model.load_weights(os.path.join(save_dir, \"adv_model_best_base_ce.h5\")) In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.6962; adversarial_accuracy: 0.4758; \n</pre> <p>In spite of our training the network using adversarially crafted images, the adversarial attack is still effective at reducing the accuracy of the network. This does not, however, mean that the efforts were wasted.</p> In\u00a0[11]: Copied! <pre>clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\")\nclean_network = fe.Network(ops=[\n        Watch(inputs=\"x\"),\n        ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),\n        FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),\n        ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),\n        UpdateOp(model=clean_model, loss_name=\"base_ce\")\n    ])\nclean_traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),\n    BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"),\n]\nclean_estimator = fe.Estimator(pipeline=pipeline,\n                         network=clean_network,\n                         epochs=epochs,\n                         traces=clean_traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         log_steps=1000)\nclean_estimator.fit()\n</pre> clean_model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\", model_name=\"clean_model\") clean_network = fe.Network(ops=[         Watch(inputs=\"x\"),         ModelOp(model=clean_model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"base_ce\"),         FGSM(data=\"x\", loss=\"base_ce\", outputs=\"x_adverse\", epsilon=epsilon, mode=\"!train\"),         ModelOp(model=clean_model, inputs=\"x_adverse\", outputs=\"y_pred_adv\", mode=\"!train\"),         UpdateOp(model=clean_model, loss_name=\"base_ce\")     ]) clean_traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\", output_name=\"clean_accuracy\"),     Accuracy(true_key=\"y\", pred_key=\"y_pred_adv\", output_name=\"adversarial_accuracy\"),     BestModelSaver(model=clean_model, save_dir=save_dir, metric=\"base_ce\", save_best_mode=\"min\"), ] clean_estimator = fe.Estimator(pipeline=pipeline,                          network=clean_network,                          epochs=epochs,                          traces=clean_traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          log_steps=1000) clean_estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; base_ce: 2.3599913; \nFastEstimator-Train: step: 1000; base_ce: 1.2336738; steps/sec: 81.68; \nFastEstimator-Train: step: 1000; epoch: 1; epoch_time: 12.53 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 1000; epoch: 1; base_ce: 1.1847152; clean_accuracy: 0.5684; adversarial_accuracy: 0.2694; since_best_base_ce: 0; min_base_ce: 1.1847152; \nFastEstimator-Train: step: 2000; base_ce: 0.81964266; steps/sec: 78.27; \nFastEstimator-Train: step: 2000; epoch: 2; epoch_time: 12.78 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 2000; epoch: 2; base_ce: 0.95957977; clean_accuracy: 0.6652; adversarial_accuracy: 0.2778; since_best_base_ce: 0; min_base_ce: 0.95957977; \nFastEstimator-Train: step: 3000; base_ce: 0.9629886; steps/sec: 78.25; \nFastEstimator-Train: step: 3000; epoch: 3; epoch_time: 12.78 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 3000; epoch: 3; base_ce: 0.8996327; clean_accuracy: 0.6946; adversarial_accuracy: 0.263; since_best_base_ce: 0; min_base_ce: 0.8996327; \nFastEstimator-Train: step: 4000; base_ce: 0.7768238; steps/sec: 77.71; \nFastEstimator-Train: step: 4000; epoch: 4; epoch_time: 12.87 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 4000; epoch: 4; base_ce: 0.86543256; clean_accuracy: 0.702; adversarial_accuracy: 0.2576; since_best_base_ce: 0; min_base_ce: 0.86543256; \nFastEstimator-Train: step: 5000; base_ce: 0.7760873; steps/sec: 77.93; \nFastEstimator-Train: step: 5000; epoch: 5; epoch_time: 12.83 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpmwpkk98d/clean_model_best_base_ce.h5\nFastEstimator-Eval: step: 5000; epoch: 5; base_ce: 0.7983937; clean_accuracy: 0.727; adversarial_accuracy: 0.2664; since_best_base_ce: 0; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 6000; base_ce: 0.56715065; steps/sec: 78.43; \nFastEstimator-Train: step: 6000; epoch: 6; epoch_time: 12.75 sec; \nFastEstimator-Eval: step: 6000; epoch: 6; base_ce: 0.79985946; clean_accuracy: 0.7318; adversarial_accuracy: 0.2704; since_best_base_ce: 1; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 7000; base_ce: 0.7633059; steps/sec: 72.81; \nFastEstimator-Train: step: 7000; epoch: 7; epoch_time: 13.74 sec; \nFastEstimator-Eval: step: 7000; epoch: 7; base_ce: 0.80506575; clean_accuracy: 0.73; adversarial_accuracy: 0.2464; since_best_base_ce: 2; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 8000; base_ce: 0.57881784; steps/sec: 76.72; \nFastEstimator-Train: step: 8000; epoch: 8; epoch_time: 13.03 sec; \nFastEstimator-Eval: step: 8000; epoch: 8; base_ce: 0.8497379; clean_accuracy: 0.733; adversarial_accuracy: 0.214; since_best_base_ce: 3; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 9000; base_ce: 0.61386603; steps/sec: 78.1; \nFastEstimator-Train: step: 9000; epoch: 9; epoch_time: 12.81 sec; \nFastEstimator-Eval: step: 9000; epoch: 9; base_ce: 0.84185517; clean_accuracy: 0.731; adversarial_accuracy: 0.2206; since_best_base_ce: 4; min_base_ce: 0.7983937; \nFastEstimator-Train: step: 10000; base_ce: 0.6447299; steps/sec: 77.0; \nFastEstimator-Train: step: 10000; epoch: 10; epoch_time: 12.99 sec; \nFastEstimator-Eval: step: 10000; epoch: 10; base_ce: 0.8941338; clean_accuracy: 0.7278; adversarial_accuracy: 0.1848; since_best_base_ce: 5; min_base_ce: 0.7983937; \nFastEstimator-Finish: step: 10000; total_time: 148.17 sec; clean_model_lr: 0.001; \n</pre> <p>As before, we will reload the best weights and the test the model</p> In\u00a0[12]: Copied! <pre>clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\"))\n</pre> clean_model.load_weights(os.path.join(save_dir, \"clean_model_best_base_ce.h5\")) In\u00a0[13]: Copied! <pre>print(\"Normal Network:\")\nnormal_results = clean_estimator.test(\"normal\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\nprint(\"Adversarially Trained Network:\")\nadversarial_results = estimator.test(\"adversarial\")\nprint(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0]))\nprint(\"-----------\")\n</pre> print(\"Normal Network:\") normal_results = clean_estimator.test(\"normal\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(normal_results.history['test']['clean_accuracy'].values())[0] - list(normal_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") print(\"Adversarially Trained Network:\") adversarial_results = estimator.test(\"adversarial\") print(\"The whitebox FGSM attack reduced accuracy by {:.2f}\".format(list(adversarial_results.history['test']['clean_accuracy'].values())[0] - list(adversarial_results.history['test']['adversarial_accuracy'].values())[0])) print(\"-----------\") <pre>Normal Network:\nFastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.7178; adversarial_accuracy: 0.2674; \nThe whitebox FGSM attack reduced accuracy by 0.45\n-----------\nAdversarially Trained Network:\nFastEstimator-Test: step: 10000; epoch: 10; clean_accuracy: 0.6962; adversarial_accuracy: 0.4758; \nThe whitebox FGSM attack reduced accuracy by 0.22\n-----------\n</pre> <p>As we can see, the normal network is significantly less robust against adversarial attacks than the one which was trained to resist them. The downside is that the adversarial network requires more epochs of training to converge, and the training steps take about twice as long since they require two forward pass operations. It is also interesting to note that as the regular model was training, it actually saw progressively worse adversarial accuracy. This may be an indication that the network is developing very brittle decision boundaries.</p> In\u00a0[14]: Copied! <pre>class_dictionary = {\n    0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"\n}\nbatch = pipeline.get_results(mode=\"test\")\n</pre> class_dictionary = {     0: \"airplane\", 1: \"car\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\" } batch = pipeline.get_results(mode=\"test\") <p>Now let's run our sample data through the network and then visualize the results</p> In\u00a0[15]: Copied! <pre>batch = clean_network.transform(batch, mode=\"test\")\n</pre> batch = clean_network.transform(batch, mode=\"test\") In\u00a0[16]: Copied! <pre>n_samples = 10\ny = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])])\ny_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])])\ny_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])])\nimg = ImgData(x=batch[\"x\"][0:n_samples], x_adverse=batch[\"x_adverse\"][0:n_samples], y=y, y_pred=y_pred, y_adv=y_adv)\nfig = img.paint_figure()\n</pre> n_samples = 10 y = np.array([class_dictionary[clazz.item()] for clazz in to_number(batch[\"y\"][0:n_samples])]) y_pred = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred\"], axis=1)[0:n_samples])]) y_adv = np.array([class_dictionary[clazz.item()] for clazz in to_number(argmax(batch[\"y_pred_adv\"], axis=1)[0:n_samples])]) img = ImgData(x=batch[\"x\"][0:n_samples], x_adverse=batch[\"x_adverse\"][0:n_samples], y=y, y_pred=y_pred, y_adv=y_adv) fig = img.paint_figure() <p>As you can see, the adversarial images appear very similar to the unmodified images, and yet they are often able to modify the class predictions of the network. Note that if a network's prediction is already wrong, the attack is unlikely to change the incorrect prediction, but rather to increase the model's confidence in its incorrect prediction.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#adversarial-training-using-the-fast-gradient-sign-method-fgsm", "title": "Adversarial Training Using the Fast Gradient Sign Method (FGSM)\u00b6", "text": "<p>In this example we will demonstrate how to train a model to resist adversarial attacks constructed using the Fast Gradient Sign Method. For more background on adversarial attacks, visit: https://arxiv.org/abs/1412.6572</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load CIFAR10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the CIFAR10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#prepare-the-pipeline", "title": "Prepare the <code>Pipeline</code>\u00b6", "text": "<p>We will use a simple pipeline that just normalizes the images</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-construction", "title": "Model Construction\u00b6", "text": "<p>Here we will leverage the LeNet implementation built in to FastEstimator</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#network-defintion", "title": "<code>Network</code> defintion\u00b6", "text": "<p>This is where the adversarial attack will be implemented. To perform an FGSM attack, we first need to monitor gradients with respect to the input image. This can be accomplished in FastEstimator using the <code>Watch</code> TensorOp. We then will run the model forward once, compute the loss, and then pass the loss value into the <code>FGSM</code> TensorOp in order to create an adversarial image. We will then run the adversarial image through the model, compute the loss again, and average the two results together in order to update the model.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to connect the <code>Network</code> with the <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>) and save the best model (<code>BestModelSaver</code>) along the way. We will compute accuracy both with respect to the clean input images ('clean accuracy') as well as with respect to the adversarial input images ('adversarial accuracy'). At the end, we use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Let's start by re-loading the weights from the best model, since the model may have overfit during training</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#comparison-vs-network-without-adversarial-training", "title": "Comparison vs Network without Adversarial Training\u00b6", "text": "<p>To see whether training using adversarial hardening was actually useful, we will compare it to a network which is trained without considering any adversarial images. The setup will be similar to before, but we will only use the adversarial images for evaluation purposes and so the second <code>CrossEntropy</code> Op as well as the <code>Average</code> Op can be omitted.</p>"}, {"location": "apphub/adversarial_training/fgsm/fgsm.html#visualizing-adversarial-samples", "title": "Visualizing Adversarial Samples\u00b6", "text": "<p>Lets visualize some images generated by these adversarial attacks to make sure that everything is working as we would expect. The first step is to get some sample data from the pipeline:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n</pre> import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 24\nbatch_size = 512\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 24 batch_size = 512 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cifar10  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        Normalize(inputs=\"x\", outputs=\"x_out\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),\n        PadIfNeeded(min_height=40, min_width=40, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),\n        RandomCrop(32, 32, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),\n        Sometimes(HorizontalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")),\n        CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\", max_holes=1),\n        ChannelTranspose(inputs=\"x_out\", outputs=\"x_out\"),\n        Onehot(inputs=\"y\", outputs=\"y_out\", mode=\"train\", num_classes=10, label_smoothing=0.2)\n    ])\n</pre> from fastestimator.op.numpyop.univariate import ChannelTranspose, CoarseDropout, Normalize, Onehot from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, PadIfNeeded, RandomCrop  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         Normalize(inputs=\"x\", outputs=\"x_out\", mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616)),         PadIfNeeded(min_height=40, min_width=40, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),         RandomCrop(32, 32, image_in=\"x_out\", image_out=\"x_out\", mode=\"train\"),         Sometimes(HorizontalFlip(image_in=\"x_out\", image_out=\"x_out\", mode=\"train\")),         CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\", max_holes=1),         ChannelTranspose(inputs=\"x_out\", outputs=\"x_out\"),         Onehot(inputs=\"y\", outputs=\"y_out\", mode=\"train\", num_classes=10, label_smoothing=0.2)     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\ndata_yin = data[\"y\"]\ndata_yout = data[\"y_out\"]\n\nprint(\"the pipeline input image size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output image size: {}\".format(data_xout.numpy().shape))\nprint(\"the pipeline input label size: {}\".format(data_yin.numpy().shape))\nprint(\"the pipeline output label size: {}\".format(data_yout.numpy().shape))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"] data_yin = data[\"y\"] data_yout = data[\"y_out\"]  print(\"the pipeline input image size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output image size: {}\".format(data_xout.numpy().shape)) print(\"the pipeline input label size: {}\".format(data_yin.numpy().shape)) print(\"the pipeline output label size: {}\".format(data_yout.numpy().shape)) <pre>the pipeline input image size: (512, 32, 32, 3)\nthe pipeline output image size: (512, 3, 32, 32)\nthe pipeline input label size: (512, 1)\nthe pipeline output label size: (512, 10)\n</pre> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 2, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input img\")\naxs[0,1].set_title(\"pipeline output img\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    # pipeline image visualization \n    img_in = data_xin.numpy()[j]\n    axs[i,0].imshow(img_in)\n    \n    img_out = data_xout.numpy()[j].transpose((1,2,0))\n    img_out[:,:,0] = img_out[:,:,0] * 0.2471 + 0.4914 \n    img_out[:,:,1] = img_out[:,:,1] * 0.2435 + 0.4822\n    img_out[:,:,2] = img_out[:,:,2] * 0.2616 + 0.4465\n    axs[i,1].imshow(img_out)\n    \n    # pipeline label print\n    label_in = data_yin.numpy()[j]\n    label_out = data_yout.numpy()[j]\n    print(\"label_in:{} -&gt; label_out:{}\".format(label_in, label_out))\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 2, figsize=(12,12))  axs[0,0].set_title(\"pipeline input img\") axs[0,1].set_title(\"pipeline output img\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     # pipeline image visualization      img_in = data_xin.numpy()[j]     axs[i,0].imshow(img_in)          img_out = data_xout.numpy()[j].transpose((1,2,0))     img_out[:,:,0] = img_out[:,:,0] * 0.2471 + 0.4914      img_out[:,:,1] = img_out[:,:,1] * 0.2435 + 0.4822     img_out[:,:,2] = img_out[:,:,2] * 0.2616 + 0.4465     axs[i,1].imshow(img_out)          # pipeline label print     label_in = data_yin.numpy()[j]     label_out = data_yout.numpy()[j]     print(\"label_in:{} -&gt; label_out:{}\".format(label_in, label_out)) <pre>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n</pre> <pre>label_in:[0] -&gt; label_out:[0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[4] -&gt; label_out:[0.02 0.02 0.02 0.02 0.82 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[9] -&gt; label_out:[0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.82]\nlabel_in:[1] -&gt; label_out:[0.02 0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\nlabel_in:[0] -&gt; label_out:[0.82 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02]\n</pre> In\u00a0[7]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\n\nclass FastCifar(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv0 = nn.Conv2d(3, 64, 3, padding=(1, 1))\n        self.conv0_bn = nn.BatchNorm2d(64, momentum=0.8)\n        self.conv1 = nn.Conv2d(64, 128, 3, padding=(1, 1))\n        self.conv1_bn = nn.BatchNorm2d(128, momentum=0.8)\n        self.residual1 = Residual(128, 128)\n        self.conv2 = nn.Conv2d(128, 256, 3, padding=(1, 1))\n        self.conv2_bn = nn.BatchNorm2d(256, momentum=0.8)\n        self.residual2 = Residual(256, 256)\n        self.conv3 = nn.Conv2d(256, 512, 3, padding=(1, 1))\n        self.conv3_bn = nn.BatchNorm2d(512, momentum=0.8)\n        self.residual3 = Residual(512, 512)\n        self.fc1 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        # prep layer\n        x = self.conv0(x)\n        x = self.conv0_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        # layer 1\n        x = self.conv1(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv1_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual1(x)\n        # layer 2\n        x = self.conv2(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv2_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual2(x)\n        # layer 3\n        x = self.conv3(x)\n        x = fn.max_pool2d(x, 2)\n        x = self.conv3_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = x + self.residual3(x)\n        # layer 4\n        x = fn.max_pool2d(x, kernel_size=x.size()[2:])\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = fn.softmax(x, dim=-1)\n        return x\n\n\nclass Residual(nn.Module):\n    def __init__(self, channel_in, channel_out):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channel_in, channel_out, 3, padding=(1, 1))\n        self.conv1_bn = nn.BatchNorm2d(channel_out)\n        self.conv2 = nn.Conv2d(channel_out, channel_out, 3, padding=(1, 1))\n        self.conv2_bn = nn.BatchNorm2d(channel_out)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv1_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        x = self.conv2(x)\n        x = self.conv2_bn(x)\n        x = fn.leaky_relu(x, negative_slope=0.1)\n        return x\n\nmodel = fe.build(model_fn=FastCifar, optimizer_fn=\"adam\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as fn  class FastCifar(nn.Module):     def __init__(self):         super().__init__()         self.conv0 = nn.Conv2d(3, 64, 3, padding=(1, 1))         self.conv0_bn = nn.BatchNorm2d(64, momentum=0.8)         self.conv1 = nn.Conv2d(64, 128, 3, padding=(1, 1))         self.conv1_bn = nn.BatchNorm2d(128, momentum=0.8)         self.residual1 = Residual(128, 128)         self.conv2 = nn.Conv2d(128, 256, 3, padding=(1, 1))         self.conv2_bn = nn.BatchNorm2d(256, momentum=0.8)         self.residual2 = Residual(256, 256)         self.conv3 = nn.Conv2d(256, 512, 3, padding=(1, 1))         self.conv3_bn = nn.BatchNorm2d(512, momentum=0.8)         self.residual3 = Residual(512, 512)         self.fc1 = nn.Linear(512, 10)      def forward(self, x):         # prep layer         x = self.conv0(x)         x = self.conv0_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         # layer 1         x = self.conv1(x)         x = fn.max_pool2d(x, 2)         x = self.conv1_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual1(x)         # layer 2         x = self.conv2(x)         x = fn.max_pool2d(x, 2)         x = self.conv2_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual2(x)         # layer 3         x = self.conv3(x)         x = fn.max_pool2d(x, 2)         x = self.conv3_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = x + self.residual3(x)         # layer 4         x = fn.max_pool2d(x, kernel_size=x.size()[2:])         x = torch.flatten(x, 1)         x = self.fc1(x)         x = fn.softmax(x, dim=-1)         return x   class Residual(nn.Module):     def __init__(self, channel_in, channel_out):         super().__init__()         self.conv1 = nn.Conv2d(channel_in, channel_out, 3, padding=(1, 1))         self.conv1_bn = nn.BatchNorm2d(channel_out)         self.conv2 = nn.Conv2d(channel_out, channel_out, 3, padding=(1, 1))         self.conv2_bn = nn.BatchNorm2d(channel_out)      def forward(self, x):         x = self.conv1(x)         x = self.conv1_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         x = self.conv2(x)         x = self.conv2_bn(x)         x = fn.leaky_relu(x, negative_slope=0.1)         return x  model = fe.build(model_fn=FastCifar, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y_out\"), outputs=\"ce\", mode=\"train\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"test\"),\n        UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y_out\"), outputs=\"ce\", mode=\"train\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"test\"),         UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")     ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\ndef lr_schedule(step):\n    if step &lt;= 490:\n        lr = step / 490 * 0.4\n    else:\n        lr = (2352 - step) / 1862 * 0.4\n    return lr * 0.1\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lr_schedule)\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n\nestimator.fit() # start the training\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  def lr_schedule(step):     if step &lt;= 490:         lr = step / 490 * 0.4     else:         lr = (2352 - step) / 1862 * 0.4     return lr * 0.1  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lr_schedule) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch)  estimator.fit() # start the training  <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \nFastEstimator-Train: step: 1; ce: 3.219695; model_lr: 8.163265e-05; \nFastEstimator-Train: step: 98; epoch: 1; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 98; epoch: 1; accuracy: 0.5402; \nFastEstimator-Train: step: 100; ce: 1.5576406; steps/sec: 10.01; model_lr: 0.008163265; \nFastEstimator-Train: step: 196; epoch: 2; epoch_time: 9.85 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 196; epoch: 2; accuracy: 0.6428; \nFastEstimator-Train: step: 200; ce: 1.4646513; steps/sec: 9.99; model_lr: 0.01632653; \nFastEstimator-Train: step: 294; epoch: 3; epoch_time: 9.82 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 294; epoch: 3; accuracy: 0.7756; \nFastEstimator-Train: step: 300; ce: 1.3107454; steps/sec: 9.98; model_lr: 0.024489796; \nFastEstimator-Train: step: 392; epoch: 4; epoch_time: 9.86 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 392; epoch: 4; accuracy: 0.7882; \nFastEstimator-Train: step: 400; ce: 1.2392472; steps/sec: 9.95; model_lr: 0.03265306; \nFastEstimator-Train: step: 490; epoch: 5; epoch_time: 9.86 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 490; epoch: 5; accuracy: 0.8258; \nFastEstimator-Train: step: 500; ce: 1.1978259; steps/sec: 9.9; model_lr: 0.039785177; \nFastEstimator-Train: step: 588; epoch: 6; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 588; epoch: 6; accuracy: 0.8466; \nFastEstimator-Train: step: 600; ce: 1.1920979; steps/sec: 9.94; model_lr: 0.03763695; \nFastEstimator-Train: step: 686; epoch: 7; epoch_time: 9.88 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 686; epoch: 7; accuracy: 0.8686; \nFastEstimator-Train: step: 700; ce: 1.1124842; steps/sec: 9.92; model_lr: 0.03548872; \nFastEstimator-Train: step: 784; epoch: 8; epoch_time: 9.92 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 784; epoch: 8; accuracy: 0.8698; \nFastEstimator-Train: step: 800; ce: 1.0877913; steps/sec: 9.91; model_lr: 0.033340495; \nFastEstimator-Train: step: 882; epoch: 9; epoch_time: 9.9 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 882; epoch: 9; accuracy: 0.885; \nFastEstimator-Train: step: 900; ce: 1.1029329; steps/sec: 9.9; model_lr: 0.031192265; \nFastEstimator-Train: step: 980; epoch: 10; epoch_time: 9.91 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 980; epoch: 10; accuracy: 0.8972; \nFastEstimator-Train: step: 1000; ce: 1.080683; steps/sec: 9.91; model_lr: 0.02904404; \nFastEstimator-Train: step: 1078; epoch: 11; epoch_time: 9.94 sec; \nFastEstimator-Eval: step: 1078; epoch: 11; accuracy: 0.895; \nFastEstimator-Train: step: 1100; ce: 1.0479429; steps/sec: 9.89; model_lr: 0.026895812; \nFastEstimator-Train: step: 1176; epoch: 12; epoch_time: 9.91 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1176; epoch: 12; accuracy: 0.899; \nFastEstimator-Train: step: 1200; ce: 1.0285817; steps/sec: 9.9; model_lr: 0.024747584; \nFastEstimator-Train: step: 1274; epoch: 13; epoch_time: 9.89 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1274; epoch: 13; accuracy: 0.901; \nFastEstimator-Train: step: 1300; ce: 0.99329174; steps/sec: 9.87; model_lr: 0.022599356; \nFastEstimator-Train: step: 1372; epoch: 14; epoch_time: 9.98 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1372; epoch: 14; accuracy: 0.9116; \nFastEstimator-Train: step: 1400; ce: 0.9965744; steps/sec: 9.85; model_lr: 0.020451128; \nFastEstimator-Train: step: 1470; epoch: 15; epoch_time: 9.97 sec; \nFastEstimator-Eval: step: 1470; epoch: 15; accuracy: 0.911; \nFastEstimator-Train: step: 1500; ce: 0.9886917; steps/sec: 9.86; model_lr: 0.0183029; \nFastEstimator-Train: step: 1568; epoch: 16; epoch_time: 9.96 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1568; epoch: 16; accuracy: 0.9168; \nFastEstimator-Train: step: 1600; ce: 0.98263824; steps/sec: 9.87; model_lr: 0.016154673; \nFastEstimator-Train: step: 1666; epoch: 17; epoch_time: 9.94 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1666; epoch: 17; accuracy: 0.924; \nFastEstimator-Train: step: 1700; ce: 0.95481974; steps/sec: 9.88; model_lr: 0.014006444; \nFastEstimator-Train: step: 1764; epoch: 18; epoch_time: 9.92 sec; \nFastEstimator-Eval: step: 1764; epoch: 18; accuracy: 0.9214; \nFastEstimator-Train: step: 1800; ce: 0.9707108; steps/sec: 9.88; model_lr: 0.011858217; \nFastEstimator-Train: step: 1862; epoch: 19; epoch_time: 9.95 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 1862; epoch: 19; accuracy: 0.9288; \nFastEstimator-Train: step: 1900; ce: 0.9509058; steps/sec: 9.86; model_lr: 0.00970999; \nFastEstimator-Train: step: 1960; epoch: 20; epoch_time: 9.97 sec; \nFastEstimator-Eval: step: 1960; epoch: 20; accuracy: 0.924; \nFastEstimator-Train: step: 2000; ce: 0.9307832; steps/sec: 9.84; model_lr: 0.0075617614; \nFastEstimator-Train: step: 2058; epoch: 21; epoch_time: 9.93 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2058; epoch: 21; accuracy: 0.9334; \nFastEstimator-Train: step: 2100; ce: 0.91421276; steps/sec: 9.84; model_lr: 0.0054135337; \nFastEstimator-Train: step: 2156; epoch: 22; epoch_time: 10.0 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2156; epoch: 22; accuracy: 0.9374; \nFastEstimator-Train: step: 2200; ce: 0.9126489; steps/sec: 9.82; model_lr: 0.0032653061; \nFastEstimator-Train: step: 2254; epoch: 23; epoch_time: 9.98 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2254; epoch: 23; accuracy: 0.9412; \nFastEstimator-Train: step: 2300; ce: 0.909637; steps/sec: 9.87; model_lr: 0.0011170784; \nFastEstimator-Train: step: 2352; epoch: 24; epoch_time: 9.96 sec; \nSaved model to /tmp/tmp8mcc0a80/model_best_accuracy.pt\nFastEstimator-Eval: step: 2352; epoch: 24; accuracy: 0.9418; \nFastEstimator-Finish: step: 2352; total_time: 257.44 sec; model_lr: 0.0; \n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: epoch: 24; accuracy: 0.9362; \n</pre> In\u00a0[11]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 3, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\naxs[0,2].set_title(\"predict result\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    data = {\"x\": test_data[\"x\"][j]}\n    axs[i,0].imshow(data[\"x\"], cmap=\"gray\")\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    img = data[\"x_out\"].squeeze(axis=0).transpose((1,2,0))\n    img[:,:,0] = img[:,:,0] * 0.2471 + 0.4914 \n    img[:,:,1] = img[:,:,1] * 0.2435 + 0.4822\n    img[:,:,2] = img[:,:,2] * 0.2616 + 0.4465\n    axs[i,1].imshow(img)\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predict = data[\"y_pred\"].numpy().squeeze(axis=(0))\n    axs[i,2].text(0.2, 0.5, \"predicted class: {}\".format(np.argmax(predict)))\n    axs[i,2].get_xaxis().set_visible(False)\n    axs[i,2].get_yaxis().set_visible(False)\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 3, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\") axs[0,2].set_title(\"predict result\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     data = {\"x\": test_data[\"x\"][j]}     axs[i,0].imshow(data[\"x\"], cmap=\"gray\")          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      img = data[\"x_out\"].squeeze(axis=0).transpose((1,2,0))     img[:,:,0] = img[:,:,0] * 0.2471 + 0.4914      img[:,:,1] = img[:,:,1] * 0.2435 + 0.4822     img[:,:,2] = img[:,:,2] * 0.2616 + 0.4465     axs[i,1].imshow(img)          # run the network     data = network.transform(data, mode=\"infer\")     predict = data[\"y_pred\"].numpy().squeeze(axis=(0))     axs[i,2].text(0.2, 0.5, \"predicted class: {}\".format(np.argmax(predict)))     axs[i,2].get_xaxis().set_visible(False)     axs[i,2].get_yaxis().set_visible(False)"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#cifar-10-image-classification-using-resnet-pytorch-backend", "title": "CIFAR-10 Image Classification Using ResNet (PyTorch Backend)\u00b6", "text": "<p>In this example we are going to demonstrate how to train a CIFAR-10 image classification model using a ResNet architecture on the PyTorch backend. All training details including model structure, data preprocessing, learning rate control, etc. come from https://github.com/davidcpage/cifar10-fast.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load CIFAR-10 training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the CIFAR-10 dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#set-up-a-pre-processing-pipeline", "title": "Set up a pre-processing <code>Pipeline</code>\u00b6", "text": "<p>Here we set up the data pipeline. This will involve a variety of data augmentation including: random cropping, horizontal flipping, image obscuration, and smoothed one-hot label encoding. Beside all of this, the image channels need to be transposed from HWC to CHW format due to PyTorch conventions. We set up these processing steps using <code>Ops</code> and also bundle the data sources and batch_size together into our <code>Pipeline</code>.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the <code>Pipeline</code> works as expected, let's visualize the output and check its size. <code>Pipeline.get_results</code> will return a batch data of pipeline output for this purpose:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the PyTorch way in this example.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-construction", "title": "Model construction\u00b6", "text": "<p>The model definitions are implemented in PyTorch and instantiated by calling <code>fe.build</code> which also associates the model with a specific optimizer.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p><code>Ops</code> are the basic components of a network that include models, loss calculation units, and post-processing units. In this step we are going to combine those pieces together into a <code>Network</code>:</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which will compute accuracy (<code>Accuracy</code>), save our best model (<code>BestModelSaver</code>), and change the learning rate (<code>LRScheduler</code>) of our optimizer over time. We will then use <code>Estimator.fit</code> to trigger the training.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> will trigger model testing using all of the test data defined in the <code>Pipeline</code>. This will allow us to check our accuracy on previously unseen data.</p>"}, {"location": "apphub/image_classification/cifar10_fast/cifar10_fast.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>In this step we run image inference directly using the model that we just trained. We randomly select 5 images from testing dataset and infer them image by image using <code>Pipeline.transform</code> and <code>Network.transform</code>. Please be aware that the <code>Pipeline</code> is no longer the same as it was during training, because we don't want to use data augmentation during inference. This detail was already defined in the <code>Pipeline</code> (mode = \"!infer\").</p>"}, {"location": "apphub/image_classification/mnist/mnist.html", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tempfile\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import matplotlib.pyplot as plt import tempfile In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 2\nbatch_size = 32\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 2 batch_size = 32 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import mnist\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import mnist  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=batch_size,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"), \n                            Minmax(inputs=\"x_out\", outputs=\"x_out\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=batch_size,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x_out\"),                              Minmax(inputs=\"x_out\", outputs=\"x_out\")]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\n\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\nprint(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy())))\nprint(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy())))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"]  print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) print(\"the maximum pixel value of output image: {}\".format(np.max(data_xout.numpy()))) print(\"the minimum pixel value of output image: {}\".format(np.min(data_xout.numpy()))) <pre>the pipeline input data size: (32, 28, 28)\nthe pipeline output data size: (32, 28, 28, 1)\nthe maximum pixel value of output image: 1.0\nthe minimum pixel value of output image: 0.0\n</pre> In\u00a0[6]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\ninputs = tf.gather(data_xin.numpy(), indices)\noutputs = tf.gather(data_xout.numpy(), indices)\nimg = fe.util.ImgData(pipeline_input=inputs, pipeline_output=outputs)\nfig = img.paint_figure()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False) inputs = tf.gather(data_xin.numpy(), indices) outputs = tf.gather(data_xout.numpy(), indices) img = fe.util.ImgData(pipeline_input=inputs, pipeline_output=outputs) fig = img.paint_figure() In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\")\n    ])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\")     ]) In\u00a0[9]: Copied! <pre>from fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy   traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3120923; model_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.3962714; steps/sec: 130.51; model_lr: 0.000998283; \nFastEstimator-Train: step: 200; ce: 0.1641247; steps/sec: 128.14; model_lr: 0.0009930746; \nFastEstimator-Train: step: 300; ce: 0.20016384; steps/sec: 123.16; model_lr: 0.0009844112; \nFastEstimator-Train: step: 400; ce: 0.139256; steps/sec: 118.37; model_lr: 0.00097235345; \nFastEstimator-Train: step: 500; ce: 0.20949002; steps/sec: 116.86; model_lr: 0.000956986; \nFastEstimator-Train: step: 600; ce: 0.08091536; steps/sec: 115.89; model_lr: 0.00093841663; \nFastEstimator-Train: step: 700; ce: 0.069529384; steps/sec: 112.6; model_lr: 0.0009167756; \nFastEstimator-Train: step: 800; ce: 0.02633699; steps/sec: 109.37; model_lr: 0.00089221465; \nFastEstimator-Train: step: 900; ce: 0.12905718; steps/sec: 104.44; model_lr: 0.0008649062; \nFastEstimator-Train: step: 1000; ce: 0.018508099; steps/sec: 108.77; model_lr: 0.0008350416; \nFastEstimator-Train: step: 1100; ce: 0.10962237; steps/sec: 106.61; model_lr: 0.00080283044; \nFastEstimator-Train: step: 1200; ce: 0.047606118; steps/sec: 101.79; model_lr: 0.0007684987; \nFastEstimator-Train: step: 1300; ce: 0.13268313; steps/sec: 97.41; model_lr: 0.0007322871; \nFastEstimator-Train: step: 1400; ce: 0.026097888; steps/sec: 94.08; model_lr: 0.00069444976; \nFastEstimator-Train: step: 1500; ce: 0.020507228; steps/sec: 92.03; model_lr: 0.0006552519; \nFastEstimator-Train: step: 1600; ce: 0.0048278654; steps/sec: 92.14; model_lr: 0.00061496865; \nFastEstimator-Train: step: 1700; ce: 0.01370596; steps/sec: 88.92; model_lr: 0.0005738824; \nFastEstimator-Train: step: 1800; ce: 0.15647383; steps/sec: 89.46; model_lr: 0.00053228147; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 19.9 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp_19zst_o/model_best_accuracy\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.048851434; accuracy: 0.985; since_best_accuracy: 0; max_accuracy: 0.985; \nFastEstimator-Train: step: 1900; ce: 0.0039410545; steps/sec: 87.28; model_lr: 0.00049045763; \nFastEstimator-Train: step: 2000; ce: 0.017347965; steps/sec: 86.28; model_lr: 0.00044870423; \nFastEstimator-Train: step: 2100; ce: 0.0798438; steps/sec: 89.34; model_lr: 0.0004073141; \nFastEstimator-Train: step: 2200; ce: 0.17821585; steps/sec: 86.08; model_lr: 0.00036657765; \nFastEstimator-Train: step: 2300; ce: 0.002756747; steps/sec: 86.16; model_lr: 0.00032678054; \nFastEstimator-Train: step: 2400; ce: 0.00071113696; steps/sec: 85.27; model_lr: 0.00028820196; \nFastEstimator-Train: step: 2500; ce: 0.0027370513; steps/sec: 80.7; model_lr: 0.00025111248; \nFastEstimator-Train: step: 2600; ce: 0.0123588; steps/sec: 84.42; model_lr: 0.00021577229; \nFastEstimator-Train: step: 2700; ce: 0.0069204723; steps/sec: 81.53; model_lr: 0.00018242926; \nFastEstimator-Train: step: 2800; ce: 0.00642678; steps/sec: 77.9; model_lr: 0.00015131726; \nFastEstimator-Train: step: 2900; ce: 0.008096467; steps/sec: 81.94; model_lr: 0.00012265453; \nFastEstimator-Train: step: 3000; ce: 0.0023379987; steps/sec: 79.61; model_lr: 9.664212e-05; \nFastEstimator-Train: step: 3100; ce: 0.104631886; steps/sec: 79.75; model_lr: 7.346248e-05; \nFastEstimator-Train: step: 3200; ce: 0.003903159; steps/sec: 78.32; model_lr: 5.3278196e-05; \nFastEstimator-Train: step: 3300; ce: 0.024989499; steps/sec: 79.65; model_lr: 3.6230853e-05; \nFastEstimator-Train: step: 3400; ce: 0.02124865; steps/sec: 81.16; model_lr: 2.2440026e-05; \nFastEstimator-Train: step: 3500; ce: 0.008722213; steps/sec: 77.56; model_lr: 1.2002448e-05; \nFastEstimator-Train: step: 3600; ce: 0.18177237; steps/sec: 77.0; model_lr: 4.9913274e-06; \nFastEstimator-Train: step: 3700; ce: 0.005134264; steps/sec: 79.63; model_lr: 1.4558448e-06; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 22.99 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp_19zst_o/model_best_accuracy\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.021919494; accuracy: 0.994; since_best_accuracy: 0; max_accuracy: 0.994; \nFastEstimator-Finish: step: 3750; total_time: 46.88 sec; model_lr: 1.0001753e-06; \n</pre> In\u00a0[10]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.9908; \n</pre> In\u00a0[11]: Copied! <pre>num_samples = 5\nindices = np.random.choice(batch_size, size=num_samples, replace=False)\n\ninputs = []\noutputs = []\npredictions = []\n\nfor idx in indices:\n    inputs.append(test_data[\"x\"][idx])\n    data = {\"x\": inputs[-1]}\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))\n\nimg = fe.util.ImgData(pipeline_input=np.stack(inputs), pipeline_output=np.stack(outputs), predictions=np.stack(predictions))\nfig = img.paint_figure()\n</pre> num_samples = 5 indices = np.random.choice(batch_size, size=num_samples, replace=False)  inputs = [] outputs = [] predictions = []  for idx in indices:     inputs.append(test_data[\"x\"][idx])     data = {\"x\": inputs[-1]}          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      outputs.append(data[\"x_out\"].squeeze(axis=(0,3)))          # run the network     data = network.transform(data, mode=\"infer\")     predictions.append(np.argmax(data[\"y_pred\"].numpy().squeeze(axis=(0))))  img = fe.util.ImgData(pipeline_input=np.stack(inputs), pipeline_output=np.stack(outputs), predictions=np.stack(predictions)) fig = img.paint_figure()"}, {"location": "apphub/image_classification/mnist/mnist.html#mnist-image-classification-using-lenet-tensorflow-backend", "title": "MNIST Image Classification Using LeNet (Tensorflow Backend)\u00b6", "text": "<p>In this example, we are going to demonstrate how to train an MNIST image classification model using a LeNet model architecture and TensorFlow backend.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_classification/mnist/mnist.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation datasets and prepare FastEstimator's pipeline.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>We use a FastEstimator API to load the MNIST dataset and then get a test set by splitting 50% of the data off of the evaluation set.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#set-up-a-preprocessing-pipeline", "title": "Set up a preprocessing pipeline\u00b6", "text": "<p>In this example, the data preprocessing steps include adding a channel to the images (since they are grey-scale) and normalizing the image pixel values to the range [0, 1]. We set up these processing steps using <code>Ops</code>. The <code>Pipeline</code> also takes our data sources and batch size as inputs.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch  of pipeline output to enable this:</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Here we are going to import one of FastEstimator's pre-defined model architectures, which was written in TensorFlow. We create a model instance by compiling our model definition function along with a specific model optimizer.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect the model and <code>Ops</code> together into a <code>Network</code>. <code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update rules, or even models themselves.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define an <code>Estimator</code> to connect our <code>Network</code> with our <code>Pipeline</code> and set the <code>traces</code> which compute accuracy (<code>Accuracy</code>), save the best model (<code>BestModelSaver</code>), and change the model learning rate over time (<code>LRScheduler</code>).</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing using the test dataset that was specified in <code>Pipeline</code>. We can evaluate the model's accuracy on this previously unseen data.</p>"}, {"location": "apphub/image_classification/mnist/mnist.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Now let's run inferencing on several images directly using the model that we just trained. We randomly select 5 images from the testing dataset and infer them image by image by leveraging <code>Pipeline.transform</code> and <code>Netowork.transform</code>:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)", "text": "In\u00a0[1]: Copied! <pre>import tensorflow as tf\nimport fastestimator as fe\nimport numpy as np\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom typing import Any, Dict, Tuple\n</pre> import tensorflow as tf import fastestimator as fe import numpy as np import tempfile import matplotlib.pyplot as plt from typing import Any, Dict, Tuple In\u00a0[2]: parameters Copied! <pre>#training parameters\nepochs = 20\nbatch_size = 100\nmax_train_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters epochs = 20 batch_size = 100 max_train_steps_per_epoch = None save_dir = tempfile.mkdtemp() In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\n\ntrain_data, test_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data  train_data, test_data = load_data() In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1) \n        Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1] \n        Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value\n    ])\n</pre> from fastestimator.op.numpyop.univariate import Binarize, ExpandDims, Minmax  pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x_out\"), # change image size: (None, 28, 28) -&gt; (None, 28, 28, 1)          Minmax(inputs=\"x_out\", outputs=\"x_out\"), # normalize pixel value: [0, 255] -&gt; [0, 1]          Binarize(inputs=\"x_out\", outputs=\"x_out\", threshold=0.5) # binarize pixel value     ]) In\u00a0[5]: Copied! <pre>data = pipeline.get_results()\ndata_xin = data[\"x\"]\ndata_xout = data[\"x_out\"]\nprint(\"the pipeline input data size: {}\".format(data_xin.numpy().shape))\nprint(\"the pipeline output data size: {}\".format(data_xout.numpy().shape))\n</pre> data = pipeline.get_results() data_xin = data[\"x\"] data_xout = data[\"x_out\"] print(\"the pipeline input data size: {}\".format(data_xin.numpy().shape)) print(\"the pipeline output data size: {}\".format(data_xout.numpy().shape)) <pre>the pipeline input data size: (100, 28, 28)\nthe pipeline output data size: (100, 28, 28, 1)\n</pre> <p>Let's randomly select 5 samples and visualize the differences between the <code>Pipeline</code> input and output.</p> In\u00a0[6]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 2, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\n\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    img_in = data_xin.numpy()[j]\n    axs[i,0].imshow(img_in, cmap=\"gray\")\n    \n    img_out = data_xout.numpy()[j,:,:,0]\n    axs[i,1].imshow(img_out, cmap=\"gray\")\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 2, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\")   for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     img_in = data_xin.numpy()[j]     axs[i,0].imshow(img_in, cmap=\"gray\")          img_out = data_xout.numpy()[j,:,:,0]     axs[i,1].imshow(img_out, cmap=\"gray\")      In\u00a0[7]: Copied! <pre>LATENT_DIM = 50\n\ndef encoder_net():\n    infer_model = tf.keras.Sequential()\n    infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))\n    infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))\n    infer_model.add(tf.keras.layers.Flatten())\n    infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))\n    return infer_model\n\n\ndef decoder_net():\n    generative_model = tf.keras.Sequential()\n    generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))\n    generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))\n    generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(\n        tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))\n    generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))\n    return generative_model\n\nencode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\")\ndecode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\")\n</pre> LATENT_DIM = 50  def encoder_net():     infer_model = tf.keras.Sequential()     infer_model.add(tf.keras.layers.InputLayer(input_shape=(28, 28, 1)))     infer_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'))     infer_model.add(tf.keras.layers.Flatten())     infer_model.add(tf.keras.layers.Dense(LATENT_DIM + LATENT_DIM))     return infer_model   def decoder_net():     generative_model = tf.keras.Sequential()     generative_model.add(tf.keras.layers.InputLayer(input_shape=(LATENT_DIM, )))     generative_model.add(tf.keras.layers.Dense(units=7 * 7 * 32, activation=tf.nn.relu))     generative_model.add(tf.keras.layers.Reshape(target_shape=(7, 7, 32)))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(         tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation='relu'))     generative_model.add(tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'))     return generative_model  encode_model = fe.build(model_fn=encoder_net, optimizer_fn=\"adam\", model_name=\"encoder\") decode_model = fe.build(model_fn=decoder_net, optimizer_fn=\"adam\", model_name=\"decoder\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass SplitOp(TensorOp):\n\"\"\"To split the infer net output into two \"\"\"\n    def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n        mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)\n        return mean, logvar\n</pre> from fastestimator.op.tensorop import TensorOp  class SplitOp(TensorOp):     \"\"\"To split the infer net output into two \"\"\"     def forward(self, data: tf.Tensor, state: Dict[str, Any]) -&gt; Tuple[tf.Tensor, tf.Tensor]:         mean, logvar = tf.split(data, num_or_size_splits=2, axis=1)         return mean, logvar In\u00a0[9]: Copied! <pre>class ReparameterizeOp(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:\n        mean, logvar = data\n        eps = tf.random.normal(shape=mean.shape)\n        return eps * tf.exp(logvar * .5) + mean\n</pre> class ReparameterizeOp(TensorOp):     def forward(self, data: Tuple[tf.Tensor, tf.Tensor], state: Dict[str, Any]) -&gt; tf.Tensor:         mean, logvar = data         eps = tf.random.normal(shape=mean.shape)         return eps * tf.exp(logvar * .5) + mean In\u00a0[10]: Copied! <pre>import math\n\nclass CVAELoss(TensorOp):\n    def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:\n        cross_ent_mean, mean, logvar, z = data   \n        \n        cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches \n                                                         # make it total cross entropy over pixels \n        logpz = self._log_normal_pdf(z, 0., 0.)\n        logqz_x = self._log_normal_pdf(z, mean, logvar)\n        total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)\n\n        return total_loss\n    \n    @staticmethod\n    def _log_normal_pdf(sample, mean, logvar, raxis=1):\n        log2pi = tf.math.log(2. * tf.constant(math.pi))\n        return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n</pre> import math  class CVAELoss(TensorOp):     def forward(self, data: Tuple[tf.Tensor, ...], state: Dict[str, Any]) -&gt; tf.Tensor:         cross_ent_mean, mean, logvar, z = data                     cross_ent_total = cross_ent_mean * (28 * 28 * 1) # cross_ent_mean is the average cross entropy over pixels and batches                                                           # make it total cross entropy over pixels          logpz = self._log_normal_pdf(z, 0., 0.)         logqz_x = self._log_normal_pdf(z, mean, logvar)         total_loss = cross_ent_total + tf.reduce_mean(-logpz + logqz_x)          return total_loss          @staticmethod     def _log_normal_pdf(sample, mean, logvar, raxis=1):         log2pi = tf.math.log(2. * tf.constant(math.pi))         return tf.reduce_sum(-.5 * ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis) In\u00a0[11]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),\n    SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),\n    ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"), \n    ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),\n    CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"), \n    CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),\n    UpdateOp(model=encode_model, loss_name=\"loss\"),\n    UpdateOp(model=decode_model, loss_name=\"loss\"),\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=encode_model, inputs=\"x_out\", outputs=\"meanlogvar\"),     SplitOp(inputs=\"meanlogvar\", outputs=(\"mean\", \"logvar\")),     ReparameterizeOp(inputs=(\"mean\", \"logvar\"), outputs=\"z\"),      ModelOp(model=decode_model, inputs=\"z\", outputs=\"x_logit\"),     CrossEntropy(inputs=(\"x_logit\", \"x_out\"), outputs=\"cross_entropy\"),      CVAELoss(inputs=(\"cross_entropy\", \"mean\", \"logvar\", \"z\"), outputs=\"loss\", mode=\"!infer\"),     UpdateOp(model=encode_model, loss_name=\"loss\"),     UpdateOp(model=decode_model, loss_name=\"loss\"), ]) In\u00a0[12]: Copied! <pre>from fastestimator.trace.io import ModelSaver\n\ntraces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs), \n          ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         log_steps=600)\n\nestimator.fit() # start the training process\n</pre> from fastestimator.trace.io import ModelSaver  traces = [ModelSaver(model=encode_model, save_dir=save_dir, frequency=epochs),            ModelSaver(model=decode_model, save_dir=save_dir, frequency=epochs)]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          log_steps=600)  estimator.fit() # start the training process <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; encoder_lr: 0.001; decoder_lr: 0.001; \nFastEstimator-Train: step: 1; loss: 543.8469; \nFastEstimator-Train: step: 600; loss: 107.70025; steps/sec: 226.96; \nFastEstimator-Train: step: 600; epoch: 1; epoch_time: 6.37 sec; \nFastEstimator-Train: step: 1200; loss: 93.69548; steps/sec: 192.28; \nFastEstimator-Train: step: 1200; epoch: 2; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 1800; loss: 86.93849; steps/sec: 193.82; \nFastEstimator-Train: step: 1800; epoch: 3; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 2400; loss: 87.41001; steps/sec: 195.48; \nFastEstimator-Train: step: 2400; epoch: 4; epoch_time: 3.06 sec; \nFastEstimator-Train: step: 3000; loss: 83.25523; steps/sec: 193.89; \nFastEstimator-Train: step: 3000; epoch: 5; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 3600; loss: 83.15831; steps/sec: 194.31; \nFastEstimator-Train: step: 3600; epoch: 6; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 4200; loss: 81.78505; steps/sec: 192.58; \nFastEstimator-Train: step: 4200; epoch: 7; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 4800; loss: 84.475464; steps/sec: 193.11; \nFastEstimator-Train: step: 4800; epoch: 8; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 5400; loss: 80.58606; steps/sec: 194.08; \nFastEstimator-Train: step: 5400; epoch: 9; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 6000; loss: 81.36517; steps/sec: 195.2; \nFastEstimator-Train: step: 6000; epoch: 10; epoch_time: 3.08 sec; \nFastEstimator-Train: step: 6600; loss: 81.0715; steps/sec: 194.17; \nFastEstimator-Train: step: 6600; epoch: 11; epoch_time: 3.08 sec; \nFastEstimator-Train: step: 7200; loss: 80.676956; steps/sec: 193.39; \nFastEstimator-Train: step: 7200; epoch: 12; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 7800; loss: 82.24495; steps/sec: 194.2; \nFastEstimator-Train: step: 7800; epoch: 13; epoch_time: 3.07 sec; \nFastEstimator-Train: step: 8400; loss: 78.083145; steps/sec: 192.18; \nFastEstimator-Train: step: 8400; epoch: 14; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 9000; loss: 78.90831; steps/sec: 193.96; \nFastEstimator-Train: step: 9000; epoch: 15; epoch_time: 3.09 sec; \nFastEstimator-Train: step: 9600; loss: 82.3984; steps/sec: 193.15; \nFastEstimator-Train: step: 9600; epoch: 16; epoch_time: 3.11 sec; \nFastEstimator-Train: step: 10200; loss: 75.02458; steps/sec: 192.84; \nFastEstimator-Train: step: 10200; epoch: 17; epoch_time: 3.12 sec; \nFastEstimator-Train: step: 10800; loss: 76.140915; steps/sec: 193.44; \nFastEstimator-Train: step: 10800; epoch: 18; epoch_time: 3.1 sec; \nFastEstimator-Train: step: 11400; loss: 76.65802; steps/sec: 190.19; \nFastEstimator-Train: step: 11400; epoch: 19; epoch_time: 3.15 sec; \nFastEstimator-Train: step: 12000; loss: 76.98837; steps/sec: 191.66; \nSaved model to /tmp/tmpbpgss6_a/encoder_epoch_20.h5\nSaved model to /tmp/tmpbpgss6_a/decoder_epoch_20.h5\nFastEstimator-Train: step: 12000; epoch: 20; epoch_time: 3.11 sec; \nFastEstimator-Finish: step: 12000; total_time: 65.35 sec; encoder_lr: 0.001; decoder_lr: 0.001; \n</pre> In\u00a0[13]: Copied! <pre>sample_num = 5\n\nfig, axs = plt.subplots(sample_num, 3, figsize=(12,12))\n\naxs[0,0].set_title(\"pipeline input\")\naxs[0,1].set_title(\"pipeline output\")\naxs[0,2].set_title(\"decoder output\")\n\nfor i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):\n    data = {\"x\": test_data[\"x\"][j]}\n    axs[i,0].imshow(data[\"x\"], cmap=\"gray\")\n    \n    # run the pipeline\n    data = pipeline.transform(data, mode=\"infer\") \n    img = data[\"x_out\"].squeeze(axis=(0,3))\n    axs[i,1].imshow(img, cmap=\"gray\")\n    \n    # run the network\n    data = network.transform(data, mode=\"infer\")\n    img = data[\"x_logit\"].numpy().squeeze(axis=(0,3))\n    axs[i,2].imshow(img, cmap=\"gray\")\n</pre> sample_num = 5  fig, axs = plt.subplots(sample_num, 3, figsize=(12,12))  axs[0,0].set_title(\"pipeline input\") axs[0,1].set_title(\"pipeline output\") axs[0,2].set_title(\"decoder output\")  for i, j in enumerate(np.random.randint(low=0, high=batch_size-1, size=sample_num)):     data = {\"x\": test_data[\"x\"][j]}     axs[i,0].imshow(data[\"x\"], cmap=\"gray\")          # run the pipeline     data = pipeline.transform(data, mode=\"infer\")      img = data[\"x_out\"].squeeze(axis=(0,3))     axs[i,1].imshow(img, cmap=\"gray\")          # run the network     data = network.transform(data, mode=\"infer\")     img = data[\"x_logit\"].numpy().squeeze(axis=(0,3))     axs[i,2].imshow(img, cmap=\"gray\")"}, {"location": "apphub/image_generation/cvae/cvae.html#convolutional-variational-autoencoder-using-the-mnist-dataset-tensorflow-backend", "title": "Convolutional Variational Autoencoder using the MNIST dataset (TensorFlow backend)\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#introduction-to-cvae", "title": "Introduction to CVAE\u00b6", "text": "<p>CVAEs are Convolutional Variational Autoencoders. They are composed of two models using convolutions: an encoder to cast the input into a latent dimension, and a decoder that will move data from the latent dimension back to the input space. The figure below illustrates the main idea behind CVAEs.</p> <p>In this example, we will use a CVAE to generate data similar to the MNIST dataset using the TensorFlow backend. All training details including model structure, data preprocessing, loss calculation, etc. come from the TensorFlow CVAE tutorial </p>"}, {"location": "apphub/image_generation/cvae/cvae.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": "<p>In this step, we will load MNIST training and validation dataset and prepare FastEstimator's data <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#load-dataset", "title": "Load dataset\u00b6", "text": "<p>Let's use a FastEstimator API to load the MNIST dataset:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#set-up-the-preprocessing-pipline", "title": "Set up the preprocessing <code>Pipline</code>\u00b6", "text": "<p>In this example, the data preprocessing steps include expanding image dimension, normalizing the image pixel values to the range [0, 1], and binarizing pixel values. We set up these processing steps using <code>Ops</code>, while also defining the data source and batch size for the <code>Pipeline</code>.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#validate-pipeline", "title": "Validate <code>Pipeline</code>\u00b6", "text": "<p>In order to make sure the pipeline works as expected, we need to visualize its output. <code>Pipeline.get_results</code> will return a batch of data for this purpose:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": "<p>FastEstimator supports both PyTorch and TensorFlow, so this section could use either backend.  We are going to only demonstrate the TensorFlow backend in this example.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#model-construction", "title": "Model construction\u00b6", "text": "<p>Both of our models' definitions are implemented in TensorFlow and instantiated by calling <code>fe.build</code> (which also associates the model with specific optimizers).</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops", "title": "Customize <code>Ops</code>\u00b6", "text": "<p><code>Ops</code> are the basic components of a <code>Network</code>. They can be logic for loss calculation, model update units, or even the model itself. Some <code>Ops</code> such as cross entropy are pre-defined in FastEstimator, but for any logic that is not there yet, users need to define their own <code>Ops</code>. Please keep all custom <code>Ops</code> backend-consistent with your model backend. In this case all <code>Ops</code> need to be implemented in TensorFlow since our model is built from Tensorflow.</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-splitop", "title": "Customize Ops - SplitOp\u00b6", "text": "<p>Because the encoder output contains both mean and log of variance, we need to split them into two outputs:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-reparameterizeop", "title": "Customize Ops - ReparameterizeOp\u00b6", "text": "<p>In this example case, the input to the decoder is a random sample from a normal distribution whose mean and variation are the output of the encoder. We are going to build an <code>Op</code> called \"ReparameterizeOp\" to accomplish this:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#customize-ops-cvaeloss", "title": "Customize Ops - CVAELoss\u00b6", "text": ""}, {"location": "apphub/image_generation/cvae/cvae.html#network-definition", "title": "<code>Network</code> definition\u00b6", "text": "<p>We are going to connect all models and <code>Ops</code> together into a <code>Network</code></p>"}, {"location": "apphub/image_generation/cvae/cvae.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": "<p>In this step, we define the <code>Estimator</code> to compile the <code>Network</code> and <code>Pipeline</code> and indicate in <code>traces</code> that we want to save the best models. We can then use <code>estimator.fit()</code> to start the training process:</p>"}, {"location": "apphub/image_generation/cvae/cvae.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the model is trained, we will try to run our models on some testing data. We randomly select 5 images from the testing dataset and infer them image by image:</p>"}, {"location": "apphub/image_generation/dcgan/dcgan.html", "title": "DCGAN on the MNIST Dataset", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.keras import layers\nfrom matplotlib import pyplot as plt\nimport fastestimator as fe\nfrom fastestimator.backend import binary_crossentropy, feed_forward\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Normalize\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import ModelSaver\n</pre> import tempfile import os import numpy as np import tensorflow as tf from tensorflow.python.keras import layers from matplotlib import pyplot as plt import fastestimator as fe from fastestimator.backend import binary_crossentropy, feed_forward from fastestimator.dataset.data import mnist from fastestimator.op.numpyop import NumpyOp from fastestimator.op.numpyop.univariate import ExpandDims, Normalize from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import ModelSaver In\u00a0[2]: parameters Copied! <pre>batch_size = 256\nepochs = 50\nmax_train_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\nmodel_name = 'model_epoch_50.h5'\n</pre> batch_size = 256 epochs = 50 max_train_steps_per_epoch = None save_dir = tempfile.mkdtemp() model_name = 'model_epoch_50.h5' Building components <p>We are loading data from tf.keras.datasets.mnist and defining a series of operations to perform on the data before the training:</p> In\u00a0[3]: Copied! <pre>train_data, _ = mnist.load_data()\npipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ExpandDims(inputs=\"x\", outputs=\"x\"),\n        Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        NumpyOp(inputs=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")\n    ])\n</pre> train_data, _ = mnist.load_data() pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ExpandDims(inputs=\"x\", outputs=\"x\"),         Normalize(inputs=\"x\", outputs=\"x\", mean=1.0, std=1.0, max_pixel_value=127.5),         NumpyOp(inputs=lambda: np.random.normal(size=[100]).astype('float32'), outputs=\"z\")     ]) <p>First, we have to define the network architecture for both our Generator and Discriminator. After defining the architecture, users are expected to feed the architecture definition, along with associated model names and optimizers, to fe.build.</p> In\u00a0[4]: Copied! <pre>def generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((7, 7, 256)))\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    return model\n</pre> def generator():     model = tf.keras.Sequential()     model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100, )))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Reshape((7, 7, 256)))     model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))     model.add(layers.BatchNormalization())     model.add(layers.LeakyReLU())     model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))     return model In\u00a0[5]: Copied! <pre>def discriminator():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    return model\n</pre> def discriminator():     model = tf.keras.Sequential()     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))     model.add(layers.LeakyReLU())     model.add(layers.Dropout(0.3))     model.add(layers.Flatten())     model.add(layers.Dense(1))     return model In\u00a0[6]: Copied! <pre>gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\ndisc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> gen_model = fe.build(model_fn=generator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) disc_model = fe.build(model_fn=discriminator, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <p>We define the generator and discriminator losses. These can have multiple inputs and outputs.</p> In\u00a0[7]: Copied! <pre>class GLoss(TensorOp):\n\"\"\"Compute generator loss.\"\"\"\n    def forward(self, data, state):\n        return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True)\n</pre> class GLoss(TensorOp):     \"\"\"Compute generator loss.\"\"\"     def forward(self, data, state):         return binary_crossentropy(y_pred=data, y_true=tf.ones_like(data), from_logits=True) In\u00a0[8]: Copied! <pre>class DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def forward(self, data, state):\n        true_score, fake_score = data\n        real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)\n        fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)\n        total_loss = real_loss + fake_loss\n        return total_loss\n</pre> class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def forward(self, data, state):         true_score, fake_score = data         real_loss = binary_crossentropy(y_pred=true_score, y_true=tf.ones_like(true_score), from_logits=True)         fake_loss = binary_crossentropy(y_pred=fake_score, y_true=tf.zeros_like(fake_score), from_logits=True)         total_loss = real_loss + fake_loss         return total_loss <p><code>fe.Network</code> takes series of operators. Here we pass our models wrapped into <code>ModelOps</code> along with our loss functions and some update rules:</p> In\u00a0[9]: Copied! <pre>network = fe.Network(ops=[\n        ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),\n        ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),\n        GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n        UpdateOp(model=gen_model, loss_name=\"gloss\"),\n        ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),\n        DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),\n        UpdateOp(model=disc_model, loss_name=\"dloss\")\n    ])\n</pre> network = fe.Network(ops=[         ModelOp(model=gen_model, inputs=\"z\", outputs=\"x_fake\"),         ModelOp(model=disc_model, inputs=\"x_fake\", outputs=\"fake_score\"),         GLoss(inputs=\"fake_score\", outputs=\"gloss\"),         UpdateOp(model=gen_model, loss_name=\"gloss\"),         ModelOp(inputs=\"x\", model=disc_model, outputs=\"true_score\"),         DLoss(inputs=(\"true_score\", \"fake_score\"), outputs=\"dloss\"),         UpdateOp(model=disc_model, loss_name=\"dloss\")     ]) <p>We will define an <code>Estimator</code> that has four notable arguments: network, pipeline, epochs and traces. Our <code>Network</code> and <code>Pipeline</code> objects are passed here as an argument along with the number of epochs and a <code>Trace</code>, in this case one designed to save our model every 5 epochs.</p> In\u00a0[10]: Copied! <pre>traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5)\n</pre> traces=ModelSaver(model=gen_model, save_dir=save_dir, frequency=5) In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch) Training In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 1e-04; model1_lr: 1e-04; \nFastEstimator-Train: step: 1; gloss: 0.7122225; dloss: 1.3922014; \nFastEstimator-Train: step: 100; gloss: 0.906471; dloss: 0.8187004; steps/sec: 10.09; \nFastEstimator-Train: step: 200; gloss: 0.59155834; dloss: 1.5896755; steps/sec: 9.93; \nFastEstimator-Train: step: 235; epoch: 1; epoch_time: 28.53 sec; \nFastEstimator-Train: step: 300; gloss: 0.7163421; dloss: 1.3333399; steps/sec: 8.9; \nFastEstimator-Train: step: 400; gloss: 0.6816584; dloss: 1.6007018; steps/sec: 9.88; \nFastEstimator-Train: step: 470; epoch: 2; epoch_time: 23.95 sec; \nFastEstimator-Train: step: 500; gloss: 0.7051203; dloss: 1.4395489; steps/sec: 9.69; \nFastEstimator-Train: step: 600; gloss: 0.75529504; dloss: 1.2358603; steps/sec: 9.86; \nFastEstimator-Train: step: 700; gloss: 0.8082159; dloss: 1.2728964; steps/sec: 9.84; \nFastEstimator-Train: step: 705; epoch: 3; epoch_time: 24.03 sec; \nFastEstimator-Train: step: 800; gloss: 0.8434949; dloss: 1.3006642; steps/sec: 9.65; \nFastEstimator-Train: step: 900; gloss: 0.84470236; dloss: 1.2344811; steps/sec: 9.79; \nFastEstimator-Train: step: 940; epoch: 4; epoch_time: 24.16 sec; \nFastEstimator-Train: step: 1000; gloss: 0.9431131; dloss: 1.0444374; steps/sec: 9.66; \nFastEstimator-Train: step: 1100; gloss: 0.6982814; dloss: 1.5213135; steps/sec: 9.77; \nSaved model to /tmp/tmpspul4xo8/model_epoch_5.h5\nFastEstimator-Train: step: 1175; epoch: 5; epoch_time: 24.17 sec; \nFastEstimator-Train: step: 1200; gloss: 1.2540445; dloss: 0.8278078; steps/sec: 9.63; \nFastEstimator-Train: step: 1300; gloss: 0.70465124; dloss: 1.7595482; steps/sec: 9.76; \nFastEstimator-Train: step: 1400; gloss: 0.83103234; dloss: 1.168882; steps/sec: 9.77; \nFastEstimator-Train: step: 1410; epoch: 6; epoch_time: 24.22 sec; \nFastEstimator-Train: step: 1500; gloss: 0.86833733; dloss: 1.2078841; steps/sec: 9.57; \nFastEstimator-Train: step: 1600; gloss: 0.82795817; dloss: 1.2242851; steps/sec: 9.75; \nFastEstimator-Train: step: 1645; epoch: 7; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 1700; gloss: 0.9743507; dloss: 1.0731742; steps/sec: 9.59; \nFastEstimator-Train: step: 1800; gloss: 0.89325964; dloss: 1.1766281; steps/sec: 9.76; \nFastEstimator-Train: step: 1880; epoch: 8; epoch_time: 24.25 sec; \nFastEstimator-Train: step: 1900; gloss: 1.0287898; dloss: 0.9916363; steps/sec: 9.6; \nFastEstimator-Train: step: 2000; gloss: 0.8240694; dloss: 1.313368; steps/sec: 9.74; \nFastEstimator-Train: step: 2100; gloss: 0.9738071; dloss: 1.1259043; steps/sec: 9.73; \nFastEstimator-Train: step: 2115; epoch: 9; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 2200; gloss: 1.0899432; dloss: 1.0272337; steps/sec: 9.57; \nFastEstimator-Train: step: 2300; gloss: 0.868231; dloss: 1.2400149; steps/sec: 9.72; \nSaved model to /tmp/tmpspul4xo8/model_epoch_10.h5\nFastEstimator-Train: step: 2350; epoch: 10; epoch_time: 24.35 sec; \nFastEstimator-Train: step: 2400; gloss: 0.9001913; dloss: 1.1931081; steps/sec: 9.58; \nFastEstimator-Train: step: 2500; gloss: 1.0865673; dloss: 0.8990781; steps/sec: 9.71; \nFastEstimator-Train: step: 2585; epoch: 11; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 2600; gloss: 0.7485407; dloss: 1.672249; steps/sec: 9.58; \nFastEstimator-Train: step: 2700; gloss: 1.045316; dloss: 1.0383615; steps/sec: 9.73; \nFastEstimator-Train: step: 2800; gloss: 0.7666995; dloss: 1.4343789; steps/sec: 9.72; \nFastEstimator-Train: step: 2820; epoch: 12; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 2900; gloss: 1.1756387; dloss: 0.96622103; steps/sec: 9.6; \nFastEstimator-Train: step: 3000; gloss: 0.9090629; dloss: 1.1984154; steps/sec: 9.72; \nFastEstimator-Train: step: 3055; epoch: 13; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 3100; gloss: 0.9301505; dloss: 1.113826; steps/sec: 9.59; \nFastEstimator-Train: step: 3200; gloss: 0.99965835; dloss: 1.0707076; steps/sec: 9.74; \nFastEstimator-Train: step: 3290; epoch: 14; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 3300; gloss: 0.80838567; dloss: 1.6384692; steps/sec: 9.55; \nFastEstimator-Train: step: 3400; gloss: 0.8714433; dloss: 1.326818; steps/sec: 9.74; \nFastEstimator-Train: step: 3500; gloss: 0.9549879; dloss: 1.2086997; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_15.h5\nFastEstimator-Train: step: 3525; epoch: 15; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 3600; gloss: 1.0164418; dloss: 1.0690243; steps/sec: 9.57; \nFastEstimator-Train: step: 3700; gloss: 1.0357686; dloss: 1.0537144; steps/sec: 9.72; \nFastEstimator-Train: step: 3760; epoch: 16; epoch_time: 24.32 sec; \nFastEstimator-Train: step: 3800; gloss: 0.7402923; dloss: 1.4840925; steps/sec: 9.6; \nFastEstimator-Train: step: 3900; gloss: 0.91192436; dloss: 1.3617609; steps/sec: 9.72; \nFastEstimator-Train: step: 3995; epoch: 17; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 4000; gloss: 1.2626994; dloss: 0.9568275; steps/sec: 9.59; \nFastEstimator-Train: step: 4100; gloss: 0.97824305; dloss: 1.300906; steps/sec: 9.74; \nFastEstimator-Train: step: 4200; gloss: 0.93075603; dloss: 1.387594; steps/sec: 9.73; \nFastEstimator-Train: step: 4230; epoch: 18; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 4300; gloss: 1.0180345; dloss: 1.0898602; steps/sec: 9.59; \nFastEstimator-Train: step: 4400; gloss: 1.051662; dloss: 1.3392837; steps/sec: 9.71; \nFastEstimator-Train: step: 4465; epoch: 19; epoch_time: 24.33 sec; \nFastEstimator-Train: step: 4500; gloss: 1.0151768; dloss: 1.1482071; steps/sec: 9.56; \nFastEstimator-Train: step: 4600; gloss: 1.107022; dloss: 0.96815336; steps/sec: 9.71; \nFastEstimator-Train: step: 4700; gloss: 1.0924942; dloss: 1.1389236; steps/sec: 9.72; \nSaved model to /tmp/tmpspul4xo8/model_epoch_20.h5\nFastEstimator-Train: step: 4700; epoch: 20; epoch_time: 24.39 sec; \nFastEstimator-Train: step: 4800; gloss: 1.1345683; dloss: 1.2068424; steps/sec: 9.54; \nFastEstimator-Train: step: 4900; gloss: 1.142304; dloss: 0.9673606; steps/sec: 9.74; \nFastEstimator-Train: step: 4935; epoch: 21; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 5000; gloss: 0.9886; dloss: 1.0960109; steps/sec: 9.57; \nFastEstimator-Train: step: 5100; gloss: 0.8936993; dloss: 1.2922779; steps/sec: 9.74; \nFastEstimator-Train: step: 5170; epoch: 22; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 5200; gloss: 1.1095095; dloss: 1.1243165; steps/sec: 9.57; \nFastEstimator-Train: step: 5300; gloss: 1.2485275; dloss: 0.89292765; steps/sec: 9.74; \nFastEstimator-Train: step: 5400; gloss: 1.0476826; dloss: 1.2994311; steps/sec: 9.75; \nFastEstimator-Train: step: 5405; epoch: 23; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 5500; gloss: 1.3308836; dloss: 0.871735; steps/sec: 9.63; \nFastEstimator-Train: step: 5600; gloss: 1.115385; dloss: 1.2837725; steps/sec: 9.74; \nFastEstimator-Train: step: 5640; epoch: 24; epoch_time: 24.23 sec; \nFastEstimator-Train: step: 5700; gloss: 1.1920481; dloss: 1.0993654; steps/sec: 9.62; \nFastEstimator-Train: step: 5800; gloss: 1.3005233; dloss: 0.914361; steps/sec: 9.74; \nSaved model to /tmp/tmpspul4xo8/model_epoch_25.h5\nFastEstimator-Train: step: 5875; epoch: 25; epoch_time: 24.27 sec; \nFastEstimator-Train: step: 5900; gloss: 1.3146336; dloss: 0.8816396; steps/sec: 9.6; \nFastEstimator-Train: step: 6000; gloss: 0.9764897; dloss: 1.289681; steps/sec: 9.75; \nFastEstimator-Train: step: 6100; gloss: 1.1467731; dloss: 1.1918977; steps/sec: 9.75; \nFastEstimator-Train: step: 6110; epoch: 26; epoch_time: 24.26 sec; \nFastEstimator-Train: step: 6200; gloss: 1.6301311; dloss: 0.9541445; steps/sec: 9.6; \nFastEstimator-Train: step: 6300; gloss: 1.2840165; dloss: 0.9587291; steps/sec: 9.73; \nFastEstimator-Train: step: 6345; epoch: 27; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 6400; gloss: 1.1097628; dloss: 1.0090048; steps/sec: 9.59; \nFastEstimator-Train: step: 6500; gloss: 1.2495477; dloss: 0.89897555; steps/sec: 9.73; \nFastEstimator-Train: step: 6580; epoch: 28; epoch_time: 24.32 sec; \nFastEstimator-Train: step: 6600; gloss: 1.1773547; dloss: 1.1330662; steps/sec: 9.58; \nFastEstimator-Train: step: 6700; gloss: 1.246088; dloss: 0.8964198; steps/sec: 9.73; \nFastEstimator-Train: step: 6800; gloss: 1.2250234; dloss: 1.0358574; steps/sec: 9.73; \nFastEstimator-Train: step: 6815; epoch: 29; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 6900; gloss: 1.1256618; dloss: 1.196687; steps/sec: 9.59; \nFastEstimator-Train: step: 7000; gloss: 1.1131527; dloss: 1.0596428; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_30.h5\nFastEstimator-Train: step: 7050; epoch: 30; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 7100; gloss: 1.1662202; dloss: 1.0555116; steps/sec: 9.57; \nFastEstimator-Train: step: 7200; gloss: 1.0653521; dloss: 1.1444951; steps/sec: 9.71; \nFastEstimator-Train: step: 7285; epoch: 31; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 7300; gloss: 1.1732882; dloss: 1.1456137; steps/sec: 9.56; \nFastEstimator-Train: step: 7400; gloss: 1.0872216; dloss: 1.128233; steps/sec: 9.74; \nFastEstimator-Train: step: 7500; gloss: 1.2431256; dloss: 1.1538315; steps/sec: 9.73; \nFastEstimator-Train: step: 7520; epoch: 32; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 7600; gloss: 1.0806718; dloss: 1.2206206; steps/sec: 9.57; \nFastEstimator-Train: step: 7700; gloss: 1.1804712; dloss: 1.1420157; steps/sec: 9.71; \nFastEstimator-Train: step: 7755; epoch: 33; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 7800; gloss: 1.1762993; dloss: 1.0413929; steps/sec: 9.54; \nFastEstimator-Train: step: 7900; gloss: 1.2267275; dloss: 0.98290396; steps/sec: 9.71; \nFastEstimator-Train: step: 7990; epoch: 34; epoch_time: 24.37 sec; \nFastEstimator-Train: step: 8000; gloss: 1.1847881; dloss: 1.0905983; steps/sec: 9.55; \nFastEstimator-Train: step: 8100; gloss: 1.1490288; dloss: 1.1739209; steps/sec: 9.72; \nFastEstimator-Train: step: 8200; gloss: 1.0283768; dloss: 1.059457; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_35.h5\nFastEstimator-Train: step: 8225; epoch: 35; epoch_time: 24.33 sec; \nFastEstimator-Train: step: 8300; gloss: 1.2351133; dloss: 1.1085691; steps/sec: 9.59; \nFastEstimator-Train: step: 8400; gloss: 1.1488228; dloss: 1.1410246; steps/sec: 9.74; \nFastEstimator-Train: step: 8460; epoch: 36; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 8500; gloss: 1.152446; dloss: 1.1371456; steps/sec: 9.55; \nFastEstimator-Train: step: 8600; gloss: 1.2175394; dloss: 1.1543391; steps/sec: 9.74; \nFastEstimator-Train: step: 8695; epoch: 37; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 8700; gloss: 1.1803217; dloss: 1.1517241; steps/sec: 9.59; \nFastEstimator-Train: step: 8800; gloss: 0.9561673; dloss: 1.2418871; steps/sec: 9.77; \nFastEstimator-Train: step: 8900; gloss: 1.0239995; dloss: 1.243228; steps/sec: 9.74; \nFastEstimator-Train: step: 8930; epoch: 38; epoch_time: 24.26 sec; \nFastEstimator-Train: step: 9000; gloss: 0.98074543; dloss: 1.2558163; steps/sec: 9.59; \nFastEstimator-Train: step: 9100; gloss: 1.0084043; dloss: 1.2773612; steps/sec: 9.74; \nFastEstimator-Train: step: 9165; epoch: 39; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 9200; gloss: 1.0313301; dloss: 1.2312038; steps/sec: 9.58; \nFastEstimator-Train: step: 9300; gloss: 1.0100834; dloss: 1.2482088; steps/sec: 9.73; \nFastEstimator-Train: step: 9400; gloss: 0.9327201; dloss: 1.3188391; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_40.h5\nFastEstimator-Train: step: 9400; epoch: 40; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 9500; gloss: 1.1315899; dloss: 1.1447232; steps/sec: 9.55; \nFastEstimator-Train: step: 9600; gloss: 1.1352619; dloss: 1.0802212; steps/sec: 9.72; \nFastEstimator-Train: step: 9635; epoch: 41; epoch_time: 24.3 sec; \nFastEstimator-Train: step: 9700; gloss: 1.01453; dloss: 1.0975459; steps/sec: 9.59; \nFastEstimator-Train: step: 9800; gloss: 0.90930146; dloss: 1.2942967; steps/sec: 9.76; \nFastEstimator-Train: step: 9870; epoch: 42; epoch_time: 24.27 sec; \nFastEstimator-Train: step: 9900; gloss: 1.0540565; dloss: 1.170531; steps/sec: 9.58; \nFastEstimator-Train: step: 10000; gloss: 1.061863; dloss: 1.2722391; steps/sec: 9.76; \nFastEstimator-Train: step: 10100; gloss: 0.9647354; dloss: 1.1689386; steps/sec: 9.73; \nFastEstimator-Train: step: 10105; epoch: 43; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 10200; gloss: 1.2080085; dloss: 1.075758; steps/sec: 9.57; \nFastEstimator-Train: step: 10300; gloss: 1.0741084; dloss: 1.1352613; steps/sec: 9.72; \nFastEstimator-Train: step: 10340; epoch: 44; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 10400; gloss: 1.1394867; dloss: 0.9788251; steps/sec: 9.58; \nFastEstimator-Train: step: 10500; gloss: 1.1983887; dloss: 1.0792823; steps/sec: 9.73; \nSaved model to /tmp/tmpspul4xo8/model_epoch_45.h5\nFastEstimator-Train: step: 10575; epoch: 45; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 10600; gloss: 0.96989757; dloss: 1.2448618; steps/sec: 9.59; \nFastEstimator-Train: step: 10700; gloss: 0.9579427; dloss: 1.2773428; steps/sec: 9.7; \nFastEstimator-Train: step: 10800; gloss: 0.97453225; dloss: 1.2194138; steps/sec: 9.72; \nFastEstimator-Train: step: 10810; epoch: 46; epoch_time: 24.36 sec; \nFastEstimator-Train: step: 10900; gloss: 1.0218571; dloss: 1.1765122; steps/sec: 9.57; \nFastEstimator-Train: step: 11000; gloss: 1.1221988; dloss: 1.1675267; steps/sec: 9.71; \nFastEstimator-Train: step: 11045; epoch: 47; epoch_time: 24.34 sec; \nFastEstimator-Train: step: 11100; gloss: 0.900293; dloss: 1.1741953; steps/sec: 9.57; \nFastEstimator-Train: step: 11200; gloss: 1.1080045; dloss: 1.1090837; steps/sec: 9.73; \nFastEstimator-Train: step: 11280; epoch: 48; epoch_time: 24.31 sec; \nFastEstimator-Train: step: 11300; gloss: 1.1028197; dloss: 1.1044464; steps/sec: 9.59; \nFastEstimator-Train: step: 11400; gloss: 1.0530615; dloss: 1.2150866; steps/sec: 9.74; \nFastEstimator-Train: step: 11500; gloss: 0.8997061; dloss: 1.3101699; steps/sec: 9.75; \nFastEstimator-Train: step: 11515; epoch: 49; epoch_time: 24.29 sec; \nFastEstimator-Train: step: 11600; gloss: 1.0087067; dloss: 1.3297874; steps/sec: 9.58; \nFastEstimator-Train: step: 11700; gloss: 1.0053492; dloss: 1.2499433; steps/sec: 9.74; \nSaved model to /tmp/tmpspul4xo8/model_epoch_50.h5\nFastEstimator-Train: step: 11750; epoch: 50; epoch_time: 24.3 sec; \nFastEstimator-Finish: step: 11750; total_time: 1219.47 sec; model_lr: 1e-04; model1_lr: 1e-04; \n</pre> Inferencing <p>For inferencing, first we have to load the trained model weights. We will load the trained generator weights using fe.build</p> In\u00a0[19]: Copied! <pre>model_path = os.path.join(save_dir, model_name)\ntrained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n</pre> model_path = os.path.join(save_dir, model_name) trained_model = fe.build(model_fn=generator, weights_path=model_path, optimizer_fn=lambda: tf.optimizers.Adam(1e-4)) <pre>Loaded model weights from /tmp/tmpspul4xo8/model_epoch_50.h5\n</pre> <p>We will the generate some images from random noise:</p> In\u00a0[20]: Copied! <pre>images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False)\n</pre> images = feed_forward(trained_model, np.random.normal(size=(16, 100)), training=False) <pre>WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n</pre> In\u00a0[21]: Copied! <pre>fig, axes = plt.subplots(4, 4)\naxes = np.ravel(axes)\nfor i in range(images.shape[0]):\n    axes[i].axis('off')\n    axes[i].imshow(np.squeeze(images[i, ...] * 127.5 + 127.5), cmap='gray')\n</pre> fig, axes = plt.subplots(4, 4) axes = np.ravel(axes) for i in range(images.shape[0]):     axes[i].axis('off')     axes[i].imshow(np.squeeze(images[i, ...] * 127.5 + 127.5), cmap='gray')"}, {"location": "apphub/image_generation/dcgan/dcgan.html#dcgan-on-the-mnist-dataset", "title": "DCGAN on the MNIST Dataset\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-1-prepare-training-and-define-a-pipeline", "title": "Step 1: Prepare training and define a <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-2-create-a-model-and-fastestimator-network", "title": "Step 2: Create a <code>model</code> and FastEstimator <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/dcgan/dcgan.html#step-3-prepare-estimator-and-configure-the-training-loop", "title": "Step 3: Prepare <code>Estimator</code> and configure the training loop\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html", "title": "Progressive Growing GAN (PGGAN)", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\nimport fastestimator as fe\nfrom fastestimator.schedule import EpochScheduler\nfrom fastestimator.util import get_num_devices\n</pre> import os import tempfile  import cv2 import numpy as np import torch import matplotlib.pyplot as plt  import fastestimator as fe from fastestimator.schedule import EpochScheduler from fastestimator.util import get_num_devices In\u00a0[2]: parameters Copied! <pre>target_size=128\nepochs=55\nsave_dir=tempfile.mkdtemp()\nmax_train_steps_per_epoch=None\ndata_dir=None\n</pre> target_size=128 epochs=55 save_dir=tempfile.mkdtemp() max_train_steps_per_epoch=None data_dir=None In\u00a0[3]: Copied! <pre>num_grow = np.log2(target_size) - 2\nassert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\"\nnum_phases = int(2 * num_grow + 1)\nassert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size)\nnum_grow, phase_length = int(num_grow), int(epochs / num_phases)\nevent_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)]\nevent_size = [4] + [2**(i + 3) for i in range(num_grow)]\n</pre> num_grow = np.log2(target_size) - 2 assert num_grow &gt;= 1 and num_grow % 1 == 0, \"need exponential of 2 and greater than 8 as target size\" num_phases = int(2 * num_grow + 1) assert epochs % num_phases == 0, \"epoch must be multiple of {} for size {}\".format(num_phases, target_size) num_grow, phase_length = int(num_grow), int(epochs / num_phases) event_epoch = [1, 1 + phase_length] + [phase_length * (2 * i + 1) + 1 for i in range(1, num_grow)] event_size = [4] + [2**(i + 3) for i in range(num_grow)] In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import nih_chestxray\n\ndataset = nih_chestxray.load_data(root_dir=data_dir)\n</pre> from fastestimator.dataset.data import nih_chestxray  dataset = nih_chestxray.load_data(root_dir=data_dir) In\u00a0[5]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage\n\nresize_map = {\n    epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map1 = {\n    epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nresize_low_res_map2 = {\n    epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_size_map = {\n    epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()\n    for (epoch, size) in zip(event_epoch, event_size)\n}\nbatch_scheduler = EpochScheduler(epoch_dict=batch_size_map)\npipeline = fe.Pipeline(\n    batch_size=batch_scheduler,\n    train_data=dataset,\n    drop_last=True,\n    ops=[\n        ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),\n        EpochScheduler(epoch_dict=resize_map),\n        EpochScheduler(epoch_dict=resize_low_res_map1),\n        EpochScheduler(epoch_dict=resize_low_res_map2),\n        Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),\n        ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),\n        NumpyOp(inputs=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\")\n    ])\n</pre> from fastestimator.op.numpyop import NumpyOp from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage  resize_map = {     epoch: Resize(image_in=\"x\", image_out=\"x\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map1 = {     epoch: Resize(image_in=\"x\", image_out=\"x_low_res\", height=size // 2, width=size // 2)     for (epoch, size) in zip(event_epoch, event_size) } resize_low_res_map2 = {     epoch: Resize(image_in=\"x_low_res\", image_out=\"x_low_res\", height=size, width=size)     for (epoch, size) in zip(event_epoch, event_size) } batch_size_map = {     epoch: max(512 // size, 4) * get_num_devices() if size &lt;= 512 else 2 * get_num_devices()     for (epoch, size) in zip(event_epoch, event_size) } batch_scheduler = EpochScheduler(epoch_dict=batch_size_map) pipeline = fe.Pipeline(     batch_size=batch_scheduler,     train_data=dataset,     drop_last=True,     ops=[         ReadImage(inputs=\"x\", outputs=\"x\", color_flag=\"gray\"),         EpochScheduler(epoch_dict=resize_map),         EpochScheduler(epoch_dict=resize_low_res_map1),         EpochScheduler(epoch_dict=resize_low_res_map2),         Normalize(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"], mean=1.0, std=1.0, max_pixel_value=127.5),         ChannelTranspose(inputs=[\"x\", \"x_low_res\"], outputs=[\"x\", \"x_low_res\"]),         NumpyOp(inputs=lambda: np.random.normal(size=[512]).astype('float32'), outputs=\"z\")     ]) <p>Let's visualize how our <code>Pipeline</code> changes image resolution at the different epochs we specified using <code>Schedulers</code>. FastEstimator as a <code>get_results</code> method to aid in this. In order to correctly visualize the output of the <code>Pipeline</code>, we need to provide epoch numbers to the <code>get_results</code> method:</p> In\u00a0[6]: Copied! <pre>plt.figure(figsize=(50,50))\nfor i, epoch in enumerate(event_epoch):\n    batch_data = pipeline.get_results(epoch=epoch)\n    img = np.squeeze(batch_data[\"x\"][0] + 0.5)\n    plt.subplot(1, 9, i+1)\n    plt.imshow(img, cmap='gray')\n</pre> plt.figure(figsize=(50,50)) for i, epoch in enumerate(event_epoch):     batch_data = pipeline.get_results(epoch=epoch)     img = np.squeeze(batch_data[\"x\"][0] + 0.5)     plt.subplot(1, 9, i+1)     plt.imshow(img, cmap='gray')  In\u00a0[7]: Copied! <pre>from torch.optim import Adam\n\ndef _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):\n    return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)\n\n\nclass EqualizedLRDense(torch.nn.Linear):\n    def __init__(self, in_features, out_features, gain=np.sqrt(2)):\n        super().__init__(in_features, out_features, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        self.wscale = np.float32(gain / np.sqrt(in_features))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\nclass ApplyBias(torch.nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.in_features = in_features\n        self.bias = torch.nn.Parameter(torch.Tensor(in_features))\n        torch.nn.init.constant_(self.bias.data, val=0.0)\n\n    def forward(self, x):\n        if len(x.shape) == 4:\n            x = x + self.bias.view(1, -1, 1, 1).expand_as(x)\n        else:\n            x = x + self.bias\n        return x\n\n\nclass EqualizedLRConv2D(torch.nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):\n        super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)\n        torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)\n        fan_in = np.float32(np.prod(self.weight.data.shape[1:]))\n        self.wscale = np.float32(gain / np.sqrt(fan_in))\n\n    def forward(self, x):\n        return super().forward(x) * self.wscale\n\n\ndef pixel_normalization(x, eps=1e-8):\n    return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)\n\n\ndef mini_batch_std(x, group_size=4, eps=1e-8):\n    b, c, h, w = x.shape\n    group_size = min(group_size, b)\n    y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]\n    y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]\n    y = torch.mean(y**2, axis=0)  # [M, C, H, W]\n    y = torch.sqrt(y + eps)  # [M, C, H, W]\n    y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]\n    y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]\n    return torch.cat((x, y), 1)\n\n\ndef fade_in(x, y, alpha):\n    return (1.0 - alpha) * x + alpha * y\n\n\nclass ToRGB(torch.nn.Module):\n    def __init__(self, in_channels, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)\n        self.bias = ApplyBias(in_features=num_channels)\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        return x\n\n\nclass FromRGB(torch.nn.Module):\n    def __init__(self, res, num_channels=3):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)\n        self.bias = ApplyBias(in_features=_nf(res - 1))\n\n    def forward(self, x):\n        x = self.elr_conv2d(x)\n        x = self.bias(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        return x\n\n\nclass BlockG1D(torch.nn.Module):\n    def __init__(self, res=2, latent_dim=512):\n        super().__init__()\n        self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512]\n        x = pixel_normalization(x)  # [batch, 512]\n        x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]\n        x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]\n        x = pixel_normalization(x)\n        return x\n\n\nclass BlockG2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias2 = ApplyBias(in_features=_nf(res - 1))\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]\n        x = self.upsample(x)\n        x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]\n        return x\n\n\ndef _block_G(res, latent_dim=512, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockG1D(res=res, latent_dim=latent_dim)\n    else:\n        model = BlockG2D(res=res)\n    return model\n\n\nclass Gen(torch.nn.Module):\n    def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.g_blocks = torch.nn.ModuleList(g_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.upsample = torch.nn.Upsample(scale_factor=2)\n\n    def forward(self, x):\n        for g in self.g_blocks[:-1]:\n            x = g(x)\n        previous_img = self.rgb_blocks[0](x)\n        previous_img = self.upsample(previous_img)\n        x = self.g_blocks[-1](x)\n        new_img = self.rgb_blocks[1](x)\n        return fade_in(previous_img, new_img, self.fade_in_alpha)\n\n\ndef build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):\n    g_blocks = [\n        _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)\n    ]\n    rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]\n    for idx in range(2, len(g_blocks) + 1):\n        generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    final_model_list = g_blocks + [rgb_blocks[-1]]\n    generators.append(torch.nn.Sequential(*final_model_list))\n    return generators\n\n\nclass BlockD1D(torch.nn.Module):\n    def __init__(self, res=2):\n        super().__init__()\n        self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)\n        self.bias3 = ApplyBias(in_features=1)\n        self.res = res\n\n    def forward(self, x):\n        # x: [batch, 512, 4, 4]\n        x = mini_batch_std(x)  # [batch, 513, 4, 4]\n        x = self.elr_conv2d(x)  # [batch, 512, 4, 4]\n        x = self.bias1(x)  # [batch, 512, 4, 4]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]\n        x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]\n        x = self.elr_dense1(x)  # [batch, 512]\n        x = self.bias2(x)  # [batch, 512]\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]\n        x = self.elr_dense2(x)  # [batch, 1]\n        x = self.bias3(x)  # [batch, 1]\n        return x\n\n\nclass BlockD2D(torch.nn.Module):\n    def __init__(self, res):\n        super().__init__()\n        self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))\n        self.bias1 = ApplyBias(in_features=_nf(res - 1))\n        self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))\n        self.bias2 = ApplyBias(in_features=_nf(res - 2))\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        x = self.elr_conv2d1(x)\n        x = self.bias1(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.elr_conv2d2(x)\n        x = self.bias2(x)\n        x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)\n        x = self.pool(x)\n        return x\n\n\ndef _block_D(res, initial_resolution=2):\n    if res == initial_resolution:\n        model = BlockD1D(res)\n    else:\n        model = BlockD2D(res)\n    return model\n\n\nclass Disc(torch.nn.Module):\n    def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):\n        super().__init__()\n        self.d_blocks = torch.nn.ModuleList(d_blocks)\n        self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)\n        self.fade_in_alpha = fade_in_alpha\n        self.pool = torch.nn.AvgPool2d(kernel_size=2)\n\n    def forward(self, x):\n        new_x = self.rgb_blocks[1](x)\n        new_x = self.d_blocks[-1](new_x)\n        downscale_x = self.pool(x)\n        downscale_x = self.rgb_blocks[0](downscale_x)\n        x = fade_in(downscale_x, new_x, self.fade_in_alpha)\n        for d in self.d_blocks[:-1][::-1]:\n            x = d(x)\n        return x\n\n\ndef build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):\n    d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]\n    rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]\n    discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]\n    for idx in range(2, len(d_blocks) + 1):\n        discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))\n    return discriminators\n\n\n\nfade_in_alpha = torch.tensor(1.0)\nd_models = fe.build(\n    model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),\n    model_name=[\"d_{}\".format(size) for size in event_size])\n\ng_models = fe.build(\n    model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),\n    optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],\n    model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"])\n</pre> from torch.optim import Adam  def _nf(stage, fmap_base=8192, fmap_decay=1.0, fmap_max=512):     return min(int(fmap_base / (2.0**(stage * fmap_decay))), fmap_max)   class EqualizedLRDense(torch.nn.Linear):     def __init__(self, in_features, out_features, gain=np.sqrt(2)):         super().__init__(in_features, out_features, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         self.wscale = np.float32(gain / np.sqrt(in_features))      def forward(self, x):         return super().forward(x) * self.wscale   class ApplyBias(torch.nn.Module):     def __init__(self, in_features):         super().__init__()         self.in_features = in_features         self.bias = torch.nn.Parameter(torch.Tensor(in_features))         torch.nn.init.constant_(self.bias.data, val=0.0)      def forward(self, x):         if len(x.shape) == 4:             x = x + self.bias.view(1, -1, 1, 1).expand_as(x)         else:             x = x + self.bias         return x   class EqualizedLRConv2D(torch.nn.Conv2d):     def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, padding_mode='zeros', gain=np.sqrt(2)):         super().__init__(in_channels, out_channels, kernel_size, padding=padding, padding_mode=padding_mode, bias=False)         torch.nn.init.normal_(self.weight.data, mean=0.0, std=1.0)         fan_in = np.float32(np.prod(self.weight.data.shape[1:]))         self.wscale = np.float32(gain / np.sqrt(fan_in))      def forward(self, x):         return super().forward(x) * self.wscale   def pixel_normalization(x, eps=1e-8):     return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdims=True) + eps)   def mini_batch_std(x, group_size=4, eps=1e-8):     b, c, h, w = x.shape     group_size = min(group_size, b)     y = x.reshape((group_size, -1, c, h, w))  # [G, M, C, H, W]     y -= torch.mean(y, dim=0, keepdim=True)  # [G, M, C, H, W]     y = torch.mean(y**2, axis=0)  # [M, C, H, W]     y = torch.sqrt(y + eps)  # [M, C, H, W]     y = torch.mean(y, dim=(1, 2, 3), keepdim=True)  # [M, 1, 1, 1]     y = y.repeat(group_size, 1, h, w)  # [B, 1, H, W]     return torch.cat((x, y), 1)   def fade_in(x, y, alpha):     return (1.0 - alpha) * x + alpha * y   class ToRGB(torch.nn.Module):     def __init__(self, in_channels, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels, num_channels, kernel_size=1, padding=0, gain=1.0)         self.bias = ApplyBias(in_features=num_channels)      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         return x   class FromRGB(torch.nn.Module):     def __init__(self, res, num_channels=3):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(num_channels, _nf(res - 1), kernel_size=1, padding=0)         self.bias = ApplyBias(in_features=_nf(res - 1))      def forward(self, x):         x = self.elr_conv2d(x)         x = self.bias(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         return x   class BlockG1D(torch.nn.Module):     def __init__(self, res=2, latent_dim=512):         super().__init__()         self.elr_dense = EqualizedLRDense(in_features=latent_dim, out_features=_nf(res - 1) * 16, gain=np.sqrt(2) / 4)         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.res = res      def forward(self, x):         # x: [batch, 512]         x = pixel_normalization(x)  # [batch, 512]         x = self.elr_dense(x)  # [batch, _nf(res - 1) * 16]         x = x.view(-1, _nf(self.res - 1), 4, 4)  # [batch, _nf(res - 1), 4, 4]         x = self.bias1(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 4, 4]         x = self.elr_conv2d(x)  # [batch, _nf(res - 1), 4, 4]         x = self.bias2(x)  # [batch, _nf(res - 1), 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 4, 4]         x = pixel_normalization(x)         return x   class BlockG2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 2), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias2 = ApplyBias(in_features=_nf(res - 1))         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         # x: [batch, _nf(res - 2), 2**(res - 1), 2**(res - 1)]         x = self.upsample(x)         x = self.elr_conv2d1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias1(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.elr_conv2d2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = self.bias2(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, _nf(res - 1), 2**res , 2**res)]         x = pixel_normalization(x)  # [batch, _nf(res - 1), 2**res , 2**res)]         return x   def _block_G(res, latent_dim=512, initial_resolution=2):     if res == initial_resolution:         model = BlockG1D(res=res, latent_dim=latent_dim)     else:         model = BlockG2D(res=res)     return model   class Gen(torch.nn.Module):     def __init__(self, g_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.g_blocks = torch.nn.ModuleList(g_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.upsample = torch.nn.Upsample(scale_factor=2)      def forward(self, x):         for g in self.g_blocks[:-1]:             x = g(x)         previous_img = self.rgb_blocks[0](x)         previous_img = self.upsample(previous_img)         x = self.g_blocks[-1](x)         new_img = self.rgb_blocks[1](x)         return fade_in(previous_img, new_img, self.fade_in_alpha)   def build_G(fade_in_alpha, latent_dim=512, initial_resolution=2, target_resolution=10, num_channels=3):     g_blocks = [         _block_G(res, latent_dim, initial_resolution) for res in range(initial_resolution, target_resolution + 1)     ]     rgb_blocks = [ToRGB(_nf(res - 1), num_channels) for res in range(initial_resolution, target_resolution + 1)]     generators = [torch.nn.Sequential(g_blocks[0], rgb_blocks[0])]     for idx in range(2, len(g_blocks) + 1):         generators.append(Gen(g_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     final_model_list = g_blocks + [rgb_blocks[-1]]     generators.append(torch.nn.Sequential(*final_model_list))     return generators   class BlockD1D(torch.nn.Module):     def __init__(self, res=2):         super().__init__()         self.elr_conv2d = EqualizedLRConv2D(in_channels=_nf(res - 1) + 1, out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_dense1 = EqualizedLRDense(in_features=_nf(res - 1) * 16, out_features=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.elr_dense2 = EqualizedLRDense(in_features=_nf(res - 2), out_features=1, gain=1.0)         self.bias3 = ApplyBias(in_features=1)         self.res = res      def forward(self, x):         # x: [batch, 512, 4, 4]         x = mini_batch_std(x)  # [batch, 513, 4, 4]         x = self.elr_conv2d(x)  # [batch, 512, 4, 4]         x = self.bias1(x)  # [batch, 512, 4, 4]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512, 4, 4]         x = x.view(-1, _nf(self.res - 1) * 16)  # [batch, 512*4*4]         x = self.elr_dense1(x)  # [batch, 512]         x = self.bias2(x)  # [batch, 512]         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)  # [batch, 512]         x = self.elr_dense2(x)  # [batch, 1]         x = self.bias3(x)  # [batch, 1]         return x   class BlockD2D(torch.nn.Module):     def __init__(self, res):         super().__init__()         self.elr_conv2d1 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 1))         self.bias1 = ApplyBias(in_features=_nf(res - 1))         self.elr_conv2d2 = EqualizedLRConv2D(in_channels=_nf(res - 1), out_channels=_nf(res - 2))         self.bias2 = ApplyBias(in_features=_nf(res - 2))         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         x = self.elr_conv2d1(x)         x = self.bias1(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.elr_conv2d2(x)         x = self.bias2(x)         x = torch.nn.functional.leaky_relu(x, negative_slope=0.2)         x = self.pool(x)         return x   def _block_D(res, initial_resolution=2):     if res == initial_resolution:         model = BlockD1D(res)     else:         model = BlockD2D(res)     return model   class Disc(torch.nn.Module):     def __init__(self, d_blocks, rgb_blocks, fade_in_alpha):         super().__init__()         self.d_blocks = torch.nn.ModuleList(d_blocks)         self.rgb_blocks = torch.nn.ModuleList(rgb_blocks)         self.fade_in_alpha = fade_in_alpha         self.pool = torch.nn.AvgPool2d(kernel_size=2)      def forward(self, x):         new_x = self.rgb_blocks[1](x)         new_x = self.d_blocks[-1](new_x)         downscale_x = self.pool(x)         downscale_x = self.rgb_blocks[0](downscale_x)         x = fade_in(downscale_x, new_x, self.fade_in_alpha)         for d in self.d_blocks[:-1][::-1]:             x = d(x)         return x   def build_D(fade_in_alpha, initial_resolution=2, target_resolution=10, num_channels=3):     d_blocks = [_block_D(res, initial_resolution) for res in range(initial_resolution, target_resolution + 1)]     rgb_blocks = [FromRGB(res, num_channels) for res in range(initial_resolution, target_resolution + 1)]     discriminators = [torch.nn.Sequential(rgb_blocks[0], d_blocks[0])]     for idx in range(2, len(d_blocks) + 1):         discriminators.append(Disc(d_blocks[0:idx], rgb_blocks[idx - 2:idx], fade_in_alpha))     return discriminators    fade_in_alpha = torch.tensor(1.0) d_models = fe.build(     model_fn=lambda: build_D(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size),     model_name=[\"d_{}\".format(size) for size in event_size])  g_models = fe.build(     model_fn=lambda: build_G(fade_in_alpha, target_resolution=int(np.log2(target_size)), num_channels=1),     optimizer_fn=[lambda x: Adam(x, lr=0.001, betas=(0.0, 0.99), eps=1e-8)] * len(event_size) + [None],     model_name=[\"g_{}\".format(size) for size in event_size] + [\"G\"]) In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.backend import feed_forward, get_gradient\n\nclass ImageBlender(TensorOp):\n    def __init__(self, alpha, inputs=None, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.alpha = alpha\n\n    def forward(self, data, state):\n        image, image_lowres = data\n        new_img = self.alpha * image + (1 - self.alpha) * image_lowres\n        return new_img\n\n\nclass Interpolate(TensorOp):\n    def forward(self, data, state):\n        fake, real = data\n        batch_size = real.shape[0]\n        coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)\n        return real + (fake - real) * coeff\n\n\nclass GradientPenalty(TensorOp):\n    def __init__(self, inputs, outputs=None, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n\n    def forward(self, data, state):\n        x_interp, interp_score = data\n        gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)\n        grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))\n        gp = (grad_l2 - 1.0)**2\n        return gp\n\n\nclass GLoss(TensorOp):\n    def forward(self, data, state):\n        return -torch.mean(data)\n\n\nclass DLoss(TensorOp):\n\"\"\"Compute discriminator loss.\"\"\"\n    def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.wgan_lambda = wgan_lambda\n        self.wgan_epsilon = wgan_epsilon\n\n    def forward(self, data, state):\n        real_score, fake_score, gp = data\n        loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon\n        return torch.mean(loss)\n\n\nfake_img_map = {\n    epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nfake_score_map = {\n    epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\nreal_score_map = {\n    epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ninterp_score_map = {\n    epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)\n    for (epoch, model) in zip(event_epoch, d_models)\n}\ng_update_map = {\n    epoch: UpdateOp(loss_name=\"gloss\", model=model)\n    for (epoch, model) in zip(event_epoch, g_models[:-1])\n}\nd_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)}\nnetwork = fe.Network(ops=[\n    EpochScheduler(fake_img_map),\n    EpochScheduler(fake_score_map),\n    ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),\n    EpochScheduler(real_score_map),\n    Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),\n    EpochScheduler(interp_score_map),\n    GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),\n    GLoss(inputs=\"fake_score\", outputs=\"gloss\"),\n    DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),\n    EpochScheduler(g_update_map),\n    EpochScheduler(d_update_map)\n])\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.backend import feed_forward, get_gradient  class ImageBlender(TensorOp):     def __init__(self, alpha, inputs=None, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.alpha = alpha      def forward(self, data, state):         image, image_lowres = data         new_img = self.alpha * image + (1 - self.alpha) * image_lowres         return new_img   class Interpolate(TensorOp):     def forward(self, data, state):         fake, real = data         batch_size = real.shape[0]         coeff = torch.rand(batch_size, 1, 1, 1).to(fake.device)         return real + (fake - real) * coeff   class GradientPenalty(TensorOp):     def __init__(self, inputs, outputs=None, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)      def forward(self, data, state):         x_interp, interp_score = data         gradient_x_interp = get_gradient(torch.sum(interp_score), x_interp, higher_order=True)         grad_l2 = torch.sqrt(torch.sum(gradient_x_interp**2, dim=(1, 2, 3)))         gp = (grad_l2 - 1.0)**2         return gp   class GLoss(TensorOp):     def forward(self, data, state):         return -torch.mean(data)   class DLoss(TensorOp):     \"\"\"Compute discriminator loss.\"\"\"     def __init__(self, inputs, outputs=None, mode=None, wgan_lambda=10, wgan_epsilon=0.001):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.wgan_lambda = wgan_lambda         self.wgan_epsilon = wgan_epsilon      def forward(self, data, state):         real_score, fake_score, gp = data         loss = fake_score - real_score + self.wgan_lambda * gp + real_score**2 * self.wgan_epsilon         return torch.mean(loss)   fake_img_map = {     epoch: ModelOp(inputs=\"z\", outputs=\"x_fake\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } fake_score_map = {     epoch: ModelOp(inputs=\"x_fake\", outputs=\"fake_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } real_score_map = {     epoch: ModelOp(inputs=\"x_blend\", outputs=\"real_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } interp_score_map = {     epoch: ModelOp(inputs=\"x_interp\", outputs=\"interp_score\", model=model)     for (epoch, model) in zip(event_epoch, d_models) } g_update_map = {     epoch: UpdateOp(loss_name=\"gloss\", model=model)     for (epoch, model) in zip(event_epoch, g_models[:-1]) } d_update_map = {epoch: UpdateOp(loss_name=\"dloss\", model=model) for (epoch, model) in zip(event_epoch, d_models)} network = fe.Network(ops=[     EpochScheduler(fake_img_map),     EpochScheduler(fake_score_map),     ImageBlender(alpha=fade_in_alpha, inputs=(\"x\", \"x_low_res\"), outputs=\"x_blend\"),     EpochScheduler(real_score_map),     Interpolate(inputs=(\"x_fake\", \"x\"), outputs=\"x_interp\"),     EpochScheduler(interp_score_map),     GradientPenalty(inputs=(\"x_interp\", \"interp_score\"), outputs=\"gp\"),     GLoss(inputs=\"fake_score\", outputs=\"gloss\"),     DLoss(inputs=(\"real_score\", \"fake_score\", \"gp\"), outputs=\"dloss\"),     EpochScheduler(g_update_map),     EpochScheduler(d_update_map) ]) In\u00a0[9]: Copied! <pre>from fastestimator.trace import Trace\nfrom fastestimator.trace.io import ModelSaver\n\nclass AlphaController(Trace):\n    def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):\n        super().__init__(inputs=None, outputs=None, mode=\"train\")\n        self.alpha = alpha\n        self.fade_start_epochs = fade_start_epochs\n        self.duration = duration\n        self.batch_scheduler = batch_scheduler\n        self.num_examples = num_examples\n        self.change_alpha = False\n        self.nimg_total = self.duration * self.num_examples\n        self._idx = 0\n        self.nimg_so_far = 0\n        self.current_batch_size = None\n\n    def on_epoch_begin(self, state):\n        # check whetehr the current epoch is in smooth transition of resolutions\n        fade_epoch = self.fade_start_epochs[self._idx]\n        if self.system.epoch_idx == fade_epoch:\n            self.change_alpha = True\n            self.nimg_so_far = 0\n            self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)\n            print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))\n        elif self.system.epoch_idx == fade_epoch + self.duration:\n            print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))\n            self.change_alpha = False\n            if self._idx + 1 &lt; len(self.fade_start_epochs):\n                self._idx += 1\n            self.alpha.data = torch.tensor(1.0)\n\n    def on_batch_begin(self, state):\n        # if in resolution transition, smoothly change the alpha from 0 to 1\n        if self.change_alpha:\n            self.nimg_so_far += self.current_batch_size\n            self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)\n            \ntraces = [\n    AlphaController(alpha=fade_in_alpha,\n                    fade_start_epochs=event_epoch[1:],\n                    duration=phase_length,\n                    batch_scheduler=batch_scheduler,\n                    num_examples=len(dataset)),\n    ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]\n            \n            \nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch)\n</pre> from fastestimator.trace import Trace from fastestimator.trace.io import ModelSaver  class AlphaController(Trace):     def __init__(self, alpha, fade_start_epochs, duration, batch_scheduler, num_examples):         super().__init__(inputs=None, outputs=None, mode=\"train\")         self.alpha = alpha         self.fade_start_epochs = fade_start_epochs         self.duration = duration         self.batch_scheduler = batch_scheduler         self.num_examples = num_examples         self.change_alpha = False         self.nimg_total = self.duration * self.num_examples         self._idx = 0         self.nimg_so_far = 0         self.current_batch_size = None      def on_epoch_begin(self, state):         # check whetehr the current epoch is in smooth transition of resolutions         fade_epoch = self.fade_start_epochs[self._idx]         if self.system.epoch_idx == fade_epoch:             self.change_alpha = True             self.nimg_so_far = 0             self.current_batch_size = self.batch_scheduler.get_current_value(self.system.epoch_idx)             print(\"FastEstimator-Alpha: Started fading in for size {}\".format(2**(self._idx + 3)))         elif self.system.epoch_idx == fade_epoch + self.duration:             print(\"FastEstimator-Alpha: Finished fading in for size {}\".format(2**(self._idx + 3)))             self.change_alpha = False             if self._idx + 1 &lt; len(self.fade_start_epochs):                 self._idx += 1             self.alpha.data = torch.tensor(1.0)      def on_batch_begin(self, state):         # if in resolution transition, smoothly change the alpha from 0 to 1         if self.change_alpha:             self.nimg_so_far += self.current_batch_size             self.alpha.data = torch.tensor(self.nimg_so_far / self.nimg_total, dtype=torch.float32)              traces = [     AlphaController(alpha=fade_in_alpha,                     fade_start_epochs=event_epoch[1:],                     duration=phase_length,                     batch_scheduler=batch_scheduler,                     num_examples=len(dataset)),     ModelSaver(model=g_models[-1], save_dir=save_dir, frequency=phase_length)]                           estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit()"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-gan-pggan", "title": "Progressive Growing GAN (PGGAN)\u00b6", "text": "<p>In this notebook, we will demonstrate the functionality of <code>Scheduler</code> which enables advanced training schemes such as the progressive training method described in Karras et al.. We will train a PGGAN to produce synthetic frontal chest X-ray images where both the generator and the discriminator grow from $4\\times4$ to $128\\times128$.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#progressive-growing-strategy", "title": "Progressive Growing Strategy\u00b6", "text": "<p>Karras et al. propose a training scheme in which both the generator and the discriminator progressively grow from a low resolution to a high resolution. Both networks begin their training based on $4\\times4$ images as illustrated below.  Then, both networks progress from $4\\times4$ to $8\\times8$ by an adding an additional block that contains a couple of convolutional layers.  Both the generator and the discriminator progressively grow until reaching the desired resolution of $1024\\times 1024$.  Image Credit: Presentation slide</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#smooth-transition-between-resolutions", "title": "Smooth Transition between Resolutions\u00b6", "text": "<p>However, when growing the networks, the new blocks must be slowly faded into the networks in order to smoothly transition between different resolutions. For example, when growing the generator from $16\\times16$ to $32\\times32$, the newly added block of $32\\times32$ is slowly faded into the already well trained $16\\times16$ network by linearly increasing a fade-factor $\\alpha$ from $0$ to $1$. Once the network is fully transitioned to $32\\times32$, the network is trained a bit further to stabilize before growing to $64\\times64$.  Image Credit: PGGAN Paper</p> <p>With this progressive training strategy, PGGAN has achieved the state-of-the-art results in producing high fidelity synthetic images.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#problem-setting", "title": "Problem Setting\u00b6", "text": "<p>In this PGGAN example, we decided the following:</p> <ul> <li>560K images will be used when transitioning from a lower resolution to a higher resolution.</li> <li>560K images will be used when stabilizing the fully transitioned network.</li> <li>Initial resolution will be $4\\times4$.</li> <li>Final resolution will be $128\\times128$.</li> </ul> <p>The number of images for both transitioning and stabilizing is equivalent to 5 epochs; the networks would smoothly grow over 5 epochs and would stabilize for 5 epochs. This yields the following schedule for growing both networks:</p> <ul> <li>From $1^{st}$ epoch to $5^{th}$ epoch: train $4\\times4$ resolution</li> <li>From $6^{th}$ epoch to $10^{th}$ epoch: transition from $4\\times4$ to $8\\times8$</li> <li>From $11^{th}$ epoch to $15^{th}$ epoch: stabilize $8\\times8$</li> <li>From $16^{th}$ epoch to $20^{th}$ epoch: transition from $8\\times8$ to $16\\times16$</li> <li>From $21^{st}$ epoch to $25^{th}$ epoch: stabilize $16\\times16$</li> <li>From $26^{th}$ epoch to $30^{th}$ epoch: transition from $16\\times16$ to $32\\times32$</li> <li>From $31^{st}$ epoch to $35^{th}$ epoch: stabilize $32\\times32$</li> </ul> <p>$\\cdots$</p> <ul> <li>From $51^{th}$ epoch to $55^{th}$ epoch: stabilize $128\\times128$</li> </ul>"}, {"location": "apphub/image_generation/pggan/pggan.html#configure-growing-parameters", "title": "Configure growing parameters\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-input-pipeline", "title": "Defining Input <code>Pipeline</code>\u00b6", "text": "<p>First, we need to download the chest frontal X-ray dataset from the National Institute of Health (NIH); the dataset has over 112,000 images with resolution $1024\\times1024$. We use the pre-built <code>fastestimator.dataset.nih_chestxray</code> API to download these images. A detailed description of the dataset is available here.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#note-please-make-sure-to-have-a-stable-internet-connection-when-downloading-the-dataset-for-the-first-time-since-the-size-of-the-dataset-is-over-40gb", "title": "Note: Please make sure to have a stable internet connection when downloading the dataset for the first time since the size of the dataset is over 40GB.\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#given-the-images-we-need-the-following-preprocessing-operations-to-execute-dynamically-for-every-batch", "title": "Given the images, we need the following preprocessing operations to execute dynamically for every batch:\u00b6", "text": "<ol> <li>Read the image.</li> <li>Resize the image to the correct size based on the current epoch.</li> <li>Create a lower resolution of the image, which is accomplished by downsampling by a factor of 2 then upsampling by a factor of 2.</li> <li>Rescale the pixels of both the original image and lower resolution image to the range [-1, 1]</li> <li>Convert both the original image and lower resolution image from channel last to channel first</li> <li>Create the latent vector used by the generator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-network", "title": "Defining <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#defining-the-generator-and-the-discriminator", "title": "Defining the generator and the discriminator\u00b6", "text": "<p>To express the progressive growing of networks, we return a list of models that progressively grow from $4 \\times 4$ to $1024 \\times 1024$ such that $i^{th}$ model in the list is a superset of the previous models. We define a <code>fade_in_alpha</code> to control the smoothness of growth. <code>fe.build</code> then bundles each model, optimizer, and model name together for use.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#the-following-operations-will-happen-in-our-network", "title": "The Following operations will happen in our <code>Network</code>:\u00b6", "text": "<ol> <li>random vector -&gt; generator -&gt; fake images</li> <li>fake images -&gt; discriminator -&gt; fake scores</li> <li>real image, low resolution real image -&gt; blender -&gt; blended real images</li> <li>blended real images -&gt; discriminator -&gt; real scores</li> <li>fake images, real images -&gt; interpolater -&gt; interpolated images</li> <li>interpolated images -&gt; discriminator -&gt; interpolated scores</li> <li>interpolated scores, interpolated image -&gt; get_gradient -&gt; gradient penalty</li> <li>fake_score -&gt; GLoss -&gt; generator loss</li> <li>real score, fake score, gradient penalty -&gt; DLoss -&gt; discriminator loss</li> <li>update generator</li> <li>update discriminator</li> </ol>"}, {"location": "apphub/image_generation/pggan/pggan.html#defining-estimator", "title": "Defining Estimator\u00b6", "text": "<p>Given that <code>Pipeline</code> and <code>Network</code> are properly defined, we need to define an <code>AlphaController</code> <code>Trace</code> to help both the generator and the discriminator smoothly grow by controlling the value of the <code>fade_in_alpha</code> tensor created previously.  We will also use <code>ModelSaver</code> to save our model during every training phase.</p>"}, {"location": "apphub/image_generation/pggan/pggan.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "apphub/image_generation/pggan/pggan.html#note-for-128x128-resolution-it-takes-about-24-hours-on-single-v100-gpu-for-1024x1024-resolution-it-takes-25-days-on-4-v100-gpus", "title": "Note: for 128x128 resolution, it takes about 24 hours on single V100 GPU.    for 1024x1024 resolution, it takes ~ 2.5 days on 4 V100 GPUs.\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html", "title": "Instance Detection with RetinaNet", "text": "<p>We are going to implement RetinaNet by Lin et al., 2017 for COCO dataset instance detection.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\nimport fastestimator as fe\n</pre> import tempfile  import cv2 import numpy as np import pandas as pd import tensorflow as tf from matplotlib import pyplot as plt  import fastestimator as fe In\u00a0[2]: Copied! <pre>#training parameters\nbatch_size = 8\nepochs = 12 \n\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\n</pre> #training parameters batch_size = 8 epochs = 12   max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() <p>The <code>class.json</code> includes a map for the class number and what object the number corresponds to.</p> In\u00a0[3]: Copied! <pre>import json\n\nwith open('class.json', 'r') as f:\n    class_map = json.load(f)\n</pre> import json  with open('class.json', 'r') as f:     class_map = json.load(f) <p>We call our <code>mscoco</code> data API to obtain the training and validation set:</p> <ul> <li>118287 images for training</li> <li>5000 images for validation</li> </ul> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\n\ntrain_ds, eval_ds = mscoco.load_data()\nassert len(train_ds) == 118287\nassert len(eval_ds) == 5000\n</pre> from fastestimator.dataset.data import mscoco  train_ds, eval_ds = mscoco.load_data() assert len(train_ds) == 118287 assert len(eval_ds) == 5000 <p>Anchors are predefined for each pixel in the feature map. In this apphub our backbone is ResNet-50, so we can precompute all the anchors we need for training.</p> In\u00a0[5]: Copied! <pre>def get_fpn_anchor_box(width: int, height: int):\n    assert height % 32 == 0 and width % 32 == 0\n    shapes = [(int(height / 8), int(width / 8))]  # P3\n    num_pixel = [np.prod(shapes)]\n    anchor_lengths = [32, 64, 128, 256, 512]\n    for _ in range(4):  # P4 through P7\n        shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))\n        num_pixel.append(np.prod(shapes[-1]))\n    total_num_pixels = np.sum(num_pixel)\n    anchorbox = np.zeros((9 * total_num_pixels, 4))\n    anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]\n    aspect_ratios = [1.0, 2.0, 0.5]  #x:y\n    anchor_idx = 0\n    for shape, anchor_length in zip(shapes, anchor_lengths):\n        p_h, p_w = shape\n        base_y = 2**np.ceil(np.log2(height / p_h))\n        base_x = 2**np.ceil(np.log2(width / p_w))\n        for i in range(p_h):\n            center_y = (i + 1 / 2) * base_y\n            for j in range(p_w):\n                center_x = (j + 1 / 2) * base_x\n                for anchor_length_multiplier in anchor_length_multipliers:\n                    area = (anchor_length * anchor_length_multiplier)**2\n                    for aspect_ratio in aspect_ratios:\n                        x1 = center_x - np.sqrt(area * aspect_ratio) / 2\n                        y1 = center_y - np.sqrt(area / aspect_ratio) / 2\n                        x2 = center_x + np.sqrt(area * aspect_ratio) / 2\n                        y2 = center_y + np.sqrt(area / aspect_ratio) / 2\n                        anchorbox[anchor_idx, 0] = x1\n                        anchorbox[anchor_idx, 1] = y1\n                        anchorbox[anchor_idx, 2] = x2 - x1\n                        anchorbox[anchor_idx, 3] = y2 - y1\n                        anchor_idx += 1\n        if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore\n            break\n    return np.float32(anchorbox), np.int32(num_pixel) * 9\n</pre> def get_fpn_anchor_box(width: int, height: int):     assert height % 32 == 0 and width % 32 == 0     shapes = [(int(height / 8), int(width / 8))]  # P3     num_pixel = [np.prod(shapes)]     anchor_lengths = [32, 64, 128, 256, 512]     for _ in range(4):  # P4 through P7         shapes.append((int(np.ceil(shapes[-1][0] / 2)), int(np.ceil(shapes[-1][1] / 2))))         num_pixel.append(np.prod(shapes[-1]))     total_num_pixels = np.sum(num_pixel)     anchorbox = np.zeros((9 * total_num_pixels, 4))     anchor_length_multipliers = [2**(0.0), 2**(1 / 3), 2**(2 / 3)]     aspect_ratios = [1.0, 2.0, 0.5]  #x:y     anchor_idx = 0     for shape, anchor_length in zip(shapes, anchor_lengths):         p_h, p_w = shape         base_y = 2**np.ceil(np.log2(height / p_h))         base_x = 2**np.ceil(np.log2(width / p_w))         for i in range(p_h):             center_y = (i + 1 / 2) * base_y             for j in range(p_w):                 center_x = (j + 1 / 2) * base_x                 for anchor_length_multiplier in anchor_length_multipliers:                     area = (anchor_length * anchor_length_multiplier)**2                     for aspect_ratio in aspect_ratios:                         x1 = center_x - np.sqrt(area * aspect_ratio) / 2                         y1 = center_y - np.sqrt(area / aspect_ratio) / 2                         x2 = center_x + np.sqrt(area * aspect_ratio) / 2                         y2 = center_y + np.sqrt(area / aspect_ratio) / 2                         anchorbox[anchor_idx, 0] = x1                         anchorbox[anchor_idx, 1] = y1                         anchorbox[anchor_idx, 2] = x2 - x1                         anchorbox[anchor_idx, 3] = y2 - y1                         anchor_idx += 1         if p_h == 1 and p_w == 1:  # the next level of 1x1 feature map is still 1x1, therefore ignore             break     return np.float32(anchorbox), np.int32(num_pixel) * 9 In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AnchorBox(NumpyOp):\n    def __init__(self, width, height, inputs, outputs, mode=None):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.anchorbox, _ = get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4\n\n    def forward(self, data, state):\n        target = self._generate_target(data)  # bbox is #obj x 5\n        return np.float32(target)\n\n    def _generate_target(self, bbox):\n        object_boxes = bbox[:, :-1]  # num_obj x 4\n        label = bbox[:, -1]  # num_obj x 1\n        ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor\n        #now for each object in image, assign the anchor box with highest iou to them\n        anchorbox_best_iou_idx = np.argmax(ious, axis=1)\n        num_obj = ious.shape[0]\n        for row in range(num_obj):\n            ious[row, anchorbox_best_iou_idx[row]] = 0.99\n        #next, begin the anchor box assignment based on iou\n        anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1\n        anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1\n        cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1\n        cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class\n        cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples\n        #finally, calculate localization target\n        single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4\n        gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)\n        ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)\n        dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)\n        dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)\n        dwidth = np.squeeze(np.log(gt_width / ac_width))\n        dheight = np.squeeze(np.log(gt_height / ac_height))\n        return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5\n\n    @staticmethod\n    def _get_iou(boxes1, boxes2):\n\"\"\"Computes the value of intersection over union (IoU) of two array of boxes.\n        Args:\n            box1 (array): first boxes in N x 4\n            box2 (array): second box in M x 4\n        Returns:\n            float: IoU value in N x M\n        \"\"\"\n        x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)\n        x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)\n        x12 = x11 + w1\n        y12 = y11 + h1\n        x22 = x21 + w2\n        y22 = y21 + h2\n        xmin = np.maximum(x11, np.transpose(x21))\n        ymin = np.maximum(y11, np.transpose(y21))\n        xmax = np.minimum(x12, np.transpose(x22))\n        ymax = np.minimum(y12, np.transpose(y22))\n        inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)\n        area1 = (w1 + 1) * (h1 + 1)\n        area2 = (w2 + 1) * (h2 + 1)\n        iou = inter_area / (area1 + area2.T - inter_area)\n        return iou\n</pre> from fastestimator.op.numpyop import NumpyOp  class AnchorBox(NumpyOp):     def __init__(self, width, height, inputs, outputs, mode=None):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.anchorbox, _ = get_fpn_anchor_box(width, height)  # anchorbox is #num_anchor x 4      def forward(self, data, state):         target = self._generate_target(data)  # bbox is #obj x 5         return np.float32(target)      def _generate_target(self, bbox):         object_boxes = bbox[:, :-1]  # num_obj x 4         label = bbox[:, -1]  # num_obj x 1         ious = self._get_iou(object_boxes, self.anchorbox)  # num_obj x num_anchor         #now for each object in image, assign the anchor box with highest iou to them         anchorbox_best_iou_idx = np.argmax(ious, axis=1)         num_obj = ious.shape[0]         for row in range(num_obj):             ious[row, anchorbox_best_iou_idx[row]] = 0.99         #next, begin the anchor box assignment based on iou         anchor_to_obj_idx = np.argmax(ious, axis=0)  # num_anchor x 1         anchor_best_iou = np.max(ious, axis=0)  # num_anchor x 1         cls_gt = np.int32([label[idx] for idx in anchor_to_obj_idx])  # num_anchor x 1         cls_gt[np.where(anchor_best_iou &lt;= 0.4)] = -1  #background class         cls_gt[np.where(np.logical_and(anchor_best_iou &gt; 0.4, anchor_best_iou &lt;= 0.5))] = -2  # ignore these examples         #finally, calculate localization target         single_loc_gt = object_boxes[anchor_to_obj_idx]  # num_anchor x 4         gt_x1, gt_y1, gt_width, gt_height = np.split(single_loc_gt, 4, axis=1)         ac_x1, ac_y1, ac_width, ac_height = np.split(self.anchorbox, 4, axis=1)         dx1 = np.squeeze((gt_x1 - ac_x1) / ac_width)         dy1 = np.squeeze((gt_y1 - ac_y1) / ac_height)         dwidth = np.squeeze(np.log(gt_width / ac_width))         dheight = np.squeeze(np.log(gt_height / ac_height))         return np.array([dx1, dy1, dwidth, dheight, cls_gt]).T  # num_anchor x 5      @staticmethod     def _get_iou(boxes1, boxes2):         \"\"\"Computes the value of intersection over union (IoU) of two array of boxes.         Args:             box1 (array): first boxes in N x 4             box2 (array): second box in M x 4         Returns:             float: IoU value in N x M         \"\"\"         x11, y11, w1, h1 = np.split(boxes1, 4, axis=1)         x21, y21, w2, h2 = np.split(boxes2, 4, axis=1)         x12 = x11 + w1         y12 = y11 + h1         x22 = x21 + w2         y22 = y21 + h2         xmin = np.maximum(x11, np.transpose(x21))         ymin = np.maximum(y11, np.transpose(y21))         xmax = np.minimum(x12, np.transpose(x22))         ymax = np.minimum(y12, np.transpose(y22))         inter_area = np.maximum((xmax - xmin + 1), 0) * np.maximum((ymax - ymin + 1), 0)         area1 = (w1 + 1) * (h1 + 1)         area2 = (w2 + 1) * (h2 + 1)         iou = inter_area / (area1 + area2.T - inter_area)         return iou <p>For our data pipeline, we resize the images such that the longer side is 512 pixels. We keep the aspect ratio the same as original image, so on the shorter side we pad zeros. The resized image is 512 by 512. For data augmentation we only flip the image horizontally.</p> In\u00a0[7]: Copied! <pre>from albumentations import BboxParams\n\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage, ToArray\n\npipeline = fe.Pipeline(\n    train_data=train_ds,\n    eval_data=eval_ds,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        LongestMaxSize(512,\n                       image_in=\"image\",\n                       image_out=\"image\",\n                       bbox_in=\"bbox\",\n                       bbox_out=\"bbox\",\n                       bbox_params=BboxParams(\"coco\", min_area=1.0)),\n        PadIfNeeded(\n            512,\n            512,\n            border_mode=cv2.BORDER_CONSTANT,\n            image_in=\"image\",\n            image_out=\"image\",\n            bbox_in=\"bbox\",\n            bbox_out=\"bbox\",\n            bbox_params=BboxParams(\"coco\", min_area=1.0),\n        ),\n        Sometimes(\n            HorizontalFlip(mode=\"train\",\n                           image_in=\"image\",\n                           image_out=\"image\",\n                           bbox_in=\"bbox\",\n                           bbox_out=\"bbox\",\n                           bbox_params='coco')),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),\n        AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=512, height=512)\n    ],\n    pad_value=0)\n</pre> from albumentations import BboxParams  from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded from fastestimator.op.numpyop.univariate import Normalize, ReadImage, ToArray  pipeline = fe.Pipeline(     train_data=train_ds,     eval_data=eval_ds,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         LongestMaxSize(512,                        image_in=\"image\",                        image_out=\"image\",                        bbox_in=\"bbox\",                        bbox_out=\"bbox\",                        bbox_params=BboxParams(\"coco\", min_area=1.0)),         PadIfNeeded(             512,             512,             border_mode=cv2.BORDER_CONSTANT,             image_in=\"image\",             image_out=\"image\",             bbox_in=\"bbox\",             bbox_out=\"bbox\",             bbox_params=BboxParams(\"coco\", min_area=1.0),         ),         Sometimes(             HorizontalFlip(mode=\"train\",                            image_in=\"image\",                            image_out=\"image\",                            bbox_in=\"bbox\",                            bbox_out=\"bbox\",                            bbox_params='coco')),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         ToArray(inputs=\"bbox\", outputs=\"bbox\", dtype=\"float32\"),         AnchorBox(inputs=\"bbox\", outputs=\"anchorbox\", width=512, height=512)     ],     pad_value=0) In\u00a0[92]: Copied! <pre>batch_data = pipeline.get_results(mode='eval', num_steps=2)\n</pre> batch_data = pipeline.get_results(mode='eval', num_steps=2) In\u00a0[93]: Copied! <pre>step_index = 1\nbatch_index = 2\n\nimg = batch_data[step_index]['image'][batch_index].numpy()\nimg = ((img + 1)/2 * 255).astype(np.uint8)\n\nkeep = batch_data[step_index]['bbox'][batch_index].numpy()[..., -1] &gt; 0\nx1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T\nx2 = x1 + w\ny2 = y1 + h\n\nfig, ax = plt.subplots(figsize=(10, 10))\nfor j in range(len(x1)):\n    cv2.rectangle(img, (x1[j], y1[j]), (x2[j], y2[j]), (0, 0, 255), 2)\n    ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0, 0, 1), fontsize=14, fontweight='bold')\n\nax.imshow(img)\nprint(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy()))\n</pre> step_index = 1 batch_index = 2  img = batch_data[step_index]['image'][batch_index].numpy() img = ((img + 1)/2 * 255).astype(np.uint8)  keep = batch_data[step_index]['bbox'][batch_index].numpy()[..., -1] &gt; 0 x1, y1, w, h, label = batch_data[step_index]['bbox'][batch_index].numpy()[keep].T x2 = x1 + w y2 = y1 + h  fig, ax = plt.subplots(figsize=(10, 10)) for j in range(len(x1)):     cv2.rectangle(img, (x1[j], y1[j]), (x2[j], y2[j]), (0, 0, 255), 2)     ax.text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0, 0, 1), fontsize=14, fontweight='bold')  ax.imshow(img) print(\"id = {}\".format(batch_data[step_index]['image_id'][batch_index].numpy())) <pre>id = 180101\n</pre> <p>We define the classification (class) subnet and regression (box) subnet. See Fig. 3 of the original paper.</p> In\u00a0[96]: Copied! <pre>from tensorflow.python.keras import layers, models, regularizers\n\ndef _classification_sub_net(num_classes, num_anchor=9):\n    model = models.Sequential()\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(num_classes * num_anchor,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='sigmoid',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n                      bias_initializer=tf.initializers.constant(np.log(1 / 99))))\n    model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]\n    return model\n\n\ndef _regression_sub_net(num_anchor=9):\n    model = models.Sequential()\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(256,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      activation='relu',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(\n        layers.Conv2D(4 * num_anchor,\n                      kernel_size=3,\n                      strides=1,\n                      padding='same',\n                      kernel_regularizer=regularizers.l2(0.0001),\n                      kernel_initializer=tf.random_normal_initializer(stddev=0.01)))\n    model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]\n    return model\n</pre> from tensorflow.python.keras import layers, models, regularizers  def _classification_sub_net(num_classes, num_anchor=9):     model = models.Sequential()     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(num_classes * num_anchor,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='sigmoid',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01),                       bias_initializer=tf.initializers.constant(np.log(1 / 99))))     model.add(layers.Reshape((-1, num_classes)))  # the output dimension is [batch, #anchor, #classes]     return model   def _regression_sub_net(num_anchor=9):     model = models.Sequential()     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(256,                       kernel_size=3,                       strides=1,                       padding='same',                       activation='relu',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(         layers.Conv2D(4 * num_anchor,                       kernel_size=3,                       strides=1,                       padding='same',                       kernel_regularizer=regularizers.l2(0.0001),                       kernel_initializer=tf.random_normal_initializer(stddev=0.01)))     model.add(layers.Reshape((-1, 4)))  # the output dimension is [batch, #anchor, 4]     return model <p>We use ResNet-50 as our backbone.</p> In\u00a0[97]: Copied! <pre>def RetinaNet(input_shape, num_classes, num_anchor=9):\n    inputs = tf.keras.Input(shape=input_shape)\n    # FPN\n    resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)\n    assert resnet50.layers[80].name == \"conv3_block4_out\"\n    C3 = resnet50.layers[80].output\n    assert resnet50.layers[142].name == \"conv4_block6_out\"\n    C4 = resnet50.layers[142].output\n    assert resnet50.layers[-1].name == \"conv5_block3_out\"\n    C5 = resnet50.layers[-1].output\n    P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C5)\n    P5_upsampling = layers.UpSampling2D()(P5)\n    P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C4)\n    P4 = layers.Add()([P5_upsampling, P4])\n    P4_upsampling = layers.UpSampling2D()(P4)\n    P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C3)\n    P3 = layers.Add()([P4_upsampling, P3])\n    P6 = layers.Conv2D(256,\n                       kernel_size=3,\n                       strides=2,\n                       padding='same',\n                       name=\"P6\",\n                       kernel_regularizer=regularizers.l2(0.0001))(C5)\n    P7 = layers.Activation('relu')(P6)\n    P7 = layers.Conv2D(256,\n                       kernel_size=3,\n                       strides=2,\n                       padding='same',\n                       name=\"P7\",\n                       kernel_regularizer=regularizers.l2(0.0001))(P7)\n    P5 = layers.Conv2D(256,\n                       kernel_size=3,\n                       strides=1,\n                       padding='same',\n                       name=\"P5\",\n                       kernel_regularizer=regularizers.l2(0.0001))(P5)\n    P4 = layers.Conv2D(256,\n                       kernel_size=3,\n                       strides=1,\n                       padding='same',\n                       name=\"P4\",\n                       kernel_regularizer=regularizers.l2(0.0001))(P4)\n    P3 = layers.Conv2D(256,\n                       kernel_size=3,\n                       strides=1,\n                       padding='same',\n                       name=\"P3\",\n                       kernel_regularizer=regularizers.l2(0.0001))(P3)\n    # classification subnet\n    cls_subnet = _classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)\n    P3_cls = cls_subnet(P3)\n    P4_cls = cls_subnet(P4)\n    P5_cls = cls_subnet(P5)\n    P6_cls = cls_subnet(P6)\n    P7_cls = cls_subnet(P7)\n    cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])\n    # localization subnet\n    loc_subnet = _regression_sub_net(num_anchor=num_anchor)\n    P3_loc = loc_subnet(P3)\n    P4_loc = loc_subnet(P4)\n    P5_loc = loc_subnet(P5)\n    P6_loc = loc_subnet(P6)\n    P7_loc = loc_subnet(P7)\n    loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])\n    return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output])\n</pre> def RetinaNet(input_shape, num_classes, num_anchor=9):     inputs = tf.keras.Input(shape=input_shape)     # FPN     resnet50 = tf.keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor=inputs, pooling=None)     assert resnet50.layers[80].name == \"conv3_block4_out\"     C3 = resnet50.layers[80].output     assert resnet50.layers[142].name == \"conv4_block6_out\"     C4 = resnet50.layers[142].output     assert resnet50.layers[-1].name == \"conv5_block3_out\"     C5 = resnet50.layers[-1].output     P5 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C5)     P5_upsampling = layers.UpSampling2D()(P5)     P4 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C4)     P4 = layers.Add()([P5_upsampling, P4])     P4_upsampling = layers.UpSampling2D()(P4)     P3 = layers.Conv2D(256, kernel_size=1, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.0001))(C3)     P3 = layers.Add()([P4_upsampling, P3])     P6 = layers.Conv2D(256,                        kernel_size=3,                        strides=2,                        padding='same',                        name=\"P6\",                        kernel_regularizer=regularizers.l2(0.0001))(C5)     P7 = layers.Activation('relu')(P6)     P7 = layers.Conv2D(256,                        kernel_size=3,                        strides=2,                        padding='same',                        name=\"P7\",                        kernel_regularizer=regularizers.l2(0.0001))(P7)     P5 = layers.Conv2D(256,                        kernel_size=3,                        strides=1,                        padding='same',                        name=\"P5\",                        kernel_regularizer=regularizers.l2(0.0001))(P5)     P4 = layers.Conv2D(256,                        kernel_size=3,                        strides=1,                        padding='same',                        name=\"P4\",                        kernel_regularizer=regularizers.l2(0.0001))(P4)     P3 = layers.Conv2D(256,                        kernel_size=3,                        strides=1,                        padding='same',                        name=\"P3\",                        kernel_regularizer=regularizers.l2(0.0001))(P3)     # classification subnet     cls_subnet = _classification_sub_net(num_classes=num_classes, num_anchor=num_anchor)     P3_cls = cls_subnet(P3)     P4_cls = cls_subnet(P4)     P5_cls = cls_subnet(P5)     P6_cls = cls_subnet(P6)     P7_cls = cls_subnet(P7)     cls_output = layers.Concatenate(axis=-2)([P3_cls, P4_cls, P5_cls, P6_cls, P7_cls])     # localization subnet     loc_subnet = _regression_sub_net(num_anchor=num_anchor)     P3_loc = loc_subnet(P3)     P4_loc = loc_subnet(P4)     P5_loc = loc_subnet(P5)     P6_loc = loc_subnet(P6)     P7_loc = loc_subnet(P7)     loc_output = layers.Concatenate(axis=-2)([P3_loc, P4_loc, P5_loc, P6_loc, P7_loc])     return tf.keras.Model(inputs=inputs, outputs=[cls_output, loc_output]) <p>We use focal loss for classification and smooth L1 for regression loss.</p> In\u00a0[98]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass RetinaLoss(TensorOp):\n    def forward(self, data, state):\n        anchorbox, cls_pred, loc_pred = data\n        batch_size = anchorbox.shape[0]\n        focal_loss, l1_loss, total_loss = [], [], []\n        for idx in range(batch_size):\n            single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], tf.cast(anchorbox[idx][:, -1], tf.int32)\n            single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]\n            single_focal_loss, anchor_obj_idx = self.focal_loss(single_cls_gt, single_cls_pred)\n            single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_idx)\n            focal_loss.append(single_focal_loss)\n            l1_loss.append(single_l1_loss)\n        focal_loss, l1_loss = tf.reduce_mean(focal_loss), tf.reduce_mean(l1_loss)\n        total_loss = focal_loss + l1_loss\n        return total_loss, focal_loss, l1_loss\n\n    def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):\n        # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]\n        num_classes = single_cls_pred.shape[-1]\n        # gather the objects and background, discard the rest\n        anchor_obj_idx = tf.where(tf.greater_equal(single_cls_gt, 0))\n        anchor_obj_bg_idx = tf.where(tf.greater_equal(single_cls_gt, -1))\n        anchor_obj_count = tf.cast(tf.shape(anchor_obj_idx)[0], tf.float32)\n        single_cls_gt = tf.one_hot(single_cls_gt, num_classes)\n        single_cls_gt = tf.gather_nd(single_cls_gt, anchor_obj_bg_idx)\n        single_cls_pred = tf.gather_nd(single_cls_pred, anchor_obj_bg_idx)\n        single_cls_gt = tf.reshape(single_cls_gt, (-1, 1))\n        single_cls_pred = tf.reshape(single_cls_pred, (-1, 1))\n        # compute the focal weight on each selected anchor box\n        alpha_factor = tf.ones_like(single_cls_gt) * alpha\n        alpha_factor = tf.where(tf.equal(single_cls_gt, 1), alpha_factor, 1 - alpha_factor)\n        focal_weight = tf.where(tf.equal(single_cls_gt, 1), 1 - single_cls_pred, single_cls_pred)\n        focal_weight = alpha_factor * focal_weight**gamma / anchor_obj_count\n        cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(single_cls_gt,\n                                                                 single_cls_pred,\n                                                                 sample_weight=focal_weight)\n        return cls_loss, anchor_obj_idx\n\n    def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_idx, beta=0.1):\n        # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]\n        single_loc_pred = tf.gather_nd(single_loc_pred, anchor_obj_idx)  #anchor_obj_count x 4\n        single_loc_gt = tf.gather_nd(single_loc_gt, anchor_obj_idx)  #anchor_obj_count x 4\n        anchor_obj_count = tf.cast(tf.shape(single_loc_pred)[0], tf.float32)\n        single_loc_gt = tf.reshape(single_loc_gt, (-1, 1))\n        single_loc_pred = tf.reshape(single_loc_pred, (-1, 1))\n        loc_diff = tf.abs(single_loc_gt - single_loc_pred)\n        cond = tf.less(loc_diff, beta)\n        loc_loss = tf.where(cond, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)\n        loc_loss = tf.reduce_sum(loc_loss) / anchor_obj_count\n        return loc_loss\n</pre> from fastestimator.op.tensorop import TensorOp  class RetinaLoss(TensorOp):     def forward(self, data, state):         anchorbox, cls_pred, loc_pred = data         batch_size = anchorbox.shape[0]         focal_loss, l1_loss, total_loss = [], [], []         for idx in range(batch_size):             single_loc_gt, single_cls_gt = anchorbox[idx][:, :-1], tf.cast(anchorbox[idx][:, -1], tf.int32)             single_loc_pred, single_cls_pred = loc_pred[idx], cls_pred[idx]             single_focal_loss, anchor_obj_idx = self.focal_loss(single_cls_gt, single_cls_pred)             single_l1_loss = self.smooth_l1(single_loc_gt, single_loc_pred, anchor_obj_idx)             focal_loss.append(single_focal_loss)             l1_loss.append(single_l1_loss)         focal_loss, l1_loss = tf.reduce_mean(focal_loss), tf.reduce_mean(l1_loss)         total_loss = focal_loss + l1_loss         return total_loss, focal_loss, l1_loss      def focal_loss(self, single_cls_gt, single_cls_pred, alpha=0.25, gamma=2.0):         # single_cls_gt shape: [num_anchor], single_cls_pred shape: [num_anchor, num_class]         num_classes = single_cls_pred.shape[-1]         # gather the objects and background, discard the rest         anchor_obj_idx = tf.where(tf.greater_equal(single_cls_gt, 0))         anchor_obj_bg_idx = tf.where(tf.greater_equal(single_cls_gt, -1))         anchor_obj_count = tf.cast(tf.shape(anchor_obj_idx)[0], tf.float32)         single_cls_gt = tf.one_hot(single_cls_gt, num_classes)         single_cls_gt = tf.gather_nd(single_cls_gt, anchor_obj_bg_idx)         single_cls_pred = tf.gather_nd(single_cls_pred, anchor_obj_bg_idx)         single_cls_gt = tf.reshape(single_cls_gt, (-1, 1))         single_cls_pred = tf.reshape(single_cls_pred, (-1, 1))         # compute the focal weight on each selected anchor box         alpha_factor = tf.ones_like(single_cls_gt) * alpha         alpha_factor = tf.where(tf.equal(single_cls_gt, 1), alpha_factor, 1 - alpha_factor)         focal_weight = tf.where(tf.equal(single_cls_gt, 1), 1 - single_cls_pred, single_cls_pred)         focal_weight = alpha_factor * focal_weight**gamma / anchor_obj_count         cls_loss = tf.losses.BinaryCrossentropy(reduction='sum')(single_cls_gt,                                                                  single_cls_pred,                                                                  sample_weight=focal_weight)         return cls_loss, anchor_obj_idx      def smooth_l1(self, single_loc_gt, single_loc_pred, anchor_obj_idx, beta=0.1):         # single_loc_gt shape: [num_anchor x 4], anchor_obj_idx shape:  [num_anchor x 4]         single_loc_pred = tf.gather_nd(single_loc_pred, anchor_obj_idx)  #anchor_obj_count x 4         single_loc_gt = tf.gather_nd(single_loc_gt, anchor_obj_idx)  #anchor_obj_count x 4         anchor_obj_count = tf.cast(tf.shape(single_loc_pred)[0], tf.float32)         single_loc_gt = tf.reshape(single_loc_gt, (-1, 1))         single_loc_pred = tf.reshape(single_loc_pred, (-1, 1))         loc_diff = tf.abs(single_loc_gt - single_loc_pred)         cond = tf.less(loc_diff, beta)         loc_loss = tf.where(cond, 0.5 * loc_diff**2 / beta, loc_diff - 0.5 * beta)         loc_loss = tf.reduce_sum(loc_loss) / anchor_obj_count         return loc_loss <p>The learning rate has a warm up phase when step number is &lt; 2k. After that, we reduce the learning rate by 10 at 120k and 160k respectively. The original batch size in the paper is 16 for 8 GPUs (2 per GPU). Here we are using 1GPU, with batch size 8. The learning rate should be proportional to the batch size, so in the learning rate function it is returning a learning divided by 2, accounting for our batch size.</p> In\u00a0[99]: Copied! <pre>def lr_fn(step):\n    if step &lt; 2000:\n        lr = (0.01 - 0.0002) / 2000 * step + 0.0002\n    elif step &lt; 120000:\n        lr = 0.01\n    elif step &lt; 160000:\n        lr = 0.001\n    else:\n        lr = 0.0001\n    return lr / 2\n</pre> def lr_fn(step):     if step &lt; 2000:         lr = (0.01 - 0.0002) / 2000 * step + 0.0002     elif step &lt; 120000:         lr = 0.01     elif step &lt; 160000:         lr = 0.001     else:         lr = 0.0001     return lr / 2   In\u00a0[100]: Copied! <pre>model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n                 optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.9))\n</pre> model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),                  optimizer_fn=lambda: tf.optimizers.SGD(momentum=0.9)) <pre>INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n</pre> In\u00a0[101]: Copied! <pre>class PredictBox(TensorOp):\n\"\"\"Convert network output to bounding boxes.\n    \"\"\"\n    def __init__(self,\n                 inputs=None,\n                 outputs=None,\n                 mode=None,\n                 input_shape=(512, 512, 3),\n                 select_top_k=1000,\n                 nms_max_outputs=100,\n                 score_threshold=0.05):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.input_shape = input_shape\n        self.select_top_k = select_top_k\n        self.nms_max_outputs = nms_max_outputs\n        self.score_threshold = score_threshold\n\n        all_anchors, num_anchors_per_level = get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])\n        self.all_anchors = tf.convert_to_tensor(all_anchors)\n        self.num_anchors_per_level = num_anchors_per_level\n\n    def forward(self, data, state):\n        pred = []\n\n        # extract max score and its class label\n        cls_pred, deltas, bbox = data\n        batch_size = bbox.shape[0]\n        labels = tf.cast(tf.argmax(cls_pred, axis=2), dtype=tf.int32)\n        scores = tf.reduce_max(cls_pred, axis=2)\n\n\n        # iterate over images\n        for i in range(batch_size):\n            # split batch into images\n            labels_per_image = labels[i]\n            scores_per_image = scores[i]\n            deltas_per_image = deltas[i]\n\n            selected_deltas_per_image = tf.constant([], shape=(0, 4))\n            selected_labels_per_image = tf.constant([], dtype=tf.int32)\n            selected_scores_per_image = tf.constant([])\n            selected_anchor_indices_per_image = tf.constant([], dtype=tf.int32)\n\n            end_index = 0\n            # iterate over each pyramid level\n            for j in range(self.num_anchors_per_level.shape[0]):\n                start_index = end_index\n                end_index += self.num_anchors_per_level[j]\n                anchor_indices = tf.range(start_index, end_index, dtype=tf.int32)\n\n                level_scores = scores_per_image[start_index:end_index]\n                level_deltas = deltas_per_image[start_index:end_index]\n                level_labels = labels_per_image[start_index:end_index]\n\n                # select top k\n                if self.num_anchors_per_level[j] &gt;= self.select_top_k:\n                    top_k = tf.math.top_k(level_scores, self.select_top_k)\n                    top_k_indices = top_k.indices\n                else:\n                    top_k_indices = tf.subtract(anchor_indices, [start_index])\n\n                # combine all pyramid levels\n                selected_deltas_per_image = tf.concat(\n                    [selected_deltas_per_image, tf.gather(level_deltas, top_k_indices)], axis=0)\n                selected_scores_per_image = tf.concat(\n                    [selected_scores_per_image, tf.gather(level_scores, top_k_indices)], axis=0)\n                selected_labels_per_image = tf.concat(\n                    [selected_labels_per_image, tf.gather(level_labels, top_k_indices)], axis=0)\n                selected_anchor_indices_per_image = tf.concat(\n                    [selected_anchor_indices_per_image, tf.gather(anchor_indices, top_k_indices)], axis=0)\n\n            # delta -&gt; (x1, y1, w, h)\n            selected_anchors_per_image = tf.gather(self.all_anchors, selected_anchor_indices_per_image)\n            x1 = (selected_deltas_per_image[:, 0] * selected_anchors_per_image[:, 2]) + selected_anchors_per_image[:, 0]\n            y1 = (selected_deltas_per_image[:, 1] * selected_anchors_per_image[:, 3]) + selected_anchors_per_image[:, 1]\n            w = tf.math.exp(selected_deltas_per_image[:, 2]) * selected_anchors_per_image[:, 2]\n            h = tf.math.exp(selected_deltas_per_image[:, 3]) * selected_anchors_per_image[:, 3]\n            x2 = x1 + w\n            y2 = y1 + h\n\n            # nms\n            # filter out low score, and perform nms\n            boxes_per_image = tf.stack([y1, x1, y2, x2], axis=1)\n            nms_indices = tf.image.non_max_suppression(boxes_per_image,\n                                                       selected_scores_per_image,\n                                                       self.nms_max_outputs,\n                                                       score_threshold=self.score_threshold)\n\n            nms_boxes = tf.gather(boxes_per_image, nms_indices)\n            final_scores = tf.gather(selected_scores_per_image, nms_indices)\n            final_labels = tf.cast(tf.gather(selected_labels_per_image, nms_indices), dtype=tf.float32)\n\n            # clip bounding boxes to image size\n            x1 = tf.clip_by_value(nms_boxes[:, 1], clip_value_min=0, clip_value_max=self.input_shape[1])\n            y1 = tf.clip_by_value(nms_boxes[:, 0], clip_value_min=0, clip_value_max=self.input_shape[0])\n            w = tf.clip_by_value(nms_boxes[:, 3], clip_value_min=0, clip_value_max=self.input_shape[1]) - x1\n            h = tf.clip_by_value(nms_boxes[:, 2], clip_value_min=0, clip_value_max=self.input_shape[0]) - y1\n\n            image_results = tf.stack([x1, y1, w, h, final_labels, final_scores], axis=1)\n            pred.append(image_results)\n            \n        return pred\n</pre> class PredictBox(TensorOp):     \"\"\"Convert network output to bounding boxes.     \"\"\"     def __init__(self,                  inputs=None,                  outputs=None,                  mode=None,                  input_shape=(512, 512, 3),                  select_top_k=1000,                  nms_max_outputs=100,                  score_threshold=0.05):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.input_shape = input_shape         self.select_top_k = select_top_k         self.nms_max_outputs = nms_max_outputs         self.score_threshold = score_threshold          all_anchors, num_anchors_per_level = get_fpn_anchor_box(width=input_shape[1], height=input_shape[0])         self.all_anchors = tf.convert_to_tensor(all_anchors)         self.num_anchors_per_level = num_anchors_per_level      def forward(self, data, state):         pred = []          # extract max score and its class label         cls_pred, deltas, bbox = data         batch_size = bbox.shape[0]         labels = tf.cast(tf.argmax(cls_pred, axis=2), dtype=tf.int32)         scores = tf.reduce_max(cls_pred, axis=2)           # iterate over images         for i in range(batch_size):             # split batch into images             labels_per_image = labels[i]             scores_per_image = scores[i]             deltas_per_image = deltas[i]              selected_deltas_per_image = tf.constant([], shape=(0, 4))             selected_labels_per_image = tf.constant([], dtype=tf.int32)             selected_scores_per_image = tf.constant([])             selected_anchor_indices_per_image = tf.constant([], dtype=tf.int32)              end_index = 0             # iterate over each pyramid level             for j in range(self.num_anchors_per_level.shape[0]):                 start_index = end_index                 end_index += self.num_anchors_per_level[j]                 anchor_indices = tf.range(start_index, end_index, dtype=tf.int32)                  level_scores = scores_per_image[start_index:end_index]                 level_deltas = deltas_per_image[start_index:end_index]                 level_labels = labels_per_image[start_index:end_index]                  # select top k                 if self.num_anchors_per_level[j] &gt;= self.select_top_k:                     top_k = tf.math.top_k(level_scores, self.select_top_k)                     top_k_indices = top_k.indices                 else:                     top_k_indices = tf.subtract(anchor_indices, [start_index])                  # combine all pyramid levels                 selected_deltas_per_image = tf.concat(                     [selected_deltas_per_image, tf.gather(level_deltas, top_k_indices)], axis=0)                 selected_scores_per_image = tf.concat(                     [selected_scores_per_image, tf.gather(level_scores, top_k_indices)], axis=0)                 selected_labels_per_image = tf.concat(                     [selected_labels_per_image, tf.gather(level_labels, top_k_indices)], axis=0)                 selected_anchor_indices_per_image = tf.concat(                     [selected_anchor_indices_per_image, tf.gather(anchor_indices, top_k_indices)], axis=0)              # delta -&gt; (x1, y1, w, h)             selected_anchors_per_image = tf.gather(self.all_anchors, selected_anchor_indices_per_image)             x1 = (selected_deltas_per_image[:, 0] * selected_anchors_per_image[:, 2]) + selected_anchors_per_image[:, 0]             y1 = (selected_deltas_per_image[:, 1] * selected_anchors_per_image[:, 3]) + selected_anchors_per_image[:, 1]             w = tf.math.exp(selected_deltas_per_image[:, 2]) * selected_anchors_per_image[:, 2]             h = tf.math.exp(selected_deltas_per_image[:, 3]) * selected_anchors_per_image[:, 3]             x2 = x1 + w             y2 = y1 + h              # nms             # filter out low score, and perform nms             boxes_per_image = tf.stack([y1, x1, y2, x2], axis=1)             nms_indices = tf.image.non_max_suppression(boxes_per_image,                                                        selected_scores_per_image,                                                        self.nms_max_outputs,                                                        score_threshold=self.score_threshold)              nms_boxes = tf.gather(boxes_per_image, nms_indices)             final_scores = tf.gather(selected_scores_per_image, nms_indices)             final_labels = tf.cast(tf.gather(selected_labels_per_image, nms_indices), dtype=tf.float32)              # clip bounding boxes to image size             x1 = tf.clip_by_value(nms_boxes[:, 1], clip_value_min=0, clip_value_max=self.input_shape[1])             y1 = tf.clip_by_value(nms_boxes[:, 0], clip_value_min=0, clip_value_max=self.input_shape[0])             w = tf.clip_by_value(nms_boxes[:, 3], clip_value_min=0, clip_value_max=self.input_shape[1]) - x1             h = tf.clip_by_value(nms_boxes[:, 2], clip_value_min=0, clip_value_max=self.input_shape[0]) - y1              image_results = tf.stack([x1, y1, w, h, final_labels, final_scores], axis=1)             pred.append(image_results)                      return pred In\u00a0[102]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n    RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),\n    UpdateOp(model=model, loss_name=\"total_loss\"),\n    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"eval\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),     RetinaLoss(inputs=[\"anchorbox\", \"cls_pred\", \"loc_pred\"], outputs=[\"total_loss\", \"focal_loss\", \"l1_loss\"]),     UpdateOp(model=model, loss_name=\"total_loss\"),     PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"eval\") ]) In\u00a0[104]: Copied! <pre>from fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import MeanAveragePrecision\n\nestimator = fe.Estimator(\n    pipeline=pipeline,\n    network=network,\n    epochs=epochs,\n    max_train_steps_per_epoch=max_train_steps_per_epoch,\n    max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n    traces=[\n        LRScheduler(model=model, lr_fn=lr_fn),\n        BestModelSaver(model=model, save_dir=save_dir, metric='mAP', save_best_mode=\"max\"),\n        MeanAveragePrecision(num_classes=90)\n    ],\n    monitor_names=[\"l1_loss\", \"focal_loss\"])\n</pre> from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import MeanAveragePrecision  estimator = fe.Estimator(     pipeline=pipeline,     network=network,     epochs=epochs,     max_train_steps_per_epoch=max_train_steps_per_epoch,     max_eval_steps_per_epoch=max_eval_steps_per_epoch,     traces=[         LRScheduler(model=model, lr_fn=lr_fn),         BestModelSaver(model=model, save_dir=save_dir, metric='mAP', save_best_mode=\"max\"),         MeanAveragePrecision(num_classes=90)     ],     monitor_names=[\"l1_loss\", \"focal_loss\"]) In\u00a0[\u00a0]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() In\u00a0[198]: Copied! <pre>import os\n\nweights_path = os.path.join(save_dir, \"model1_best_mAP.h5\")\n</pre> import os  weights_path = os.path.join(save_dir, \"model1_best_mAP.h5\") In\u00a0[199]: Copied! <pre>model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),\n                 optimizer_fn=None,\n                 model_name=\"retinanet\",\n                 weights_path=weights_path)\n</pre> model = fe.build(model_fn=lambda: RetinaNet(input_shape=(512, 512, 3), num_classes=90),                  optimizer_fn=None,                  model_name=\"retinanet\",                  weights_path=weights_path) In\u00a0[200]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),\n    PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"infer\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(model=model, inputs=\"image\", outputs=[\"cls_pred\", \"loc_pred\"]),     PredictBox(inputs=[\"cls_pred\", \"loc_pred\", \"bbox\"], outputs=\"pred\", mode=\"infer\") ]) <p>We randomly select 4 images and visualize the model predictions.</p> In\u00a0[255]: Copied! <pre>num_images = 4\n</pre> num_images = 4 In\u00a0[256]: Copied! <pre>np.random.seed(379)\nselected = np.random.randint(len(eval_ds), size=num_images).tolist()\ndata = [pipeline.transform(eval_ds[i], mode=\"infer\") for i in selected]\nim = np.array([item['image'][0] for item in data])\npad = max(item['bbox'][0].shape[0] for item in data)\nbo = np.array([\n    np.pad(item['bbox'][0], ((0, pad - item['bbox'][0].shape[0]), (0, 0)), mode='constant', constant_values=0)\n    for item in data\n])\n</pre> np.random.seed(379) selected = np.random.randint(len(eval_ds), size=num_images).tolist() data = [pipeline.transform(eval_ds[i], mode=\"infer\") for i in selected] im = np.array([item['image'][0] for item in data]) pad = max(item['bbox'][0].shape[0] for item in data) bo = np.array([     np.pad(item['bbox'][0], ((0, pad - item['bbox'][0].shape[0]), (0, 0)), mode='constant', constant_values=0)     for item in data ]) In\u00a0[257]: Copied! <pre>network_out = network.transform({'image': im, 'bbox': bo}, mode=\"infer\")\n</pre> network_out = network.transform({'image': im, 'bbox': bo}, mode=\"infer\") In\u00a0[260]: Copied! <pre>fig, ax = plt.subplots(num_images, 2, figsize=(20, 30))\nfor batch_index in range(num_images):\n    img = network_out['image'].numpy()[batch_index, ...]\n    img = ((img + 1) / 2 * 255).astype(np.uint8)\n\n    img2 = img.copy()\n\n    keep = network_out['bbox'][batch_index].numpy()[..., -1] &gt; 0\n    gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T\n    gt_x2 = gt_x1 + gt_w\n    gt_y2 = gt_y1 + gt_h\n\n    scores = network_out['pred'][batch_index].numpy()[..., -1]\n    labels = network_out['pred'][batch_index].numpy()[..., -2]\n    keep = scores &gt; 0.5\n    x1, y1, w, h, label, _ = network_out['pred'][batch_index].numpy()[keep].T\n    x2 = x1 + w\n    y2 = y1 + h\n\n    for i in range(len(gt_x1)):\n        cv2.rectangle(img, (gt_x1[i], gt_y1[i]), (gt_x2[i], gt_y2[i]), (255, 0, 0), 2)\n        ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')\n        ax[batch_index, 0].text(gt_x1[i] + 3,\n                   gt_y1[i] + 12,\n                   class_map[str(int(gt_label[i]))],\n                   color=(1, 0, 0),\n                   fontsize=14,\n                   fontweight='bold')\n\n    for j in range(len(x1)):\n        cv2.rectangle(img2, (x1[j], y1[j]), (x2[j], y2[j]), (100, 240, 240), 2)\n        ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')\n        ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')\n\n    ax[batch_index, 0].imshow(img)\n    ax[batch_index, 1].imshow(img2)\n    print(scores)\n    print(list(map(class_map.get, labels.astype(int).astype(str))))\n\nplt.tight_layout()\n</pre> fig, ax = plt.subplots(num_images, 2, figsize=(20, 30)) for batch_index in range(num_images):     img = network_out['image'].numpy()[batch_index, ...]     img = ((img + 1) / 2 * 255).astype(np.uint8)      img2 = img.copy()      keep = network_out['bbox'][batch_index].numpy()[..., -1] &gt; 0     gt_x1, gt_y1, gt_w, gt_h, gt_label = network_out['bbox'][batch_index].numpy()[keep].T     gt_x2 = gt_x1 + gt_w     gt_y2 = gt_y1 + gt_h      scores = network_out['pred'][batch_index].numpy()[..., -1]     labels = network_out['pred'][batch_index].numpy()[..., -2]     keep = scores &gt; 0.5     x1, y1, w, h, label, _ = network_out['pred'][batch_index].numpy()[keep].T     x2 = x1 + w     y2 = y1 + h      for i in range(len(gt_x1)):         cv2.rectangle(img, (gt_x1[i], gt_y1[i]), (gt_x2[i], gt_y2[i]), (255, 0, 0), 2)         ax[batch_index, 0].set_xlabel('GT', fontsize=14, fontweight='bold')         ax[batch_index, 0].text(gt_x1[i] + 3,                    gt_y1[i] + 12,                    class_map[str(int(gt_label[i]))],                    color=(1, 0, 0),                    fontsize=14,                    fontweight='bold')      for j in range(len(x1)):         cv2.rectangle(img2, (x1[j], y1[j]), (x2[j], y2[j]), (100, 240, 240), 2)         ax[batch_index, 1].set_xlabel('Prediction', fontsize=14, fontweight='bold')         ax[batch_index, 1].text(x1[j] + 3, y1[j] + 12, class_map[str(int(label[j]))], color=(0.5, 0.9, 0.9), fontsize=14, fontweight='bold')      ax[batch_index, 0].imshow(img)     ax[batch_index, 1].imshow(img2)     print(scores)     print(list(map(class_map.get, labels.astype(int).astype(str))))  plt.tight_layout() <pre>[0.9880271  0.6860561  0.34332335 0.3304865 ]\n['airplane', 'truck', 'person', 'person']\n[0.99191636 0.0548539  0.05163975 0.99191636]\n['bear', 'bear', 'bear', 'bear']\n[0.870187   0.50000274 0.12686375 0.09993405]\n['vase', 'cup', 'bowl', 'vase']\n[0.83461    0.78291625 0.7454702  0.46012864]\n['laptop', 'cat', 'chair', 'cell phone']\n</pre>"}, {"location": "apphub/instance_detection/retinanet/retinanet.html#instance-detection-with-retinanet", "title": "Instance Detection with RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-1-data-and-pipeline-preparation", "title": "Step 1 - Data and <code>Pipeline</code> preparation\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#load-data", "title": "Load data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#generate-anchors", "title": "Generate Anchors\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#pipeline", "title": "<code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualization-of-batch-data", "title": "Visualization of batch data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-2-network-construction", "title": "Step 2 - <code>Network</code> construction\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#class-and-box-subnets", "title": "Class and box subnets\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#retinanet", "title": "RetinaNet\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#loss-functions", "title": "Loss functions\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#learning-rate", "title": "Learning rate\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#predict-box", "title": "Predict Box\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#step-3-estimator-definition-and-training", "title": "Step 3 - <code>Estimator</code> definition and training\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#inference", "title": "Inference\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#load-trained-model-and-reconstruct-network", "title": "Load trained model and reconstruct <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#feed-data", "title": "Feed data\u00b6", "text": ""}, {"location": "apphub/instance_detection/retinanet/retinanet.html#visualize-predictions", "title": "Visualize predictions\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html", "title": "Multi-Task Learning using Uncertainty Weighted Loss", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\nfrom torch.nn.init import kaiming_normal_ as he_normal\nfrom torchvision import models\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape\nfrom fastestimator.op.tensorop import TensorOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy, Dice\n</pre> import os import tempfile  import cv2 import torch import torch.nn as nn import torch.nn.functional as fn from torch.nn.init import kaiming_normal_ as he_normal from torchvision import models  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, LongestMaxSize, PadIfNeeded, ReadMat, ShiftScaleRotate from fastestimator.op.numpyop.univariate import ChannelTranspose, Normalize, ReadImage, Reshape from fastestimator.op.tensorop import TensorOp from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy, Dice In\u00a0[2]: parameters Copied! <pre>#parameters\nepochs = 25\nbatch_size = 8\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> #parameters epochs = 25 batch_size = 8 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import cub200\n\ntrain_data = cub200.load_data(root_dir=data_dir)\neval_data = train_data.split(0.3)\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import cub200  train_data = cub200.load_data(root_dir=data_dir) eval_data = train_data.split(0.3) test_data = eval_data.split(0.5)  In\u00a0[4]: Copied! <pre>pipeline = fe.Pipeline(batch_size=batch_size,\n                       train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       ops=[\n                           ReadImage(inputs=\"image\",\n                                     outputs=\"image\",\n                                     parent_path=train_data.parent_path),\n                           Normalize(inputs=\"image\",\n                                     outputs=\"image\",\n                                     mean=1.0,\n                                     std=1.0,\n                                     max_pixel_value=127.5),\n                           ReadMat(file='annotation',\n                                   keys=\"seg\",\n                                   parent_path=train_data.parent_path),\n                           Delete(keys=\"annotation\"),\n                           LongestMaxSize(max_size=512,\n                                          image_in=\"image\",\n                                          image_out=\"image\",\n                                          mask_in=\"seg\",\n                                          mask_out=\"seg\"),\n                           PadIfNeeded(min_height=512,\n                                       min_width=512,\n                                       image_in=\"image\",\n                                       image_out=\"image\",\n                                       mask_in=\"seg\",\n                                       mask_out=\"seg\",\n                                       border_mode=cv2.BORDER_CONSTANT,\n                                       value=0,\n                                       mask_value=0),\n                           ShiftScaleRotate(\n                               image_in=\"image\",\n                               mask_in=\"seg\",\n                               image_out=\"image\",\n                               mask_out=\"seg\",\n                               mode=\"train\",\n                               shift_limit=0.2,\n                               rotate_limit=15.0,\n                               scale_limit=0.2,\n                               border_mode=cv2.BORDER_CONSTANT,\n                               value=0,\n                               mask_value=0),\n                           Sometimes(\n                               HorizontalFlip(image_in=\"image\",\n                                              mask_in=\"seg\",\n                                              image_out=\"image\",\n                                              mask_out=\"seg\",\n                                              mode=\"train\")),\n                           ChannelTranspose(inputs=\"image\",\n                                            outputs=\"image\"),\n                           Reshape(shape=(1, 512, 512),\n                                   inputs=\"seg\",\n                                   outputs=\"seg\")\n                       ])\n</pre> pipeline = fe.Pipeline(batch_size=batch_size,                        train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        ops=[                            ReadImage(inputs=\"image\",                                      outputs=\"image\",                                      parent_path=train_data.parent_path),                            Normalize(inputs=\"image\",                                      outputs=\"image\",                                      mean=1.0,                                      std=1.0,                                      max_pixel_value=127.5),                            ReadMat(file='annotation',                                    keys=\"seg\",                                    parent_path=train_data.parent_path),                            Delete(keys=\"annotation\"),                            LongestMaxSize(max_size=512,                                           image_in=\"image\",                                           image_out=\"image\",                                           mask_in=\"seg\",                                           mask_out=\"seg\"),                            PadIfNeeded(min_height=512,                                        min_width=512,                                        image_in=\"image\",                                        image_out=\"image\",                                        mask_in=\"seg\",                                        mask_out=\"seg\",                                        border_mode=cv2.BORDER_CONSTANT,                                        value=0,                                        mask_value=0),                            ShiftScaleRotate(                                image_in=\"image\",                                mask_in=\"seg\",                                image_out=\"image\",                                mask_out=\"seg\",                                mode=\"train\",                                shift_limit=0.2,                                rotate_limit=15.0,                                scale_limit=0.2,                                border_mode=cv2.BORDER_CONSTANT,                                value=0,                                mask_value=0),                            Sometimes(                                HorizontalFlip(image_in=\"image\",                                               mask_in=\"seg\",                                               image_out=\"image\",                                               mask_out=\"seg\",                                               mode=\"train\")),                            ChannelTranspose(inputs=\"image\",                                             outputs=\"image\"),                            Reshape(shape=(1, 512, 512),                                    inputs=\"seg\",                                    outputs=\"seg\")                        ]) In\u00a0[5]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef Minmax(data):\n    data_max = np.max(data)\n    data_min = np.min(data)\n    data = (data - data_min) / max((data_max - data_min), 1e-7)\n    return data\n\n\ndef visualize_image_mask(img, mask):\n    img = (img*255).astype(np.uint8)\n    \n    mask = mask.astype(np.uint8)\n    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n    \n    ret, mask_thres = cv2.threshold(mask, 0.5,1, cv2.THRESH_BINARY)\n    mask_overlay = mask * mask_thres\n    mask_overlay = np.where( mask_overlay != [0,0,0], [255,0,0] ,[0,0,0])\n    mask_overlay = mask_overlay.astype(np.uint8)\n    img_with_mask = cv2.addWeighted(img, 0.7, mask_overlay, 0.3,0 )\n\n    maskgt_with_maskpred = cv2.addWeighted(mask, 0.7, mask_overlay, 0.3, 0)\n\n    fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(18,8))\n    ax[0].imshow(img)\n    ax[0].set_title('original image')\n    ax[1].imshow(img_with_mask)\n    ax[1].set_title('img - mask overlay')\n    plt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def Minmax(data):     data_max = np.max(data)     data_min = np.min(data)     data = (data - data_min) / max((data_max - data_min), 1e-7)     return data   def visualize_image_mask(img, mask):     img = (img*255).astype(np.uint8)          mask = mask.astype(np.uint8)     mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)          ret, mask_thres = cv2.threshold(mask, 0.5,1, cv2.THRESH_BINARY)     mask_overlay = mask * mask_thres     mask_overlay = np.where( mask_overlay != [0,0,0], [255,0,0] ,[0,0,0])     mask_overlay = mask_overlay.astype(np.uint8)     img_with_mask = cv2.addWeighted(img, 0.7, mask_overlay, 0.3,0 )      maskgt_with_maskpred = cv2.addWeighted(mask, 0.7, mask_overlay, 0.3, 0)      fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(18,8))     ax[0].imshow(img)     ax[0].set_title('original image')     ax[1].imshow(img_with_mask)     ax[1].set_title('img - mask overlay')     plt.show() In\u00a0[6]: Copied! <pre>idx = np.random.randint(low=0, high=batch_size)\nresult = pipeline.get_results()\nimg = Minmax(result[\"image\"][idx].numpy())\nmsk = np.squeeze(result[\"seg\"][idx].numpy())\nimg = np.transpose(img, (1, 2, 0))\n\nvisualize_image_mask(img, msk)\n</pre> idx = np.random.randint(low=0, high=batch_size) result = pipeline.get_results() img = Minmax(result[\"image\"][idx].numpy()) msk = np.squeeze(result[\"seg\"][idx].numpy()) img = np.transpose(img, (1, 2, 0))  visualize_image_mask(img, msk) In\u00a0[7]: Copied! <pre>class Upsample2D(nn.Module):\n\"\"\"Upsampling Block\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.upsample = nn.Sequential(\n            nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.upsample:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass DecBlock(nn.Module):\n\"\"\"Decoder Block\"\"\"\n    def __init__(self, upsample_in_ch, conv_in_ch, out_ch):\n        super().__init__()\n        self.upsample = Upsample2D(upsample_in_ch, out_ch)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True))\n\n        for l in self.conv_layers:\n            if isinstance(l, nn.Conv2d):\n                he_normal(l.weight.data)\n\n    def forward(self, x_up, x_down):\n        x = self.upsample(x_up)\n        x = torch.cat([x, x_down], 1)\n        x = self.conv_layers(x)\n        return x\n\n\nclass ResUnet50(nn.Module):\n\"\"\"Network Architecture\"\"\"\n    def __init__(self, num_classes=200):\n        super().__init__()\n        base_model = models.resnet50(pretrained=True)\n\n        self.enc1 = nn.Sequential(*list(base_model.children())[:3])\n        self.input_pool = list(base_model.children())[3]\n        self.enc2 = nn.Sequential(*list(base_model.children())[4])\n        self.enc3 = nn.Sequential(*list(base_model.children())[5])\n        self.enc4 = nn.Sequential(*list(base_model.children())[6])\n        self.enc5 = nn.Sequential(*list(base_model.children())[7])\n        self.fc = nn.Linear(2048, num_classes)\n\n        self.dec6 = DecBlock(2048, 1536, 512)\n        self.dec7 = DecBlock(512, 768, 256)\n        self.dec8 = DecBlock(256, 384, 128)\n        self.dec9 = DecBlock(128, 128, 64)\n        self.dec10 = Upsample2D(64, 2)\n        self.mask = nn.Conv2d(2, 1, kernel_size=1)\n\n    def forward(self, x):\n        x_e1 = self.enc1(x)\n        x_e1_1 = self.input_pool(x_e1)\n        x_e2 = self.enc2(x_e1_1)\n        x_e3 = self.enc3(x_e2)\n        x_e4 = self.enc4(x_e3)\n        x_e5 = self.enc5(x_e4)\n\n        x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])\n        x_label = x_label.view(x_label.shape[0], -1)\n        x_label = self.fc(x_label)\n        x_label = torch.softmax(x_label, dim=-1)\n\n        x_d6 = self.dec6(x_e5, x_e4)\n        x_d7 = self.dec7(x_d6, x_e3)\n        x_d8 = self.dec8(x_d7, x_e2)\n        x_d9 = self.dec9(x_d8, x_e1)\n        x_d10 = self.dec10(x_d9)\n        x_mask = self.mask(x_d10)\n        x_mask = torch.sigmoid(x_mask)\n        return x_label, x_mask\n</pre> class Upsample2D(nn.Module):     \"\"\"Upsampling Block\"\"\"     def __init__(self, in_channels, out_channels):         super().__init__()         self.upsample = nn.Sequential(             nn.Upsample(mode=\"bilinear\", scale_factor=2, align_corners=True),             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.upsample:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x):         return self.upsample(x)   class DecBlock(nn.Module):     \"\"\"Decoder Block\"\"\"     def __init__(self, upsample_in_ch, conv_in_ch, out_ch):         super().__init__()         self.upsample = Upsample2D(upsample_in_ch, out_ch)         self.conv_layers = nn.Sequential(             nn.Conv2d(conv_in_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True),             nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),             nn.ReLU(inplace=True))          for l in self.conv_layers:             if isinstance(l, nn.Conv2d):                 he_normal(l.weight.data)      def forward(self, x_up, x_down):         x = self.upsample(x_up)         x = torch.cat([x, x_down], 1)         x = self.conv_layers(x)         return x   class ResUnet50(nn.Module):     \"\"\"Network Architecture\"\"\"     def __init__(self, num_classes=200):         super().__init__()         base_model = models.resnet50(pretrained=True)          self.enc1 = nn.Sequential(*list(base_model.children())[:3])         self.input_pool = list(base_model.children())[3]         self.enc2 = nn.Sequential(*list(base_model.children())[4])         self.enc3 = nn.Sequential(*list(base_model.children())[5])         self.enc4 = nn.Sequential(*list(base_model.children())[6])         self.enc5 = nn.Sequential(*list(base_model.children())[7])         self.fc = nn.Linear(2048, num_classes)          self.dec6 = DecBlock(2048, 1536, 512)         self.dec7 = DecBlock(512, 768, 256)         self.dec8 = DecBlock(256, 384, 128)         self.dec9 = DecBlock(128, 128, 64)         self.dec10 = Upsample2D(64, 2)         self.mask = nn.Conv2d(2, 1, kernel_size=1)      def forward(self, x):         x_e1 = self.enc1(x)         x_e1_1 = self.input_pool(x_e1)         x_e2 = self.enc2(x_e1_1)         x_e3 = self.enc3(x_e2)         x_e4 = self.enc4(x_e3)         x_e5 = self.enc5(x_e4)          x_label = fn.max_pool2d(x_e5, kernel_size=x_e5.size()[2:])         x_label = x_label.view(x_label.shape[0], -1)         x_label = self.fc(x_label)         x_label = torch.softmax(x_label, dim=-1)          x_d6 = self.dec6(x_e5, x_e4)         x_d7 = self.dec7(x_d6, x_e3)         x_d8 = self.dec8(x_d7, x_e2)         x_d9 = self.dec9(x_d8, x_e1)         x_d10 = self.dec10(x_d9)         x_mask = self.mask(x_d10)         x_mask = torch.sigmoid(x_mask)         return x_label, x_mask <p>Other than the ResUnet50, we will have another network to contain the trainable weighted parameter in the weighted loss. We call it our uncertainty model. In the network <code>ops</code>, ResUnet50 produces both a predicted label and predicted mask. These two predictions are then fed to classification loss and segmentation loss operators respectively. Finally, both losses are passed to the uncertainty model to create a final loss.</p> In\u00a0[8]: Copied! <pre>class UncertaintyLossNet(nn.Module):\n\"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.w1 = nn.Parameter(torch.zeros(1))\n        self.w2 = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(\n            -self.w2) * x[1] + self.w2\n        return loss\n</pre> class UncertaintyLossNet(nn.Module):     \"\"\"Creates Uncertainty weighted loss model https://arxiv.org/abs/1705.07115     \"\"\"     def __init__(self):         super().__init__()         self.w1 = nn.Parameter(torch.zeros(1))         self.w2 = nn.Parameter(torch.zeros(1))      def forward(self, x):         loss = torch.exp(-self.w1) * x[0] + self.w1 + torch.exp(             -self.w2) * x[1] + self.w2         return loss <p>We also implement a <code>TensorOp</code> to average the output of <code>UncertaintyLossNet</code> for each batch:</p> In\u00a0[9]: Copied! <pre>class ReduceLoss(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> class ReduceLoss(TensorOp):     def forward(self, data, state):         return reduce_mean(data) In\u00a0[10]: Copied! <pre>resunet50 = fe.build(model_fn=ResUnet50,\n                     model_name=\"resunet50\",\n                     optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4))\nuncertainty = fe.build(model_fn=UncertaintyLossNet,\n                       model_name=\"uncertainty\",\n                       optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs='image',\n            model=resunet50,\n            outputs=[\"label_pred\", \"mask_pred\"]),\n    CrossEntropy(inputs=[\"label_pred\", \"label\"],\n                 outputs=\"cls_loss\",\n                 form=\"sparse\",\n                 average_loss=False),\n    CrossEntropy(inputs=[\"mask_pred\", \"seg\"],\n                 outputs=\"seg_loss\",\n                 form=\"binary\",\n                 average_loss=False),\n    ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],\n            model=uncertainty,\n            outputs=\"total_loss\"),\n    ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),\n    UpdateOp(model=resunet50, loss_name=\"total_loss\"),\n    UpdateOp(model=uncertainty, loss_name=\"total_loss\")\n])\n</pre> resunet50 = fe.build(model_fn=ResUnet50,                      model_name=\"resunet50\",                      optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-4)) uncertainty = fe.build(model_fn=UncertaintyLossNet,                        model_name=\"uncertainty\",                        optimizer_fn=lambda x: torch.optim.Adam(x, lr=1e-5))  network = fe.Network(ops=[     ModelOp(inputs='image',             model=resunet50,             outputs=[\"label_pred\", \"mask_pred\"]),     CrossEntropy(inputs=[\"label_pred\", \"label\"],                  outputs=\"cls_loss\",                  form=\"sparse\",                  average_loss=False),     CrossEntropy(inputs=[\"mask_pred\", \"seg\"],                  outputs=\"seg_loss\",                  form=\"binary\",                  average_loss=False),     ModelOp(inputs=[\"cls_loss\", \"seg_loss\"],             model=uncertainty,             outputs=\"total_loss\"),     ReduceLoss(inputs=\"total_loss\", outputs=\"total_loss\"),     UpdateOp(model=resunet50, loss_name=\"total_loss\"),     UpdateOp(model=uncertainty, loss_name=\"total_loss\") ]) In\u00a0[11]: Copied! <pre>traces = [\n    Accuracy(true_key=\"label\", pred_key=\"label_pred\"),\n    Dice(true_key=\"seg\", pred_key='mask_pred'),\n    BestModelSaver(model=resunet50,\n                   save_dir=save_dir,\n                   metric=\"total_loss\",\n                   save_best_mode=\"min\"),\n    LRScheduler(model=resunet50,\n                lr_fn=lambda step: cosine_decay(\n                    step, cycle_length=13200, init_lr=1e-4))\n]\nestimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=traces,\n                         epochs=epochs,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch,\n                         log_steps=500)\n</pre> traces = [     Accuracy(true_key=\"label\", pred_key=\"label_pred\"),     Dice(true_key=\"seg\", pred_key='mask_pred'),     BestModelSaver(model=resunet50,                    save_dir=save_dir,                    metric=\"total_loss\",                    save_best_mode=\"min\"),     LRScheduler(model=resunet50,                 lr_fn=lambda step: cosine_decay(                     step, cycle_length=13200, init_lr=1e-4)) ] estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=traces,                          epochs=epochs,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch,                          log_steps=500) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; resunet50_lr: 0.0001; uncertainty_lr: 1e-05; \nFastEstimator-Train: step: 1; total_loss: 8.121616; resunet50_lr: 1e-04; \nFastEstimator-Train: step: 500; total_loss: 4.7089643; steps/sec: 3.17; resunet50_lr: 9.9651326e-05; \nFastEstimator-Train: step: 528; epoch: 1; epoch_time: 167.79 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 528; epoch: 1; total_loss: 3.9346602; min_total_loss: 3.9346602; since_best: 0; accuracy: 0.16022099447513813; Dice: 0.7908390168388019; \nFastEstimator-Train: step: 1000; total_loss: 2.6967134; steps/sec: 3.13; resunet50_lr: 9.860745e-05; \nFastEstimator-Train: step: 1056; epoch: 2; epoch_time: 168.37 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1056; epoch: 2; total_loss: 2.358711; min_total_loss: 2.358711; since_best: 0; accuracy: 0.430939226519337; Dice: 0.8358255033320947; \nFastEstimator-Train: step: 1500; total_loss: 2.5349426; steps/sec: 3.14; resunet50_lr: 9.688313e-05; \nFastEstimator-Train: step: 1584; epoch: 3; epoch_time: 168.17 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 1584; epoch: 3; total_loss: 1.8696523; min_total_loss: 1.8696523; since_best: 0; accuracy: 0.5138121546961326; Dice: 0.823370761791696; \nFastEstimator-Train: step: 2000; total_loss: 1.288121; steps/sec: 3.14; resunet50_lr: 9.450275e-05; \nFastEstimator-Train: step: 2112; epoch: 4; epoch_time: 168.15 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 2112; epoch: 4; total_loss: 1.5971442; min_total_loss: 1.5971442; since_best: 0; accuracy: 0.6077348066298343; Dice: 0.8283221605740853; \nFastEstimator-Train: step: 2500; total_loss: 1.1395; steps/sec: 3.14; resunet50_lr: 9.149999e-05; \nFastEstimator-Train: step: 2640; epoch: 5; epoch_time: 168.23 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 2640; epoch: 5; total_loss: 1.2618546; min_total_loss: 1.2618546; since_best: 0; accuracy: 0.6696132596685083; Dice: 0.8558863977253409; \nFastEstimator-Train: step: 3000; total_loss: 0.4348533; steps/sec: 3.14; resunet50_lr: 8.791732e-05; \nFastEstimator-Train: step: 3168; epoch: 6; epoch_time: 168.23 sec; \nFastEstimator-Eval: step: 3168; epoch: 6; total_loss: 1.269778; min_total_loss: 1.2618546; since_best: 1; accuracy: 0.6828729281767956; Dice: 0.8506440013132466; \nFastEstimator-Train: step: 3500; total_loss: 1.0252838; steps/sec: 3.14; resunet50_lr: 8.3805404e-05; \nFastEstimator-Train: step: 3696; epoch: 7; epoch_time: 168.16 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 3696; epoch: 7; total_loss: 1.1601604; min_total_loss: 1.1601604; since_best: 0; accuracy: 0.7071823204419889; Dice: 0.8529923493590581; \nFastEstimator-Train: step: 4000; total_loss: 0.02089151; steps/sec: 3.14; resunet50_lr: 7.922241e-05; \nFastEstimator-Train: step: 4224; epoch: 8; epoch_time: 168.09 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 4224; epoch: 8; total_loss: 1.0023108; min_total_loss: 1.0023108; since_best: 0; accuracy: 0.7370165745856354; Dice: 0.8545488100818929; \nFastEstimator-Train: step: 4500; total_loss: 0.8392416; steps/sec: 3.14; resunet50_lr: 7.423316e-05; \nFastEstimator-Train: step: 4752; epoch: 9; epoch_time: 167.95 sec; \nFastEstimator-Eval: step: 4752; epoch: 9; total_loss: 1.1233779; min_total_loss: 1.0023108; since_best: 1; accuracy: 0.7303867403314918; Dice: 0.8615670240361439; \nFastEstimator-Train: step: 5000; total_loss: 0.16346264; steps/sec: 3.14; resunet50_lr: 6.890823e-05; \nFastEstimator-Train: step: 5280; epoch: 10; epoch_time: 168.11 sec; \nFastEstimator-Eval: step: 5280; epoch: 10; total_loss: 1.087766; min_total_loss: 1.0023108; since_best: 2; accuracy: 0.712707182320442; Dice: 0.8528682840144792; \nFastEstimator-Train: step: 5500; total_loss: 0.06831953; steps/sec: 3.14; resunet50_lr: 6.332292e-05; \nFastEstimator-Train: step: 5808; epoch: 11; epoch_time: 168.11 sec; \nFastEstimator-Eval: step: 5808; epoch: 11; total_loss: 1.0286766; min_total_loss: 1.0023108; since_best: 3; accuracy: 0.7535911602209945; Dice: 0.8548840333571784; \nFastEstimator-Train: step: 6000; total_loss: 0.7263755; steps/sec: 3.14; resunet50_lr: 5.7556244e-05; \nFastEstimator-Train: step: 6336; epoch: 12; epoch_time: 168.1 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 6336; epoch: 12; total_loss: 0.9133773; min_total_loss: 0.9133773; since_best: 0; accuracy: 0.7701657458563536; Dice: 0.8557864413210411; \nFastEstimator-Train: step: 6500; total_loss: 0.054296676; steps/sec: 3.14; resunet50_lr: 5.1689763e-05; \nFastEstimator-Train: step: 6864; epoch: 13; epoch_time: 168.02 sec; \nFastEstimator-Eval: step: 6864; epoch: 13; total_loss: 0.9712434; min_total_loss: 0.9133773; since_best: 1; accuracy: 0.7558011049723757; Dice: 0.8675903102123168; \nFastEstimator-Train: step: 7000; total_loss: -0.041223913; steps/sec: 3.14; resunet50_lr: 4.5806453e-05; \nFastEstimator-Train: step: 7392; epoch: 14; epoch_time: 168.12 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 7392; epoch: 14; total_loss: 0.9070398; min_total_loss: 0.9070398; since_best: 0; accuracy: 0.7668508287292818; Dice: 0.8670388874359952; \nFastEstimator-Train: step: 7500; total_loss: -0.053999424; steps/sec: 3.14; resunet50_lr: 3.998953e-05; \nFastEstimator-Train: step: 7920; epoch: 15; epoch_time: 168.13 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 7920; epoch: 15; total_loss: 0.7376718; min_total_loss: 0.7376718; since_best: 0; accuracy: 0.8077348066298342; Dice: 0.8687608480992627; \nFastEstimator-Train: step: 8000; total_loss: -0.053805795; steps/sec: 3.14; resunet50_lr: 3.432127e-05; \nFastEstimator-Train: step: 8448; epoch: 16; epoch_time: 168.08 sec; \nFastEstimator-Eval: step: 8448; epoch: 16; total_loss: 0.7989601; min_total_loss: 0.7376718; since_best: 1; accuracy: 0.7790055248618785; Dice: 0.8679111249415221; \nFastEstimator-Train: step: 8500; total_loss: 0.09030403; steps/sec: 3.14; resunet50_lr: 2.8881845e-05; \nFastEstimator-Train: step: 8976; epoch: 17; epoch_time: 168.27 sec; \nFastEstimator-Eval: step: 8976; epoch: 17; total_loss: 0.78494877; min_total_loss: 0.7376718; since_best: 2; accuracy: 0.7977900552486188; Dice: 0.8651717848164342; \nFastEstimator-Train: step: 9000; total_loss: -0.03528821; steps/sec: 3.14; resunet50_lr: 2.374819e-05; \nFastEstimator-Train: step: 9500; total_loss: -0.08958718; steps/sec: 3.15; resunet50_lr: 1.8992921e-05; \nFastEstimator-Train: step: 9504; epoch: 18; epoch_time: 168.3 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 9504; epoch: 18; total_loss: 0.6974553; min_total_loss: 0.6974553; since_best: 0; accuracy: 0.8121546961325967; Dice: 0.8689569823798876; \nFastEstimator-Train: step: 10000; total_loss: -0.09731047; steps/sec: 3.14; resunet50_lr: 1.4683296e-05; \nFastEstimator-Train: step: 10032; epoch: 19; epoch_time: 168.06 sec; \nFastEstimator-Eval: step: 10032; epoch: 19; total_loss: 0.70396554; min_total_loss: 0.6974553; since_best: 1; accuracy: 0.8187845303867404; Dice: 0.8683503213115263; \nFastEstimator-Train: step: 10500; total_loss: -0.11295703; steps/sec: 3.13; resunet50_lr: 1.088027e-05; \nFastEstimator-Train: step: 10560; epoch: 20; epoch_time: 168.39 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 10560; epoch: 20; total_loss: 0.6501377; min_total_loss: 0.6501377; since_best: 0; accuracy: 0.8209944751381215; Dice: 0.8626990602523178; \nFastEstimator-Train: step: 11000; total_loss: -0.13782492; steps/sec: 3.14; resunet50_lr: 7.637635e-06; \nFastEstimator-Train: step: 11088; epoch: 21; epoch_time: 168.3 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11088; epoch: 21; total_loss: 0.606383; min_total_loss: 0.606383; since_best: 0; accuracy: 0.8375690607734807; Dice: 0.8694501119973564; \nFastEstimator-Train: step: 11500; total_loss: -0.1537084; steps/sec: 3.14; resunet50_lr: 5.001254e-06; \nFastEstimator-Train: step: 11616; epoch: 22; epoch_time: 168.09 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 11616; epoch: 22; total_loss: 0.6024005; min_total_loss: 0.6024005; since_best: 0; accuracy: 0.8320441988950277; Dice: 0.8696893037950605; \nFastEstimator-Train: step: 12000; total_loss: -0.15201315; steps/sec: 3.14; resunet50_lr: 3.0084182e-06; \nFastEstimator-Train: step: 12144; epoch: 23; epoch_time: 168.18 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12144; epoch: 23; total_loss: 0.5858978; min_total_loss: 0.5858978; since_best: 0; accuracy: 0.8320441988950277; Dice: 0.8712486869963797; \nFastEstimator-Train: step: 12500; total_loss: -0.14848635; steps/sec: 3.14; resunet50_lr: 1.6873145e-06; \nFastEstimator-Train: step: 12672; epoch: 24; epoch_time: 168.18 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 12672; epoch: 24; total_loss: 0.581293; min_total_loss: 0.581293; since_best: 0; accuracy: 0.8386740331491712; Dice: 0.8710742498357834; \nFastEstimator-Train: step: 13000; total_loss: -0.13124007; steps/sec: 3.14; resunet50_lr: 1.0566287e-06; \nFastEstimator-Train: step: 13200; epoch: 25; epoch_time: 167.95 sec; \nFastEstimator-ModelSaver: saved model to /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Eval: step: 13200; epoch: 25; total_loss: 0.5735652; min_total_loss: 0.5735652; since_best: 0; accuracy: 0.8353591160220994; Dice: 0.8705187608361179; \nFastEstimator-Finish: step: 13200; total_time: 4575.41 sec; resunet50_lr: 1.0000014e-06; uncertainty_lr: 1e-05; \n</pre> <p>Let's load the model with best loss and check our performance on the test set:</p> In\u00a0[13]: Copied! <pre>fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt'))\nestimator.test()\n</pre> fe.backend.load_model(resunet50, os.path.join(save_dir, 'resunet50_best_total_loss.pt')) estimator.test() <pre>Loaded model weights from /tmp/tmpij3gx2qr/resunet50_best_total_loss.pt\nFastEstimator-Test: epoch: 25; accuracy: 0.8342541436464088; Dice: 0.8676798139291644; \n</pre> <p>We randomly select an image from the test dataset and use <code>pipeline.transform</code> to process the image. We generate the results using <code>network.transform</code> and visualize the prediction.</p> In\u00a0[14]: Copied! <pre>data = test_data[np.random.randint(low=0, high=len(test_data))]\nresult = pipeline.transform(data, mode=\"infer\")\n\nimg = np.squeeze(result[\"image\"])\nimg = np.transpose(img, (1, 2, 0))\nmask_gt = np.squeeze(result[\"seg\"])\n</pre> data = test_data[np.random.randint(low=0, high=len(test_data))] result = pipeline.transform(data, mode=\"infer\")  img = np.squeeze(result[\"image\"]) img = np.transpose(img, (1, 2, 0)) mask_gt = np.squeeze(result[\"seg\"]) In\u00a0[15]: Copied! <pre>visualize_image_mask(Minmax(img), mask_gt)\n</pre> visualize_image_mask(Minmax(img), mask_gt) In\u00a0[16]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"])\n])\n\npredictions = network.transform(result, mode=\"infer\")\npredicted_mask = predictions[\"mask_pred\"].numpy() \npred_mask = np.squeeze(predicted_mask)\npred_mask = np.round(pred_mask).astype(mask_gt.dtype)\n\nvisualize_image_mask(Minmax(img), pred_mask)\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=resunet50, outputs=[\"label_pred\", \"mask_pred\"]) ])  predictions = network.transform(result, mode=\"infer\") predicted_mask = predictions[\"mask_pred\"].numpy()  pred_mask = np.squeeze(predicted_mask) pred_mask = np.round(pred_mask).astype(mask_gt.dtype)  visualize_image_mask(Minmax(img), pred_mask)"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#multi-task-learning-using-uncertainty-weighted-loss", "title": "Multi-Task Learning using Uncertainty Weighted Loss\u00b6", "text": "<p>Multi-task learning is popular in many deep learning applications. For example, in object detection the network performs both classification and localization for each object. As a result, the final loss will be a combination of classification loss and regression loss. The most frequent way of combining two losses is by simply adding them together:</p> <p>$loss_{total} = loss_1 + loss_2$</p> <p>However, a problem emerges when the two losses are on different numerical scales. To resolve this issue, people usually manually design/experimentally determine the best weight, which is very time consuming and computationally expensive:</p> <p>$loss_{total} = w_1loss_1 + w_2loss_2$</p> <p>This paper presents an interesting idea: make the weights w1 and w2 trainable parameters based on the uncertainty of each task, such that the network can dynamically focus more on the task with higher uncertainty.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#dataset", "title": "Dataset\u00b6", "text": "<p>We will use the CUB200 2010 dataset by Caltech. It contains 6033 bird images from 200 categories, where each image also has a corresponding mask. Therefore, our task is to classify and segment the bird given the image.</p> <p>We use a FastEstimator API to load the CUB200 dataset and split the dataset to get train, evaluation and test sets.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": "<p>We read the images with <code>ReadImage</code>, and the masks stored in a MAT file with <code>ReadMat</code>. There is other information stored in the MAT file, so we specify the key <code>seg</code> to retrieve the mask only.</p> <p>Here the main task is to resize the images and masks into 512 by 512 pixels. We use <code>LongestMaxSize</code> (to preserve the aspect ratio) and <code>PadIfNeeded</code> to resize the image. We will augment both image and mask in the same way and rescale the image pixel values between -1 and 1 since we are using pre-trained ImageNet weights.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#lets-visualize-our-pipeline-results", "title": "Let's visualize our <code>Pipeline</code> results\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>In this implementation, the network architecture is not the focus. Therefore, we are going to create something out of the blue :). How about a combination of resnet50 and Unet that can do both classification and segmentation? We can call it - ResUnet50</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": "<p>We will have four different traces to control/monitor the training: <code>Dice</code> and <code>Accuracy</code> will be used to measure segmentation and classification results, <code>BestModelSaver</code> will save the model with best loss, and <code>LRScheduler</code> will apply a cosine learning rate decay throughout the training loop.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#training-and-testing", "title": "Training and Testing\u00b6", "text": "<p>The whole training (25 epochs) will take about 1 hour 20 mins on single V100 GPU. We are going to reach ~0.87 dice and ~83% accuracy by the end of the training.</p>"}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-ground-truth", "title": "Visualize Ground Truth\u00b6", "text": ""}, {"location": "apphub/multi_task_learning/uncertainty_weighted_loss/uncertainty_loss.html#visualize-prediction", "title": "Visualize Prediction\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html", "title": "One-Shot Learning using a Siamese Network in FastEstimator", "text": "<p>This notebook demonstrates how to perform one-shot learning using a Siamese Network in FastEstimator.</p> <p>In one-shot learning we classify based on only a single example of each class. This ability to learn from very little data could be useful in many machine learning problems. The details of the method are presented in Siamese neural networks for one-shot image recognition.</p> <p>We will use the Omniglot dataset for training and evaluation. The Omniglot dataset consists of 50 different alphabets split into background (30 alphabets) and evaluation (20 alphabets) sets. Each alphabet has a number of characters, with 20 images for each character.</p> In\u00a0[1]: Copied! <pre>import tempfile\n\nimport os\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\nimport fastestimator as fe\n\nfrom matplotlib import pyplot as plt\n</pre> import tempfile  import os import cv2 import numpy as np  import tensorflow as tf import fastestimator as fe  from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre># Parameters\nepochs = 200\nbatch_size = 128\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> # Parameters epochs = 200 batch_size = 128 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None In\u00a0[3]: Copied! <pre>from fastestimator.dataset.data import omniglot\n\ntrain_data, eval_data = omniglot.load_data()\ntest_data = eval_data.split(0.5)\n</pre> from fastestimator.dataset.data import omniglot  train_data, eval_data = omniglot.load_data() test_data = eval_data.split(0.5) <p>For training, batches of data are created with half of the batch consisting of image pairs drawn from the same character, and the other half consisting of image pairs drawn from different characters. The target label is 1 for image pairs from the same character and 0 otherwise. The aim is to learn to quantify similarity between any given pair of images.</p> In\u00a0[4]: Copied! <pre>from fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import ShiftScaleRotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage\n\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),\n        ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_a\",\n                             image_out=\"x_a\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Sometimes(\n            ShiftScaleRotate(image_in=\"x_b\",\n                             image_out=\"x_b\",\n                             shift_limit=0.05,\n                             scale_limit=0.2,\n                             rotate_limit=10.0,\n                             mode=\"train\"),\n            prob=0.89),\n        Minmax(inputs=\"x_a\", outputs=\"x_a\"),\n        Minmax(inputs=\"x_b\", outputs=\"x_b\")\n    ])\n</pre> from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import ShiftScaleRotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage  pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"x_a\", outputs=\"x_a\", color_flag=\"gray\"),         ReadImage(inputs=\"x_b\", outputs=\"x_b\", color_flag=\"gray\"),         Sometimes(             ShiftScaleRotate(image_in=\"x_a\",                              image_out=\"x_a\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Sometimes(             ShiftScaleRotate(image_in=\"x_b\",                              image_out=\"x_b\",                              shift_limit=0.05,                              scale_limit=0.2,                              rotate_limit=10.0,                              mode=\"train\"),             prob=0.89),         Minmax(inputs=\"x_a\", outputs=\"x_a\"),         Minmax(inputs=\"x_b\", outputs=\"x_b\")     ]) <p>We can visualize sample images from the <code>Pipeline</code> using the <code>get_results</code> method:</p> In\u00a0[5]: Copied! <pre>sample_batch = pipeline.get_results()\n\npair1_img_a = sample_batch[\"x_a\"][0]\npair1_img_b = sample_batch[\"x_b\"][0]\n\npair2_img_a = sample_batch[\"x_a\"][1]\npair2_img_b = sample_batch[\"x_b\"][1]\n\nif sample_batch[\"y\"][0] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair1_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair1_img_b))\n\nplt.show()\n    \nif sample_batch[\"y\"][1] ==1:\n    print('Image pair from same character')\nelse:\n    print('Image pair from different characters')\n    \nplt.subplot(121)\nplt.imshow(np.squeeze(pair2_img_a))\n\nplt.subplot(122)\nplt.imshow(np.squeeze(pair2_img_b))\n\nplt.show()\n</pre> sample_batch = pipeline.get_results()  pair1_img_a = sample_batch[\"x_a\"][0] pair1_img_b = sample_batch[\"x_b\"][0]  pair2_img_a = sample_batch[\"x_a\"][1] pair2_img_b = sample_batch[\"x_b\"][1]  if sample_batch[\"y\"][0] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair1_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair1_img_b))  plt.show()      if sample_batch[\"y\"][1] ==1:     print('Image pair from same character') else:     print('Image pair from different characters')      plt.subplot(121) plt.imshow(np.squeeze(pair2_img_a))  plt.subplot(122) plt.imshow(np.squeeze(pair2_img_b))  plt.show() <pre>Image pair from same character\n</pre> <pre>Image pair from different characters\n</pre> <p>Our siamese network has two convolutional arms which accept distinct inputs. However, the weights on both these convolutional arms are shared. Each convolutional arm works as a feature extractor which produces a feature vector. L1 component-wise distance between these vectors is computed which is used to classify whether the image pair belongs to the same or different classes (characters).</p> In\u00a0[6]: Copied! <pre>from tensorflow.python.keras import Model, Sequential, layers\nfrom tensorflow.python.keras.initializers import RandomNormal\nfrom tensorflow.python.keras.regularizers import l2\n    \n\ndef siamese_network(input_shape=(105, 105, 1), classes=1):\n\"\"\"Network Architecture\"\"\"\n    left_input = layers.Input(shape=input_shape)\n    right_input = layers.Input(shape=input_shape)\n\n    #Creating the convnet which shares weights between the left and right legs of Siamese network\n    siamese_convnet = Sequential()\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=64,\n                      kernel_size=10,\n                      strides=1,\n                      input_shape=input_shape,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=7,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=128,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n    siamese_convnet.add(\n        layers.Conv2D(filters=256,\n                      kernel_size=4,\n                      strides=1,\n                      activation='relu',\n                      kernel_initializer=RandomNormal(mean=0, stddev=0.01),\n                      kernel_regularizer=l2(1e-2),\n                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    siamese_convnet.add(layers.Flatten())\n\n    siamese_convnet.add(\n        layers.Dense(4096,\n                     activation='sigmoid',\n                     kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                     kernel_regularizer=l2(1e-4),\n                     bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))\n\n    encoded_left_input = siamese_convnet(left_input)\n    encoded_right_input = siamese_convnet(right_input)\n\n    l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])\n\n    output = layers.Dense(classes,\n                          activation='sigmoid',\n                          kernel_initializer=RandomNormal(mean=0, stddev=0.2),\n                          bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)\n\n    return Model(inputs=[left_input, right_input], outputs=output)\n</pre> from tensorflow.python.keras import Model, Sequential, layers from tensorflow.python.keras.initializers import RandomNormal from tensorflow.python.keras.regularizers import l2       def siamese_network(input_shape=(105, 105, 1), classes=1):     \"\"\"Network Architecture\"\"\"     left_input = layers.Input(shape=input_shape)     right_input = layers.Input(shape=input_shape)      #Creating the convnet which shares weights between the left and right legs of Siamese network     siamese_convnet = Sequential()      siamese_convnet.add(         layers.Conv2D(filters=64,                       kernel_size=10,                       strides=1,                       input_shape=input_shape,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=7,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=128,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.MaxPooling2D(pool_size=(2, 2)))      siamese_convnet.add(         layers.Conv2D(filters=256,                       kernel_size=4,                       strides=1,                       activation='relu',                       kernel_initializer=RandomNormal(mean=0, stddev=0.01),                       kernel_regularizer=l2(1e-2),                       bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      siamese_convnet.add(layers.Flatten())      siamese_convnet.add(         layers.Dense(4096,                      activation='sigmoid',                      kernel_initializer=RandomNormal(mean=0, stddev=0.2),                      kernel_regularizer=l2(1e-4),                      bias_initializer=RandomNormal(mean=0.5, stddev=0.01)))      encoded_left_input = siamese_convnet(left_input)     encoded_right_input = siamese_convnet(right_input)      l1_encoded = layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([encoded_left_input, encoded_right_input])      output = layers.Dense(classes,                           activation='sigmoid',                           kernel_initializer=RandomNormal(mean=0, stddev=0.2),                           bias_initializer=RandomNormal(mean=0.5, stddev=0.01))(l1_encoded)      return Model(inputs=[left_input, right_input], outputs=output) <p>We now prepare the <code>model</code> and define a <code>Network</code> object.</p> In\u00a0[7]: Copied! <pre>from fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\nmodel = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   model = fe.build(model_fn=siamese_network, model_name=\"siamese_net\", optimizer_fn=\"adam\")  network = fe.Network(ops=[     ModelOp(inputs=[\"x_a\", \"x_b\"], model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) <p>In this example we will also use the following traces:</p> <ol> <li>LRScheduler with a constant decay schedule as described in the paper.</li> <li>BestModelSaver for saving the best model. For illustration purpose, we will save these models in a temporary directory.</li> <li>EarlyStopping for stopping training if the monitored metric doesn't improve within a specified number of epochs.</li> <li>A custom trace to calculate one shot classification accuracy as described in the paper. This trace performs a 20-way within-alphabet classification task in which an alphabet is first chosen from among those reserved for the evaluation set. Then, nineteen other characters are taken uniformly at random from the alphabet. The first character's image is compared with another image of the same character and with images of the other nineteen characters. This is called a one-shot trial. The trial is considered a success if the network outputs the highest similarity (probability) score for the image pair belonging to same character.</li> </ol> In\u00a0[8]: Copied! <pre>from fastestimator.backend import feed_forward\nfrom fastestimator.trace import Trace\nfrom fastestimator.trace.adapt import EarlyStopping, LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.util import Data\n\n\ndef lr_schedule(epoch):\n\"\"\"Learning rate schedule\"\"\"\n    lr = 0.0001*np.power(0.99, epoch)\n    return lr\n\n\nclass OneShotAccuracy(Trace):\n\"\"\"Trace for calculating one shot accuracy\"\"\"\n    def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):\n\n        super().__init__(mode=mode, outputs=output_name)\n        self.dataset = dataset\n        self.model = model\n        self.total = 0\n        self.correct = 0\n        self.output_name = output_name\n        self.N = N\n        self.trials = trials\n\n    def on_epoch_begin(self, data: Data):\n        self.total = 0\n        self.correct = 0\n\n    def on_epoch_end(self, data: Data):\n        for _ in range(self.trials):\n            img_path = self.dataset.one_shot_trial(self.N)\n            input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                                  dtype=np.float32),\n                         np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                                  dtype=np.float32))\n            prediction_score = feed_forward(self.model, input_img, training=False).numpy()\n\n            if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:\n                self.correct += 1\n\n            self.total += 1\n\n        data.write_with_log(self.outputs[0], self.correct / self.total)\n\n        \ntraces = [\n    LRScheduler(model=model, lr_fn=lr_schedule),\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),\n    EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\")\n]\n</pre> from fastestimator.backend import feed_forward from fastestimator.trace import Trace from fastestimator.trace.adapt import EarlyStopping, LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.util import Data   def lr_schedule(epoch):     \"\"\"Learning rate schedule\"\"\"     lr = 0.0001*np.power(0.99, epoch)     return lr   class OneShotAccuracy(Trace):     \"\"\"Trace for calculating one shot accuracy\"\"\"     def __init__(self, dataset, model, N=20, trials=400, mode=[\"eval\", \"test\"], output_name=\"one_shot_accuracy\"):          super().__init__(mode=mode, outputs=output_name)         self.dataset = dataset         self.model = model         self.total = 0         self.correct = 0         self.output_name = output_name         self.N = N         self.trials = trials      def on_epoch_begin(self, data: Data):         self.total = 0         self.correct = 0      def on_epoch_end(self, data: Data):         for _ in range(self.trials):             img_path = self.dataset.one_shot_trial(self.N)             input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                                   dtype=np.float32),                          np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                                   dtype=np.float32))             prediction_score = feed_forward(self.model, input_img, training=False).numpy()              if np.argmax(prediction_score) == 0 and prediction_score.std() &gt; 0.01:                 self.correct += 1              self.total += 1          data.write_with_log(self.outputs[0], self.correct / self.total)           traces = [     LRScheduler(model=model, lr_fn=lr_schedule),     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     OneShotAccuracy(dataset=eval_data, model=model, output_name='one_shot_accuracy'),     BestModelSaver(model=model, save_dir=save_dir, metric=\"one_shot_accuracy\", save_best_mode=\"max\"),     EarlyStopping(monitor=\"one_shot_accuracy\", patience=20, compare='max', mode=\"eval\") ] In\u00a0[9]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         traces=traces, \n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          traces=traces,                           max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[10]: Copied! <pre># Training\nestimator.fit()\n</pre> # Training estimator.fit() <p>Now, we can load the best model to check its one-shot accuracy on the test set:</p> In\u00a0[11]: Copied! <pre># Testing\nfe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5'))\nestimator.test()\n</pre> # Testing fe.backend.load_model(model, os.path.join(save_dir, 'siamese_net_best_one_shot_accuracy.h5')) estimator.test() <pre>Loaded model weights from /tmp/tmptw2czltd/siamese_net_best_one_shot_accuracy.h5\nFastEstimator-Test: epoch: 110; accuracy: 0.9256060606060607; one_shot_accuracy: 0.7875; \n</pre> <p>Let's perform inferencing on some elements in the test dataset. Here, we generate a 5-way one shot trial for demo purposes.</p> In\u00a0[12]: Copied! <pre>#Generating one-shot trial set for 5-way one shot trial\nimg_path = test_data.one_shot_trial(5)\ninput_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],\n                      dtype=np.float32),\n             np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],\n                      dtype=np.float32))\n\nprediction_score = feed_forward(model, input_img, training=False).numpy()\n</pre> #Generating one-shot trial set for 5-way one shot trial img_path = test_data.one_shot_trial(5) input_img = (np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[0]],                       dtype=np.float32),              np.array([np.expand_dims(cv2.imread(i, cv2.IMREAD_GRAYSCALE), -1) / 255. for i in img_path[1]],                       dtype=np.float32))  prediction_score = feed_forward(model, input_img, training=False).numpy() <p>The test image is predicted to be belonging to the class with the maximum similarity.</p> In\u00a0[13]: Copied! <pre>plt.figure(figsize=(4, 4))\nplt.imshow(np.squeeze(input_img[0][0]));\nplt.title('test image')\nplt.axis('off');\nplt.show()\n\nplt.figure(figsize=(18, 18))\nplt.subplot(151)\nplt.imshow(np.squeeze(input_img[1][0]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0]))\nplt.axis('off');\n\nplt.subplot(152)\nplt.imshow(np.squeeze(input_img[1][1]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0]))\nplt.axis('off');\n\nplt.subplot(153)\nplt.imshow(np.squeeze(input_img[1][2]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0]))\nplt.axis('off');\n\nplt.subplot(154)\nplt.imshow(np.squeeze(input_img[1][3]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0]))\nplt.axis('off');\n\nplt.subplot(155)\nplt.imshow(np.squeeze(input_img[1][4]));\nplt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0]))\nplt.axis('off');\n\nplt.tight_layout()\n</pre> plt.figure(figsize=(4, 4)) plt.imshow(np.squeeze(input_img[0][0])); plt.title('test image') plt.axis('off'); plt.show()  plt.figure(figsize=(18, 18)) plt.subplot(151) plt.imshow(np.squeeze(input_img[1][0])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[0][0])) plt.axis('off');  plt.subplot(152) plt.imshow(np.squeeze(input_img[1][1])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[1][0])) plt.axis('off');  plt.subplot(153) plt.imshow(np.squeeze(input_img[1][2])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[2][0])) plt.axis('off');  plt.subplot(154) plt.imshow(np.squeeze(input_img[1][3])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[3][0])) plt.axis('off');  plt.subplot(155) plt.imshow(np.squeeze(input_img[1][4])); plt.title('Similarity with test image: {:0.2f}%'.format(100*prediction_score[4][0])) plt.axis('off');  plt.tight_layout()"}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#one-shot-learning-using-a-siamese-network-in-fastestimator", "title": "One-Shot Learning using a Siamese Network in FastEstimator\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#training-and-testing", "title": "Training and Testing\u00b6", "text": ""}, {"location": "apphub/one_shot_learning/siamese_network/siamese.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html", "title": "Lung Segmentation Using the Montgomery Dataset", "text": "In\u00a0[1]: Copied! <pre>import os\nimport tempfile\nfrom typing import Any, Dict, List\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt\n\nimport fastestimator as fe\nfrom fastestimator.architecture.pytorch import UNet\nfrom fastestimator.dataset.data import montgomery\nfrom fastestimator.op.numpyop import Delete, NumpyOp\nfrom fastestimator.op.numpyop.meta import Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate\nfrom fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Dice\n</pre> import os import tempfile from typing import Any, Dict, List  import cv2 import numpy as np import pandas as pd import torch from matplotlib import pyplot as plt  import fastestimator as fe from fastestimator.architecture.pytorch import UNet from fastestimator.dataset.data import montgomery from fastestimator.op.numpyop import Delete, NumpyOp from fastestimator.op.numpyop.meta import Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, Resize, Rotate from fastestimator.op.numpyop.univariate import Minmax, ReadImage, Reshape from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Dice In\u00a0[2]: Copied! <pre>pd.set_option('display.max_colwidth', 500)\n</pre> pd.set_option('display.max_colwidth', 500) In\u00a0[3]: parameters Copied! <pre>batch_size = 4\nepochs = 25\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\nsave_dir = tempfile.mkdtemp()\ndata_dir = None\n</pre> batch_size = 4 epochs = 25 max_train_steps_per_epoch = None max_eval_steps_per_epoch = None save_dir = tempfile.mkdtemp() data_dir = None <p>We download the Montgomery data first:</p> In\u00a0[4]: Copied! <pre>csv = montgomery.load_data(root_dir=data_dir)\n</pre> csv = montgomery.load_data(root_dir=data_dir) <p>This creates a <code>CSVDataset</code>. Let's see what is inside:</p> In\u00a0[5]: Copied! <pre>df = pd.DataFrame.from_dict(csv.data, orient='index')\n</pre> df = pd.DataFrame.from_dict(csv.data, orient='index') In\u00a0[6]: Copied! <pre>df.head()\n</pre> df.head() Out[6]: image mask_left mask_right 0 MontgomerySet/CXR_png/MCUCXR_0243_1.png MontgomerySet/ManualMask/leftMask/MCUCXR_0243_1.png MontgomerySet/ManualMask/rightMask/MCUCXR_0243_1.png 1 MontgomerySet/CXR_png/MCUCXR_0022_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0022_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0022_0.png 2 MontgomerySet/CXR_png/MCUCXR_0086_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0086_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0086_0.png 3 MontgomerySet/CXR_png/MCUCXR_0008_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0008_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0008_0.png 4 MontgomerySet/CXR_png/MCUCXR_0094_0.png MontgomerySet/ManualMask/leftMask/MCUCXR_0094_0.png MontgomerySet/ManualMask/rightMask/MCUCXR_0094_0.png <p>Now let's set the stage for training:</p> In\u00a0[7]: Copied! <pre>class CombineLeftRightMask(NumpyOp):    \n    def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\n        mask_left, mask_right = data\n        data = mask_left + mask_right\n        return data\n</pre> class CombineLeftRightMask(NumpyOp):         def forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:         mask_left, mask_right = data         data = mask_left + mask_right         return data In\u00a0[8]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=csv,\n    eval_data=csv.split(0.2),\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),\n        ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),\n        ReadImage(inputs=\"mask_right\",\n                  parent_path=csv.parent_path,\n                  outputs=\"mask_right\",\n                  color_flag=\"gray\",\n                  mode='!infer'),\n        CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),\n        Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),\n        Resize(image_in=\"image\", width=512, height=512),\n        Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),\n        Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),\n        Sometimes(numpy_op=Rotate(\n            image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),\n        Minmax(inputs=\"image\", outputs=\"image\"),\n        Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),\n        Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),\n        Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=csv,     eval_data=csv.split(0.2),     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", parent_path=csv.parent_path, outputs=\"image\", color_flag=\"gray\"),         ReadImage(inputs=\"mask_left\", parent_path=csv.parent_path, outputs=\"mask_left\", color_flag=\"gray\", mode='!infer'),         ReadImage(inputs=\"mask_right\",                   parent_path=csv.parent_path,                   outputs=\"mask_right\",                   color_flag=\"gray\",                   mode='!infer'),         CombineLeftRightMask(inputs=(\"mask_left\", \"mask_right\"), outputs=\"mask\", mode='!infer'),         Delete(keys=[\"mask_left\", \"mask_right\"], mode='!infer'),         Resize(image_in=\"image\", width=512, height=512),         Resize(image_in=\"mask\", width=512, height=512, mode='!infer'),         Sometimes(numpy_op=HorizontalFlip(image_in=\"image\", mask_in=\"mask\", mode='train')),         Sometimes(numpy_op=Rotate(             image_in=\"image\", mask_in=\"mask\", limit=(-10, 10), border_mode=cv2.BORDER_CONSTANT, mode='train')),         Minmax(inputs=\"image\", outputs=\"image\"),         Minmax(inputs=\"mask\", outputs=\"mask\", mode='!infer'),         Reshape(shape=(1, 512, 512), inputs=\"image\", outputs=\"image\"),         Reshape(shape=(1, 512, 512), inputs=\"mask\", outputs=\"mask\", mode='!infer')     ]) <p>Let's see if the <code>Pipeline</code> output is reasonable. We call <code>get_results</code> to get outputs from <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre>batch_data = pipeline.get_results()\n</pre> batch_data = pipeline.get_results() In\u00a0[10]: Copied! <pre>batch_index = 1\nfig, ax = plt.subplots(1, 2, figsize=(15,6))\nax[0].imshow(np.squeeze(batch_data['image'][batch_index]), cmap='gray')\nax[1].imshow(np.squeeze(batch_data['mask'][batch_index]), cmap='gray')\n</pre> batch_index = 1 fig, ax = plt.subplots(1, 2, figsize=(15,6)) ax[0].imshow(np.squeeze(batch_data['image'][batch_index]), cmap='gray') ax[1].imshow(np.squeeze(batch_data['mask'][batch_index]), cmap='gray') Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x7f62440ace80&gt;</pre> In\u00a0[11]: Copied! <pre>model = fe.build(\n    model_fn=lambda: UNet(input_size=(1, 512, 512)),\n    optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n    model_name=\"lung_segmentation\"\n)\n</pre> model = fe.build(     model_fn=lambda: UNet(input_size=(1, 512, 512)),     optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),     model_name=\"lung_segmentation\" ) In\u00a0[12]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),\n    CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\"),     CrossEntropy(inputs=(\"pred_segment\", \"mask\"), outputs=\"loss\", form=\"binary\"),     UpdateOp(model=model, loss_name=\"loss\") ]) In\u00a0[13]: Copied! <pre>traces = [\n    Dice(true_key=\"mask\", pred_key=\"pred_segment\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max')\n]\n</pre> traces = [     Dice(true_key=\"mask\", pred_key=\"pred_segment\"),     BestModelSaver(model=model, save_dir=save_dir, metric='Dice', save_best_mode='max') ] In\u00a0[14]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         epochs=epochs,\n                         log_steps=20,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          epochs=epochs,                          log_steps=20,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[15]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 0; lung_segmentation_lr: 0.0001; \nFastEstimator-Train: step: 0; loss: 0.65074736; \nFastEstimator-Train: step: 20; loss: 0.32184097; steps/sec: 2.84; \nFastEstimator-Train: step: 28; epoch: 0; epoch_time: 12.08 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 28; epoch: 0; loss: 0.6737924; min_loss: 0.6737924; since_best: 0; dice: 1.7335249448973935e-13; \nFastEstimator-Train: step: 40; loss: 0.3740636; steps/sec: 2.14; \nFastEstimator-Train: step: 56; epoch: 1; epoch_time: 12.1 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 56; epoch: 1; loss: 0.32339695; min_loss: 0.32339695; since_best: 0; dice: 0.07768369491807968; \nFastEstimator-Train: step: 60; loss: 0.44676578; steps/sec: 2.16; \nFastEstimator-Train: step: 80; loss: 0.14473557; steps/sec: 2.83; \nFastEstimator-Train: step: 84; epoch: 2; epoch_time: 12.12 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 84; epoch: 2; loss: 0.12985013; min_loss: 0.12985013; since_best: 0; dice: 0.8881443049809153; \nFastEstimator-Train: step: 100; loss: 0.1680123; steps/sec: 2.12; \nFastEstimator-Train: step: 112; epoch: 3; epoch_time: 12.24 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 112; epoch: 3; loss: 0.12533109; min_loss: 0.12533109; since_best: 0; dice: 0.8912838752069556; \nFastEstimator-Train: step: 120; loss: 0.09625991; steps/sec: 2.11; \nFastEstimator-Train: step: 140; epoch: 4; epoch_time: 12.45 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 140; epoch: 4; loss: 0.1327874; min_loss: 0.12533109; since_best: 1; dice: 0.9037457108044139; \nFastEstimator-Train: step: 140; loss: 0.100700244; steps/sec: 2.11; \nFastEstimator-Train: step: 160; loss: 0.076236516; steps/sec: 2.78; \nFastEstimator-Train: step: 168; epoch: 5; epoch_time: 12.42 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 168; epoch: 5; loss: 0.12693708; min_loss: 0.12533109; since_best: 2; dice: 0.9092143670270832; \nFastEstimator-Train: step: 180; loss: 0.074325085; steps/sec: 2.11; \nFastEstimator-Train: step: 196; epoch: 6; epoch_time: 12.26 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 196; epoch: 6; loss: 0.08061478; min_loss: 0.08061478; since_best: 0; dice: 0.9364716513786071; \nFastEstimator-Train: step: 200; loss: 0.050859306; steps/sec: 2.04; \nFastEstimator-Train: step: 220; loss: 0.0795434; steps/sec: 2.81; \nFastEstimator-Train: step: 224; epoch: 7; epoch_time: 12.73 sec; \nFastEstimator-Eval: step: 224; epoch: 7; loss: 0.077932164; min_loss: 0.077932164; since_best: 0; dice: 0.9363544687013216; \nFastEstimator-Train: step: 240; loss: 0.04766827; steps/sec: 2.09; \nFastEstimator-Train: step: 252; epoch: 8; epoch_time: 12.39 sec; \nFastEstimator-Eval: step: 252; epoch: 8; loss: 0.08354305; min_loss: 0.077932164; since_best: 1; dice: 0.9357203667668574; \nFastEstimator-Train: step: 260; loss: 0.07521939; steps/sec: 1.98; \nFastEstimator-Train: step: 280; epoch: 9; epoch_time: 12.96 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 280; epoch: 9; loss: 0.07668398; min_loss: 0.07668398; since_best: 0; dice: 0.9423949873504999; \nFastEstimator-Train: step: 280; loss: 0.040693548; steps/sec: 2.13; \nFastEstimator-Train: step: 300; loss: 0.056012712; steps/sec: 2.8; \nFastEstimator-Train: step: 308; epoch: 10; epoch_time: 12.25 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 308; epoch: 10; loss: 0.07509731; min_loss: 0.07509731; since_best: 0; dice: 0.9461569423558578; \nFastEstimator-Train: step: 320; loss: 0.04584109; steps/sec: 1.99; \nFastEstimator-Train: step: 336; epoch: 11; epoch_time: 12.89 sec; \nFastEstimator-Eval: step: 336; epoch: 11; loss: 0.071265906; min_loss: 0.071265906; since_best: 0; dice: 0.9425017790028412; \nFastEstimator-Train: step: 340; loss: 0.06591544; steps/sec: 2.06; \nFastEstimator-Train: step: 360; loss: 0.039461873; steps/sec: 2.77; \nFastEstimator-Train: step: 364; epoch: 12; epoch_time: 12.69 sec; \nFastEstimator-Eval: step: 364; epoch: 12; loss: 0.06946295; min_loss: 0.06946295; since_best: 0; dice: 0.9453714568046383; \nFastEstimator-Train: step: 380; loss: 0.053401247; steps/sec: 2.03; \nFastEstimator-Train: step: 392; epoch: 13; epoch_time: 12.69 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 392; epoch: 13; loss: 0.06035184; min_loss: 0.06035184; since_best: 0; dice: 0.9530510743823034; \nFastEstimator-Train: step: 400; loss: 0.032455094; steps/sec: 2.12; \nFastEstimator-Train: step: 420; epoch: 14; epoch_time: 12.33 sec; \nFastEstimator-Eval: step: 420; epoch: 14; loss: 0.072278894; min_loss: 0.06035184; since_best: 1; dice: 0.9376849732175224; \nFastEstimator-Train: step: 420; loss: 0.053798117; steps/sec: 2.07; \nFastEstimator-Train: step: 440; loss: 0.038134858; steps/sec: 2.77; \nFastEstimator-Train: step: 448; epoch: 15; epoch_time: 12.62 sec; \nFastEstimator-Eval: step: 448; epoch: 15; loss: 0.06692966; min_loss: 0.06035184; since_best: 2; dice: 0.9515013302881467; \nFastEstimator-Train: step: 460; loss: 0.039841093; steps/sec: 2.09; \nFastEstimator-Train: step: 476; epoch: 16; epoch_time: 12.42 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 476; epoch: 16; loss: 0.062103588; min_loss: 0.06035184; since_best: 3; dice: 0.9542675866982232; \nFastEstimator-Train: step: 480; loss: 0.029947285; steps/sec: 2.14; \nFastEstimator-Train: step: 500; loss: 0.027964272; steps/sec: 2.78; \nFastEstimator-Train: step: 504; epoch: 17; epoch_time: 12.24 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 504; epoch: 17; loss: 0.05789433; min_loss: 0.05789433; since_best: 0; dice: 0.9561937779850502; \nFastEstimator-Train: step: 520; loss: 0.030686911; steps/sec: 2.12; \nFastEstimator-Train: step: 532; epoch: 18; epoch_time: 12.29 sec; \nFastEstimator-Eval: step: 532; epoch: 18; loss: 0.05837614; min_loss: 0.05789433; since_best: 1; dice: 0.9551540080564349; \nFastEstimator-Train: step: 540; loss: 0.02688715; steps/sec: 2.12; \nFastEstimator-Train: step: 560; epoch: 19; epoch_time: 12.26 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 560; epoch: 19; loss: 0.05515228; min_loss: 0.05515228; since_best: 0; dice: 0.9569575317963865; \nFastEstimator-Train: step: 560; loss: 0.045194946; steps/sec: 2.12; \nFastEstimator-Train: step: 580; loss: 0.029270299; steps/sec: 2.81; \nFastEstimator-Train: step: 588; epoch: 20; epoch_time: 12.29 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 588; epoch: 20; loss: 0.054214593; min_loss: 0.054214593; since_best: 0; dice: 0.9588091257130298; \nFastEstimator-Train: step: 600; loss: 0.03187306; steps/sec: 2.15; \nFastEstimator-Train: step: 616; epoch: 21; epoch_time: 12.14 sec; \nFastEstimator-Eval: step: 616; epoch: 21; loss: 0.056209736; min_loss: 0.054214593; since_best: 1; dice: 0.9565499194603396; \nFastEstimator-Train: step: 620; loss: 0.03272804; steps/sec: 2.16; \nFastEstimator-Train: step: 640; loss: 0.034923207; steps/sec: 2.82; \nFastEstimator-Train: step: 644; epoch: 22; epoch_time: 12.12 sec; \nFastEstimator-Eval: step: 644; epoch: 22; loss: 0.05843257; min_loss: 0.054214593; since_best: 2; dice: 0.9549386241705727; \nFastEstimator-Train: step: 660; loss: 0.03908976; steps/sec: 2.14; \nFastEstimator-Train: step: 672; epoch: 23; epoch_time: 12.16 sec; \nSaved model to /tmp/tmpnogbnnb5/lung_segmentation_best_dice.pt\nFastEstimator-Eval: step: 672; epoch: 23; loss: 0.05370887; min_loss: 0.05370887; since_best: 0; dice: 0.9596281608464776; \nFastEstimator-Train: step: 680; loss: 0.031742807; steps/sec: 2.15; \nFastEstimator-Train: step: 700; epoch: 24; epoch_time: 12.16 sec; \nFastEstimator-Eval: step: 700; epoch: 24; loss: 0.054277744; min_loss: 0.05370887; since_best: 1; dice: 0.9582266396901049; \nFastEstimator-Finish: step: 700; total_time: 392.64 sec; lung_segmentation_lr: 0.0001; \n</pre> <p>Let's visualize the prediction from the neural network. We select a random image from the dataset:</p> In\u00a0[16]: Copied! <pre>image_path = df['image'].sample(random_state=3).values[0]\n</pre> image_path = df['image'].sample(random_state=3).values[0] <p>We create a data dict, and call <code>Pipeline.transform()</code>.</p> In\u00a0[17]: Copied! <pre>data = {'image': image_path}\ndata = pipeline.transform(data, mode=\"infer\")\n</pre> data = {'image': image_path} data = pipeline.transform(data, mode=\"infer\") <p>After the <code>Pipeline</code>, we rebuild our model by providing the trained weights path and pass it to a new <code>Network</code>:</p> In\u00a0[18]: Copied! <pre>weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path\n\nmodel = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),\n                 optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),\n                 model_name=\"lung_segmentation\",\n                 weights_path=weights_path)\n</pre> weights_path = os.path.join(save_dir, \"lung_segmentation_best_Dice.pt\") # your model_path  model = fe.build(model_fn=lambda: UNet(input_size=(1, 512, 512)),                  optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.0001),                  model_name=\"lung_segmentation\",                  weights_path=weights_path) <pre>Loaded model weights from /tmp/tmpsqurxgnr/lung_segmentation_best_dice.pt\n</pre> In\u00a0[19]: Copied! <pre>network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")])\n</pre> network = fe.Network(ops=[ModelOp(inputs=\"image\", model=model, outputs=\"pred_segment\")]) <p>We call <code>Network.transform()</code> to get outputs from our <code>Network</code>:</p> In\u00a0[20]: Copied! <pre>pred = network.transform(data, mode=\"infer\")\n</pre> pred = network.transform(data, mode=\"infer\") In\u00a0[21]: Copied! <pre>img = np.squeeze(pred['image'].numpy())\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\nimg_rgb = (img_rgb * 255).astype(np.uint8)\n</pre> img = np.squeeze(pred['image'].numpy()) img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) img_rgb = (img_rgb * 255).astype(np.uint8) In\u00a0[22]: Copied! <pre>mask = pred['pred_segment'].numpy()\nmask = np.squeeze(mask)\nmask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\nmask_rgb = (mask_rgb * 255).astype(np.uint8)\n</pre> mask = pred['pred_segment'].numpy() mask = np.squeeze(mask) mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) mask_rgb = (mask_rgb * 255).astype(np.uint8) In\u00a0[23]: Copied! <pre>_, mask_thres = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY)\nmask_overlay = mask_rgb * np.expand_dims(mask_thres, axis=-1)\nmask_overlay = np.where(mask_overlay != [0, 0, 0], [255, 0, 0], [0, 0, 0])\nmask_overlay = mask_overlay.astype(np.uint8)\nimg_with_mask = cv2.addWeighted(img_rgb, 0.7, mask_overlay, 0.3, 0)\nmaskgt_with_maskpred = cv2.addWeighted(mask_rgb, 0.7, mask_overlay, 0.3, 0)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\nax[0].imshow(img_rgb)\nax[0].set_title('original lung')\nax[1].imshow(img_with_mask)\nax[1].set_title('predict mask ')\nplt.show()\n</pre> _, mask_thres = cv2.threshold(mask, 0.5, 1, cv2.THRESH_BINARY) mask_overlay = mask_rgb * np.expand_dims(mask_thres, axis=-1) mask_overlay = np.where(mask_overlay != [0, 0, 0], [255, 0, 0], [0, 0, 0]) mask_overlay = mask_overlay.astype(np.uint8) img_with_mask = cv2.addWeighted(img_rgb, 0.7, mask_overlay, 0.3, 0) maskgt_with_maskpred = cv2.addWeighted(mask_rgb, 0.7, mask_overlay, 0.3, 0)  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6)) ax[0].imshow(img_rgb) ax[0].set_title('original lung') ax[1].imshow(img_with_mask) ax[1].set_title('predict mask ') plt.show()"}, {"location": "apphub/semantic_segmentation/unet/unet.html#lung-segmentation-using-the-montgomery-dataset", "title": "Lung Segmentation Using the Montgomery Dataset\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#download-data", "title": "Download Data\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#inferencing", "title": "Inferencing\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#pass-the-image-through-pipeline-and-network", "title": "Pass the image through <code>Pipeline</code> and <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/semantic_segmentation/unet/unet.html#visualize-outputs", "title": "Visualize Outputs\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html", "title": "Fast Style Transfer with FastEstimator", "text": "In\u00a0[1]: Copied! <pre>import tempfile\nimport os\nimport cv2\nimport tensorflow as tf\nimport numpy as np\n\nimport fastestimator as fe\nfrom fastestimator.backend import reduce_mean\nfrom fastestimator.op.numpyop.multivariate import Resize\nfrom fastestimator.op.numpyop.univariate import Normalize, ReadImage\nfrom fastestimator.trace.io import ModelSaver\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n</pre> import tempfile import os import cv2 import tensorflow as tf import numpy as np  import fastestimator as fe from fastestimator.backend import reduce_mean from fastestimator.op.numpyop.multivariate import Resize from fastestimator.op.numpyop.univariate import Normalize, ReadImage from fastestimator.trace.io import ModelSaver  import matplotlib from matplotlib import pyplot as plt In\u00a0[2]: parameters Copied! <pre>#Parameters\nbatch_size = 4\nepochs = 2\nmax_train_steps_per_epoch = None\nlog_steps = 2000\nstyle_weight=5.0\ncontent_weight=1.0\ntv_weight=1e-4\nsave_dir = tempfile.mkdtemp()\nstyle_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg'\ntest_img_path = 'panda.jpeg'\ndata_dir = None\n</pre> #Parameters batch_size = 4 epochs = 2 max_train_steps_per_epoch = None log_steps = 2000 style_weight=5.0 content_weight=1.0 tv_weight=1e-4 save_dir = tempfile.mkdtemp() style_img_path = 'Vassily_Kandinsky,_1913_-_Composition_7.jpg' test_img_path = 'panda.jpeg' data_dir = None <p>In this notebook we will use Vassily Kandinsky's Composition 7 as a style image. We will also resize the style image to $256 \\times 256$ to make the dimension consistent with that of COCO images.</p> In\u00a0[3]: Copied! <pre>style_img = cv2.imread(style_img_path)\nassert style_img is not None, \"cannot load the style image, please go to the folder with style image\"\nstyle_img = cv2.resize(style_img, (256, 256))\nstyle_img = (style_img.astype(np.float32) - 127.5) / 127.5\nstyle_img_t = tf.convert_to_tensor(np.expand_dims(style_img, axis=0))\n\nstyle_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB)\nplt.imshow(style_img_disp)\nplt.title('Vassily Kandinsky\\'s Composition 7')\nplt.axis('off');\n</pre> style_img = cv2.imread(style_img_path) assert style_img is not None, \"cannot load the style image, please go to the folder with style image\" style_img = cv2.resize(style_img, (256, 256)) style_img = (style_img.astype(np.float32) - 127.5) / 127.5 style_img_t = tf.convert_to_tensor(np.expand_dims(style_img, axis=0))  style_img_disp = cv2.cvtColor((style_img + 1) * 0.5, cv2.COLOR_BGR2RGB) plt.imshow(style_img_disp) plt.title('Vassily Kandinsky\\'s Composition 7') plt.axis('off'); In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mscoco\ntrain_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False)\n</pre> from fastestimator.dataset.data import mscoco train_data, _ = mscoco.load_data(root_dir=data_dir, load_bboxes=False, load_masks=False, load_captions=False) In\u00a0[5]: Copied! <pre>pipeline = fe.Pipeline(\n    train_data=train_data,\n    batch_size=batch_size,\n    ops=[\n        ReadImage(inputs=\"image\", outputs=\"image\"),\n        Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),\n        Resize(height=256, width=256, image_in=\"image\", image_out=\"image\")\n    ])\n</pre> pipeline = fe.Pipeline(     train_data=train_data,     batch_size=batch_size,     ops=[         ReadImage(inputs=\"image\", outputs=\"image\"),         Normalize(inputs=\"image\", outputs=\"image\", mean=1.0, std=1.0, max_pixel_value=127.5),         Resize(height=256, width=256, image_in=\"image\", image_out=\"image\")     ]) <p>We can visualize sample images from our <code>Pipeline</code> using the 'get_results' method:</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef Minmax(data):\n    data_max = np.max(data)\n    data_min = np.min(data)\n    data = (data - data_min) / max((data_max - data_min), 1e-7)\n    return data\n\nsample_batch = pipeline.get_results()\nimg = Minmax(sample_batch[\"image\"][0].numpy())\nplt.imshow(img)\nplt.show()\n</pre> import matplotlib.pyplot as plt import numpy as np  def Minmax(data):     data_max = np.max(data)     data_min = np.min(data)     data = (data - data_min) / max((data_max - data_min), 1e-7)     return data  sample_batch = pipeline.get_results() img = Minmax(sample_batch[\"image\"][0].numpy()) plt.imshow(img) plt.show() In\u00a0[7]: Copied! <pre>from typing import Dict, List, Tuple, Union\n\nimport tensorflow as tf\n\nfrom fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D\n\n\ndef _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x0)\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               kernel_initializer=initializer)(x)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.Add()([x, x0_cropped])\n    return x\n\n\ndef _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    if apply_relu:\n        x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2DTranspose(filters=num_filter,\n                                        kernel_size=kernel_size,\n                                        strides=strides,\n                                        padding=padding,\n                                        kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = tf.keras.layers.Conv2D(filters=num_filter,\n                               kernel_size=kernel_size,\n                               strides=strides,\n                               padding=padding,\n                               kernel_initializer=initializer)(x0)\n\n    x = InstanceNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n    return x\n\n\ndef StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):\n\"\"\"Creates the Style Transfer Network.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    x = ReflectionPadding2D(padding=(40, 40))(x0)\n    x = _conv_block(x, num_filter=32)\n    x = _downsample(x, num_filter=64)\n    x = _downsample(x, num_filter=128)\n\n    for _ in range(num_resblock):\n        x = _residual_block(x, num_filter=128)\n\n    x = _upsample(x, num_filter=64)\n    x = _upsample(x, num_filter=32)\n    x = _conv_block(x, num_filter=3, apply_relu=False)\n    x = tf.keras.layers.Activation(\"tanh\")(x)\n    return tf.keras.Model(inputs=x0, outputs=x)\n\n\ndef LossNet(input_shape=(256, 256, 3),\n            style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],\n            content_layers=[\"block3_conv3\"]):\n\"\"\"Creates the network to compute the style loss.\n    This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16\n    for each.\n    \"\"\"\n    x0 = tf.keras.layers.Input(shape=input_shape)\n    mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)\n    # Compute style loss\n    style_output = [mdl.get_layer(name).output for name in style_layers]\n    content_output = [mdl.get_layer(name).output for name in content_layers]\n    output = {\"style\": style_output, \"content\": content_output}\n    return tf.keras.Model(inputs=x0, outputs=output)\n</pre> from typing import Dict, List, Tuple, Union  import tensorflow as tf  from fastestimator.layers.tensorflow import InstanceNormalization, ReflectionPadding2D   def _residual_block(x0, num_filter, kernel_size=(3, 3), strides=(1, 1)):     initializer = tf.random_normal_initializer(0., 0.02)     x0_cropped = tf.keras.layers.Cropping2D(cropping=2)(x0)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x0)     x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)      x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                kernel_initializer=initializer)(x)      x = InstanceNormalization()(x)     x = tf.keras.layers.Add()([x, x0_cropped])     return x   def _conv_block(x0, num_filter, kernel_size=(9, 9), strides=(1, 1), padding=\"same\", apply_relu=True):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     if apply_relu:         x = tf.keras.layers.ReLU()(x)     return x   def _upsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2DTranspose(filters=num_filter,                                         kernel_size=kernel_size,                                         strides=strides,                                         padding=padding,                                         kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def _downsample(x0, num_filter, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"):     initializer = tf.random_normal_initializer(0., 0.02)     x = tf.keras.layers.Conv2D(filters=num_filter,                                kernel_size=kernel_size,                                strides=strides,                                padding=padding,                                kernel_initializer=initializer)(x0)      x = InstanceNormalization()(x)     x = tf.keras.layers.ReLU()(x)     return x   def StyleTransferNet(input_shape=(256, 256, 3), num_resblock=5):     \"\"\"Creates the Style Transfer Network.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     x = ReflectionPadding2D(padding=(40, 40))(x0)     x = _conv_block(x, num_filter=32)     x = _downsample(x, num_filter=64)     x = _downsample(x, num_filter=128)      for _ in range(num_resblock):         x = _residual_block(x, num_filter=128)      x = _upsample(x, num_filter=64)     x = _upsample(x, num_filter=32)     x = _conv_block(x, num_filter=3, apply_relu=False)     x = tf.keras.layers.Activation(\"tanh\")(x)     return tf.keras.Model(inputs=x0, outputs=x)   def LossNet(input_shape=(256, 256, 3),             style_layers=[\"block1_conv2\", \"block2_conv2\", \"block3_conv3\", \"block4_conv3\"],             content_layers=[\"block3_conv3\"]):     \"\"\"Creates the network to compute the style loss.     This network outputs a dictionary with outputs values for style and content, based on a list of layers from VGG16     for each.     \"\"\"     x0 = tf.keras.layers.Input(shape=input_shape)     mdl = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_tensor=x0)     # Compute style loss     style_output = [mdl.get_layer(name).output for name in style_layers]     content_output = [mdl.get_layer(name).output for name in content_layers]     output = {\"style\": style_output, \"content\": content_output}     return tf.keras.Model(inputs=x0, outputs=output) In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=StyleTransferNet, \n                 model_name=\"style_transfer_net\",\n                 optimizer_fn=lambda: tf.optimizers.Adam(1e-3))\n</pre> model = fe.build(model_fn=StyleTransferNet,                   model_name=\"style_transfer_net\",                  optimizer_fn=lambda: tf.optimizers.Adam(1e-3)) In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\n\nclass ExtractVGGFeatures(TensorOp):\n    def __init__(self, inputs, outputs, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.vgg = LossNet()\n\n    def forward(self, data, state):\n        return self.vgg(data)\n\n\nclass StyleContentLoss(TensorOp):\n    def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):\n        super().__init__(inputs=inputs, outputs=outputs, mode=mode)\n        self.style_weight = style_weight\n        self.content_weight = content_weight\n        self.tv_weight = tv_weight\n        self.average_loss = average_loss\n\n    def calculate_style_recon_loss(self, y_true, y_pred):\n        y_true_gram = self.calculate_gram_matrix(y_true)\n        y_pred_gram = self.calculate_gram_matrix(y_pred)\n        y_diff_gram = y_pred_gram - y_true_gram\n        y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))\n        return y_norm\n\n    def calculate_feature_recon_loss(self, y_true, y_pred):\n        y_diff = y_pred - y_true\n        num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)\n        y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts\n        return y_diff_norm\n\n    def calculate_gram_matrix(self, x):\n        x = tf.cast(x, tf.float32)\n        num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)\n        gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)\n        gram_matrix /= num_elts\n        return gram_matrix\n\n    def calculate_total_variation(self, y_pred):\n        return tf.image.total_variation(y_pred)\n\n    def forward(self, data, state):\n        y_pred, y_style, y_content, image_out = data\n\n        style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]\n        style_loss = tf.add_n(style_loss)\n        style_loss *= self.style_weight\n\n        content_loss = [\n            self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])\n        ]\n        content_loss = tf.add_n(content_loss)\n        content_loss *= self.content_weight\n\n        total_variation_reg = self.calculate_total_variation(image_out)\n        total_variation_reg *= self.tv_weight\n        loss = style_loss + content_loss + total_variation_reg\n\n        if self.average_loss:\n            loss = reduce_mean(loss)\n\n        return loss\n</pre> from fastestimator.op.tensorop import TensorOp  class ExtractVGGFeatures(TensorOp):     def __init__(self, inputs, outputs, mode=None):         super().__init__(inputs, outputs, mode)         self.vgg = LossNet()      def forward(self, data, state):         return self.vgg(data)   class StyleContentLoss(TensorOp):     def __init__(self, style_weight, content_weight, tv_weight, inputs, outputs=None, mode=None, average_loss=True):         super().__init__(inputs=inputs, outputs=outputs, mode=mode)         self.style_weight = style_weight         self.content_weight = content_weight         self.tv_weight = tv_weight         self.average_loss = average_loss      def calculate_style_recon_loss(self, y_true, y_pred):         y_true_gram = self.calculate_gram_matrix(y_true)         y_pred_gram = self.calculate_gram_matrix(y_pred)         y_diff_gram = y_pred_gram - y_true_gram         y_norm = tf.math.sqrt(tf.reduce_sum(tf.math.square(y_diff_gram), axis=(1, 2)))         return y_norm      def calculate_feature_recon_loss(self, y_true, y_pred):         y_diff = y_pred - y_true         num_elts = tf.cast(tf.reduce_prod(y_diff.shape[1:]), tf.float32)         y_diff_norm = tf.reduce_sum(tf.square(y_diff), axis=(1, 2, 3)) / num_elts         return y_diff_norm      def calculate_gram_matrix(self, x):         x = tf.cast(x, tf.float32)         num_elts = tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)         gram_matrix = tf.einsum('bijc,bijd-&gt;bcd', x, x)         gram_matrix /= num_elts         return gram_matrix      def calculate_total_variation(self, y_pred):         return tf.image.total_variation(y_pred)      def forward(self, data, state):         y_pred, y_style, y_content, image_out = data          style_loss = [self.calculate_style_recon_loss(a, b) for a, b in zip(y_style['style'], y_pred['style'])]         style_loss = tf.add_n(style_loss)         style_loss *= self.style_weight          content_loss = [             self.calculate_feature_recon_loss(a, b) for a, b in zip(y_content['content'], y_pred['content'])         ]         content_loss = tf.add_n(content_loss)         content_loss *= self.content_weight          total_variation_reg = self.calculate_total_variation(image_out)         total_variation_reg *= self.tv_weight         loss = style_loss + content_loss + total_variation_reg          if self.average_loss:             loss = reduce_mean(loss)          return loss <p>We now define the <code>Network</code> object:</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),\n    ExtractVGGFeatures(inputs=lambda: style_img_t, outputs=\"y_style\"),\n    ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),\n    ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),\n    StyleContentLoss(style_weight=style_weight,\n                     content_weight=content_weight,\n                     tv_weight=tv_weight,\n                     inputs=('y_pred', 'y_style', 'y_content', 'image_out'),\n                     outputs='loss'),\n    UpdateOp(model=model, loss_name=\"loss\")\n])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp  network = fe.Network(ops=[     ModelOp(inputs=\"image\", model=model, outputs=\"image_out\"),     ExtractVGGFeatures(inputs=lambda: style_img_t, outputs=\"y_style\"),     ExtractVGGFeatures(inputs=\"image\", outputs=\"y_content\"),     ExtractVGGFeatures(inputs=\"image_out\", outputs=\"y_pred\"),     StyleContentLoss(style_weight=style_weight,                      content_weight=content_weight,                      tv_weight=tv_weight,                      inputs=('y_pred', 'y_style', 'y_content', 'image_out'),                      outputs='loss'),     UpdateOp(model=model, loss_name=\"loss\") ]) In\u00a0[11]: Copied! <pre>estimator = fe.Estimator(network=network,\n                         pipeline=pipeline,\n                         traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),\n                         epochs=epochs,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         log_steps=log_steps)\n</pre> estimator = fe.Estimator(network=network,                          pipeline=pipeline,                          traces=ModelSaver(model=model, save_dir=save_dir, frequency=1),                          epochs=epochs,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          log_steps=log_steps) In\u00a0[12]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; style_transfer_net_lr: 0.001; \nFastEstimator-Train: step: 1; loss: 686.76025; \nFastEstimator-Train: step: 2000; loss: 194.85736; steps/sec: 10.82; \nFastEstimator-Train: step: 4000; loss: 172.5411; steps/sec: 10.81; \nFastEstimator-Train: step: 6000; loss: 173.80026; steps/sec: 10.81; \nFastEstimator-Train: step: 8000; loss: 168.31807; steps/sec: 10.81; \nFastEstimator-Train: step: 10000; loss: 169.65088; steps/sec: 10.81; \nFastEstimator-Train: step: 12000; loss: 157.52707; steps/sec: 10.81; \nFastEstimator-Train: step: 14000; loss: 157.95462; steps/sec: 10.82; \nFastEstimator-Train: step: 16000; loss: 156.0791; steps/sec: 10.82; \nFastEstimator-Train: step: 18000; loss: 141.07487; steps/sec: 10.82; \nFastEstimator-Train: step: 20000; loss: 165.1513; steps/sec: 10.82; \nFastEstimator-Train: step: 22000; loss: 142.25858; steps/sec: 10.82; \nFastEstimator-Train: step: 24000; loss: 141.33316; steps/sec: 10.82; \nFastEstimator-Train: step: 26000; loss: 136.16362; steps/sec: 10.82; \nFastEstimator-Train: step: 28000; loss: 159.05832; steps/sec: 10.82; \nFastEstimator-ModelSaver: saved model to /tmp/tmpyby4rzao/style_transfer_net_epoch_1.h5\nFastEstimator-Train: step: 29572; epoch: 1; epoch_time: 2746.03 sec; \nFastEstimator-Train: step: 30000; loss: 140.72166; steps/sec: 10.52; \nFastEstimator-Train: step: 32000; loss: 153.47067; steps/sec: 10.82; \nFastEstimator-Train: step: 34000; loss: 157.17432; steps/sec: 10.82; \nFastEstimator-Train: step: 36000; loss: 145.79706; steps/sec: 10.82; \nFastEstimator-Train: step: 38000; loss: 152.90091; steps/sec: 10.81; \nFastEstimator-Train: step: 40000; loss: 146.31168; steps/sec: 10.81; \nFastEstimator-Train: step: 42000; loss: 148.24469; steps/sec: 10.82; \nFastEstimator-Train: step: 44000; loss: 149.6113; steps/sec: 10.82; \nFastEstimator-Train: step: 46000; loss: 136.75755; steps/sec: 10.82; \nFastEstimator-Train: step: 48000; loss: 149.79001; steps/sec: 10.82; \nFastEstimator-Train: step: 50000; loss: 144.61955; steps/sec: 10.82; \nFastEstimator-Train: step: 52000; loss: 138.5368; steps/sec: 10.82; \nFastEstimator-Train: step: 54000; loss: 144.22594; steps/sec: 10.82; \nFastEstimator-Train: step: 56000; loss: 140.38748; steps/sec: 10.82; \nFastEstimator-Train: step: 58000; loss: 150.7169; steps/sec: 10.82; \nFastEstimator-ModelSaver: saved model to /tmp/tmpyby4rzao/style_transfer_net_epoch_2.h5\nFastEstimator-Train: step: 59144; epoch: 2; epoch_time: 2734.43 sec; \nFastEstimator-Finish: step: 59144; total_time: 5480.64 sec; style_transfer_net_lr: 0.001; \n</pre> In\u00a0[13]: Copied! <pre>data = {\"image\":test_img_path}\nresult = pipeline.transform(data, mode=\"infer\")\ntest_img = np.squeeze(result[\"image\"])\n</pre> data = {\"image\":test_img_path} result = pipeline.transform(data, mode=\"infer\") test_img = np.squeeze(result[\"image\"]) In\u00a0[14]: Copied! <pre>network = fe.Network(ops=[\n    ModelOp(inputs='image', model=model, outputs=\"image_out\")\n])\n\npredictions = network.transform(result, mode=\"infer\")\noutput_img = np.squeeze(predictions[\"image_out\"])\n</pre> network = fe.Network(ops=[     ModelOp(inputs='image', model=model, outputs=\"image_out\") ])  predictions = network.transform(result, mode=\"infer\") output_img = np.squeeze(predictions[\"image_out\"]) In\u00a0[15]: Copied! <pre>output_img_disp = (output_img + 1) * 0.5\ntest_img_disp = (test_img + 1) * 0.5\nplt.figure(figsize=(20,20))\n\nplt.subplot(131)\nplt.imshow(cv2.cvtColor(test_img_disp, cv2.COLOR_BGR2RGB))\nplt.title('Original Image')\nplt.axis('off');\n\nplt.subplot(132)\nplt.imshow(style_img_disp)\nplt.title('Style Image')\nplt.axis('off');\n\nplt.subplot(133)\nplt.imshow(cv2.cvtColor(output_img_disp, cv2.COLOR_BGR2RGB));\nplt.title('Transferred Image')\nplt.axis('off');\n</pre> output_img_disp = (output_img + 1) * 0.5 test_img_disp = (test_img + 1) * 0.5 plt.figure(figsize=(20,20))  plt.subplot(131) plt.imshow(cv2.cvtColor(test_img_disp, cv2.COLOR_BGR2RGB)) plt.title('Original Image') plt.axis('off');  plt.subplot(132) plt.imshow(style_img_disp) plt.title('Style Image') plt.axis('off');  plt.subplot(133) plt.imshow(cv2.cvtColor(output_img_disp, cv2.COLOR_BGR2RGB)); plt.title('Transferred Image') plt.axis('off');"}, {"location": "apphub/style_transfer/fst_coco/fst.html#fast-style-transfer-with-fastestimator", "title": "Fast Style Transfer with FastEstimator\u00b6", "text": "<p>In this notebook we will demonstrate how to do a neural image style transfer with perceptual loss as described in Perceptual Losses for Real-Time Style Transfer and Super-Resolution. Typical neural style transfer involves two images: an image containing semantics that you want to preserve, and another image serving as a reference style. The first image is often referred as the content image and the other image as the style image. In this paper training images from the COCO2014 dataset are used to learn style transfer from any content image.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#downloading-the-data", "title": "Downloading the data\u00b6", "text": "<p>First, we will download training images from the COCO2014 dataset via our dataset API. Downloading the images will take a while.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": "<p>The architecture of our model is a modified ResNet:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#defining-loss", "title": "Defining Loss\u00b6", "text": "<p>The perceptual loss described in the paper is computed based on intermediate layers of VGG16 pretrained on ImageNet; specifically, <code>relu1_2</code>, <code>relu2_2</code>, <code>relu3_3</code>, and <code>relu4_3</code> of VGG16 are used.</p> <p>The style loss term is computed as the squared l2 norm of the difference in Gram Matrix of these feature maps between an input image and the reference style image.</p> <p>The content loss is simply the l2 norm of the difference in <code>relu3_3</code> of the input image and the reference style image. In addition, the method also uses total variation loss to enforce spatial smoothness in the output image.</p> <p>The final loss is a weighted sum of the style loss term, the content loss term (feature reconstruction term in the paper), and the total variation term.</p> <p>We first define a custom <code>TensorOp</code> that outputs intermediate layers of VGG16. Given these intermediate layers returned by the loss network as a dictionary, we define a custom <code>StyleContentLoss</code> class that encapsulates all the logic of the loss calculation.</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#step-3-estimator", "title": "Step 3: Estimator\u00b6", "text": "<p>We can now define the <code>Estimator</code>. We will use <code>Trace</code> to save intermediate models:</p>"}, {"location": "apphub/style_transfer/fst_coco/fst.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/style_transfer/fst_coco/fst.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>Once the training is finished, we will apply the model to perform style transfer on arbitrary images. Here we use a photo of a panda.</p>"}, {"location": "apphub/tabular/dnn/dnn.html", "title": "Breast Cancer Detection", "text": "In\u00a0[1]: Copied! <pre>import tempfile\n\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nimport fastestimator as fe\nfrom fastestimator.dataset.data import breast_cancer\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n</pre> import tempfile  import tensorflow as tf import pandas as pd from sklearn.preprocessing import StandardScaler  import fastestimator as fe from fastestimator.dataset.data import breast_cancer from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy In\u00a0[2]: parameters Copied! <pre>#training parameters\nbatch_size = 4\nepochs = 10\nsave_dir = tempfile.mkdtemp()\nmax_train_steps_per_epoch = None\nmax_eval_steps_per_epoch = None\n</pre> #training parameters batch_size = 4 epochs = 10 save_dir = tempfile.mkdtemp() max_train_steps_per_epoch = None max_eval_steps_per_epoch = None <p>This downloads some tabular data with different features stored in numerical format in a table. We then split the data into train, evaluation, and testing data sets.</p> In\u00a0[3]: Copied! <pre>train_data, eval_data = breast_cancer.load_data()\ntest_data = eval_data.split(0.5)\n</pre> train_data, eval_data = breast_cancer.load_data() test_data = eval_data.split(0.5) <p>This is what the raw data looks like:</p> In\u00a0[4]: Copied! <pre>df = pd.DataFrame.from_dict(train_data.data, orient='index')\ndf.head()\n</pre> df = pd.DataFrame.from_dict(train_data.data, orient='index') df.head() Out[4]: x y 0 [9.029, 17.33, 58.79, 250.5, 0.1066, 0.1413, 0... 1 1 [21.09, 26.57, 142.7, 1311.0, 0.1141, 0.2832, ... 0 2 [9.173, 13.86, 59.2, 260.9, 0.07721, 0.08751, ... 1 3 [10.65, 25.22, 68.01, 347.0, 0.09657, 0.07234,... 1 4 [10.17, 14.88, 64.55, 311.9, 0.1134, 0.08061, ... 1 In\u00a0[5]: Copied! <pre>scaler = StandardScaler()\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\ntest_data[\"x\"] = scaler.transform(test_data[\"x\"])\n</pre> scaler = StandardScaler() train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) test_data[\"x\"] = scaler.transform(test_data[\"x\"]) <p>We create the <code>Pipeline</code> with the usual train, eval, and test data along with the batch size:</p> In\u00a0[6]: Copied! <pre>pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size)\n</pre> pipeline = fe.Pipeline(train_data=train_data, eval_data=eval_data, test_data=test_data, batch_size=batch_size) <p>We first define the neural network in a function that can then be passed on to the FastEstimator <code>Network</code>:</p> In\u00a0[7]: Copied! <pre>def create_dnn():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(16, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n    return model\n</pre> def create_dnn():     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=(30, )))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(16, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dropout(0.5))     model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))     return model In\u00a0[8]: Copied! <pre>model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\")\n])\n</pre> model = fe.build(model_fn=create_dnn, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(inputs=\"x\", model=model, outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\", mode=\"!infer\") ]) In\u00a0[9]: Copied! <pre>traces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=epochs,\n                         log_steps=10,\n                         traces=traces,\n                         max_train_steps_per_epoch=max_train_steps_per_epoch,\n                         max_eval_steps_per_epoch=max_eval_steps_per_epoch)\n</pre> traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     BestModelSaver(model=model, save_dir=save_dir, metric=\"accuracy\", save_best_mode=\"max\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=epochs,                          log_steps=10,                          traces=traces,                          max_train_steps_per_epoch=max_train_steps_per_epoch,                          max_eval_steps_per_epoch=max_eval_steps_per_epoch) In\u00a0[10]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; model_lr: 0.001; \nFastEstimator-Train: step: 1; ce: 0.58930933; \nFastEstimator-Train: step: 10; ce: 1.2191963; steps/sec: 342.02; \nFastEstimator-Train: step: 20; ce: 0.6330318; steps/sec: 422.95; \nFastEstimator-Train: step: 30; ce: 0.68403095; steps/sec: 400.86; \nFastEstimator-Train: step: 40; ce: 0.70622563; steps/sec: 277.93; \nFastEstimator-Train: step: 50; ce: 0.7649698; steps/sec: 443.68; \nFastEstimator-Train: step: 60; ce: 0.70189; steps/sec: 455.18; \nFastEstimator-Train: step: 70; ce: 0.6120157; steps/sec: 486.19; \nFastEstimator-Train: step: 80; ce: 0.6461396; steps/sec: 495.01; \nFastEstimator-Train: step: 90; ce: 0.709924; steps/sec: 397.38; \nFastEstimator-Train: step: 100; ce: 0.69695604; steps/sec: 545.18; \nFastEstimator-Train: step: 110; ce: 0.5406225; steps/sec: 587.24; \nFastEstimator-Train: step: 114; epoch: 1; epoch_time: 3.3 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 114; epoch: 1; ce: 0.49621966; min_ce: 0.49621966; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 120; ce: 0.55340487; steps/sec: 7.94; \nFastEstimator-Train: step: 130; ce: 0.31839967; steps/sec: 308.44; \nFastEstimator-Train: step: 140; ce: 0.16889682; steps/sec: 482.01; \nFastEstimator-Train: step: 150; ce: 0.5158031; steps/sec: 407.94; \nFastEstimator-Train: step: 160; ce: 0.6378304; steps/sec: 386.75; \nFastEstimator-Train: step: 170; ce: 1.1647241; steps/sec: 447.16; \nFastEstimator-Train: step: 180; ce: 0.5274984; steps/sec: 500.64; \nFastEstimator-Train: step: 190; ce: 0.68258667; steps/sec: 481.22; \nFastEstimator-Train: step: 200; ce: 0.35559005; steps/sec: 476.14; \nFastEstimator-Train: step: 210; ce: 0.46034247; steps/sec: 508.85; \nFastEstimator-Train: step: 220; ce: 0.95580393; steps/sec: 548.84; \nFastEstimator-Train: step: 228; epoch: 2; epoch_time: 1.39 sec; \nFastEstimator-Eval: step: 228; epoch: 2; ce: 0.3193086; min_ce: 0.3193086; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 230; ce: 0.33260575; steps/sec: 8.49; \nFastEstimator-Train: step: 240; ce: 0.2510308; steps/sec: 232.08; \nFastEstimator-Train: step: 250; ce: 0.2878321; steps/sec: 666.82; \nFastEstimator-Train: step: 260; ce: 0.1154226; steps/sec: 368.11; \nFastEstimator-Train: step: 270; ce: 0.26300237; steps/sec: 414.99; \nFastEstimator-Train: step: 280; ce: 0.5653368; steps/sec: 421.39; \nFastEstimator-Train: step: 290; ce: 0.5872185; steps/sec: 402.68; \nFastEstimator-Train: step: 300; ce: 0.27621573; steps/sec: 440.24; \nFastEstimator-Train: step: 310; ce: 0.5477217; steps/sec: 481.69; \nFastEstimator-Train: step: 320; ce: 0.4602429; steps/sec: 398.85; \nFastEstimator-Train: step: 330; ce: 0.38244748; steps/sec: 546.57; \nFastEstimator-Train: step: 340; ce: 0.5337428; steps/sec: 571.02; \nFastEstimator-Train: step: 342; epoch: 3; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 342; epoch: 3; ce: 0.18308732; min_ce: 0.18308732; since_best: 0; accuracy: 0.9824561403508771; \nFastEstimator-Train: step: 350; ce: 0.13466343; steps/sec: 8.53; \nFastEstimator-Train: step: 360; ce: 0.22628057; steps/sec: 368.34; \nFastEstimator-Train: step: 370; ce: 0.5836228; steps/sec: 485.06; \nFastEstimator-Train: step: 380; ce: 0.37300625; steps/sec: 409.87; \nFastEstimator-Train: step: 390; ce: 0.2717349; steps/sec: 413.59; \nFastEstimator-Train: step: 400; ce: 0.07554119; steps/sec: 433.84; \nFastEstimator-Train: step: 410; ce: 0.20552614; steps/sec: 439.36; \nFastEstimator-Train: step: 420; ce: 0.28509304; steps/sec: 448.96; \nFastEstimator-Train: step: 430; ce: 0.32158756; steps/sec: 492.58; \nFastEstimator-Train: step: 440; ce: 1.1102628; steps/sec: 525.49; \nFastEstimator-Train: step: 450; ce: 0.31964102; steps/sec: 548.06; \nFastEstimator-Train: step: 456; epoch: 4; epoch_time: 1.4 sec; \nFastEstimator-ModelSaver: saved model to ./model_best_accuracy.h5\nFastEstimator-Eval: step: 456; epoch: 4; ce: 0.105911165; min_ce: 0.105911165; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 460; ce: 0.4391592; steps/sec: 8.37; \nFastEstimator-Train: step: 470; ce: 0.29870045; steps/sec: 297.42; \nFastEstimator-Train: step: 480; ce: 0.03247342; steps/sec: 597.74; \nFastEstimator-Train: step: 490; ce: 0.13323224; steps/sec: 393.92; \nFastEstimator-Train: step: 500; ce: 0.58429027; steps/sec: 405.0; \nFastEstimator-Train: step: 510; ce: 0.2376658; steps/sec: 455.97; \nFastEstimator-Train: step: 520; ce: 0.4150503; steps/sec: 424.88; \nFastEstimator-Train: step: 530; ce: 0.22695109; steps/sec: 451.62; \nFastEstimator-Train: step: 540; ce: 0.42051294; steps/sec: 402.12; \nFastEstimator-Train: step: 550; ce: 0.17364319; steps/sec: 389.83; \nFastEstimator-Train: step: 560; ce: 0.06320181; steps/sec: 466.97; \nFastEstimator-Train: step: 570; ce: 0.13996354; steps/sec: 518.8; \nFastEstimator-Train: step: 570; epoch: 5; epoch_time: 1.46 sec; \nFastEstimator-Eval: step: 570; epoch: 5; ce: 0.066059396; min_ce: 0.066059396; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 580; ce: 0.12985338; steps/sec: 8.17; \nFastEstimator-Train: step: 590; ce: 0.6419388; steps/sec: 373.15; \nFastEstimator-Train: step: 600; ce: 0.2857446; steps/sec: 404.92; \nFastEstimator-Train: step: 610; ce: 0.21400735; steps/sec: 381.65; \nFastEstimator-Train: step: 620; ce: 0.27899668; steps/sec: 394.87; \nFastEstimator-Train: step: 630; ce: 0.31599885; steps/sec: 472.31; \nFastEstimator-Train: step: 640; ce: 0.036415085; steps/sec: 457.09; \nFastEstimator-Train: step: 650; ce: 0.10052729; steps/sec: 461.82; \nFastEstimator-Train: step: 660; ce: 0.40688303; steps/sec: 474.46; \nFastEstimator-Train: step: 670; ce: 0.40816957; steps/sec: 517.75; \nFastEstimator-Train: step: 680; ce: 0.40120217; steps/sec: 555.53; \nFastEstimator-Train: step: 684; epoch: 6; epoch_time: 1.44 sec; \nFastEstimator-Eval: step: 684; epoch: 6; ce: 0.04396173; min_ce: 0.04396173; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 690; ce: 0.20741543; steps/sec: 8.33; \nFastEstimator-Train: step: 700; ce: 0.12485474; steps/sec: 324.64; \nFastEstimator-Train: step: 710; ce: 2.8970864e-05; steps/sec: 534.71; \nFastEstimator-Train: step: 720; ce: 0.110491954; steps/sec: 402.98; \nFastEstimator-Train: step: 730; ce: 0.10486858; steps/sec: 432.34; \nFastEstimator-Train: step: 740; ce: 0.2951797; steps/sec: 421.45; \nFastEstimator-Train: step: 750; ce: 0.65293443; steps/sec: 433.71; \nFastEstimator-Train: step: 760; ce: 0.32570755; steps/sec: 461.43; \nFastEstimator-Train: step: 770; ce: 0.35400242; steps/sec: 433.46; \nFastEstimator-Train: step: 780; ce: 0.023054674; steps/sec: 483.0; \nFastEstimator-Train: step: 790; ce: 0.16433364; steps/sec: 540.17; \nFastEstimator-Train: step: 798; epoch: 7; epoch_time: 1.43 sec; \nFastEstimator-Eval: step: 798; epoch: 7; ce: 0.040205613; min_ce: 0.040205613; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 800; ce: 0.42427045; steps/sec: 8.52; \nFastEstimator-Train: step: 810; ce: 0.39827985; steps/sec: 266.34; \nFastEstimator-Train: step: 820; ce: 0.43165076; steps/sec: 775.07; \nFastEstimator-Train: step: 830; ce: 0.06976031; steps/sec: 412.9; \nFastEstimator-Train: step: 840; ce: 0.37039524; steps/sec: 441.05; \nFastEstimator-Train: step: 850; ce: 0.10960688; steps/sec: 418.01; \nFastEstimator-Train: step: 860; ce: 0.0070317476; steps/sec: 450.1; \nFastEstimator-Train: step: 870; ce: 0.020452987; steps/sec: 434.06; \nFastEstimator-Train: step: 880; ce: 0.12914097; steps/sec: 476.49; \nFastEstimator-Train: step: 890; ce: 0.25528443; steps/sec: 466.42; \nFastEstimator-Train: step: 900; ce: 0.18017673; steps/sec: 549.95; \nFastEstimator-Train: step: 910; ce: 0.31777602; steps/sec: 583.67; \nFastEstimator-Train: step: 912; epoch: 8; epoch_time: 1.41 sec; \nFastEstimator-Eval: step: 912; epoch: 8; ce: 0.028554583; min_ce: 0.028554583; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 920; ce: 0.24684253; steps/sec: 8.42; \nFastEstimator-Train: step: 930; ce: 0.19438684; steps/sec: 365.05; \nFastEstimator-Train: step: 940; ce: 0.1568121; steps/sec: 477.85; \nFastEstimator-Train: step: 950; ce: 0.3368427; steps/sec: 371.39; \nFastEstimator-Train: step: 960; ce: 0.20518681; steps/sec: 411.72; \nFastEstimator-Train: step: 970; ce: 0.13320616; steps/sec: 401.91; \nFastEstimator-Train: step: 980; ce: 0.1800138; steps/sec: 470.79; \nFastEstimator-Train: step: 990; ce: 0.10868286; steps/sec: 421.41; \nFastEstimator-Train: step: 1000; ce: 0.040300086; steps/sec: 467.66; \nFastEstimator-Train: step: 1010; ce: 0.42622733; steps/sec: 505.31; \nFastEstimator-Train: step: 1020; ce: 0.06701453; steps/sec: 530.27; \nFastEstimator-Train: step: 1026; epoch: 9; epoch_time: 1.42 sec; \nFastEstimator-Eval: step: 1026; epoch: 9; ce: 0.019402837; min_ce: 0.019402837; since_best: 0; accuracy: 1.0; \nFastEstimator-Train: step: 1030; ce: 0.27714887; steps/sec: 8.63; \nFastEstimator-Train: step: 1040; ce: 0.074241355; steps/sec: 302.11; \nFastEstimator-Train: step: 1050; ce: 0.025415465; steps/sec: 640.07; \nFastEstimator-Train: step: 1060; ce: 0.21693969; steps/sec: 447.73; \nFastEstimator-Train: step: 1070; ce: 0.120441705; steps/sec: 432.52; \nFastEstimator-Train: step: 1080; ce: 0.25360084; steps/sec: 459.08; \nFastEstimator-Train: step: 1090; ce: 0.22401881; steps/sec: 493.29; \nFastEstimator-Train: step: 1100; ce: 0.112028226; steps/sec: 485.33; \nFastEstimator-Train: step: 1110; ce: 2.4293017; steps/sec: 446.65; \nFastEstimator-Train: step: 1120; ce: 0.19810674; steps/sec: 514.16; \nFastEstimator-Train: step: 1130; ce: 0.12599353; steps/sec: 529.87; \nFastEstimator-Train: step: 1140; ce: 0.23468983; steps/sec: 580.97; \nFastEstimator-Train: step: 1140; epoch: 10; epoch_time: 1.38 sec; \nFastEstimator-Eval: step: 1140; epoch: 10; ce: 0.017586827; min_ce: 0.017586827; since_best: 0; accuracy: 1.0; \nFastEstimator-Finish: step: 1140; total_time: 28.21 sec; model_lr: 0.001; \n</pre> In\u00a0[11]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: epoch: 10; accuracy: 0.9649122807017544; \n</pre>"}, {"location": "apphub/tabular/dnn/dnn.html#breast-cancer-detection", "title": "Breast Cancer Detection\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#import-the-required-libraries", "title": "Import the required libraries\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#download-data", "title": "Download data\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#building-components", "title": "Building Components\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-1-create-pipeline", "title": "Step 1: Create <code>Pipeline</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-2-create-network", "title": "Step 2: Create <code>Network</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#step-3-create-estimator", "title": "Step 3: Create <code>Estimator</code>\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#training", "title": "Training\u00b6", "text": ""}, {"location": "apphub/tabular/dnn/dnn.html#model-testing", "title": "Model testing\u00b6", "text": "<p><code>Estimator.test</code> triggers model testing with the test dataset that was specified in our <code>Pipeline</code>. We can use this to evaluate our model's accuracy on previously unseen data:</p>"}, {"location": "fastestimator/estimator.html", "title": "estimator", "text": ""}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.EarlyStop", "title": "<code>EarlyStop</code>", "text": "<p>         Bases: <code>Exception</code></p> <p>An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>class EarlyStop(Exception):\n\"\"\"An exception raised when the system.stop_training flag is flipped by a Trace in order to abort the training.\n    \"\"\"\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator", "title": "<code>Estimator</code>", "text": "<p>One class to rule them all.</p> <p>Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train (estimator.fit) or test (estimator.test) models. It wraps <code>Pipeline</code>, <code>Network</code>, <code>Trace</code> objects together and defines the whole optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p>An fe.Pipeline object that defines the data processing workflow.</p> required <code>network</code> <code>BaseNetwork</code> <p>An fe.Network object that contains models and other training graph definitions.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to run.</p> required <code>max_train_steps_per_epoch</code> <code>Optional[int]</code> <p>Training will complete after n steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>max_eval_steps_per_epoch</code> <code>Optional[int]</code> <p>Evaluation will complete after n steps even if loader is not yet exhausted. If None, all data will be used.</p> <code>None</code> <code>traces</code> <code>Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]]</code> <p>What Traces to run during training. If None, only the system's default Traces will be included.</p> <code>None</code> <code>log_steps</code> <code>Optional[int]</code> <p>Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch information will still print). None to completely disable printing.</p> <code>100</code> <code>monitor_names</code> <code>Union[None, str, Iterable[str]]</code> <p>Additional keys from the data dictionary to be written into the logs.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>class Estimator:\n\"\"\"One class to rule them all.\n    Estimator is the highest level class within FastEstimator. It is the class which is invoked to actually train\n    (estimator.fit) or test (estimator.test) models. It wraps `Pipeline`, `Network`, `Trace` objects together and\n    defines the whole optimization process.\n    Args:\n        pipeline: An fe.Pipeline object that defines the data processing workflow.\n        network: An fe.Network object that contains models and other training graph definitions.\n        epochs: The number of epochs to run.\n        max_train_steps_per_epoch: Training will complete after n steps even if loader is not yet exhausted. If None,\n            all data will be used.\n        max_eval_steps_per_epoch: Evaluation will complete after n steps even if loader is not yet exhausted. If None,\n            all data will be used.\n        traces: What Traces to run during training. If None, only the system's default Traces will be included.\n        log_steps: Frequency (in steps) for printing log messages. 0 to disable all step-based printing (though epoch\n            information will still print). None to completely disable printing.\n        monitor_names: Additional keys from the data dictionary to be written into the logs.\n    \"\"\"\npipeline: Pipeline\ntraces: List[Union[Trace, Scheduler[Trace]]]\nmonitor_names: Set[str]\ndef __init__(self,\npipeline: Pipeline,\nnetwork: BaseNetwork,\nepochs: int,\nmax_train_steps_per_epoch: Optional[int] = None,\nmax_eval_steps_per_epoch: Optional[int] = None,\ntraces: Union[None, Trace, Scheduler[Trace], Iterable[Union[Trace, Scheduler[Trace]]]] = None,\nlog_steps: Optional[int] = 100,\nmonitor_names: Union[None, str, Iterable[str]] = None):\nself.pipeline = pipeline\nself.network = network\nself.traces = to_list(traces)\nself.traces_in_use = None\nassert log_steps is None or log_steps &gt;= 0, \\\n            \"log_steps must be None or positive (or 0 to disable only train logging)\"\nself.monitor_names = to_set(monitor_names) | self.network.get_loss_keys()\nself.system = System(network=network,\nlog_steps=log_steps,\ntotal_epochs=epochs,\nmax_train_steps_per_epoch=max_train_steps_per_epoch,\nmax_eval_steps_per_epoch=max_eval_steps_per_epoch)\ndef fit(self, summary: Optional[str] = None, warmup: bool = True) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training.\n            warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n                epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n                also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n                mismatch.\n        Returns:\n            A summary object containing the training history for this session iff a `summary` name was provided.\n        \"\"\"\ndraw()\nself.system.reset(summary)\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup()\nself._start(run_modes={\"train\", \"eval\"})\nreturn self.system.summary or None\ndef _prepare_traces(self, run_modes: Set[str]) -&gt; None:\n\"\"\"Prepare information about the traces for training.\n        Add default traces into the traces_in_use list, also prints a warning if no model saver trace is detected.\n        Args:\n            run_modes: The current execution modes.\n        \"\"\"\nself.traces_in_use = [trace for trace in self.traces]\nif self.system.log_steps is not None:\nself.traces_in_use.append(Logger())\nif \"train\" in run_modes:\nself.traces_in_use.insert(0, TrainEssential(monitor_names=self.monitor_names))\nno_save_warning = True\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\nif isinstance(trace, (ModelSaver, BestModelSaver)):\nno_save_warning = False\nif no_save_warning:\nprint(\"FastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\")\nif \"eval\" in run_modes and \"eval\" in self.pipeline.get_modes():\nself.traces_in_use.insert(1, EvalEssential(monitor_names=self.monitor_names))\n# insert system instance to trace\nfor trace in get_current_items(self.traces_in_use, run_modes=run_modes):\ntrace.system = self.system\ndef test(self, summary: Optional[str] = None) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n        Args:\n            summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n                a summary object at the end of training. If None, the default value will be whatever `summary` name was\n                most recently provided to this Estimator's .fit() or .test() methods.\n        Returns:\n            A summary object containing the training history for this session iff the `summary` name is not None (after\n            considering the default behavior above).\n        \"\"\"\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"})\nreturn self.system.summary or None\n@staticmethod\ndef _sort_traces(traces: List[Trace], available_outputs: Optional[Set[str]] = None) -&gt; List[Trace]:\n\"\"\"Sort traces to attempt to resolve any dependency issues.\n        This is essentially a topological sort, but it doesn't seem worthwhile to convert the data into a graph\n        representation in order to get the slightly better asymptotic runtime complexity.\n        Args:\n            traces: A list of traces (not inside schedulers) to be sorted.\n            available_outputs: What output keys are already available for the traces to use. If None are provided, the\n                sorting algorithm will assume that any keys not generated by traces are being provided by the system.\n                This results in a less rigorous sorting.\n        Returns:\n            The sorted list of `traces`.\n        Raises:\n            AssertionError: If Traces have circular dependencies or require input keys which are not available.\n        \"\"\"\nsorted_traces = []\ntrace_outputs = {output for trace in traces for output in trace.outputs}\nif available_outputs is None:\n# Assume that anything not generated by a Trace is provided by the system\navailable_outputs = {inp for trace in traces for inp in trace.inputs} - trace_outputs\nweak_sort = True\nelse:\navailable_outputs = to_set(available_outputs)\nweak_sort = False\nend_traces = deque()\nintermediate_traces = deque()\nintermediate_outputs = set()\ntrace_deque = deque(traces)\nwhile trace_deque:\ntrace = trace_deque.popleft()\nins = set(trace.inputs)\nouts = set(trace.outputs)\nif not ins or isinstance(trace, (TrainEssential, EvalEssential)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelif \"*\" in ins:\nif outs:\nend_traces.appendleft(trace)\nelse:\nend_traces.append(trace)\nelif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nelse:\nintermediate_traces.append(trace)\nintermediate_outputs |= outs\nalready_seen = set()\nwhile intermediate_traces:\ntrace = intermediate_traces.popleft()\nins = set(trace.inputs)\nouts = set(trace.outputs)\nalready_seen.add(trace)\nif ins &lt;= available_outputs or (weak_sort and (ins - outs - available_outputs).isdisjoint(trace_outputs)):\nsorted_traces.append(trace)\navailable_outputs |= outs\nalready_seen.clear()\nelif ins &lt;= (available_outputs | intermediate_outputs):\nintermediate_traces.append(trace)\nelse:\nraise AssertionError(\"The {} trace has unsatisfiable inputs: {}\".format(\ntype(trace).__name__, \", \".join(ins - (available_outputs | intermediate_outputs))))\nif intermediate_traces and len(already_seen) == len(intermediate_traces):\nraise AssertionError(\"Dependency cycle detected amongst traces: {}\".format(\", \".join(\n[type(tr).__name__ for tr in already_seen])))\nsorted_traces.extend(list(end_traces))\nreturn sorted_traces\ndef _warmup(self) -&gt; None:\n\"\"\"Perform a test run of each pipeline and network signature epoch to make sure that training won't fail later.\n        Traces are not executed in the warmup since they are likely to contain state variables which could become\n        corrupted by running extra steps.\n        \"\"\"\nall_traces = get_current_items(self.traces_in_use, run_modes={\"train\", \"eval\"})\nself._sort_traces(all_traces)\nmonitor_names = self.monitor_names\nfor mode in self.pipeline.get_modes() - {\"test\"}:\nscheduled_items = self.pipeline.get_scheduled_items(mode) + self.network.get_scheduled_items(\nmode) + self.get_scheduled_items(mode)\nsignature_epochs = get_signature_epochs(scheduled_items, self.system.total_epochs, mode=mode)\nepochs_with_data = self.pipeline.get_epochs_with_data(total_epochs=self.system.total_epochs, mode=mode)\nfor epoch in signature_epochs:\nif epoch not in epochs_with_data:\ncontinue\n# key checking\nloader = self._configure_loader(self.pipeline.get_loader(mode, epoch))\nwith Suppressor():\nif isinstance(loader, tf.data.Dataset):\nbatch = list(loader.take(1))[0]\nelse:\nbatch = next(iter(loader))\nbatch = self._configure_tensor(loader, batch)\nassert isinstance(batch, dict), \"please make sure data output format is dictionary\"\npipeline_output_keys = to_set(batch.keys())\nnetwork_output_keys = self.network.get_all_output_keys(mode, epoch)\ntrace_input_keys = set()\ntrace_output_keys = {\"*\"}\ntraces = get_current_items(self.traces_in_use, run_modes=mode, epoch=epoch)\nfor idx, trace in enumerate(traces):\nif idx &gt; 0:  # ignore TrainEssential and EvalEssential's inputs for unmet requirement checking\ntrace_input_keys.update(trace.inputs)\ntrace_output_keys.update(trace.outputs)\nmonitor_names = monitor_names - (pipeline_output_keys | network_output_keys)\nunmet_requirements = trace_input_keys - (pipeline_output_keys | network_output_keys | trace_output_keys)\nassert not unmet_requirements, \\\n                    \"found missing key(s) during epoch {} mode {}: {}\".format(epoch, mode, unmet_requirements)\nself._sort_traces(traces, available_outputs=pipeline_output_keys | network_output_keys)\ntrace_input_keys.update(traces[0].inputs)\nself.network.load_epoch(mode, epoch, output_keys=trace_input_keys, warmup=True)\nself.network.run_step(batch)\nself.network.unload_epoch()\nassert not monitor_names, \"found missing key(s): {}\".format(monitor_names)\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in estimator.\n        \"\"\"\nreturn self.traces_in_use\ndef _start(self, run_modes: Set[str]) -&gt; None:\n\"\"\"The outer training loop.\n        This method invokes the trace on_begin method, runs the necessary 'train' and 'eval' epochs, and then invokes\n        the trace on_end method.\n        Args:\n            run_modes: The current execution modes.\n        \"\"\"\nall_traces = get_current_items(self.traces_in_use, run_modes=run_modes)\nself._sort_traces(all_traces)\nself._run_traces_on_begin(traces=all_traces)\ntry:\nif \"train\" in run_modes or \"eval\" in run_modes:\nfor self.system.epoch_idx in range(self.system.epoch_idx + 1, self.system.total_epochs + 1):\nif \"train\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"train\"\nself._run_epoch()\nif \"eval\" in self.pipeline.get_modes(epoch=self.system.epoch_idx):\nself.system.mode = \"eval\"\nself._run_epoch()\nelse:\nself._run_epoch()\nexcept EarlyStop:\npass  # On early stopping we still want to run the final traces and return results\nself._run_traces_on_end(traces=all_traces)\ndef _run_epoch(self) -&gt; None:\n\"\"\"A method to perform an epoch of activity.\n        This method requires that the current mode and epoch already be specified within the self.system object.\n        \"\"\"\ntraces = get_current_items(self.traces_in_use, run_modes=self.system.mode, epoch=self.system.epoch_idx)\ntrace_input_keys = set()\nfor trace in traces:\ntrace_input_keys.update(trace.inputs)\nloader = self._configure_loader(self.pipeline.get_loader(self.system.mode, self.system.epoch_idx))\niterator = iter(loader)\nself.network.load_epoch(mode=self.system.mode, epoch=self.system.epoch_idx, output_keys=trace_input_keys)\nself.system.batch_idx = None\nwith Suppressor():\nbatch = next(iterator)\ntraces = self._sort_traces(\ntraces,\navailable_outputs=to_set(batch.keys())\n| self.network.get_all_output_keys(self.system.mode, self.system.epoch_idx))\nself._run_traces_on_epoch_begin(traces=traces)\nwhile True:\ntry:\nif self.system.mode == \"train\":\nself.system.update_global_step()\nself.system.update_batch_idx()\nself._run_traces_on_batch_begin(traces=traces)\nbatch = self._configure_tensor(loader, batch)\nbatch, prediction = self.network.run_step(batch)\nself._run_traces_on_batch_end(batch, prediction, traces=traces)\nif isinstance(loader, DataLoader) and (\n(self.system.batch_idx == self.system.max_train_steps_per_epoch and self.system.mode == \"train\") or\n(self.system.batch_idx == self.system.max_eval_steps_per_epoch and self.system.mode == \"eval\")):\nraise StopIteration\nwith Suppressor():\nbatch = next(iterator)\nexcept StopIteration:\nbreak\nself._run_traces_on_epoch_end(traces=traces)\nself.network.unload_epoch()\ndef _configure_loader(self, loader: Union[DataLoader, tf.data.Dataset]) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"A method to configure a given dataloader for use with this Estimator's Network.\n        This method will ensure that the `loader` returns the correct data type (tf.Tensor or torch.Tensor) depending on\n         the requirements of the Network. It also handles issues with multi-gpu data sharding.\n        Args:\n            loader: A data loader to be modified.\n        Returns:\n            The potentially modified dataloader to be used for training.\n        \"\"\"\nnew_loader = loader\nif isinstance(new_loader, DataLoader) and isinstance(self.network, TFNetwork):\nadd_batch = True\nif hasattr(loader.dataset, \"dataset\") and isinstance(loader.dataset.dataset, BatchDataset):\nadd_batch = False\nbatch = to_tensor(loader.dataset[0], target_type=\"tf\")\ndata_type = to_type(batch)\ndata_shape = to_shape(batch, add_batch=add_batch, exact_shape=False)\nnew_loader = tf.data.Dataset.from_generator(lambda: loader, data_type, output_shapes=data_shape)\nnew_loader = new_loader.prefetch(1)\nif isinstance(new_loader, tf.data.Dataset):\nif self.system.max_train_steps_per_epoch and self.system.mode == \"train\":\nnew_loader = new_loader.take(self.system.max_train_steps_per_epoch)\nif self.system.max_eval_steps_per_epoch and self.system.mode == \"eval\":\nnew_loader = new_loader.take(self.system.max_eval_steps_per_epoch)\nif isinstance(tf.distribute.get_strategy(),\ntf.distribute.MirroredStrategy) and not isinstance(new_loader, DistributedDataset):\nnew_loader = tf.distribute.get_strategy().experimental_distribute_dataset(new_loader)\nreturn new_loader\ndef _configure_tensor(self, loader: Union[DataLoader, tf.data.Dataset], batch: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"A function to convert a batch of tf.Tensors to torch.Tensors if required.\n        Returns:\n            Either the original `batch`, or the `batch` converted to torch.Tensors if required.\n        \"\"\"\nif isinstance(loader, tf.data.Dataset) and isinstance(self.network, TorchNetwork):\nbatch = to_tensor(batch, target_type=\"torch\")\nreturn batch\ndef _run_traces_on_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_begin(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_epoch_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_epoch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_begin(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_begin methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_batch_begin(data)\nself._check_early_exit()\ndef _run_traces_on_batch_end(self, batch: Dict[str, Any], prediction: Dict[str, Any],\ntraces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_batch_end methods of given traces.\n        Args:\n            batch: The batch data which was provided by the pipeline.\n            prediction: The prediction data which was generated by the network.\n            traces: List of traces.\n        \"\"\"\ndata = Data(ChainMap(prediction, batch))\nfor trace in traces:\ntrace.on_batch_end(data)\nself._check_early_exit()\ndef _run_traces_on_epoch_end(self, traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_epoch_end methods of of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_epoch_end(data)\nself._check_early_exit()\n@staticmethod\ndef _run_traces_on_end(traces: Iterable[Trace]) -&gt; None:\n\"\"\"Invoke the on_end methods of given traces.\n        Args:\n            traces: List of traces.\n        \"\"\"\ndata = Data()\nfor trace in traces:\ntrace.on_end(data)\ndef _check_early_exit(self) -&gt; None:\n\"\"\"Determine whether training should be prematurely aborted.\n        Raises:\n            EarlyStop: If the system.stop_training flag has been set to True.\n        \"\"\"\nif self.system.stop_training:\nraise EarlyStop\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.fit", "title": "<code>fit</code>", "text": "<p>Train the network for the number of epochs specified by the estimator's constructor.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to perform warmup before training begins. The warmup procedure will test one step at every epoch where schedulers cause the execution graph to change. This can take some time up front, but can also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size mismatch.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff a <code>summary</code> name was provided.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def fit(self, summary: Optional[str] = None, warmup: bool = True) -&gt; Optional[Summary]:\n\"\"\"Train the network for the number of epochs specified by the estimator's constructor.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training.\n        warmup: Whether to perform warmup before training begins. The warmup procedure will test one step at every\n            epoch where schedulers cause the execution graph to change. This can take some time up front, but can\n            also save significant heartache on epoch 300 when the training unexpectedly fails due to a tensor size\n            mismatch.\n    Returns:\n        A summary object containing the training history for this session iff a `summary` name was provided.\n    \"\"\"\ndraw()\nself.system.reset(summary)\nself._prepare_traces(run_modes={\"train\", \"eval\"})\nif warmup:\nself._warmup()\nself._start(run_modes={\"train\", \"eval\"})\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in estimator.</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in estimator.\n    \"\"\"\nreturn self.traces_in_use\n</code></pre>"}, {"location": "fastestimator/estimator.html#fastestimator.fastestimator.estimator.Estimator.test", "title": "<code>test</code>", "text": "<p>Run the pipeline / network in test mode for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>summary</code> <code>Optional[str]</code> <p>A name for the experiment. If provided, the log history will be recorded in-memory and returned as a summary object at the end of training. If None, the default value will be whatever <code>summary</code> name was most recently provided to this Estimator's .fit() or .test() methods.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Summary]</code> <p>A summary object containing the training history for this session iff the <code>summary</code> name is not None (after</p> <code>Optional[Summary]</code> <p>considering the default behavior above).</p> Source code in <code>fastestimator\\fastestimator\\estimator.py</code> <pre><code>def test(self, summary: Optional[str] = None) -&gt; Optional[Summary]:\n\"\"\"Run the pipeline / network in test mode for one epoch.\n    Args:\n        summary: A name for the experiment. If provided, the log history will be recorded in-memory and returned as\n            a summary object at the end of training. If None, the default value will be whatever `summary` name was\n            most recently provided to this Estimator's .fit() or .test() methods.\n    Returns:\n        A summary object containing the training history for this session iff the `summary` name is not None (after\n        considering the default behavior above).\n    \"\"\"\nself.system.reset_for_test(summary)\nself._prepare_traces(run_modes={\"test\"})\nself._start(run_modes={\"test\"})\nreturn self.system.summary or None\n</code></pre>"}, {"location": "fastestimator/network.html", "title": "network", "text": ""}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork", "title": "<code>BaseNetwork</code>", "text": "<p>A base class for Network objects.</p> <p>Networks are used to define the computation graph surrounding one or more models during training.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The operators to be executed throughout training / testing / inference. These are likely to contain one or more model ops, as well as loss ops and update ops.</p> required Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>class BaseNetwork:\n\"\"\"A base class for Network objects.\n    Networks are used to define the computation graph surrounding one or more models during training.\n    Args:\n        ops: The operators to be executed throughout training / testing / inference. These are likely to contain one or\n            more model ops, as well as loss ops and update ops.\n    \"\"\"\ndef __init__(self, ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; None:\nself.ops = to_list(ops)\nself.models = to_list(_collect_models(ops))\nself._verify_inputs()\nself.effective_inputs = dict()\nself.effective_outputs = dict()\nself.epoch_ops = []\nself.epoch_models = set()\nself.epoch_state = dict()\ndef _verify_inputs(self) -&gt; None:\n\"\"\"Ensure that all ops are TensorOps.\n        Raises:\n            AssertionError: If any of the ops are not TensorOps.\n        \"\"\"\nfor op in get_current_items(self.ops):\nassert isinstance(op, TensorOp), \"unsupported op format, must provide TensorOp in Network\"\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Network.\n        \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models]\nelse:\nall_items = self.ops\nreturn all_items\ndef load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch.\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(output_keys)\nself.epoch_ops = get_current_items(self.ops, mode, epoch)\nself.epoch_models = set(op.model for op in self.epoch_ops if isinstance(op, (UpdateOp, ModelOp)))\ngradient_ops = [op for op in self.epoch_ops if hasattr(op, \"retain_graph\")]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.retain_graph = idx != len(gradient_ops) - 1\nself.epoch_state = {\"warmup\": warmup, \"mode\": mode, \"req_grad\": len(gradient_ops) &gt; 0}\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        \"\"\"\npass\ndef get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n        Returns:\n            All of the keys associated with model losses in this network.\n        \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nif isinstance(op, UpdateOp):\nloss_keys.update(op.inputs)\nreturn loss_keys\ndef get_effective_input_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider for determining inputs.\n        Returns:\n            The necessary inputs for the network to execute the given `epoch` and `mode`.\n        \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\ndef get_all_output_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n        Args:\n            mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch number to consider when searching for outputs.\n        Returns:\n            The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n        \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\noutput_keys.update(op.outputs)\nreturn output_keys\n@staticmethod\ndef _forward_batch(batch: MutableMapping[str, Any], state: Dict[str, Any], ops: List[TensorOp]) -&gt; None:\n\"\"\"Run a forward pass through the network's Op chain given a `batch` of data.\n        Args:\n            batch: A batch of input data. Predictions from the network will be written back into this dictionary.\n            state: A dictionary holding information about the current execution context. The TF gradient tape, for\n                example will be stored here.\n            ops: Which ops to execute.\n        \"\"\"\nfor op in ops:\ndata = get_inputs_by_op(op, batch)\ndata = op.forward(data, state)\nif op.outputs:\nwrite_outputs_by_op(op, batch, data)\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nraise NotImplementedError\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_all_output_keys", "title": "<code>get_all_output_keys</code>", "text": "<p>Get all of the keys that will be generated by the network during the given <code>epoch</code> and <code>mode</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider when searching for outputs.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The keys that will be generated by the network's Ops during the <code>epoch</code> for the given <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_all_output_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Get all of the keys that will be generated by the network during the given `epoch` and `mode`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider when searching for outputs.\n    Returns:\n        The keys that will be generated by the network's Ops during the `epoch` for the given `mode`.\n    \"\"\"\noutput_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\noutput_keys.update(op.outputs)\nreturn output_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_effective_input_keys", "title": "<code>get_effective_input_keys</code>", "text": "<p>Determine which keys need to be provided as input to the network during the given <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to consider for determining inputs.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The necessary inputs for the network to execute the given <code>epoch</code> and <code>mode</code>.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_effective_input_keys(self, mode: str, epoch: int) -&gt; Set[str]:\n\"\"\"Determine which keys need to be provided as input to the network during the given `epoch`.\n    Args:\n        mode: The execution mode to consider. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch number to consider for determining inputs.\n    Returns:\n        The necessary inputs for the network to execute the given `epoch` and `mode`.\n    \"\"\"\ninput_keys = set()\nproduced_keys = set()\nfor op in get_current_items(self.ops, mode, epoch):\ninput_keys.update(set(key for key in op.inputs if key not in produced_keys))\nproduced_keys.update(op.outputs)\nreturn input_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_loss_keys", "title": "<code>get_loss_keys</code>", "text": "<p>Find all of the keys associated with model losses.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>All of the keys associated with model losses in this network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_loss_keys(self) -&gt; Set[str]:\n\"\"\"Find all of the keys associated with model losses.\n    Returns:\n        All of the keys associated with model losses in this network.\n    \"\"\"\nloss_keys = set()\nfor op in get_current_items(self.ops):\nif isinstance(op, UpdateOp):\nloss_keys.update(op.inputs)\nreturn loss_keys\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Network.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Network.\n    \"\"\"\nif mode == \"train\":\nall_items = self.ops + [model.optimizer for model in self.models]\nelse:\nall_items = self.ops\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch.\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n    \"\"\"\nself.effective_inputs[mode] = self.get_effective_input_keys(mode, epoch)\nself.effective_outputs[mode] = self.get_all_output_keys(mode, epoch)\nif output_keys:\nself.effective_outputs[mode] = self.effective_outputs[mode].intersection(output_keys)\nself.epoch_ops = get_current_items(self.ops, mode, epoch)\nself.epoch_models = set(op.model for op in self.epoch_ops if isinstance(op, (UpdateOp, ModelOp)))\ngradient_ops = [op for op in self.epoch_ops if hasattr(op, \"retain_graph\")]\nfor idx, gradient_op in enumerate(gradient_ops):\ngradient_op.retain_graph = idx != len(gradient_ops) - 1\nself.epoch_state = {\"warmup\": warmup, \"mode\": mode, \"req_grad\": len(gradient_ops) &gt; 0}\nfor model in self.epoch_models:\nif hasattr(model, \"optimizer\") and model.optimizer is not None:\nif isinstance(model.optimizer, Scheduler):\nmodel.current_optimizer = model.optimizer.get_current_value(epoch)\nelse:\nmodel.current_optimizer = model.optimizer\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:  # Batch, Prediction\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.BaseNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork", "title": "<code>TFNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for TensorFlow models.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>class TFNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for TensorFlow models.\n    \"\"\"\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nif self.epoch_state[\"warmup\"]:\nprediction = strategy.experimental_run_v2(\nself._forward_step_eager,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nelse:\nprediction = strategy.experimental_run_v2(\nself._forward_step_static,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nbatch = self._per_replica_to_global(batch)\nprediction = self._per_replica_to_global(prediction)\nelse:\nif self.epoch_state[\"warmup\"]:\nprediction = self._forward_step_eager(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nelse:\nprediction = self._forward_step_static(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nreturn batch, prediction\ndef _per_replica_to_global(self, data: T) -&gt; T:\n\"\"\"Combine data from \"per-replica\" values recursively.\n        For multi-GPU training, data are distributed using `tf.distribute.Strategy.experimental_distribute_dataset`.\n        This method collects data from all replicas and combines them into one.\n        Args:\n            data: Distributed data.\n        Returns:\n            Combined data from all replicas.\n        \"\"\"\nif isinstance(data, DistributedValues):\nif data.values[0].shape.rank == 0:\nreturn tf.reduce_mean(tuple(d for d in data.values if not tf.math.is_nan(d)))\nelse:\nreturn tf.concat(data.values, axis=0)\nelif isinstance(data, dict):\nresult = {}\nfor key, val in data.items():\nresult[key] = self._per_replica_to_global(val)\nreturn result\nelif isinstance(data, list):\nreturn [self._per_replica_to_global(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._per_replica_to_global(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._per_replica_to_global(val) for val in data])\nelse:\nreturn data\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Filter input data so that only the data required by the Network is moved onto the GPU.\n        Args:\n            batch: An unfiltered batch of input data.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The filtered input data ready for use on GPU(s).\n        \"\"\"\nnew_batch = {}\nfor key in self.effective_inputs[mode]:\nif key in batch:\nnew_batch[key] = batch[key]\nreturn new_batch\ndef _forward_step_eager(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in eager (non-static graph) mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = ChainMap({}, batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\n@tf.function\ndef _forward_step_static(self,\nbatch: Dict[str, Any],\nstate: Dict[str, Any],\nops: List[TensorOp],\neffective_outputs: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step of the Network in static graph mode.\n        Args:\n            batch: The input data for the Network.\n            state: A dictionary containing information about the current execution environment, including the active\n                gradient tape.\n            ops: A list of Ops to run during the forward step.\n            effective_outputs: Which outputs should be copied from the GPU back onto the CPU for further use in Traces.\n        Returns:\n            The prediction dictionary resulting from a forward pass of the Network.\n        \"\"\"\nbatch = ChainMap({}, batch)\nprediction = {}\nwith tf.GradientTape(persistent=True) if state[\"req_grad\"] else NonContext() as tape:\nstate['tape'] = tape\nself._forward_batch(batch, state, ops)\ndel state['tape']\ndel tape\nfor key in effective_outputs:\nif key in batch:\nprediction[key] = batch[key]\nreturn prediction\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nself.load_epoch(mode, epoch, warmup=True)\ndata = to_tensor(data, target_type=\"tf\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\n# handle tensorflow multi-gpu inferencing issue, it will replicate data on each device\nif isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\nprediction = self._subsample_data(prediction, get_batch_size(data))\ndata.update(prediction)\nreturn data\ndef _subsample_data(self, data: T, n: int) -&gt; T:\n\"\"\"Subsample data by selecting the first n indices recursively.\n        Args:\n            data: The data to be subsampled.\n        Returns:\n            Subsampled data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._subsample_data(val, n) for (key, val) in data.items()}\nelif isinstance(data, list):\nreturn [self._subsample_data(val, n) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._subsample_data(val, n) for val in data])\nelif isinstance(data, set):\nreturn set([self._subsample_data(val, n) for val in data])\nelif hasattr(data, \"shape\") and list(data.shape) and data.shape[0] &gt; n:\nreturn data[0:n]\nelse:\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nif self.epoch_state[\"warmup\"]:\nprediction = strategy.experimental_run_v2(\nself._forward_step_eager,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nelse:\nprediction = strategy.experimental_run_v2(\nself._forward_step_static,\nargs=(batch_in, self.epoch_state, self.epoch_ops, to_list(self.effective_outputs[mode])))\nbatch = self._per_replica_to_global(batch)\nprediction = self._per_replica_to_global(prediction)\nelse:\nif self.epoch_state[\"warmup\"]:\nprediction = self._forward_step_eager(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nelse:\nprediction = self._forward_step_static(batch_in,\nself.epoch_state,\nself.epoch_ops,\nto_list(self.effective_outputs[mode]))\nreturn batch, prediction\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TFNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nself.load_epoch(mode, epoch, warmup=True)\ndata = to_tensor(data, target_type=\"tf\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\n# handle tensorflow multi-gpu inferencing issue, it will replicate data on each device\nif isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\nprediction = self._subsample_data(prediction, get_batch_size(data))\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork", "title": "<code>TorchNetwork</code>", "text": "<p>         Bases: <code>BaseNetwork</code></p> <p>An extension of BaseNetwork for PyTorch models.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>The ops defining the execution graph for this Network.</p> required Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>class TorchNetwork(BaseNetwork):\n\"\"\"An extension of BaseNetwork for PyTorch models.\n    Args:\n        ops: The ops defining the execution graph for this Network.\n    \"\"\"\ndef __init__(self, ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; None:\nsuper().__init__(ops)\nself.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndef load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n        This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n        every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n        Args:\n            mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch to prepare to execute.\n            output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n            warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n        \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\ndef _move_optimizer_between_device(self, data: Dict[str, Any], device: str) -&gt; None:\n\"\"\"Move optimizer state between gpu and cpu recursively.\n        Args:\n            data: Optimizer state.\n            device: The target device.\n        \"\"\"\nfor key in data:\nif isinstance(data[key], dict):\nself._move_optimizer_between_device(data[key], device)\nelse:\ntry:\ndata[key] = data[key].to(device)\nexcept:\npass\ndef unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n        In this case we move all of the models from the GPU(s) back to the CPU.\n        \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\ndef _get_effective_batch_input(self, batch: MutableMapping[str, Any], mode: str) -&gt; Dict[str, Any]:\n\"\"\"Copy input data from the the CPU onto the GPU(s).\n        This method will filter inputs from the batch so that only data required by the network during execution will be\n        copied to the GPU.\n        Args:\n            batch: The input data to be moved.\n            mode: The current execution mode. One of 'train', 'eval', 'test', or 'infer'.\n        Returns:\n            The input data ready for use on GPU(s).\n        \"\"\"\nif self.device.type == \"cuda\":\nnew_batch = {\nkey: self._move_tensor_between_device(batch[key], self.device)\nfor key in self.effective_inputs[mode] if key in batch\n}\nelse:\nnew_batch = {key: batch[key] for key in self.effective_inputs[mode] if key in batch}\nreturn new_batch\ndef run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n        Implementations of this method within derived classes should handle bringing the prediction data back from the\n        (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n        Args:\n            batch: The batch of data serving as input to the Network.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nself.epoch_state[\"tape\"] = NonContext()\n# gpu operation\nwith torch.no_grad() if not self.epoch_state[\"req_grad\"] else NonContext():\nself._forward_batch(batch_in, self.epoch_state, self.epoch_ops)\n# copy data to cpu\nif self.device.type == \"cuda\":\nprediction = {\nkey: self._move_tensor_between_device(self._detach_tensor(batch_in[key]), \"cpu\")\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nelse:\nprediction = {\nkey: self._detach_tensor(batch_in[key])\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nreturn batch, prediction\ndef _move_tensor_between_device(self, data: T, device: str) -&gt; T:\n\"\"\"Move tensor between gpu and cpu recursively.\n        Args:\n            data: The input data to be moved.\n            device: The target device.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._move_tensor_between_device(value, device) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._move_tensor_between_device(val, device) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, set):\nreturn set([self._move_tensor_between_device(val, device) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.to(device)\nelse:\nreturn data\ndef _detach_tensor(self, data: T) -&gt; T:\n\"\"\"Detach tensor from current graph recursively.\n        Args:\n            data: The data to be detached.\n        Returns:\n            Output data.\n        \"\"\"\nif isinstance(data, dict):\nreturn {key: self._detach_tensor(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [self._detach_tensor(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([self._detach_tensor(val) for val in data])\nelif isinstance(data, set):\nreturn set([self._detach_tensor(val) for val in data])\nelif isinstance(data, torch.Tensor):\nreturn data.detach()\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n        Args:\n            data: The element to data to use as input.\n            mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n            epoch: The epoch in which to run the transform.\n        Returns:\n            (batch_data, prediction_data)\n        \"\"\"\nself.load_epoch(mode, epoch, warmup=True)\ndata = to_tensor(data, \"torch\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.load_epoch", "title": "<code>load_epoch</code>", "text": "<p>Prepare the network to run a given epoch and mode.</p> <p>This method is necessary since schedulers and op mode restrictions may result in different computation graphs every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch to prepare to execute.</p> required <code>output_keys</code> <code>Optional[Set[str]]</code> <p>What keys must be moved from the GPU back to the CPU after executing a step.</p> <code>None</code> <code>warmup</code> <code>bool</code> <p>Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def load_epoch(self, mode: str, epoch: int, output_keys: Optional[Set[str]] = None, warmup: bool = False) -&gt; None:\n\"\"\"Prepare the network to run a given epoch and mode.\n    This method is necessary since schedulers and op mode restrictions may result in different computation graphs\n    every epoch. This also moves all of the necessary models from the CPU onto the GPU(s).\n    Args:\n        mode: The mode to prepare to execute. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch to prepare to execute.\n        output_keys: What keys must be moved from the GPU back to the CPU after executing a step.\n        warmup: Whether to prepare to execute it warmup mode or not (end users can likely ignore this argument).\n    \"\"\"\nsuper().load_epoch(mode, epoch, output_keys, warmup)\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to gpu\nmodel.to(self.device)\n# move optimizer variables to gpu\nself._move_optimizer_between_device(model.current_optimizer.state, self.device)\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.run_step", "title": "<code>run_step</code>", "text": "<p>Run a forward step through the Network on a batch of data.</p> <p>Implementations of this method within derived classes should handle bringing the prediction data back from the (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch of data serving as input to the Network.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, Any], Dict[str, Any]]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def run_step(self, batch: Dict[str, Any]) -&gt; Tuple[Dict[str, Any], Dict[str, Any]]:\n\"\"\"Run a forward step through the Network on a batch of data.\n    Implementations of this method within derived classes should handle bringing the prediction data back from the\n    (multi-)GPU environment to the CPU. This method expects that Network.load_epoch() has already been invoked.\n    Args:\n        batch: The batch of data serving as input to the Network.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nmode = self.epoch_state[\"mode\"]\nbatch_in = self._get_effective_batch_input(batch, mode)\nself.epoch_state[\"tape\"] = NonContext()\n# gpu operation\nwith torch.no_grad() if not self.epoch_state[\"req_grad\"] else NonContext():\nself._forward_batch(batch_in, self.epoch_state, self.epoch_ops)\n# copy data to cpu\nif self.device.type == \"cuda\":\nprediction = {\nkey: self._move_tensor_between_device(self._detach_tensor(batch_in[key]), \"cpu\")\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nelse:\nprediction = {\nkey: self._detach_tensor(batch_in[key])\nfor key in self.effective_outputs[mode] if key in batch_in\n}\nreturn batch, prediction\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.transform", "title": "<code>transform</code>", "text": "<p>Run a forward step through the Network on an element of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The element to data to use as input.</p> required <code>mode</code> <code>str</code> <p>The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.</p> required <code>epoch</code> <code>int</code> <p>The epoch in which to run the transform.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>(batch_data, prediction_data)</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Run a forward step through the Network on an element of data.\n    Args:\n        data: The element to data to use as input.\n        mode: The mode in which to run the transform. One of 'train', 'eval', 'test', or 'infer'.\n        epoch: The epoch in which to run the transform.\n    Returns:\n        (batch_data, prediction_data)\n    \"\"\"\nself.load_epoch(mode, epoch, warmup=True)\ndata = to_tensor(data, \"torch\")\ndata, prediction = self.run_step(data)\nself.unload_epoch()\ndata.update(prediction)\nreturn data\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.TorchNetwork.unload_epoch", "title": "<code>unload_epoch</code>", "text": "<p>Clean up the network after running an epoch.</p> <p>In this case we move all of the models from the GPU(s) back to the CPU.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def unload_epoch(self) -&gt; None:\n\"\"\"Clean up the network after running an epoch.\n    In this case we move all of the models from the GPU(s) back to the CPU.\n    \"\"\"\nif self.device.type == \"cuda\":\nfor model in self.epoch_models:\n# move model variables to cpu\nmodel.to(\"cpu\")\n# move optimizer variables to cpu\nself._move_optimizer_between_device(model.current_optimizer.state, \"cpu\")\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.Network", "title": "<code>Network</code>", "text": "<p>A function to automatically instantiate the correct Network derived class based on the given <code>ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>Iterable[Union[TensorOp, Scheduler[TensorOp]]]</code> <p>A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch models within the same network.</p> required <p>Returns:</p> Type Description <code>BaseNetwork</code> <p>A network instance containing the given <code>ops</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If TensorFlow and PyTorch models are mixed, or if no models are provided.</p> <code>ValueError</code> <p>If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def Network(ops: Iterable[Union[TensorOp, Scheduler[TensorOp]]]) -&gt; BaseNetwork:\n\"\"\"A function to automatically instantiate the correct Network derived class based on the given `ops`.\n    Args:\n        ops: A collection of Ops defining the graph for this Network. It should contain at least one ModelOp, and all\n            models should be either TensorFlow or Pytorch. We currently do not support mixing TensorFlow and Pytorch\n            models within the same network.\n    Returns:\n        A network instance containing the given `ops`.\n    Raises:\n        AssertionError: If TensorFlow and PyTorch models are mixed, or if no models are provided.\n        ValueError: If a model is provided whose type cannot be identified as either TensorFlow or PyTorch.\n    \"\"\"\nmodels = _collect_models(ops)\nassert models, \"cannot find model in Network ops\"\nframework = set()\nfor model in models:\nif isinstance(model, tf.keras.Model):\nframework.add(\"tf\")\nelif isinstance(model, torch.nn.Module):\nframework.add(\"torch\")\nelse:\nframework.add(\"unknown\")\nassert len(framework) == 1, \"please make sure either tensorflow or torch model is used in network\"\nframework = framework.pop()\nif framework == \"tf\":\nnetwork = TFNetwork(ops)\nelif framework == \"torch\":\nnetwork = TorchNetwork(ops)\nelse:\nraise ValueError(\"Unknown model type\")\nreturn network\n</code></pre>"}, {"location": "fastestimator/network.html#fastestimator.fastestimator.network.build", "title": "<code>build</code>", "text": "<p>Build model instances and associate them with optimizers.</p> <p>This method can be used with TensorFlow models / optimizers: <pre><code>model_def = fe.architecture.tensorflow.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models / optimizers: <pre><code>model_def = fe.architecture.pytorch.LeNet\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\nmodel = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\nmodel = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model_fn</code> <code>Callable[[], Union[Model, List[Model]]]</code> <p>A function that define model(s).</p> required <code>optimizer_fn</code> <code>Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None]</code> <p>Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers provided here should match the number of models generated by the <code>model_fn</code>.</p> required <code>model_name</code> <code>Union[str, List[str], None]</code> <p>Name(s) of the model(s) that will be used for logging purpose. If None, a name will be automatically generated and assigned.</p> <code>None</code> <code>weights_path</code> <code>Union[str, None, List[Union[str, None]]]</code> <p>Path(s) from which to load model weights. If not None, then the number of weight paths provided should match the number of models generated by the <code>model_fn</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>models</code> <code>Union[Model, List[Model]]</code> <p>The model(s) built by FastEstimator.</p> Source code in <code>fastestimator\\fastestimator\\network.py</code> <pre><code>def build(model_fn: Callable[[], Union[Model, List[Model]]],\noptimizer_fn: Union[str, Scheduler, Callable, List[str], List[Callable], List[Scheduler], None],\nweights_path: Union[str, None, List[Union[str, None]]] = None,\nmodel_name: Union[str, List[str], None] = None) -&gt; Union[Model, List[Model]]:\n\"\"\"Build model instances and associate them with optimizers.\n    This method can be used with TensorFlow models / optimizers:\n    ```python\n    model_def = fe.architecture.tensorflow.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda: tf.optimizers.Adam(lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.h5\")\n    ```\n    This method can be used with PyTorch models / optimizers:\n    ```python\n    model_def = fe.architecture.pytorch.LeNet\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\")\n    model = fe.build(model_fn = model_def, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=0.1))\n    model = fe.build(model_fn = model_def, optimizer_fn=\"adam\", weights_path=\"~/weights.pt)\n    ```\n    Args:\n        model_fn: A function that define model(s).\n        optimizer_fn: Optimizer string/definition or a list of optimizer instances/strings. The number of optimizers\n            provided here should match the number of models generated by the `model_fn`.\n        model_name: Name(s) of the model(s) that will be used for logging purpose. If None, a name will be\n            automatically generated and assigned.\n        weights_path: Path(s) from which to load model weights. If not None, then the number of weight paths provided\n            should match the number of models generated by the `model_fn`.\n    Returns:\n        models: The model(s) built by FastEstimator.\n    \"\"\"\ndef _generate_model_names(num_names):\nnames = [\"model\" if i + build.count == 0 else \"model{}\".format(i + build.count) for i in range(num_names)]\nbuild.count += num_names\nreturn names\nif not hasattr(build, \"count\"):\nbuild.count = 0\nmodels, optimizer_fn = to_list(model_fn()), to_list(optimizer_fn)\n# fill optimizer\nif not optimizer_fn:\noptimizer_fn = [None]\n# check framework\nif isinstance(models[0], tf.keras.Model):\nframework = \"tf\"\nelif isinstance(models[0], torch.nn.Module):\nframework = \"torch\"\nelse:\nraise ValueError(\"unrecognized model format: {}\".format(type(models[0])))\n# multi-gpu handling\nif torch.cuda.device_count() &gt; 1:\nif framework == \"tf\" and not isinstance(tf.distribute.get_strategy(), tf.distribute.MirroredStrategy):\ntf.distribute.experimental_set_strategy(tf.distribute.MirroredStrategy())\nmodels = to_list(model_fn())\nif framework == \"torch\":\nmodels = [torch.nn.DataParallel(model) for model in models]\n# generate names\nif not model_name:\nmodel_name = _generate_model_names(len(models))\nmodel_name = to_list(model_name)\n# load weights\nif weights_path:\nweights_path = to_list(weights_path)\nelse:\nweights_path = [None] * len(models)\nassert len(models) == len(optimizer_fn) == len(weights_path) == len(model_name), \\\n        \"Found inconsistency in number of models, optimizers, model_name or weights\"\n# create optimizer\nfor idx, (model, optimizer_def, weight, name) in enumerate(zip(models, optimizer_fn, weights_path, model_name)):\nmodels[idx] = _fe_compile(model, optimizer_def, weight, name, framework)\nif len(models) == 1:\nmodels = models[0]\nreturn models\n</code></pre>"}, {"location": "fastestimator/pipeline.html", "title": "pipeline", "text": ""}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline", "title": "<code>Pipeline</code>", "text": "<p>A data pipeline class that takes care of data pre-processing.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The training data, or None if no training data is available.</p> <code>None</code> <code>eval_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The evaluation data, or None if no evaluation data is available.</p> <code>None</code> <code>test_data</code> <code>Union[None, DataSource, Scheduler[DataSource]]</code> <p>The testing data, or None if no evaluation data is available.</p> <code>None</code> <code>batch_size</code> <code>Union[None, int, Scheduler[int]]</code> <p>The batch size to be used by the pipeline. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>ops</code> <code>Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]]</code> <p>NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>num_process</code> <code>Optional[int]</code> <p>Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when using a FastEstimator Dataset. None will default to the system CPU count. Multiprocessing can be disabled by passing 0 here, which can be useful for debugging.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last batch if the last batch is incomplete.</p> <code>False</code> <code>pad_value</code> <code>Optional[Union[int, float]]</code> <p>The padding value if batch padding is needed. None indicates that no padding is needed. NOTE: This argument is only applicable when using a FastEstimator Dataset.</p> <code>None</code> <code>collate_fn</code> <code>Optional[Callable]</code> <p>Function to merge data into one batch with input being list of elements.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>class Pipeline:\n\"\"\"A data pipeline class that takes care of data pre-processing.\n    Args:\n        train_data: The training data, or None if no training data is available.\n        eval_data: The evaluation data, or None if no evaluation data is available.\n        test_data: The testing data, or None if no evaluation data is available.\n        batch_size: The batch size to be used by the pipeline. NOTE: This argument is only applicable when using a\n            FastEstimator Dataset.\n        ops: NumpyOps to be used for pre-processing. NOTE: This argument is only applicable when using a FastEstimator\n            Dataset.\n        num_process: Number of CPU threads to use for data pre-processing. NOTE: This argument is only applicable when\n            using a FastEstimator Dataset. None will default to the system CPU count. Multiprocessing can be disabled by\n            passing 0 here, which can be useful for debugging.\n        drop_last: Whether to drop the last batch if the last batch is incomplete.\n        pad_value: The padding value if batch padding is needed. None indicates that no padding is needed. NOTE: This\n            argument is only applicable when using a FastEstimator Dataset.\n        collate_fn: Function to merge data into one batch with input being list of elements.\n    \"\"\"\nops: List[Union[NumpyOp, Scheduler[NumpyOp]]]\ndef __init__(self,\ntrain_data: Union[None, DataSource, Scheduler[DataSource]] = None,\neval_data: Union[None, DataSource, Scheduler[DataSource]] = None,\ntest_data: Union[None, DataSource, Scheduler[DataSource]] = None,\nbatch_size: Union[None, int, Scheduler[int]] = None,\nops: Union[None, NumpyOp, Scheduler[NumpyOp], List[Union[NumpyOp, Scheduler[NumpyOp]]]] = None,\nnum_process: Optional[int] = None,\ndrop_last: bool = False,\npad_value: Optional[Union[int, float]] = None,\ncollate_fn: Optional[Callable] = None):\nself.data = {x: y for (x, y) in zip([\"train\", \"eval\", \"test\"], [train_data, eval_data, test_data]) if y}\nself.batch_size = batch_size\nself.ops = to_list(ops)\nself.num_process = num_process if num_process is not None else os.cpu_count() if os.name != 'nt' else 0\nself.drop_last = drop_last\nself.pad_value = pad_value\nself.collate_fn = collate_fn\nself._verify_inputs(**{k: v for k, v in locals().items() if k != 'self'})\ndef _verify_inputs(self, **kwargs) -&gt; None:\n\"\"\"A helper method to ensure that the Pipeline inputs are valid.\n        Args:\n            **kwargs: A collection of variable / value pairs to validate.\n        Raises:\n            AssertionError: If `batch_size`, `ops`, or `num_process` were specified in the absence of a FastEstimator\n                Dataset.\n        \"\"\"\nfe_dataset = False\nfor dataset in get_current_items(self.data.values()):\nfe_dataset = self._verify_dataset(dataset, **kwargs) or fe_dataset\nif not fe_dataset:\nassert kwargs['batch_size'] is None, \"Pipeline only supports batch_size with built-in (FE) datasets\"\nassert kwargs['ops'] is None, \"Pipeline only supports ops with built-in (FE) datasets\"\nassert kwargs['num_process'] is None, \"Pipeline only support num_process with built-in (FE) datasets\"\ndef _verify_dataset(self, dataset: DataSource, **kwargs) -&gt; bool:\n\"\"\"A helper function to ensure that all of a dataset's arguments are correct.\n        Args:\n            dataset: The dataset to validate against.\n            **kwargs: A selection of variables and their values which must be validated.\n        Returns:\n            True iff the `dataset` is a PyTorch Dataset (as opposed to a DataLoader or tf.data.Dataset).\n        Raises:\n            AssertionError: If the `kwargs` are found to be invalid based on the given `dataset`.\n            ValueError: If the `dataset` is of an unknown type.\n        \"\"\"\nif isinstance(dataset, Dataset):\n# batch_size check\nfor batch_size in get_current_items(to_list(self.batch_size)):\nassert isinstance(batch_size, int), \"unsupported batch_size format: {}\".format(type(batch_size))\n# ops check\nfor op in get_current_items(self.ops):\nassert isinstance(op, NumpyOp), \"unsupported op format, must provide NumpyOp in Pipeline\"\n# num_process check\nassert isinstance(self.num_process, int), \"number of processes must be an integer\"\nreturn True\nelif isinstance(dataset, (DataLoader, tf.data.Dataset)):\nif kwargs['batch_size'] is not None:\nwarnings.warn(\"batch_size will only be used for built-in dataset\")\nif kwargs['ops'] is not None:\nwarnings.warn(\"ops will only be used for built-in dataset\")\nif kwargs['num_process'] is not None:\nwarnings.warn(\"num_process will only be used for built-in dataset\")\nreturn False\nelse:\nraise ValueError(\"Unsupported dataset type: {}\".format(type(dataset)))\ndef get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n        Args:\n            epoch: The current epoch index\n        Returns:\n            The modes for which the Pipeline has data.\n        \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, dataset in self.data.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\ndef benchmark(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1000, log_interval: int = 100) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n        Args:\n            mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n            num_steps: The maximum number of steps over which to perform the benchmark.\n            log_interval: The logging interval.\n        \"\"\"\nloader = self.get_loader(mode=mode, epoch=epoch)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nprint(\"FastEstimator: Step: {}, Epoch: {}, Steps/sec: {}\".format(idx, epoch, iters_per_sec))\nstart = time.perf_counter()\nif idx == num_steps:\nbreak\ndef get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n        Args:\n            mode: Current execution mode.\n        Returns:\n            List of schedulable items in Pipeline.\n        \"\"\"\nall_items = self.ops + [self.batch_size] + [self.data[mode]]\nreturn all_items\ndef get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n        Args:\n            total_epochs: Total number of epochs.\n            mode: Current execution mode.\n        Returns:\n            Set of epoch indices.\n        \"\"\"\nepochs_with_data = set()\ndataset = self.data[mode]\nif isinstance(dataset, Scheduler):\nepochs_with_data = set(epoch for epoch in range(1, total_epochs + 1) if dataset.get_current_value(epoch))\nelif dataset:\nepochs_with_data = set(range(1, total_epochs + 1))\nreturn epochs_with_data\ndef transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n        Args:\n            data: Input data in dictionary format.\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        Returns:\n            The transformed data.\n        \"\"\"\ndata = deepcopy(data)\nops = get_current_items(self.ops, mode, epoch)\nforward_numpyop(ops, data, mode)\nfor key, value in data.items():\ndata[key] = np.expand_dims(value, 0)\nreturn data\ndef get_results(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n        Args:\n            mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n            epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n            num_steps: Number of steps (batches) to get.\n            shuffle: Whether to use shuffling.\n        Returns:\n            A list of batches of Pipeline outputs.\n        \"\"\"\nresults = []\nloader = self.get_loader(mode=mode, epoch=epoch, shuffle=shuffle)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef get_loader(self, mode: str, epoch: int = 1,\nshuffle: Optional[bool] = None) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"Get a data loader from the Pipeline for a given `mode` and `epoch`.\n        Args:\n            mode: The execution mode for the loader. This can be 'train', 'eval' or 'test'.\n            epoch: The epoch index for the loader. Note that epoch indices are 1-indexed.\n            shuffle: Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument\n                is only used with FastEstimator Datasets.\n        Returns:\n            A data loader for the given `mode` and `epoch`.\n        \"\"\"\ndata = self.data[mode]\nif isinstance(data, Scheduler):\ndata = data.get_current_value(epoch)\nif isinstance(data, Dataset):\n# batch size\nbatch_size = self.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\n# batch dataset\nif isinstance(data, BatchDataset):\ndata.pad_value = self.pad_value\n# shuffle\nif shuffle is None:\nshuffle = mode == \"train\" and batch_size is not None\n# collate_fn\ncollate_fn = self.collate_fn\nif collate_fn is None and self.pad_value is not None:\ncollate_fn = self._pad_batch_collate\nop_dataset = OpDataset(data, get_current_items(self.ops, mode, epoch), mode)\ndata = DataLoader(op_dataset,\nbatch_size=None if isinstance(data, BatchDataset) else batch_size,\nshuffle=False if isinstance(data, BatchDataset) else shuffle,\nsampler=RandomSampler(op_dataset) if isinstance(data, BatchDataset) and shuffle else None,\nnum_workers=self.num_process,\ndrop_last=self.drop_last,\nworker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),\ncollate_fn=collate_fn)\nreturn data\ndef _pad_batch_collate(self, batch: List[MutableMapping[str, Any]]) -&gt; Dict[str, Any]:\n\"\"\"A collate function which pads a batch of data.\n        Args:\n            batch: The data to be batched and collated.\n        Returns:\n            A padded and collated batch of data.\n        \"\"\"\npad_batch(batch, self.pad_value)\nreturn default_collate(batch)\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.benchmark", "title": "<code>benchmark</code>", "text": "<p>Benchmark the pipeline processing speed.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode to benchmark. This can be 'train', 'eval' or 'test'.</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to benchmark. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>num_steps</code> <code>int</code> <p>The maximum number of steps over which to perform the benchmark.</p> <code>1000</code> <code>log_interval</code> <code>int</code> <p>The logging interval.</p> <code>100</code> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def benchmark(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1000, log_interval: int = 100) -&gt; None:\n\"\"\"Benchmark the pipeline processing speed.\n    Args:\n        mode: The execution mode to benchmark. This can be 'train', 'eval' or 'test'.\n        epoch: The epoch index to benchmark. Note that epoch indices are 1-indexed.\n        num_steps: The maximum number of steps over which to perform the benchmark.\n        log_interval: The logging interval.\n    \"\"\"\nloader = self.get_loader(mode=mode, epoch=epoch)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nstart = time.perf_counter()\nfor idx, _ in enumerate(loader, start=1):\nif idx % log_interval == 0:\nduration = time.perf_counter() - start\niters_per_sec = log_interval / duration\nprint(\"FastEstimator: Step: {}, Epoch: {}, Steps/sec: {}\".format(idx, epoch, iters_per_sec))\nstart = time.perf_counter()\nif idx == num_steps:\nbreak\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_epochs_with_data", "title": "<code>get_epochs_with_data</code>", "text": "<p>Get a set of epoch indices that contains data given mode.</p> <p>Parameters:</p> Name Type Description Default <code>total_epochs</code> <code>int</code> <p>Total number of epochs.</p> required <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>Set[int]</code> <p>Set of epoch indices.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_epochs_with_data(self, total_epochs: int, mode: str) -&gt; Set[int]:\n\"\"\"Get a set of epoch indices that contains data given mode.\n    Args:\n        total_epochs: Total number of epochs.\n        mode: Current execution mode.\n    Returns:\n        Set of epoch indices.\n    \"\"\"\nepochs_with_data = set()\ndataset = self.data[mode]\nif isinstance(dataset, Scheduler):\nepochs_with_data = set(epoch for epoch in range(1, total_epochs + 1) if dataset.get_current_value(epoch))\nelif dataset:\nepochs_with_data = set(range(1, total_epochs + 1))\nreturn epochs_with_data\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_loader", "title": "<code>get_loader</code>", "text": "<p>Get a data loader from the Pipeline for a given <code>mode</code> and <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode for the loader. This can be 'train', 'eval' or 'test'.</p> required <code>epoch</code> <code>int</code> <p>The epoch index for the loader. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>shuffle</code> <code>Optional[bool]</code> <p>Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument is only used with FastEstimator Datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataLoader, tf.data.Dataset]</code> <p>A data loader for the given <code>mode</code> and <code>epoch</code>.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_loader(self, mode: str, epoch: int = 1,\nshuffle: Optional[bool] = None) -&gt; Union[DataLoader, tf.data.Dataset]:\n\"\"\"Get a data loader from the Pipeline for a given `mode` and `epoch`.\n    Args:\n        mode: The execution mode for the loader. This can be 'train', 'eval' or 'test'.\n        epoch: The epoch index for the loader. Note that epoch indices are 1-indexed.\n        shuffle: Whether to shuffle the data. If None, the value for shuffle is based on mode. NOTE: This argument\n            is only used with FastEstimator Datasets.\n    Returns:\n        A data loader for the given `mode` and `epoch`.\n    \"\"\"\ndata = self.data[mode]\nif isinstance(data, Scheduler):\ndata = data.get_current_value(epoch)\nif isinstance(data, Dataset):\n# batch size\nbatch_size = self.batch_size\nif isinstance(batch_size, Scheduler):\nbatch_size = batch_size.get_current_value(epoch)\n# batch dataset\nif isinstance(data, BatchDataset):\ndata.pad_value = self.pad_value\n# shuffle\nif shuffle is None:\nshuffle = mode == \"train\" and batch_size is not None\n# collate_fn\ncollate_fn = self.collate_fn\nif collate_fn is None and self.pad_value is not None:\ncollate_fn = self._pad_batch_collate\nop_dataset = OpDataset(data, get_current_items(self.ops, mode, epoch), mode)\ndata = DataLoader(op_dataset,\nbatch_size=None if isinstance(data, BatchDataset) else batch_size,\nshuffle=False if isinstance(data, BatchDataset) else shuffle,\nsampler=RandomSampler(op_dataset) if isinstance(data, BatchDataset) and shuffle else None,\nnum_workers=self.num_process,\ndrop_last=self.drop_last,\nworker_init_fn=lambda _: np.random.seed(random.randint(0, 2**32 - 1)),\ncollate_fn=collate_fn)\nreturn data\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_modes", "title": "<code>get_modes</code>", "text": "<p>Get the modes for which the Pipeline has data.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>Optional[int]</code> <p>The current epoch index</p> <code>None</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes for which the Pipeline has data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_modes(self, epoch: Optional[int] = None) -&gt; Set[str]:\n\"\"\"Get the modes for which the Pipeline has data.\n    Args:\n        epoch: The current epoch index\n    Returns:\n        The modes for which the Pipeline has data.\n    \"\"\"\nif epoch is None:\nall_modes = set(self.data.keys())\nelse:\nall_modes = []\nfor mode, dataset in self.data.items():\nif isinstance(dataset, Scheduler):\ndataset = dataset.get_current_value(epoch)\nif dataset:\nall_modes.append(mode)\nreturn to_set(all_modes)\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_results", "title": "<code>get_results</code>", "text": "<p>Get sample Pipeline outputs.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".</p> <code>'train'</code> <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <code>num_steps</code> <code>int</code> <p>Number of steps (batches) to get.</p> <code>1</code> <code>shuffle</code> <code>bool</code> <p>Whether to use shuffling.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[Dict[str, Any]], Dict[str, Any]]</code> <p>A list of batches of Pipeline outputs.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_results(self, mode: str = \"train\", epoch: int = 1, num_steps: int = 1,\nshuffle: bool = False) -&gt; Union[List[Dict[str, Any]], Dict[str, Any]]:\n\"\"\"Get sample Pipeline outputs.\n    Args:\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", or \"test\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n        num_steps: Number of steps (batches) to get.\n        shuffle: Whether to use shuffling.\n    Returns:\n        A list of batches of Pipeline outputs.\n    \"\"\"\nresults = []\nloader = self.get_loader(mode=mode, epoch=epoch, shuffle=shuffle)\nif isinstance(loader, tf.data.Dataset):\nloader = loader.take(num_steps)\nfor idx, batch in enumerate(loader, start=1):\nresults.append(batch)\nif idx == num_steps:\nbreak\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.get_scheduled_items", "title": "<code>get_scheduled_items</code>", "text": "<p>Get a list of items considered for scheduling.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Current execution mode.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of schedulable items in Pipeline.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def get_scheduled_items(self, mode: str) -&gt; List[Any]:\n\"\"\"Get a list of items considered for scheduling.\n    Args:\n        mode: Current execution mode.\n    Returns:\n        List of schedulable items in Pipeline.\n    \"\"\"\nall_items = self.ops + [self.batch_size] + [self.data[mode]]\nreturn all_items\n</code></pre>"}, {"location": "fastestimator/pipeline.html#fastestimator.fastestimator.pipeline.Pipeline.transform", "title": "<code>transform</code>", "text": "<p>Apply all pipeline operations on a given data instance for the specified <code>mode</code> and <code>epoch</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Input data in dictionary format.</p> required <code>mode</code> <code>str</code> <p>The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".</p> required <code>epoch</code> <code>int</code> <p>The epoch index to run. Note that epoch indices are 1-indexed.</p> <code>1</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>The transformed data.</p> Source code in <code>fastestimator\\fastestimator\\pipeline.py</code> <pre><code>def transform(self, data: Dict[str, Any], mode: str, epoch: int = 1) -&gt; Dict[str, Any]:\n\"\"\"Apply all pipeline operations on a given data instance for the specified `mode` and `epoch`.\n    Args:\n        data: Input data in dictionary format.\n        mode: The execution mode in which to run. This can be \"train\", \"eval\", \"test\" or \"infer\".\n        epoch: The epoch index to run. Note that epoch indices are 1-indexed.\n    Returns:\n        The transformed data.\n    \"\"\"\ndata = deepcopy(data)\nops = get_current_items(self.ops, mode, epoch)\nforward_numpyop(ops, data, mode)\nfor key, value in data.items():\ndata[key] = np.expand_dims(value, 0)\nreturn data\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/pytorch/lenet.html#fastestimator.fastestimator.architecture.pytorch.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>         Bases: <code>torch.nn.Module</code></p> <p>A standard LeNet implementation in pytorch.</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>The shape of the model input (channels, height, width).</p> <code>(1, 28, 28)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\lenet.py</code> <pre><code>class LeNet(torch.nn.Module):\n\"\"\"A standard LeNet implementation in pytorch.\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: The shape of the model input (channels, height, width).\n        classes: The number of outputs the model should generate.\n    \"\"\"\ndef __init__(self, input_shape: Tuple[int, int, int] = (1, 28, 28), classes: int = 10) -&gt; None:\nsuper().__init__()\nconv_kernel = 3\nself.pool_kernel = 2\nself.conv1 = nn.Conv2d(input_shape[0], 32, conv_kernel)\nself.conv2 = nn.Conv2d(32, 64, conv_kernel)\nself.conv3 = nn.Conv2d(64, 64, conv_kernel)\nflat_x = ((((input_shape[1] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nflat_y = ((((input_shape[2] - (conv_kernel - 1)) // self.pool_kernel) -\n(conv_kernel - 1)) // self.pool_kernel) - (conv_kernel - 1)\nself.fc1 = nn.Linear(flat_x * flat_y * 64, 64)\nself.fc2 = nn.Linear(64, classes)\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx = fn.relu(self.conv1(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv2(x))\nx = fn.max_pool2d(x, self.pool_kernel)\nx = fn.relu(self.conv3(x))\nx = x.view(x.size(0), -1)\nx = fn.relu(self.fc1(x))\nx = fn.softmax(self.fc2(x), dim=-1)\nreturn x\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNet", "title": "<code>UNet</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A standard UNet implementation in PyTorch.</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (channels, height, width).</p> <code>(1, 128, 128)</code> Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNet(nn.Module):\n\"\"\"A standard UNet implementation in PyTorch.\n    Args:\n        input_size: The size of the input tensor (channels, height, width).\n    \"\"\"\ndef __init__(self, input_size: Tuple[int, int, int] = (1, 128, 128)) -&gt; None:\nsuper().__init__()\nself.input_size = input_size\nself.enc1 = UNetEncoderBlock(in_channels=input_size[0], out_channels=64)\nself.enc2 = UNetEncoderBlock(in_channels=64, out_channels=128)\nself.enc3 = UNetEncoderBlock(in_channels=128, out_channels=256)\nself.enc4 = UNetEncoderBlock(in_channels=256, out_channels=512)\nself.bottle_neck = UNetDecoderBlock(in_channels=512, mid_channels=1024, out_channels=512)\nself.dec4 = UNetDecoderBlock(in_channels=1024, mid_channels=512, out_channels=256)\nself.dec3 = UNetDecoderBlock(in_channels=512, mid_channels=256, out_channels=128)\nself.dec2 = UNetDecoderBlock(in_channels=256, mid_channels=128, out_channels=64)\nself.dec1 = nn.Sequential(nn.Conv2d(128, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 64, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(64, 1, 1),\nnn.Sigmoid())\nfor layer in self.dec1:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nx1, x_e1 = self.enc1(x)\nx2, x_e2 = self.enc2(x_e1)\nx3, x_e3 = self.enc3(x_e2)\nx4, x_e4 = self.enc4(x_e3)\nx_bottle_neck = self.bottle_neck(x_e4)\nx_d4 = self.dec4(torch.cat((x_bottle_neck, x4), 1))\nx_d3 = self.dec3(torch.cat((x_d4, x3), 1))\nx_d2 = self.dec2(torch.cat((x_d3, x2), 1))\nx_out = self.dec1(torch.cat((x_d2, x1), 1))\nreturn x_out\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetDecoderBlock", "title": "<code>UNetDecoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet decoder block.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the decoder.</p> required <code>mid_channels</code> <code>int</code> <p>How many channels are used for the decoder's intermediate layer.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the decoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetDecoderBlock(nn.Module):\n\"\"\"A UNet decoder block.\n    Args:\n        in_channels: How many channels enter the decoder.\n        mid_channels: How many channels are used for the decoder's intermediate layer.\n        out_channels: How many channels leave the decoder.\n    \"\"\"\ndef __init__(self, in_channels: int, mid_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(mid_channels, mid_channels, 3, padding=1),\nnn.ReLU(inplace=True),\nnn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\nnn.Conv2d(mid_channels, out_channels, 3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn self.layers(x)\n</code></pre>"}, {"location": "fastestimator/architecture/pytorch/unet.html#fastestimator.fastestimator.architecture.pytorch.unet.UNetEncoderBlock", "title": "<code>UNetEncoderBlock</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A UNet encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>How many channels enter the encoder.</p> required <code>out_channels</code> <code>int</code> <p>How many channels leave the encoder.</p> required Source code in <code>fastestimator\\fastestimator\\architecture\\pytorch\\unet.py</code> <pre><code>class UNetEncoderBlock(nn.Module):\n\"\"\"A UNet encoder block.\n    Args:\n        in_channels: How many channels enter the encoder.\n        out_channels: How many channels leave the encoder.\n    \"\"\"\ndef __init__(self, in_channels: int, out_channels: int) -&gt; None:\nsuper().__init__()\nself.layers = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True),\nnn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\nnn.ReLU(inplace=True))\nfor layer in self.layers:\nif isinstance(layer, nn.Conv2d):\nhe_normal(layer.weight.data)\nlayer.bias.data.zero_()\ndef forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\nout = self.layers(x)\nreturn out, F.max_pool2d(out, 2)\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/lenet.html", "title": "lenet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/lenet.html#fastestimator.fastestimator.architecture.tensorflow.lenet.LeNet", "title": "<code>LeNet</code>", "text": "<p>A standard LeNet implementation in TensorFlow.</p> <p>The LeNet model has 3 convolution layers and 2 dense layers.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>Tuple[int, int, int]</code> <p>shape of the input data (height, width, channels).</p> <code>(28, 28, 1)</code> <code>classes</code> <code>int</code> <p>The number of outputs the model should generate.</p> <code>10</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow LeNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\lenet.py</code> <pre><code>def LeNet(input_shape: Tuple[int, int, int] = (28, 28, 1), classes: int = 10) -&gt; tf.keras.Model:\n\"\"\"A standard LeNet implementation in TensorFlow.\n    The LeNet model has 3 convolution layers and 2 dense layers.\n    Args:\n        input_shape: shape of the input data (height, width, channels).\n        classes: The number of outputs the model should generate.\n    Returns:\n        A TensorFlow LeNet model.\n    \"\"\"\nmodel = Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(classes, activation='softmax'))\nreturn model\n</code></pre>"}, {"location": "fastestimator/architecture/tensorflow/unet.html", "title": "unet", "text": ""}, {"location": "fastestimator/architecture/tensorflow/unet.html#fastestimator.fastestimator.architecture.tensorflow.unet.UNet", "title": "<code>UNet</code>", "text": "<p>A standard UNet implementation in pytorch</p> <p>Parameters:</p> Name Type Description Default <code>input_size</code> <code>Tuple[int, int, int]</code> <p>The size of the input tensor (height, width, channels).</p> <code>(128, 128, 1)</code> <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A TensorFlow LeNet model.</p> Source code in <code>fastestimator\\fastestimator\\architecture\\tensorflow\\unet.py</code> <pre><code>def UNet(input_size: Tuple[int, int, int] = (128, 128, 1)) -&gt; tf.keras.Model:\n\"\"\"A standard UNet implementation in pytorch\n    Args:\n        input_size: The size of the input tensor (height, width, channels).\n    Returns:\n        A TensorFlow LeNet model.\n    \"\"\"\nconv_config = {'activation': 'relu', 'padding': 'same', 'kernel_initializer': 'he_normal'}\nup_config = {'size': (2, 2), 'interpolation': 'bilinear'}\ninputs = Input(input_size)\nconv1 = Conv2D(64, 3, **conv_config)(inputs)\nconv1 = Conv2D(64, 3, **conv_config)(conv1)\npool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = Conv2D(128, 3, **conv_config)(pool1)\nconv2 = Conv2D(128, 3, **conv_config)(conv2)\npool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = Conv2D(256, 3, **conv_config)(pool2)\nconv3 = Conv2D(256, 3, **conv_config)(conv3)\npool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = Conv2D(512, 3, **conv_config)(pool3)\nconv4 = Conv2D(512, 3, **conv_config)(conv4)\ndrop4 = Dropout(0.5)(conv4)\npool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\nconv5 = Conv2D(1024, 3, **conv_config)(pool4)\nconv5 = Conv2D(1024, 3, **conv_config)(conv5)\ndrop5 = Dropout(0.5)(conv5)\nup6 = Conv2D(512, 3, **conv_config)(UpSampling2D(**up_config)(drop5))\nmerge6 = concatenate([drop4, up6], axis=-1)\nconv6 = Conv2D(512, 3, **conv_config)(merge6)\nconv6 = Conv2D(512, 3, **conv_config)(conv6)\nup7 = Conv2D(256, 3, **conv_config)(UpSampling2D(**up_config)(conv6))\nmerge7 = concatenate([conv3, up7], axis=-1)\nconv7 = Conv2D(256, 3, **conv_config)(merge7)\nconv7 = Conv2D(256, 3, **conv_config)(conv7)\nup8 = Conv2D(128, 3, **conv_config)(UpSampling2D(**up_config)(conv7))\nmerge8 = concatenate([conv2, up8], axis=-1)\nconv8 = Conv2D(128, 3, **conv_config)(merge8)\nconv8 = Conv2D(128, 3, **conv_config)(conv8)\nup9 = Conv2D(64, 3, **conv_config)(UpSampling2D(**up_config)(conv8))\nmerge9 = concatenate([conv1, up9], axis=-1)\nconv9 = Conv2D(64, 3, **conv_config)(merge9)\nconv9 = Conv2D(64, 3, **conv_config)(conv9)\nconv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\nmodel = Model(inputs=inputs, outputs=conv10)\nreturn model\n</code></pre>"}, {"location": "fastestimator/backend/abs.html", "title": "abs", "text": ""}, {"location": "fastestimator/backend/abs.html#fastestimator.fastestimator.backend.abs.abs", "title": "<code>abs</code>", "text": "<p>Compute the absolute value of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.abs(n)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.abs(t)  # [2, 7, 19]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.abs(p)  # [2, 7, 19]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute value of <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\abs.py</code> <pre><code>def abs(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the absolute value of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.abs(n)  # [2, 7, 19]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.abs(t)  # [2, 7, 19]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.abs(p)  # [2, 7, 19]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The absolute value of `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.abs(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.abs(tensor)\nelif isinstance(tensor, np.ndarray):\nreturn np.abs(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/argmax.html", "title": "argmax", "text": ""}, {"location": "fastestimator/backend/argmax.html#fastestimator.fastestimator.backend.argmax.argmax", "title": "<code>argmax</code>", "text": "<p>Compute the index of the maximum value along a given axis of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\nb = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\nb = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>int</code> <p>Which axis to compute the index along.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indices corresponding to the maximum values within <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\argmax.py</code> <pre><code>def argmax(tensor: Tensor, axis: int = 0) -&gt; Tensor:\n\"\"\"Compute the index of the maximum value along a given axis of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(n, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(n, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(t, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(t, axis=1)  # [1, 0, 1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[2,7,5],[9,1,3],[4,8,2]])\n    b = fe.backend.argmax(p, axis=0)  # [1, 2, 0]\n    b = fe.backend.argmax(p, axis=1)  # [1, 0, 1]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to compute the index along.\n    Returns:\n        The indices corresponding to the maximum values within `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.argmax(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.max(dim=axis, keepdim=False)[1]\nelif isinstance(tensor, np.ndarray):\nreturn np.argmax(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/binary_crossentropy.html", "title": "binary_crossentropy", "text": ""}, {"location": "fastestimator/backend/binary_crossentropy.html#fastestimator.fastestimator.backend.binary_crossentropy.binary_crossentropy", "title": "<code>binary_crossentropy</code>", "text": "<p>Compute binary crossentropy.</p> <p>This method is applicable when there are only two label classes (zero and one). There should be a single floating point prediction per example.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [1], [0]])\npred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [1], [0]])\npred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\nb = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (batch, ...). dtype: float32.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with the same shape as <code>y_pred</code>. dtype: int or float32.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The binary crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a tensor with</p> <code>Tensor</code> <p>the same shape as <code>y_true</code>.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\binary_crossentropy.py</code> <pre><code>def binary_crossentropy(y_pred: Tensor, y_true: Tensor, from_logits: bool = False, average_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute binary crossentropy.\n    This method is applicable when there are only two label classes (zero and one). There should be a single floating\n    point prediction per example.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [1], [0]])\n    pred = tf.constant([[0.9], [0.3], [0.8], [0.1]])\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [1], [0]])\n    pred = torch.tensor([[0.9], [0.3], [0.8], [0.1]])\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true)  # 0.197\n    b = fe.backend.binary_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.105, 0.356, 0.223, 0.105]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (batch, ...). dtype: float32.\n        y_true: Ground truth class labels with the same shape as `y_pred`. dtype: int or float32.\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The binary crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a tensor with\n        the same shape as `y_true`.\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) is type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif isinstance(y_pred, tf.Tensor):\nce = tf.losses.binary_crossentropy(y_pred=y_pred,\ny_true=tf.reshape(y_true, y_pred.shape),\nfrom_logits=from_logits)\nce = tf.reshape(ce, [ce.shape[0], -1])\nce = tf.reduce_mean(ce, 1)\nelse:\ny_true = y_true.to(torch.float)\nif from_logits:\nce = torch.nn.BCEWithLogitsLoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nelse:\nce = torch.nn.BCELoss(reduction=\"none\")(input=y_pred, target=y_true.view(y_pred.size()))\nce = ce.view(ce.shape[0], -1)\nce = torch.mean(ce, dim=1)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/categorical_crossentropy.html", "title": "categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/categorical_crossentropy.html#fastestimator.fastestimator.backend.categorical_crossentropy.categorical_crossentropy", "title": "<code>categorical_crossentropy</code>", "text": "<p>Compute categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like <code>y_pred</code>. dtype: int or float32.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\categorical_crossentropy.py</code> <pre><code>def categorical_crossentropy(y_pred: Tensor, y_true: Tensor, from_logits: bool = False,\naverage_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.223, 0.105, 0.356]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32.\n        y_true: Ground truth class labels with a shape like `y_pred`. dtype: int or float32.\n        from_logits: Whether y_pred is from logits. If True, a sigmoid will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif isinstance(y_pred, tf.Tensor):\nce = tf.losses.categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nelse:\ny_true = y_true.to(torch.float)\nce = _categorical_crossentropy_torch(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/clip_by_value.html", "title": "clip_by_value", "text": ""}, {"location": "fastestimator/backend/clip_by_value.html#fastestimator.fastestimator.backend.clip_by_value.clip_by_value", "title": "<code>clip_by_value</code>", "text": "<p>Clip a tensor such that <code>min_value</code> &lt;= tensor &lt;= <code>max_value</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-5, 4, 2, 0, 9, -2])\nb = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>min_value</code> <code>Union[int, float, Tensor]</code> <p>The minimum value to clip to.</p> required <code>max_value</code> <code>Union[int, float, Tensor]</code> <p>The maximum value to clip to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code>, with it's values clipped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\clip_by_value.py</code> <pre><code>def clip_by_value(tensor: Tensor, min_value: Union[int, float, Tensor], max_value: Union[int, float, Tensor]) -&gt; Tensor:\n\"\"\"Clip a tensor such that `min_value` &lt;= tensor &lt;= `max_value`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(n, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(t, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-5, 4, 2, 0, 9, -2])\n    b = fe.backend.clip_by_value(p, min_value=-2, max_value=3)  # [-2, 3, 2, 0, 3, -2]\n    ```\n    Args:\n        tensor: The input value.\n        min_value: The minimum value to clip to.\n        max_value: The maximum value to clip to.\n    Returns:\n        The `tensor`, with it's values clipped.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.clip_by_value(tensor, clip_value_min=min_value, clip_value_max=max_value)\nelif isinstance(tensor, torch.Tensor):\nif isinstance(min_value, torch.Tensor):\nmin_value = min_value.item()\nif isinstance(max_value, torch.Tensor):\nmax_value = max_value.item()\nreturn tensor.clamp(min=min_value, max=max_value)\nelif isinstance(tensor, np.ndarray):\nreturn np.clip(tensor, a_min=min_value, a_max=max_value)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/concat.html", "title": "concat", "text": ""}, {"location": "fastestimator/backend/concat.html#fastestimator.fastestimator.backend.concat.concat", "title": "<code>concat</code>", "text": "<p>Concatenate a list of <code>tensors</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\nb = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\nb = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\nb = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\nb = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>List[Tensor]</code> <p>A list of tensors to be concatenated.</p> required <code>axis</code> <code>int</code> <p>The axis along which to concatenate the input.</p> <code>0</code> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensors</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\concat.py</code> <pre><code>def concat(tensors: List[Tensor], axis: int = 0) -&gt; Optional[Tensor]:\n\"\"\"Concatenate a list of `tensors` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = [np.array([[0, 1]]), np.array([[2, 3]]), np.array([[4, 5]])]\n    b = fe.backend.concat(n, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(n, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = [tf.constant([[0, 1]]), tf.constant([[2, 3]]), tf.constant([[4, 5]])]\n    b = fe.backend.concat(t, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(t, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = [torch.tensor([[0, 1]]), torch.tensor([[2, 3]]), torch.tensor([[4, 5]])]\n    b = fe.backend.concat(p, axis=0)  # [[0, 1], [2, 3], [4, 5]]\n    b = fe.backend.concat(p, axis=1)  # [[0, 1, 2, 3, 4, 5]]\n    ```\n    Args:\n        tensors: A list of tensors to be concatenated.\n        axis: The axis along which to concatenate the input.\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensors` is an unacceptable data type.\n    \"\"\"\nif len(tensors) == 0:\nreturn None\nif isinstance(tensors[0], tf.Tensor):\nreturn tf.concat(tensors, axis=axis)\nelif isinstance(tensors[0], torch.Tensor):\nreturn torch.cat(tensors, dim=axis)\nelif isinstance(tensors[0], np.ndarray):\nreturn np.concatenate(tensors, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensors[0])))\n</code></pre>"}, {"location": "fastestimator/backend/expand_dims.html", "title": "expand_dims", "text": ""}, {"location": "fastestimator/backend/expand_dims.html#fastestimator.fastestimator.backend.expand_dims.expand_dims", "title": "<code>expand_dims</code>", "text": "<p>Create a new dimension in <code>tensor</code> along a given <code>axis</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([2,7,5])\nb = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([2,7,5])\nb = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([2,7,5])\nb = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\nb = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input to be modified, having n dimensions.</p> required <code>axis</code> <code>int</code> <p>Which axis should the new axis be inserted along. Must be in the range [-n-1, n].</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A concatenated representation of the <code>tensors</code>, or None if the list of <code>tensors</code> was empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\expand_dims.py</code> <pre><code>def expand_dims(tensor: Tensor, axis: int = 1) -&gt; Tensor:\n\"\"\"Create a new dimension in `tensor` along a given `axis`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([2,7,5])\n    b = fe.backend.expand_dims(n, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(n, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([2,7,5])\n    b = fe.backend.expand_dims(t, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(t, axis=1)  # [[2], [5], [7]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([2,7,5])\n    b = fe.backend.expand_dims(p, axis=0)  # [[2, 5, 7]]\n    b = fe.backend.expand_dims(p, axis=1)  # [[2], [5], [7]]\n    ```\n    Args:\n        tensor: The input to be modified, having n dimensions.\n        axis: Which axis should the new axis be inserted along. Must be in the range [-n-1, n].\n    Returns:\n        A concatenated representation of the `tensors`, or None if the list of `tensors` was empty.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.expand_dims(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.unsqueeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.expand_dims(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/feed_forward.html", "title": "feed_forward", "text": ""}, {"location": "fastestimator/backend/feed_forward.html#fastestimator.fastestimator.backend.feed_forward.feed_forward", "title": "<code>feed_forward</code>", "text": "<p>Run a forward step on a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.architecture.tensorflow.LeNet(classes=2)\nx = tf.ones((3,28,28,1))  # (batch, height, width, channels)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.architecture.pytorch.LeNet(classes=2)\nx = torch.ones((3,1,28,28))  # (batch, channels, height, width)\nb = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network to run the forward step through.</p> required <code>x</code> <code>Union[Tensor, np.ndarray]</code> <p>An input tensor for the <code>model</code>. This value will be auto-cast to either a tf.Tensor or torch.Tensor as applicable for the <code>model</code>.</p> required <code>training</code> <code>bool</code> <p>Whether this forward step is part of training or not. This may impact the behavior of <code>model</code> layers such as dropout.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The result of <code>model(x)</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\feed_forward.py</code> <pre><code>def feed_forward(model: Union[tf.keras.Model, torch.nn.Module], x: Union[Tensor, np.ndarray],\ntraining: bool = True) -&gt; Tensor:\n\"\"\"Run a forward step on a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.architecture.tensorflow.LeNet(classes=2)\n    x = tf.ones((3,28,28,1))  # (batch, height, width, channels)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.architecture.pytorch.LeNet(classes=2)\n    x = torch.ones((3,1,28,28))  # (batch, channels, height, width)\n    b = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    ```\n    Args:\n        model: A neural network to run the forward step through.\n        x: An input tensor for the `model`. This value will be auto-cast to either a tf.Tensor or torch.Tensor as\n            applicable for the `model`.\n        training: Whether this forward step is part of training or not. This may impact the behavior of `model` layers\n            such as dropout.\n    Returns:\n        The result of `model(x)`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nif isinstance(model, tf.keras.Model):\nif not isinstance(x, tf.Tensor):\nx = to_tensor(x, \"tf\")\nx = model(x, training=training)\nelif isinstance(model, torch.nn.Module):\nmodel.train(mode=training)\nif not isinstance(x, torch.Tensor):\nx = to_tensor(x, \"torch\")\nx = model(x)\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn x\n</code></pre>"}, {"location": "fastestimator/backend/gather_from_batch.html", "title": "gather_from_batch", "text": ""}, {"location": "fastestimator/backend/gather_from_batch.html#fastestimator.fastestimator.backend.gather_from_batch.gather_from_batch", "title": "<code>gather_from_batch</code>", "text": "<p>Gather specific indices from a batch of data.</p> <p>This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values. The <code>indices</code> will automatically be cast to the correct type (tf, torch, np) based on the type of the <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>ind = np.array([1, 0, 1])\nn = np.array([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\nn = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>ind = tf.constant([1, 0, 1])\nt = tf.constant([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\nt = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>ind = torch.tensor([1, 0, 1])\np = torch.tensor([[0, 1], [2, 3], [4, 5]])\nb = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\np = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>A tensor of shape (batch, d1, ..., dn).</p> required <code>indices</code> <code>Tensor</code> <p>A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape (batch, d2, ..., dn) containing the elements from <code>tensor</code> at the given <code>indices</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\gather_from_batch.py</code> <pre><code>def gather_from_batch(tensor: Tensor, indices: Tensor) -&gt; Tensor:\n\"\"\"Gather specific indices from a batch of data.\n    This method can be useful if you need to compute gradients based on a specific subset of a tensor's output values.\n    The `indices` will automatically be cast to the correct type (tf, torch, np) based on the type of the `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    ind = np.array([1, 0, 1])\n    n = np.array([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(n, ind)  # [1, 2, 5]\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(n, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    ind = tf.constant([1, 0, 1])\n    t = tf.constant([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(t, ind)  # [1, 2, 5]\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(t, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    ind = torch.tensor([1, 0, 1])\n    p = torch.tensor([[0, 1], [2, 3], [4, 5]])\n    b = fe.backend.gather_from_batch(p, ind)  # [1, 2, 5]\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.gather_from_batch(p, ind)  # [[2, 3], [4, 5], [10, 11]]\n    ```\n    Args:\n        tensor: A tensor of shape (batch, d1, ..., dn).\n        indices: A tensor of shape (batch, ) or (batch, 1) indicating which indices should be selected.\n    Returns:\n        A tensor of shape (batch, d2, ..., dn) containing the elements from `tensor` at the given `indices`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nindices = to_tensor(indices, 'tf')\nindices = tf.cast(indices, tf.int64)\nif len(indices.shape) == 1:  # Indices not batched\nindices = expand_dims(indices, 1)\nreturn tf.gather_nd(tensor, indices=indices, batch_dims=1)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor[torch.arange(tensor.shape[0]), squeeze(indices)]\nelif isinstance(tensor, np.ndarray):\nreturn tensor[np.arange(tensor.shape[0]), squeeze(indices)]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/get_gradient.html", "title": "get_gradient", "text": ""}, {"location": "fastestimator/backend/get_gradient.html#fastestimator.fastestimator.backend.get_gradient.get_gradient", "title": "<code>get_gradient</code>", "text": "<p>Calculate gradients of a target w.r.t sources.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.Variable([1.0, 2.0, 3.0])\nwith tf.GradientTape(persistent=True) as tape:\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\nb = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = x * x\nb = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\nb = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\nb = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>The target (final) tensor.</p> required <code>sources</code> <code>Union[Iterable[Tensor], Tensor]</code> <p>A sequence of source (initial) tensors.</p> required <code>higher_order</code> <code>bool</code> <p>Whether the gradient will be used for higher order gradients.</p> <code>False</code> <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>TensorFlow gradient tape. Only needed when using the TensorFlow backend.</p> <code>None</code> <code>retain_graph</code> <code>bool</code> <p>Whether to retain PyTorch graph. Only valid when using the PyTorch backend.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Iterable[Tensor], Tensor]</code> <p>Gradient(s) of the <code>target</code> with respect to the <code>sources</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\get_gradient.py</code> <pre><code>def get_gradient(target: Tensor,\nsources: Union[Iterable[Tensor], Tensor],\nhigher_order: bool = False,\ntape: Optional[tf.GradientTape] = None,\nretain_graph: bool = True) -&gt; Union[Iterable[Tensor], Tensor]:\n\"\"\"Calculate gradients of a target w.r.t sources.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.Variable([1.0, 2.0, 3.0])\n    with tf.GradientTape(persistent=True) as tape:\n        y = x * x\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # None\n        b = fe.backend.get_gradient(target=y, sources=x, tape=tape, higher_order=True)  # [2.0, 4.0, 6.0]\n        b = fe.backend.get_gradient(target=b, sources=x, tape=tape)  # [2.0, 2.0, 2.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n    y = x * x\n    b = fe.backend.get_gradient(target=y, sources=x)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # Error - b does not have a backwards function\n    b = fe.backend.get_gradient(target=y, sources=x, higher_order=True)  # [2.0, 4.0, 6.0]\n    b = fe.backend.get_gradient(target=b, sources=x)  # [2.0, 2.0, 2.0]\n    ```\n    Args:\n        target: The target (final) tensor.\n        sources: A sequence of source (initial) tensors.\n        higher_order: Whether the gradient will be used for higher order gradients.\n        tape: TensorFlow gradient tape. Only needed when using the TensorFlow backend.\n        retain_graph: Whether to retain PyTorch graph. Only valid when using the PyTorch backend.\n    Returns:\n        Gradient(s) of the `target` with respect to the `sources`.\n    Raises:\n        ValueError: If `target` is an unacceptable data type.\n    \"\"\"\nif isinstance(target, tf.Tensor):\nwith NonContext() if higher_order else tape.stop_recording():\ngradients = tape.gradient(target, sources)\nelif isinstance(target, torch.Tensor):\ngradients = torch.autograd.grad(target,\nsources,\ngrad_outputs=torch.ones_like(target),\nretain_graph=retain_graph,\ncreate_graph=higher_order,\nonly_inputs=True)\nif isinstance(gradients, Tuple) and len(gradients) == 1:\n# PyTorch seems to unnecessarily wrap results into a tuple when a non-scalar target is provided\ngradients = gradients[0]\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(target)))\nreturn gradients\n</code></pre>"}, {"location": "fastestimator/backend/get_lr.html", "title": "get_lr", "text": ""}, {"location": "fastestimator/backend/get_lr.html#fastestimator.fastestimator.backend.get_lr.get_lr", "title": "<code>get_lr</code>", "text": "<p>Get the learning rate of a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nb = fe.backend.get_lr(model=m)  # 0.001\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to inspect.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The learning rate of <code>model</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\get_lr.py</code> <pre><code>def get_lr(model: Union[tf.keras.Model, torch.nn.Module]) -&gt; float:\n\"\"\"Get the learning rate of a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    b = fe.backend.get_lr(model=m)  # 0.001\n    ```\n    Args:\n        model: A neural network instance to inspect.\n    Returns:\n        The learning rate of `model`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nif isinstance(model, tf.keras.Model):\nlr = tf.keras.backend.get_value(model.current_optimizer.lr)\nelif isinstance(model, torch.nn.Module):\nlr = model.current_optimizer.param_groups[0]['lr']\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\nreturn lr\n</code></pre>"}, {"location": "fastestimator/backend/load_model.html", "title": "load_model", "text": ""}, {"location": "fastestimator/backend/load_model.html#fastestimator.fastestimator.backend.load_model.load_model", "title": "<code>load_model</code>", "text": "<p>Load saved weights for a given model.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\nfe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to load.</p> required <code>weights_path</code> <code>str</code> <p>Path to the <code>model</code> weights.</p> required <code>load_optimizer</code> <code>bool</code> <p>Whether to load optimizer. If True, then it will load  file in the path. <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\load_model.py</code> <pre><code>def load_model(model: Union[tf.keras.Model, torch.nn.Module], weights_path: str, load_optimizer: bool = False):\n\"\"\"Load saved weights for a given model.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.h5\")\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"tmp\", model_name=\"test\")\n    fe.backend.load_model(m, weights_path=\"tmp/test.pt\")\n    ```\n    Args:\n        model: A neural network instance to load.\n        weights_path: Path to the `model` weights.\n        load_optimizer: Whether to load optimizer. If True, then it will load &lt;weights_opt&gt; file in the path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif isinstance(model, tf.keras.Model):\nmodel.load_weights(weights_path)\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pkl\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nwith open(optimizer_path, 'rb') as f:\nweight_values = pickle.load(f)\nmodel.current_optimizer.set_weights(weight_values)\nelif isinstance(model, torch.nn.Module):\nmodel.load_state_dict(torch.load(weights_path))\nif load_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = \"{}_opt.pt\".format(os.path.splitext(weights_path)[0])\nassert os.path.exists(optimizer_path), \"cannot find optimizer path: {}\".format(optimizer_path)\nmodel.current_optimizer.load_state_dict(torch.load(optimizer_path))\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/mean_squared_error.html", "title": "mean_squared_error", "text": ""}, {"location": "fastestimator/backend/mean_squared_error.html#fastestimator.fastestimator.backend.mean_squared_error.mean_squared_error", "title": "<code>mean_squared_error</code>", "text": "<p>Calculate mean squared error between two tensors.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\npred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\ntrue = tf.constant([[1], [3], [2], [0]])\npred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\nb = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int or float32.</p> required <code>y_pred</code> <code>Tensor</code> <p>Prediction score for each class, with a shape like y_true. dtype: float32.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MSE between <code>y_true</code> and <code>y_pred</code></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> and <code>y_pred</code> have mismatched shapes or disparate types.</p> <code>ValueError</code> <p>If <code>y_pred</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\mean_squared_error.py</code> <pre><code>def mean_squared_error(y_true: Tensor, y_pred: Tensor) -&gt; Tensor:\n\"\"\"Calculate mean squared error between two tensors.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = tf.constant([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[0,1,0,0], [0,0,0,1], [0,0,1,0], [1,0,0,0]])\n    pred = torch.tensor([[0.1,0.9,0.05,0.05], [0.1,0.2,0.0,0.7], [0.0,0.15,0.8,0.05], [1.0,0.0,0.0,0.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [0.0063, 0.035, 0.016, 0.0]\n    true = tf.constant([[1], [3], [2], [0]])\n    pred = tf.constant([[2.0], [0.0], [2.0], [1.0]])\n    b = fe.backend.mean_squared_error(y_pred=pred, y_true=true)  # [1.0, 9.0, 0.0, 1.0]\n    ```\n    Args:\n        y_true: Ground truth class labels with a shape like (batch) or (batch, n_classes). dtype: int or float32.\n        y_pred: Prediction score for each class, with a shape like y_true. dtype: float32.\n    Returns:\n        The MSE between `y_true` and `y_pred`\n    Raises:\n        AssertionError: If `y_true` and `y_pred` have mismatched shapes or disparate types.\n        ValueError: If `y_pred` is an unacceptable data type.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be of the same tensor type\"\nassert y_pred.shape == y_true.shape, \\\n        f\"MSE requires y_true and y_pred to have the same shape, but found {y_true.shape} and {y_pred.shape}\"\nif isinstance(y_pred, tf.Tensor):\nmse = tf.losses.MSE(y_true, y_pred)\nelif isinstance(y_pred, torch.Tensor):\nmse = reduce_mean(\ntorch.nn.MSELoss(reduction=\"none\")(y_pred, y_true), axis=[ax for ax in range(y_pred.ndim)][1:])\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(y_pred)))\nreturn mse\n</code></pre>"}, {"location": "fastestimator/backend/percentile.html", "title": "percentile", "text": ""}, {"location": "fastestimator/backend/percentile.html#fastestimator.fastestimator.backend.percentile.percentile", "title": "<code>percentile</code>", "text": "<p>Compute the <code>percentiles</code> of a <code>tensor</code>.</p> <p>The n-th percentile of <code>tensor</code> is the value n/100 of the way from the minimum to the maximum in a sorted copy of <code>tensor</code>. If the percentile falls in between two values, the nearest of the two values will be used.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\nb = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor from which to extract percentiles.</p> required <code>percentiles</code> <code>Union[int, List[int]]</code> <p>One or more percentile values to be computed.</p> required <code>axis</code> <code>Union[None, int, List[int]]</code> <p>Along which axes to compute the percentile (None to compute over all axes).</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to maintain the number of dimensions from <code>tensor</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>percentiles</code> of the given <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\percentile.py</code> <pre><code>def percentile(tensor: Tensor,\npercentiles: Union[int, List[int]],\naxis: Union[None, int, List[int]] = None,\nkeepdims: bool = True) -&gt; Tensor:\n\"\"\"Compute the `percentiles` of a `tensor`.\n    The n-th percentile of `tensor` is the value n/100 of the way from the minimum to the maximum in a sorted copy of\n    `tensor`. If the percentile falls in between two values, the nearest of the two values will be used.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(n, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(n, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(t, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(t, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    b = fe.backend.percentile(p, percentiles=[66])  # [[[6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=0)  # [[[4, 5, 6]]]\n    b = fe.backend.percentile(p, percentiles=[66], axis=1)  # [[[2], [5], [8]]]\n    ```\n    Args:\n        tensor: The tensor from which to extract percentiles.\n        percentiles: One or more percentile values to be computed.\n        axis: Along which axes to compute the percentile (None to compute over all axes).\n        keepdims: Whether to maintain the number of dimensions from `tensor`.\n    Returns:\n        The `percentiles` of the given `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nif isinstance(percentiles, List):\npercentiles = tf.convert_to_tensor(percentiles)\nreturn tfp.stats.percentile(tensor, percentiles, axis=axis, keep_dims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nn_dims = len(tensor.shape)\nif axis is None:\n# Default behavior in tf without axis is to compress all dimensions\naxis = list(range(n_dims))\n# Convert negative axis values to their positive counterparts\nif isinstance(axis, int):\naxis = [axis]\nfor idx, elem in enumerate(axis):\naxis[idx] = elem % n_dims\n# Extract dims which are not being considered\nother_dims = sorted(set(range(n_dims)).difference(axis))\n# Flatten all of the permutation axis down for kth-value computation\npermutation = other_dims + list(axis)\npermuted = tensor.permute(*permutation)\nother_shape = [tensor.shape[i] for i in other_dims]\nother_shape.append(np.prod([tensor.shape[i] for i in axis]))\npermuted = torch.reshape(permuted, other_shape)\nresults = []\nfor tile in to_list(percentiles):\ntarget = min(round(tile / 100.0 * permuted.shape[-1]), permuted.shape[-1])\nkth_val = torch.kthvalue(permuted, k=target, dim=-1, keepdim=True)[0]\nfor dim in range(n_dims - len(kth_val.shape)):\nkth_val = torch.unsqueeze(kth_val, dim=-1)\n# Undo the permutation from earlier\nkth_val = kth_val.permute(*np.argsort(permutation))\nif not keepdims:\nfor dim in reversed(axis):\nkth_val = torch.squeeze(kth_val, dim=dim)\nresults.append(kth_val)\nif isinstance(percentiles, int):\nreturn results[0]\nelse:\nreturn torch.stack(results, dim=0)\nelif isinstance(tensor, np.ndarray):\nreturn np.percentile(tensor, percentiles, axis=axis, keepdims=keepdims, interpolation='nearest')\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/permute.html", "title": "permute", "text": ""}, {"location": "fastestimator/backend/permute.html#fastestimator.fastestimator.backend.permute.permute", "title": "<code>permute</code>", "text": "<p>Perform the specified <code>permutation</code> on the axes of a given <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\nb = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\nb = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to permute.</p> required <code>permutation</code> <code>List[int]</code> <p>The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> with axes swapped according to the <code>permutation</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\permute.py</code> <pre><code>def permute(tensor: Tensor, permutation: List[int]) -&gt; Tensor:\n\"\"\"Perform the specified `permutation` on the axes of a given `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(n, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(n, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(t, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(t, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]], [[8, 9], [10, 11]]])\n    b = fe.backend.permute(p, [2, 0, 1])  # [[[0, 2], [4, 6], [8, 10]], [[1, 3], [5, 7], [9, 11]]]\n    b = fe.backend.permute(P, [0, 2, 1])  # [[[0, 2], [1, 3]], [[4, 6], [5, 7]], [[8, 10], [9, 11]]]\n    ```\n    Args:\n        tensor: The tensor to permute.\n        permutation: The new axis order to be used. Should be a list containing all integers in range [0, tensor.ndim).\n    Returns:\n        The `tensor` with axes swapped according to the `permutation`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.transpose(tensor, perm=permutation)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.permute(*permutation)\nelif isinstance(tensor, np.ndarray):\nreturn np.transpose(tensor, axes=permutation)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/random_normal_like.html", "title": "random_normal_like", "text": ""}, {"location": "fastestimator/backend/random_normal_like.html#fastestimator.fastestimator.backend.random_normal_like.random_normal_like", "title": "<code>random_normal_like</code>", "text": "<p>Generate noise shaped like <code>tensor</code> from a random normal distribution with a given <code>mean</code> and <code>std</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\nb = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>mean</code> <code>float</code> <p>The mean of the normal distribution to be sampled.</p> <code>0.0</code> <code>std</code> <code>float</code> <p>The standard deviation of the normal distribution to be sampled.</p> <code>1.0</code> <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. This should be one of the floating point types.</p> <code>'float32'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of random normal noise with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\random_normal_like.py</code> <pre><code>def random_normal_like(\ntensor: Tensor,\nmean: float = 0.0,\nstd: float = 1.0,\ndtype: Union[None, str] = 'float32',\n) -&gt; Tensor:\n\"\"\"Generate noise shaped like `tensor` from a random normal distribution with a given `mean` and `std`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(n)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(n, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(t)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(t, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.random_normal_like(p)  # [[-0.6, 0.2], [1.9, -0.02]]\n    b = fe.backend.random_normal_like(P, mean=5.0)  # [[3.7, 5.7], [5.6, 3.6]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        mean: The mean of the normal distribution to be sampled.\n        std: The standard deviation of the normal distribution to be sampled.\n        dtype: The data type to be used when generating the resulting tensor. This should be one of the floating point\n            types.\n    Returns:\n        A tensor of random normal noise with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.random.normal(shape=tensor.shape, mean=mean, stddev=std, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.randn_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype]) * std + mean\nelif isinstance(tensor, np.ndarray):\nreturn np.random.normal(loc=mean, scale=std, size=tensor.shape).astype(dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_max.html", "title": "reduce_max", "text": ""}, {"location": "fastestimator/backend/reduce_max.html#fastestimator.fastestimator.backend.reduce_max.reduce_max", "title": "<code>reduce_max</code>", "text": "<p>Compute the maximum value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(n)  # 8\nb = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(t)  # 8\nb = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\nb = fe.backend.reduce_max(p)  # 8\nb = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\nb = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\nb = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the maximum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The maximum values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_max.py</code> <pre><code>def reduce_max(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the maximum value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(n)  # 8\n    b = fe.backend.reduce_max(n, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(n, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(t)  # 8\n    b = fe.backend.reduce_max(t, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(t, axis=[0,2])  # [6, 8]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    b = fe.backend.reduce_max(p)  # 8\n    b = fe.backend.reduce_max(p, axis=0)  # [[5, 6], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=1)  # [[3, 4], [7, 8]]\n    b = fe.backend.reduce_max(p, axis=[0,2])  # [6, 8]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the maximum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The maximum values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.reduce_max(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.max(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.max(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_mean.html", "title": "reduce_mean", "text": ""}, {"location": "fastestimator/backend/reduce_mean.html#fastestimator.fastestimator.backend.reduce_mean.reduce_mean", "title": "<code>reduce_mean</code>", "text": "<p>Compute the mean value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(n)  # 4.5\nb = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(t)  # 4.5\nb = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\nb = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_mean(p)  # 4.5\nb = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\nb = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\nb = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the mean along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_mean.py</code> <pre><code>def reduce_mean(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the mean value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(n)  # 4.5\n    b = fe.backend.reduce_mean(n, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(n, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(n, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(t)  # 4.5\n    b = fe.backend.reduce_mean(t, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(t, axis=1)  # [[2, 3], [3, 7]]\n    b = fe.backend.reduce_mean(t, axis=[0,2])  # [3.5, 5.5]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_mean(p)  # 4.5\n    b = fe.backend.reduce_mean(p, axis=0)  # [[3, 4], [5, 6]]\n    b = fe.backend.reduce_mean(p, axis=1)  # [[2, 3], [6, 7]]\n    b = fe.backend.reduce_mean(p, axis=[0,2])  # [3.5, 5.5]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the mean along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The mean values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.reduce_mean(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nif not keepdims:\nreturn tensor.mean()\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.mean(dim=ax, keepdim=keepdims)\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.mean(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_min.html", "title": "reduce_min", "text": ""}, {"location": "fastestimator/backend/reduce_min.html#fastestimator.fastestimator.backend.reduce_min.reduce_min", "title": "<code>reduce_min</code>", "text": "<p>Compute the min value along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(n)  # 1\nb = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(t)  # 1\nb = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_min(p)  # 1\nb = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\nb = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\nb = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the min along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The min values of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_min.py</code> <pre><code>def reduce_min(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the min value along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(n)  # 1\n    b = fe.backend.reduce_min(n, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(n, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(n, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(t)  # 1\n    b = fe.backend.reduce_min(t, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(t, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(t, axis=[0,2])  # [1, 3]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_min(p)  # 1\n    b = fe.backend.reduce_min(p, axis=0)  # [[1, 2], [3, 4]]\n    b = fe.backend.reduce_min(p, axis=1)  # [[1, 2], [5, 6]]\n    b = fe.backend.reduce_min(p, axis=[0,2])  # [1, 3]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the min along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The min values of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.reduce_min(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\naxis = to_list(axis)\naxis = reversed(sorted(axis))\nfor ax in axis:\ntensor = tensor.min(dim=ax, keepdim=keepdims)[0]\nreturn tensor\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.min(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reduce_sum.html", "title": "reduce_sum", "text": ""}, {"location": "fastestimator/backend/reduce_sum.html#fastestimator.fastestimator.backend.reduce_sum.reduce_sum", "title": "<code>reduce_sum</code>", "text": "<p>Compute the sum along a given <code>axis</code> of a <code>tensor</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(n)  # 36\nb = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(t)  # 36\nb = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reduce_sum(p)  # 36\nb = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\nb = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\nb = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Union[None, int, Sequence[int]]</code> <p>Which axis or collection of axes to compute the sum along.</p> <code>None</code> <code>keepdims</code> <code>bool</code> <p>Whether to preserve the number of dimensions during the reduction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sum of <code>tensor</code> along <code>axis</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reduce_sum.py</code> <pre><code>def reduce_sum(tensor: Tensor, axis: Union[None, int, Sequence[int]] = None, keepdims: bool = False) -&gt; Tensor:\n\"\"\"Compute the sum along a given `axis` of a `tensor`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(n)  # 36\n    b = fe.backend.reduce_sum(n, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(n, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(n, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(t)  # 36\n    b = fe.backend.reduce_sum(t, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(t, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(t, axis=[0,2])  # [14, 22]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reduce_sum(p)  # 36\n    b = fe.backend.reduce_sum(p, axis=0)  # [[6, 8], [10, 12]]\n    b = fe.backend.reduce_sum(p, axis=1)  # [[4, 6], [12, 14]]\n    b = fe.backend.reduce_sum(p, axis=[0,2])  # [14, 22]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis or collection of axes to compute the sum along.\n        keepdims: Whether to preserve the number of dimensions during the reduction.\n    Returns:\n        The sum of `tensor` along `axis`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.reduce_sum(tensor, axis=axis, keepdims=keepdims)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\naxis = list(range(len(tensor.shape)))\nreturn tensor.sum(dim=axis, keepdim=keepdims)\nelif isinstance(tensor, np.ndarray):\nif isinstance(axis, list):\naxis = tuple(axis)\nreturn np.sum(tensor, axis=axis, keepdims=keepdims)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/backend/reshape.html#fastestimator.fastestimator.backend.reshape.reshape", "title": "<code>reshape</code>", "text": "<p>Reshape a <code>tensor</code> to conform to a given shape.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\nb = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\nb = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\nb = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\nb = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>shape</code> <code>List[int]</code> <p>The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left should be packed into that axis.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\reshape.py</code> <pre><code>def reshape(tensor: Tensor, shape: List[int]) -&gt; Tensor:\n\"\"\"Reshape a `tensor` to conform to a given shape.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(n, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(n, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(n, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(n, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(t, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(t, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(t, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(t, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\n    b = fe.backend.reshape(p, shape=[-1])  # [1, 2, 3, 4, 5, 6, 7, 8]\n    b = fe.backend.reshape(p, shape=[2, 4])  # [[1, 2, 3, 4], [5, 6, 7, 8]]\n    b = fe.backend.reshape(p, shape=[4, 2])  # [[1, 2], [3, 4], [5, 6], [7, 8]]\n    b = fe.backend.reshape(p, shape=[2, 2, 2, 1])  # [[[[1], [2]], [[3], [4]]], [[[5], [6]], [[7], [8]]]]\n    ```\n    Args:\n        tensor: The input value.\n        shape: The new shape of the tensor. At most one value may be -1 which indicates that whatever values are left\n            should be packed into that axis.\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.reshape(tensor, shape=shape)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.reshape(tensor, shape=shape)\nelif isinstance(tensor, np.ndarray):\nreturn np.reshape(tensor, shape)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/save_model.html", "title": "save_model", "text": ""}, {"location": "fastestimator/backend/save_model.html#fastestimator.fastestimator.backend.save_model.save_model", "title": "<code>save_model</code>", "text": "<p>Save <code>model</code> weights to a specific directory.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nfe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to save.</p> required <code>save_dir</code> <code>str</code> <p>Directory into which to write the <code>model</code> weights.</p> required <code>model_name</code> <code>Optional[str]</code> <p>The name of the model (used for naming the weights file). If None, model.model_name will be used.</p> <code>None</code> <code>save_optimizer</code> <code>bool</code> <p>Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.</p> <code>False</code> <p>Returns:</p> Type Description <p>The saved model path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\save_model.py</code> <pre><code>def save_model(model: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmodel_name: Optional[str] = None,\nsave_optimizer: bool = False):\n\"\"\"Save `model` weights to a specific directory.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.h5' file inside /tmp directory\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    fe.backend.save_model(m, save_dir=\"/tmp\", model_name=\"test\")  # Generates 'test.pt' file inside /tmp directory\n    ```\n    Args:\n        model: A neural network instance to save.\n        save_dir: Directory into which to write the `model` weights.\n        model_name: The name of the model (used for naming the weights file). If None, model.model_name will be used.\n        save_optimizer: Whether to save optimizer. If True, optimizer will be saved in a separate file at same folder.\n    Returns:\n        The saved model path.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nassert hasattr(model, \"fe_compiled\") and model.fe_compiled, \"model must be built by fe.build\"\nif model_name is None:\nmodel_name = model.model_name\nsave_dir = os.path.normpath(save_dir)\nos.makedirs(save_dir, exist_ok=True)\nif isinstance(model, tf.keras.Model):\nmodel_path = os.path.join(save_dir, \"{}.h5\".format(model_name))\nmodel.save_weights(model_path)\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pkl\".format(model_name))\nwith open(optimizer_path, 'wb') as f:\npickle.dump(model.current_optimizer.get_weights(), f)\nreturn model_path\nelif isinstance(model, torch.nn.Module):\nmodel_path = os.path.join(save_dir, \"{}.pt\".format(model_name))\ntorch.save(model.state_dict(), model_path)\nif save_optimizer:\nassert model.current_optimizer, \"optimizer does not exist\"\noptimizer_path = os.path.join(save_dir, \"{}_opt.pt\".format(model_name))\ntorch.save(model.current_optimizer.state_dict(), optimizer_path)\nreturn model_path\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/set_lr.html", "title": "set_lr", "text": ""}, {"location": "fastestimator/backend/set_lr.html#fastestimator.fastestimator.backend.set_lr.set_lr", "title": "<code>set_lr</code>", "text": "<p>Set the learning rate of a given <code>model</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\nfe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to modify.</p> required <code>lr</code> <code>float</code> <p>The learning rate to assign to the <code>model</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\set_lr.py</code> <pre><code>def set_lr(model: Union[tf.keras.Model, torch.nn.Module], lr: float):\n\"\"\"Set the learning rate of a given `model`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")  # m.optimizer.lr == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.lr == 0.8\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")  # m.optimizer.param_groups[-1]['lr'] == 0.001\n    fe.backend.set_lr(m, lr=0.8)  # m.optimizer.param_groups[-1]['lr'] == 0.8\n    ```\n    Args:\n        model: A neural network instance to modify.\n        lr: The learning rate to assign to the `model`.\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nif isinstance(model, tf.keras.Model):\ntf.keras.backend.set_value(model.current_optimizer.lr, lr)\nelif isinstance(model, torch.nn.Module):\nfor param_group in model.current_optimizer.param_groups:\nparam_group['lr'] = lr\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/sign.html", "title": "sign", "text": ""}, {"location": "fastestimator/backend/sign.html#fastestimator.fastestimator.backend.sign.sign", "title": "<code>sign</code>", "text": "<p>Compute the sign of a tensor.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([-2, 7, -19])\nb = fe.backend.sign(n)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([-2, 7, -19])\nb = fe.backend.sign(t)  # [-1, 1, -1]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([-2, 7, -19])\nb = fe.backend.sign(p)  # [-1, 1, -1]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The sign of each value of the <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\sign.py</code> <pre><code>def sign(tensor: Tensor) -&gt; Tensor:\n\"\"\"Compute the sign of a tensor.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([-2, 7, -19])\n    b = fe.backend.sign(n)  # [-1, 1, -1]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([-2, 7, -19])\n    b = fe.backend.sign(t)  # [-1, 1, -1]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([-2, 7, -19])\n    b = fe.backend.sign(p)  # [-1, 1, -1]\n    ```\n    Args:\n        tensor: The input value.\n    Returns:\n        The sign of each value of the `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.sign(tensor)\nelif isinstance(tensor, torch.Tensor):\nreturn tensor.sign()\nelif isinstance(tensor, np.ndarray):\nreturn np.sign(tensor)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/sparse_categorical_crossentropy.html", "title": "sparse_categorical_crossentropy", "text": ""}, {"location": "fastestimator/backend/sparse_categorical_crossentropy.html#fastestimator.fastestimator.backend.sparse_categorical_crossentropy.sparse_categorical_crossentropy", "title": "<code>sparse_categorical_crossentropy</code>", "text": "<p>Compute sparse categorical crossentropy.</p> <p>Note that if any of the <code>y_pred</code> values are exactly 0, this will result in a NaN output. If <code>from_logits</code> is False, then each entry of <code>y_pred</code> should sum to 1. If they don't sum to 1 then tf and torch backends will result in different numerical values.</p> <p>This method can be used with TensorFlow tensors: <pre><code>true = tf.constant([[1], [0], [2]])\npred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>true = torch.tensor([[1], [0], [2]])\npred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\nb = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>y_pred</code> <code>Tensor</code> <p>Prediction with a shape like (Batch, C). dtype: float32.</p> required <code>y_true</code> <code>Tensor</code> <p>Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.</p> required <code>from_logits</code> <code>bool</code> <p>Whether y_pred is from logits. If True, a softmax will be applied to the prediction.</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The sparse categorical crossentropy between <code>y_pred</code> and <code>y_true</code>. A scalar if <code>average_loss</code> is True, else a</p> <code>Tensor</code> <p>tensor with the shape (Batch).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>y_true</code> or <code>y_pred</code> are unacceptable data types.</p> Source code in <code>fastestimator\\fastestimator\\backend\\sparse_categorical_crossentropy.py</code> <pre><code>def sparse_categorical_crossentropy(y_pred: Tensor,\ny_true: Tensor,\nfrom_logits: bool = False,\naverage_loss: bool = True) -&gt; Tensor:\n\"\"\"Compute sparse categorical crossentropy.\n    Note that if any of the `y_pred` values are exactly 0, this will result in a NaN output. If `from_logits` is\n    False, then each entry of `y_pred` should sum to 1. If they don't sum to 1 then tf and torch backends will\n    result in different numerical values.\n    This method can be used with TensorFlow tensors:\n    ```python\n    true = tf.constant([[1], [0], [2]])\n    pred = tf.constant([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    true = torch.tensor([[1], [0], [2]])\n    pred = torch.tensor([[0.1, 0.8, 0.1], [0.9, 0.05, 0.05], [0.1, 0.2, 0.7]])\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true)  # 0.228\n    b = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=true, average_loss=False)  # [0.22, 0.11, 0.36]\n    ```\n    Args:\n        y_pred: Prediction with a shape like (Batch, C). dtype: float32.\n        y_true: Ground truth class labels with a shape like (Batch) or (Batch, 1). dtype: int.\n        from_logits: Whether y_pred is from logits. If True, a softmax will be applied to the prediction.\n        average_loss: Whether to average the element-wise loss.\n    Returns:\n        The sparse categorical crossentropy between `y_pred` and `y_true`. A scalar if `average_loss` is True, else a\n        tensor with the shape (Batch).\n    Raises:\n        AssertionError: If `y_true` or `y_pred` are unacceptable data types.\n    \"\"\"\nassert type(y_pred) == type(y_true), \"y_pred and y_true must be same tensor type\"\nassert isinstance(y_pred, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_pred\"\nassert isinstance(y_true, (tf.Tensor, torch.Tensor)), \"only support tf.Tensor or torch.Tensor as y_true\"\nif isinstance(y_pred, tf.Tensor):\nce = tf.losses.sparse_categorical_crossentropy(y_pred=y_pred, y_true=y_true, from_logits=from_logits)\nelse:\ny_true = y_true.view(-1)\nif from_logits:\nce = torch.nn.CrossEntropyLoss(reduction=\"none\")(input=y_pred, target=y_true.long())\nelse:\nce = torch.nn.NLLLoss(reduction=\"none\")(input=torch.log(y_pred), target=y_true.long())\nif average_loss:\nce = reduce_mean(ce)\nreturn ce\n</code></pre>"}, {"location": "fastestimator/backend/squeeze.html", "title": "squeeze", "text": ""}, {"location": "fastestimator/backend/squeeze.html#fastestimator.fastestimator.backend.squeeze.squeeze", "title": "<code>squeeze</code>", "text": "<p>Remove an <code>axis</code> from a <code>tensor</code> if that axis has length 1.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\nb = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\nb = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\nb = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input value.</p> required <code>axis</code> <code>Optional[int]</code> <p>Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The reshaped <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\squeeze.py</code> <pre><code>def squeeze(tensor: Tensor, axis: Optional[int] = None) -&gt; Tensor:\n\"\"\"Remove an `axis` from a `tensor` if that axis has length 1.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(n)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(n, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(n, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(t)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(t, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(t, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[[[1],[2]]],[[[3],[4]]],[[[5],[6]]]])  # shape == (3, 1, 2, 1)\n    b = fe.backend.squeeze(p)  # [[1, 2], [3, 4], [5, 6]]\n    b = fe.backend.squeeze(p, axis=1)  # [[[1], [2]], [[3], [4]], [[5], [6]]]\n    b = fe.backend.squeeze(p, axis=3)  # [[[1, 2]], [[3, 4]], [[5, 6]]]\n    ```\n    Args:\n        tensor: The input value.\n        axis: Which axis to squeeze along, which must have length==1 (or pass None to squeeze all length 1 axes).\n    Returns:\n        The reshaped `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.squeeze(tensor, axis=axis)\nelif isinstance(tensor, torch.Tensor):\nif axis is None:\nreturn torch.squeeze(tensor)\nelse:\nreturn torch.squeeze(tensor, dim=axis)\nelif isinstance(tensor, np.ndarray):\nreturn np.squeeze(tensor, axis=axis)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/to_number.html", "title": "to_number", "text": ""}, {"location": "fastestimator/backend/to_number.html#fastestimator.fastestimator.backend.to_number.to_number", "title": "<code>to_number</code>", "text": "<p>Convert an input value into a Numpy ndarray.</p> <p>This method can be used with Python and Numpy data: <pre><code>b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\nb = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\nn = np.array([1, 2, 3])\nb = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([1, 2, 3])\nb = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([1, 2, 3])\nb = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[tf.Tensor, torch.Tensor, np.ndarray, int, float]</code> <p>The value to be converted into a np.ndarray.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>An ndarray corresponding to the given <code>data</code>.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_number.py</code> <pre><code>def to_number(data: Union[tf.Tensor, torch.Tensor, np.ndarray, int, float]) -&gt; np.ndarray:\n\"\"\"Convert an input value into a Numpy ndarray.\n    This method can be used with Python and Numpy data:\n    ```python\n    b = fe.backend.to_number(5)  # 5 (type==np.ndarray)\n    b = fe.backend.to_number(4.0)  # 4.0 (type==np.ndarray)\n    n = np.array([1, 2, 3])\n    b = fe.backend.to_number(n)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([1, 2, 3])\n    b = fe.backend.to_number(t)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([1, 2, 3])\n    b = fe.backend.to_number(p)  # [1, 2, 3] (type==np.ndarray)\n    ```\n    Args:\n        data: The value to be converted into a np.ndarray.\n    Returns:\n        An ndarray corresponding to the given `data`.\n    \"\"\"\nif isinstance(data, tf.Tensor):\ndata = data.numpy()\nelif isinstance(data, torch.Tensor):\nif data.requires_grad:\ndata = data.detach().numpy()\nelse:\ndata = data.numpy()\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/backend/to_shape.html", "title": "to_shape", "text": ""}, {"location": "fastestimator/backend/to_shape.html#fastestimator.fastestimator.backend.to_shape.to_shape", "title": "<code>to_shape</code>", "text": "<p>Compute the shape of tensors within a collection of <code>data</code>recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nshape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\nshape = fe.backend.to_shape(data, add_batch=True)\n# {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\nshape = fe.backend.to_shape(data, exact_shape=False)\n# {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>add_batch</code> <p>Whether to prepend a batch dimension to the shapes.</p> <code>False</code> <code>exact_shape</code> <p>Whether to return the exact shapes, or if False to fill the shapes with None values.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Collection, Tensor]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their shapes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_shape.py</code> <pre><code>def to_shape(data: Union[Collection, Tensor], add_batch=False, exact_shape=True) -&gt; Union[Collection, Tensor]:\n\"\"\"Compute the shape of tensors within a collection of `data`recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    shape = fe.backend.to_shape(data)  # {\"x\": (10, 15), \"y\":[(4), (5, 3)], \"z\": {\"key\": (2, 2)}}\n    shape = fe.backend.to_shape(data, add_batch=True)\n    # {\"x\": (None, 10, 15), \"y\":[(None, 4), (None, 5, 3)], \"z\": {\"key\": (None, 2, 2)}}\n    shape = fe.backend.to_shape(data, exact_shape=False)\n    # {\"x\": (None, None), \"y\":[(None), (None, None)], \"z\": {\"key\": (None, None)}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        add_batch: Whether to prepend a batch dimension to the shapes.\n        exact_shape: Whether to return the exact shapes, or if False to fill the shapes with None values.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their shapes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_shape(value, add_batch, exact_shape) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_shape(val, add_batch, exact_shape) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_shape(val, add_batch, exact_shape) for val in data])\nelif isinstance(data, set):\nreturn set([to_shape(val, add_batch, exact_shape) for val in data])\nelif hasattr(data, \"shape\"):\nshape = data.shape\nif not exact_shape:\nshape = [None] * len(shape)\nif add_batch:\nshape = [None] + list(shape)\nreturn shape\nelse:\nreturn to_shape(np.array(data), add_batch, exact_shape)\n</code></pre>"}, {"location": "fastestimator/backend/to_tensor.html", "title": "to_tensor", "text": ""}, {"location": "fastestimator/backend/to_tensor.html#fastestimator.fastestimator.backend.to_tensor.to_tensor", "title": "<code>to_tensor</code>", "text": "<p>Convert tensors within a collection of <code>data</code> to a given <code>target_type</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\np = fe.backend.to_tensor(data, target_type='torch')\n# {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\nt = fe.backend.to_tensor(data, target_type='tf')\n# {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <code>target_type</code> <code>str</code> <p>What kind of tensor(s) to create, either \"tf\" or \"torch\".</p> required <p>Returns:</p> Type Description <code>Union[Collection, Tensor]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors converted to the <code>target_type</code>.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_tensor.py</code> <pre><code>def to_tensor(data: Union[Collection, Tensor], target_type: str) -&gt; Union[Collection, Tensor]:\n\"\"\"Convert tensors within a collection of `data` to a given `target_type` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15)), \"y\":[np.ones((4)), np.ones((5, 3))], \"z\":{\"key\":np.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15)), \"y\":[tf.ones((4)), tf.ones((5, 3))], \"z\":{\"key\":tf.ones((2,2))}}\n    p = fe.backend.to_tensor(data, target_type='torch')\n    # {\"x\": &lt;torch.Tensor&gt;, \"y\":[&lt;torch.Tensor&gt;, &lt;torch.Tensor&gt;], \"z\": {\"key\": &lt;torch.Tensor&gt;}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15)), \"y\":[torch.ones((4)), torch.ones((5, 3))], \"z\":{\"key\":torch.ones((2,2))}}\n    t = fe.backend.to_tensor(data, target_type='tf')\n    # {\"x\": &lt;tf.Tensor&gt;, \"y\":[&lt;tf.Tensor&gt;, &lt;tf.Tensor&gt;], \"z\": {\"key\": &lt;tf.Tensor&gt;}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n        target_type: What kind of tensor(s) to create, either \"tf\" or \"torch\".\n    Returns:\n        A collection with the same structure as `data`, but with any tensors converted to the `target_type`.\n    \"\"\"\ntarget_instance = {\"tf\": tf.Tensor, \"torch\": torch.Tensor}\nconversion_function = {\"tf\": tf.convert_to_tensor, \"torch\": torch.from_numpy}\nif isinstance(data, target_instance[target_type]):\nreturn data\nelif isinstance(data, dict):\nreturn {key: to_tensor(value, target_type) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_tensor(val, target_type) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_tensor(val, target_type) for val in data])\nelif isinstance(data, set):\nreturn set([to_tensor(val, target_type) for val in data])\nelse:\nreturn conversion_function[target_type](np.array(data))\n</code></pre>"}, {"location": "fastestimator/backend/to_type.html", "title": "to_type", "text": ""}, {"location": "fastestimator/backend/to_type.html#fastestimator.fastestimator.backend.to_type.to_type", "title": "<code>to_type</code>", "text": "<p>Compute the data types of tensors within a collection of <code>data</code> recursively.</p> <p>This method can be used with Numpy data: <pre><code>data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n\"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\ntypes = fe.backend.to_type(data)\n# {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\ndtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\ntypes = fe.backend.to_type(data)\n# {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Collection, Tensor]</code> <p>A tensor or possibly nested collection of tensors.</p> required <p>Returns:</p> Type Description <code>Union[Collection, str]</code> <p>A collection with the same structure as <code>data</code>, but with any tensors substituted for their dtypes.</p> Source code in <code>fastestimator\\fastestimator\\backend\\to_type.py</code> <pre><code>def to_type(data: Union[Collection, Tensor]) -&gt; Union[Collection, str]:\n\"\"\"Compute the data types of tensors within a collection of `data` recursively.\n    This method can be used with Numpy data:\n    ```python\n    data = {\"x\": np.ones((10,15), dtype=\"float32\"), \"y\":[np.ones((4), dtype=\"int8\"), np.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":np.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': dtype('float32'), 'y': [dtype('int8'), dtype('float64')], 'z': {'key': dtype('int64')}}\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    data = {\"x\": tf.ones((10,15), dtype=\"float32\"), \"y\":[tf.ones((4), dtype=\"int8\"), tf.ones((5, 3), dtype=\"double\")],\n        \"z\":{\"key\":tf.ones((2,2), dtype=\"int64\")}}\n    types = fe.backend.to_type(data)\n    # {'x': tf.float32, 'y': [tf.int8, tf.float64], 'z': {'key': tf.int64}}\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    data = {\"x\": torch.ones((10,15), dtype=torch.float32), \"y\":[torch.ones((4), dtype=torch.int8), torch.ones((5, 3),\n        dtype=torch.double)], \"z\":{\"key\":torch.ones((2,2), dtype=torch.long)}}\n    types = fe.backend.to_type(data)\n    # {'x': torch.float32, 'y': [torch.int8, torch.float64], 'z': {'key': torch.int64}}\n    ```\n    Args:\n        data: A tensor or possibly nested collection of tensors.\n    Returns:\n        A collection with the same structure as `data`, but with any tensors substituted for their dtypes.\n    \"\"\"\nif isinstance(data, dict):\nreturn {key: to_type(value) for (key, value) in data.items()}\nelif isinstance(data, list):\nreturn [to_type(val) for val in data]\nelif isinstance(data, tuple):\nreturn tuple([to_type(val) for val in data])\nelif isinstance(data, set):\nreturn set([to_type(val) for val in data])\nelif hasattr(data, \"dtype\"):\nreturn data.dtype\nelse:\nreturn np.array(data).dtype\n</code></pre>"}, {"location": "fastestimator/backend/update_model.html", "title": "update_model", "text": ""}, {"location": "fastestimator/backend/update_model.html#fastestimator.fastestimator.backend.update_model.update_model", "title": "<code>update_model</code>", "text": "<p>Update <code>model</code> weights based on a given <code>loss</code>.</p> <p>This method can be used with TensorFlow models: <pre><code>m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\nx = tf.ones((3,28,28,1))  # (batch, height, width, channels)\ny = tf.constant((1, 0, 1))\nwith tf.GradientTape(persistent=True) as tape:\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\nfe.backend.update_model(m, loss=loss, tape=tape)\n</code></pre></p> <p>This method can be used with PyTorch models: <pre><code>m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\nx = torch.ones((3,1,28,28))  # (batch, channels, height, width)\ny = torch.tensor((1, 0, 1))\npred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\nloss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\nfe.backend.update_model(m, loss=loss)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A neural network instance to update.</p> required <code>loss</code> <code>Union[tf.Tensor, torch.Tensor]</code> <p>A loss value to compute gradients from.</p> required <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>A TensorFlow GradientTape which was recording when the <code>loss</code> was computed (iff using TensorFlow).</p> <code>None</code> <code>retain_graph</code> <code>bool</code> <p>Whether to keep the model graph in memory (applicable only for PyTorch).</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\update_model.py</code> <pre><code>def update_model(model: Union[tf.keras.Model, torch.nn.Module],\nloss: Union[tf.Tensor, torch.Tensor],\ntape: Optional[tf.GradientTape] = None,\nretain_graph: bool = True):\n\"\"\"Update `model` weights based on a given `loss`.\n    This method can be used with TensorFlow models:\n    ```python\n    m = fe.build(fe.architecture.tensorflow.LeNet, optimizer_fn=\"adam\")\n    x = tf.ones((3,28,28,1))  # (batch, height, width, channels)\n    y = tf.constant((1, 0, 1))\n    with tf.GradientTape(persistent=True) as tape:\n        pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n        loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n        fe.backend.update_model(m, loss=loss, tape=tape)\n    ```\n    This method can be used with PyTorch models:\n    ```python\n    m = fe.build(fe.architecture.pytorch.LeNet, optimizer_fn=\"adam\")\n    x = torch.ones((3,1,28,28))  # (batch, channels, height, width)\n    y = torch.tensor((1, 0, 1))\n    pred = fe.backend.feed_forward(m, x)  # [[~0.5, ~0.5], [~0.5, ~0.5], [~0.5, ~0.5]]\n    loss = fe.backend.sparse_categorical_crossentropy(y_pred=pred, y_true=y)  # ~2.3\n    fe.backend.update_model(m, loss=loss)\n    ```\n    Args:\n        model: A neural network instance to update.\n        loss: A loss value to compute gradients from.\n        tape: A TensorFlow GradientTape which was recording when the `loss` was computed (iff using TensorFlow).\n        retain_graph: Whether to keep the model graph in memory (applicable only for PyTorch).\n    Raises:\n        ValueError: If `model` is an unacceptable data type.\n    \"\"\"\nloss = reduce_mean(loss)\nif isinstance(model, tf.keras.Model):\nstrategy = tf.distribute.get_strategy()\nif isinstance(strategy, tf.distribute.MirroredStrategy):\nloss = loss / strategy.num_replicas_in_sync\ngradients = get_gradient(loss, model.trainable_variables, tape=tape)\nwith tape.stop_recording():\nmodel.current_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\nelif isinstance(model, torch.nn.Module):\ngradients = get_gradient(loss, model.parameters(), retain_graph=retain_graph)\nfor gradient, parameter in zip(gradients, model.parameters()):\nparameter.grad = gradient\nmodel.current_optimizer.step()\nelse:\nraise ValueError(\"Unrecognized model instance {}\".format(type(model)))\n</code></pre>"}, {"location": "fastestimator/backend/watch.html", "title": "watch", "text": ""}, {"location": "fastestimator/backend/watch.html#fastestimator.fastestimator.backend.watch.watch", "title": "<code>watch</code>", "text": "<p>Monitor the given <code>tensor</code> for later gradient computations.</p> <p>This method can be used with TensorFlow tensors: <pre><code>x = tf.ones((3,28,28,1))\nwith tf.GradientTape(persistent=True) as tape:\nx = fe.backend.watch(x, tape=tape)\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>x = torch.ones((3,1,28,28))  # x.requires_grad == False\nx = fe.backend.watch(x)  # x.requires_grad == True\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to be monitored.</p> required <code>tape</code> <code>Optional[tf.GradientTape]</code> <p>A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The <code>tensor</code> or a copy of the <code>tensor</code> which is being tracked for gradient computations. This value is only</p> <code>Tensor</code> <p>needed if using PyTorch as the backend.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\watch.py</code> <pre><code>def watch(tensor: Tensor, tape: Optional[tf.GradientTape] = None) -&gt; Tensor:\n\"\"\"Monitor the given `tensor` for later gradient computations.\n    This method can be used with TensorFlow tensors:\n    ```python\n    x = tf.ones((3,28,28,1))\n    with tf.GradientTape(persistent=True) as tape:\n        x = fe.backend.watch(x, tape=tape)\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    x = torch.ones((3,1,28,28))  # x.requires_grad == False\n    x = fe.backend.watch(x)  # x.requires_grad == True\n    ```\n    Args:\n        tensor: The tensor to be monitored.\n        tape: A TensorFlow GradientTape which will be used to record gradients (iff using TensorFlow for the backend).\n    Returns:\n        The `tensor` or a copy of the `tensor` which is being tracked for gradient computations. This value is only\n        needed if using PyTorch as the backend.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\ntape.watch(tensor)\nreturn tensor\nelif isinstance(tensor, torch.Tensor):\nif tensor.requires_grad:\nreturn tensor\n# It is tempting to just do tensor.requires_grad = True here, but that will lead to trouble\nreturn tensor.detach().requires_grad_(True)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/backend/zeros_like.html", "title": "zeros_like", "text": ""}, {"location": "fastestimator/backend/zeros_like.html#fastestimator.fastestimator.backend.zeros_like.zeros_like", "title": "<code>zeros_like</code>", "text": "<p>Generate zeros shaped like <code>tensor</code> with a specified <code>dtype</code>.</p> <p>This method can be used with Numpy data: <pre><code>n = np.array([[0,1],[2,3]])\nb = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with TensorFlow tensors: <pre><code>t = tf.constant([[0,1],[2,3]])\nb = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>This method can be used with PyTorch tensors: <pre><code>p = torch.tensor([[0,1],[2,3]])\nb = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\nb = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor whose shape will be copied.</p> required <code>dtype</code> <code>Union[None, str]</code> <p>The data type to be used when generating the resulting tensor. If None then the <code>tensor</code> dtype is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of zeros with the same shape as <code>tensor</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>tensor</code> is an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\backend\\zeros_like.py</code> <pre><code>def zeros_like(tensor: Tensor, dtype: Union[None, str] = None) -&gt; Tensor:\n\"\"\"Generate zeros shaped like `tensor` with a specified `dtype`.\n    This method can be used with Numpy data:\n    ```python\n    n = np.array([[0,1],[2,3]])\n    b = fe.backend.zeros_like(n)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(n, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with TensorFlow tensors:\n    ```python\n    t = tf.constant([[0,1],[2,3]])\n    b = fe.backend.zeros_like(t)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(t, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    This method can be used with PyTorch tensors:\n    ```python\n    p = torch.tensor([[0,1],[2,3]])\n    b = fe.backend.zeros_like(p)  # [[0, 0], [0, 0]]\n    b = fe.backend.zeros_like(p, dtype=\"float32\")  # [[0.0, 0.0], [0.0, 0.0]]\n    ```\n    Args:\n        tensor: The tensor whose shape will be copied.\n        dtype: The data type to be used when generating the resulting tensor. If None then the `tensor` dtype is used.\n    Returns:\n        A tensor of zeros with the same shape as `tensor`.\n    Raises:\n        ValueError: If `tensor` is an unacceptable data type.\n    \"\"\"\nif isinstance(tensor, tf.Tensor):\nreturn tf.zeros_like(tensor, dtype=dtype)\nelif isinstance(tensor, torch.Tensor):\nreturn torch.zeros_like(tensor, dtype=STRING_TO_TORCH_DTYPE[dtype])\nelif isinstance(tensor, np.ndarray):\nreturn np.zeros_like(tensor, dtype=dtype)\nelse:\nraise ValueError(\"Unrecognized tensor type {}\".format(type(tensor)))\n</code></pre>"}, {"location": "fastestimator/cli/cli_util.html", "title": "cli_util", "text": ""}, {"location": "fastestimator/cli/cli_util.html#fastestimator.fastestimator.cli.cli_util.SaveAction", "title": "<code>SaveAction</code>", "text": "<p>         Bases: <code>argparse.Action</code></p> <p>A customized save action for use with argparse.</p> <p>A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file is invoked directly during argument parsing.</p> <p>Parameters:</p> Name Type Description Default <code>option_strings</code> <code>Sequence[str]</code> <p>A list of command-line option strings which should be associated with this action.</p> required <code>dest</code> <code>str</code> <p>The name of the attribute to hold the created object(s).</p> required <code>nargs</code> <code>Union[int, str, None]</code> <p>The number of command line arguments to be consumed.</p> <code>'?'</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Pass-through keyword arguments.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\cli\\cli_util.py</code> <pre><code>class SaveAction(argparse.Action):\n\"\"\"A customized save action for use with argparse.\n    A custom save action which is used to populate a secondary variable inside of an exclusive group. Used if this file\n    is invoked directly during argument parsing.\n    Args:\n        option_strings: A list of command-line option strings which should be associated with this action.\n        dest: The name of the attribute to hold the created object(s).\n        nargs: The number of command line arguments to be consumed.\n        **kwargs: Pass-through keyword arguments.\n    \"\"\"\ndef __init__(self,\noption_strings: Sequence[str],\ndest: str,\nnargs: Union[int, str, None] = '?',\n**kwargs: Dict[str, Any]) -&gt; None:\nif '?' != nargs:\nraise ValueError(\"nargs must be \\'?\\'\")\nsuper().__init__(option_strings, dest, nargs, **kwargs)\ndef __call__(self,\nparser: argparse.ArgumentParser,\nnamespace: argparse.Namespace,\nvalues: Optional[str],\noption_string: Optional[str] = None) -&gt; None:\n\"\"\"Invokes the save action, writing two values into the namespace.\n        Args:\n            parser: The active argument parser (ignored by this implementation).\n            namespace: The current namespace to be written to.\n            values: The value to write into the namespace.\n            option_string: An option_string (ignored by this implementation).\n        \"\"\"\nsetattr(namespace, self.dest, True)\nsetattr(namespace, self.dest + '_dir', values if values is None else os.path.join(values, ''))\n</code></pre>"}, {"location": "fastestimator/cli/cli_util.html#fastestimator.fastestimator.cli.cli_util.parse_cli_to_dictionary", "title": "<code>parse_cli_to_dictionary</code>", "text": "<p>Convert a list of strings into a dictionary with python objects as values.</p> <pre><code>a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"]) \n# {'epochs': 5, 'test': 'this', 'lr': 0.74}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[str]</code> <p>A list of input strings from the cli.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary constructed from the <code>input_list</code>, with values converted to python objects where applicable.</p> Source code in <code>fastestimator\\fastestimator\\cli\\cli_util.py</code> <pre><code>def parse_cli_to_dictionary(input_list: List[str]) -&gt; Dict[str, Any]:\n\"\"\"Convert a list of strings into a dictionary with python objects as values.\n    ```python\n    a = parse_cli_to_dictionary([\"--epochs\", \"5\", \"--test\", \"this\", \"--lr\", \"0.74\"]) \n    # {'epochs': 5, 'test': 'this', 'lr': 0.74}\n    ```\n    Args:\n        input_list: A list of input strings from the cli.\n    Returns:\n        A dictionary constructed from the `input_list`, with values converted to python objects where applicable.\n    \"\"\"\nresult = {}\nif input_list is None:\nreturn result\nkey = \"\"\nval = \"\"\nidx = 0\nwhile idx &lt; len(input_list):\nif input_list[idx].startswith(\"--\"):\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nval = \"\"\nkey = input_list[idx].strip('--')\nelse:\nval += input_list[idx]\nidx += 1\nif len(key) &gt; 0:\nresult[key] = parse_string_to_python(val)\nreturn result\n</code></pre>"}, {"location": "fastestimator/cli/logs.html", "title": "logs", "text": ""}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.configure_log_parser", "title": "<code>configure_log_parser</code>", "text": "<p>Add a logging parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def configure_log_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a logging parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('logs',\ndescription='Generates comparison graphs amongst one or more log files',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\nparser.add_argument('log_dir',\nmetavar='&lt;Log Dir&gt;',\ntype=str,\nhelp=\"The path to a folder containing one or more log files\")\nparser.add_argument('--extension',\nmetavar='E',\ntype=str,\nhelp=\"The file type / extension of your logs\",\ndefault=\".txt\")\nparser.add_argument('--recursive', action='store_true', help=\"Recursively search sub-directories for log files\")\nparser.add_argument('--ignore',\nmetavar='I',\ntype=str,\nnargs='+',\nhelp=\"The names of metrics to ignore though they may be present in the log files\")\nparser.add_argument('--smooth',\nmetavar='&lt;float&gt;',\ntype=float,\nhelp=\"The amount of gaussian smoothing to apply (zero for no smoothing)\",\ndefault=1)\nparser.add_argument('--pretty_names', help=\"Clean up the metric names for display\", action='store_true')\nlegend_group = parser.add_argument_group('legend arguments')\nlegend_x_group = legend_group.add_mutually_exclusive_group(required=False)\nlegend_x_group.add_argument('--common_legend',\ndest='share_legend',\nhelp=\"Generate one legend total\",\naction='store_true',\ndefault=True)\nlegend_x_group.add_argument('--split_legend',\ndest='share_legend',\nhelp=\"Generate one legend per graph\",\naction='store_false',\ndefault=False)\nsave_group = parser.add_argument_group('output arguments')\nsave_x_group = save_group.add_mutually_exclusive_group(required=False)\nsave_x_group.add_argument(\n'--save',\nnargs='?',\nmetavar='&lt;Save Dir&gt;',\ndest='save',\naction=SaveAction,\ndefault=False,\nhelp=\"Save the output image. May be accompanied by a directory into \\\n                                              which the file is saved. If no output directory is specified, the log \\\n                                              directory will be used\")\nsave_x_group.add_argument('--display',\ndest='save',\naction='store_false',\nhelp=\"Render the image to the UI (rather than saving it)\",\ndefault=True)\nsave_x_group.set_defaults(save_dir=None)\nparser.set_defaults(func=logs)\n</code></pre>"}, {"location": "fastestimator/cli/logs.html#fastestimator.fastestimator.cli.logs.logs", "title": "<code>logs</code>", "text": "<p>A method to invoke the FE logging function using CLI-provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>The arguments to be fed to the parse_log_dir() method.</p> required <code>unknown</code> <code>List[str]</code> <p>Any cli arguments not matching known inputs for the parse_log_dir() method.</p> required <p>Raises:</p> Type Description <code>SystemExit</code> <p>If <code>unknown</code> arguments were provided by the user.</p> Source code in <code>fastestimator\\fastestimator\\cli\\logs.py</code> <pre><code>def logs(args: Dict[str, Any], unknown: List[str]) -&gt; None:\n\"\"\"A method to invoke the FE logging function using CLI-provided arguments.\n    Args:\n        args: The arguments to be fed to the parse_log_dir() method.\n        unknown: Any cli arguments not matching known inputs for the parse_log_dir() method.\n    Raises:\n        SystemExit: If `unknown` arguments were provided by the user.\n    \"\"\"\nif len(unknown) &gt; 0:\nprint(\"error: unrecognized arguments: \", str.join(\", \", unknown))\nsys.exit(-1)\nparse_log_dir(args['log_dir'],\nargs['extension'],\nargs['recursive'],\nargs['smooth'],\nargs['save'],\nargs['save_dir'],\nargs['ignore'],\nargs['share_legend'],\nargs['pretty_names'])\n</code></pre>"}, {"location": "fastestimator/cli/main.html", "title": "main", "text": ""}, {"location": "fastestimator/cli/main.html#fastestimator.fastestimator.cli.main.run", "title": "<code>run</code>", "text": "<p>A function which invokes the various argument parsers and then runs the requested subroutine.</p> Source code in <code>fastestimator\\fastestimator\\cli\\main.py</code> <pre><code>def run() -&gt; None:\n\"\"\"A function which invokes the various argument parsers and then runs the requested subroutine.\n    \"\"\"\nparser = argparse.ArgumentParser(allow_abbrev=False)\nsubparsers = parser.add_subparsers()\n# In python 3.7 the following 2 lines could be put into the .add_subparsers() call\nsubparsers.required = True\nsubparsers.dest = 'mode'\nconfigure_train_parser(subparsers)\nconfigure_test_parser(subparsers)\nconfigure_log_parser(subparsers)\nargs, unknown = parser.parse_known_args()\nargs.func(vars(args), unknown)\n</code></pre>"}, {"location": "fastestimator/cli/train.html", "title": "train", "text": ""}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_test_parser", "title": "<code>configure_test_parser</code>", "text": "<p>Add a testing parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_test_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a testing parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('test',\ndescription='Test a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--no_warmup', help=\"Disable warmup for this session\", action='store_true')\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=test)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.configure_train_parser", "title": "<code>configure_train_parser</code>", "text": "<p>Add a training parser to an existing argparser.</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>argparse._SubParsersAction</code> <p>The parser object to be appended to.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def configure_train_parser(subparsers: argparse._SubParsersAction) -&gt; None:\n\"\"\"Add a training parser to an existing argparser.\n    Args:\n        subparsers: The parser object to be appended to.\n    \"\"\"\nparser = subparsers.add_parser('train',\ndescription='Train a FastEstimator model',\nformatter_class=argparse.ArgumentDefaultsHelpFormatter,\nallow_abbrev=False)\n# use an argument group for required flag arguments since otherwise they will show up as optional in the help\nparser.add_argument('entry_point', type=str, help='The path to the model python file')\nparser.add_argument('--hyperparameters',\ndest='hyperparameters_json',\ntype=str,\nhelp=\"The path to the hyperparameters JSON file\")\nparser.add_argument('--no_warmup', help=\"Disable warmup for this session\", action='store_true')\nparser.add_argument_group(\n'hyperparameter arguments',\n'Arguments to be passed through to the get_estimator() call. \\\n        Examples might look like --epochs &lt;int&gt;, --batch_size &lt;int&gt;, --optimizer &lt;str&gt;, etc...')\nparser.set_defaults(func=train)\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.test", "title": "<code>test</code>", "text": "<p>Load an Estimator from a file and invoke its .test() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def test(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .test() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nestimator.test()\n</code></pre>"}, {"location": "fastestimator/cli/train.html#fastestimator.fastestimator.cli.train.train", "title": "<code>train</code>", "text": "<p>Load an Estimator from a file and invoke its .fit() method.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Dict[str, Any]</code> <p>A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional 'hyperparameters_json' key if the user is storing their parameters in a file.</p> required <code>unknown</code> <code>Optional[List[str]]</code> <p>The remainder of the command line arguments to be passed along to the get_estimator() method.</p> required Source code in <code>fastestimator\\fastestimator\\cli\\train.py</code> <pre><code>def train(args: Dict[str, Any], unknown: Optional[List[str]]) -&gt; None:\n\"\"\"Load an Estimator from a file and invoke its .fit() method.\n    Args:\n        args: A dictionary containing location of the FE file under the 'entry_point' key, as well as an optional\n            'hyperparameters_json' key if the user is storing their parameters in a file.\n        unknown: The remainder of the command line arguments to be passed along to the get_estimator() method.\n    \"\"\"\nestimator = _get_estimator(args, unknown)\nestimator.fit(warmup=not args['no_warmup'])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html", "title": "batch_dataset", "text": ""}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset", "title": "<code>BatchDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.</p> <p>This dataset helps to enable several use-cases: 1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.     <pre><code>ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\nds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\nunpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n# {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n</code></pre> 2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n</code></pre> 3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.     <pre><code>class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nclass2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\nds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n# {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[FEDataset, Iterable[FEDataset]]</code> <p>The dataset(s) to use for batch sampling.</p> required <code>num_samples</code> <code>Union[int, Iterable[int]]</code> <p>Number of samples to draw from the <code>datasets</code>. May be a single int if used in conjunction with <code>probability</code>, otherwise a list of ints of len(<code>datasets</code>) is required.</p> required <code>probability</code> <code>Optional[Iterable[float]]</code> <p>Probability to draw from each dataset. Only allowed if <code>num_samples</code> is an integer.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>class BatchDataset(FEDataset):\n\"\"\"BatchDataset extracts a list (batch) of data from a single dataset or multiple datasets.\n    This dataset helps to enable several use-cases:\n    1. Creating an unpaired dataset from two or more completely disjoint (no common keys) datasets.\n        ```python\n        ds1 = fe.dataset.DirDataset(...)  # {\"a\": &lt;32x32&gt;}\n        ds2 = fe.dataset.DirDataset(...)  # {\"b\": &lt;28x28&gt;}\n        unpaired_ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[4, 4])\n        # {\"a\": &lt;4x32x32&gt;, \"b\": &lt;4x28x28&gt;}\n        ```\n    2. Deterministic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=[3, 5])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (3 of the samples are from class1_ds, 5 of the samples from class2_ds)\n        ```\n    3. Probabilistic class balanced sampling from two or more similar (all keys in common) datasets.\n        ```python\n        class1_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        class2_ds = fe.dataset.DirDataset(...)  # {\"x\": &lt;32x32&gt;, \"y\": &lt;&gt;}\n        ds = fe.dataset.BatchDataset(datasets=[ds1, ds2], num_samples=8, probability=[0.7, 0.3])\n        # {\"x\": &lt;8x32x32&gt;, \"y\": &lt;8&gt;}  (~70% of the samples are from class1_ds, ~30% of the samples from class2_ds)\n        ```\n    Args:\n        datasets: The dataset(s) to use for batch sampling.\n        num_samples: Number of samples to draw from the `datasets`. May be a single int if used in conjunction with\n            `probability`, otherwise a list of ints of len(`datasets`) is required.\n        probability: Probability to draw from each dataset. Only allowed if `num_samples` is an integer.\n    \"\"\"\ndef __init__(self,\ndatasets: Union[FEDataset, Iterable[FEDataset]],\nnum_samples: Union[int, Iterable[int]],\nprobability: Optional[Iterable[float]] = None) -&gt; None:\nself.datasets = to_list(datasets)\nself.num_samples = to_list(num_samples)\nself.probability = to_list(probability)\nself.same_feature = False\nself._check_input()\nself.reset_index_maps()\nself.pad_value = None\ndef _check_input(self) -&gt; None:\n\"\"\"Verify that the given input values are valid.\n        Raises:\n            AssertionError: If any of the parameters are found to by unacceptable for a variety of reasons.\n        \"\"\"\nassert len(self.datasets) &gt; 1, \"must provide multiple datasets as input\"\nfor num_sample in self.num_samples:\nassert isinstance(num_sample, int) and num_sample &gt; 0, \"only accept positive integer type as num_sample\"\n# check dataset keys\ndataset_keys = [set(dataset[0].keys()) for dataset in self.datasets]\nfor key in dataset_keys:\nassert key, \"found no key in datasets\"\nis_same_key = all([dataset_keys[0] == key for key in dataset_keys])\nis_disjoint_key = sum([len(key) for key in dataset_keys]) == len(set.union(*dataset_keys))\nif len(self.datasets) &gt; 1:\nassert is_same_key != is_disjoint_key, \"dataset keys must be all same or all disjoint\"\nself.same_feature = is_same_key\nif self.probability:\nassert self.same_feature, \"keys must be exactly same among datasets when using probability distribution\"\nassert len(self.datasets) == len(self.probability), \"the length of dataset must match probability\"\nassert len(self.num_samples) == 1, \"num_sample must be scalar for probability mode\"\nassert len(self.datasets) &gt; 1, \"number of datasets must be more than one to use probability mode\"\nassert sum(self.probability) == 1, \"sum of probability must be 1\"\nfor p in self.probability:\nassert isinstance(p, float) and p &gt; 0, \"must provide positive float for probability distribution\"\nelse:\nassert len(self.datasets) == len(self.num_samples), \"the number of dataset must match num_samples\"\nif not self.same_feature:\nassert len(set(self.num_samples)) == 1, \"the number of samples must be the same for disjoint features\"\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['UnpairedDataset']:\n\"\"\"This class overwrites the .split() method instead of _do_split().\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every element of the `splits` sequence.\n        Raises:\n            AssertionError: This method should never by invoked.\n        \"\"\"\nraise AssertionError(\"This method should not have been invoked. Please file a bug report\")\ndef split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['UnpairedDataset', List['UnpairedDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        \"\"\"\nnew_datasets = [to_list(ds.split(*fractions)) for ds in self.datasets]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\n# Re-compute personal variables\nself.reset_index_maps()\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\ndef __len__(self) -&gt; int:\n\"\"\"Compute the length of this dataset.\n        Returns:\n            How many batches of data can this dataset serve per epoch.\n        \"\"\"\nif len(self.num_samples) &gt; 1:\nlength = max([math.ceil(len(ds) / num_sample) for ds, num_sample in zip(self.datasets, self.num_samples)])\nelse:\nnum_sample = self.num_samples[0]\nlength = max([math.ceil(len(ds) / num_sample / p) for ds, p in zip(self.datasets, self.probability)])\nreturn length\ndef __getitem__(self, batch_idx: int) -&gt; List[Dict[str, Any]]:\n\"\"\"Extract items from the underlying datasets based on the given `batch_idx`.\n        Args:\n            batch_idx: Which batch is it.\n        Returns:\n            A list of data instance dictionaries corresponding to the current `batch_idx`.\n        \"\"\"\nitems = []\nif self.same_feature:\nif self.probability:\nindex = list(np.random.choice(range(len(self.datasets)), size=self.num_samples, p=self.probability))\nnum_samples = [index.count(i) for i in range(len(self.datasets))]\nelse:\nnum_samples = self.num_samples\nfor dataset, num_sample, index_map in zip(self.datasets, num_samples, self.index_maps):\nfor idx in range(num_sample):\nitems.append(dataset[index_map[batch_idx * num_sample + idx]])\nelse:\nnum_sample = self.num_samples[0]\nfor idx in range(num_sample):\npaired_items = [\ndataset[index_map[batch_idx * num_sample + idx]] for dataset,\nindex_map in zip(self.datasets, self.index_maps)\n]\nitems.append({k: v for d in paired_items for k, v in d.items()})\nrandom.shuffle(items)\nreturn items\ndef reset_index_maps(self) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n        This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the\n        basis datasets.\n        \"\"\"\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nrandom.shuffle(mapping)\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.reset_index_maps", "title": "<code>reset_index_maps</code>", "text": "<p>Rearrange the index maps of this BatchDataset.</p> <p>This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the basis datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def reset_index_maps(self) -&gt; None:\n\"\"\"Rearrange the index maps of this BatchDataset.\n    This method is invoked every epoch by OpDataset which allows each epoch to have different random pairings of the\n    basis datasets.\n    \"\"\"\nnum_samples = self.num_samples\nif self.probability:\nnum_samples = num_samples * len(self.datasets)\nself.index_maps = []\nfor dataset, num_sample in zip(self.datasets, num_samples):\nindex_map = [list(range(len(dataset))) for _ in range(math.ceil(len(self) * num_sample / len(dataset)))]\nfor mapping in index_map:\nrandom.shuffle(mapping)\nself.index_maps.append([item for sublist in index_map for item in sublist])\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\nds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>*fractions</code> <code>Union[float, int, Iterable[int]]</code> <p>Floating point values will be interpreted as percentages, integers as an absolute number of datapoints, and an iterable of integers as the exact indices of the data that should be removed in order to create the new dataset.</p> <code>()</code> <p>Returns:</p> Type Description <code>Union[UnpairedDataset, List[UnpairedDataset]]</code> <p>One or more new datasets which are created by removing elements from the current dataset. The number of</p> <code>Union[UnpairedDataset, List[UnpairedDataset]]</code> <p>datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided</p> <code>Union[UnpairedDataset, List[UnpairedDataset]]</code> <p>then the return will be a single dataset rather than a list of datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['UnpairedDataset', List['UnpairedDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    \"\"\"\nnew_datasets = [to_list(ds.split(*fractions)) for ds in self.datasets]\nnum_splits = len(new_datasets[0])\nnew_datasets = [[ds[i] for ds in new_datasets] for i in range(num_splits)]\nresults = [BatchDataset(ds, self.num_samples, self.probability) for ds in new_datasets]\n# Re-compute personal variables\nself.reset_index_maps()\n# Unpack response if only a single split\nif len(results) == 1:\nresults = results[0]\nreturn results\n</code></pre>"}, {"location": "fastestimator/dataset/batch_dataset.html#fastestimator.fastestimator.dataset.batch_dataset.BatchDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\batch_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummaries = [ds.summary() for ds in self.datasets]\nkeys = {k: v for summary in summaries for k, v in summary.keys.items()}\nreturn DatasetSummary(num_instances=len(self), keys=keys)\n</code></pre>"}, {"location": "fastestimator/dataset/csv_dataset.html", "title": "csv_dataset", "text": ""}, {"location": "fastestimator/dataset/csv_dataset.html#fastestimator.fastestimator.dataset.csv_dataset.CSVDataset", "title": "<code>CSVDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a CSV file.</p> <p>CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the CSV file.</p> required <code>delimiter</code> <code>str</code> <p>What delimiter is used by the file.</p> <code>','</code> <code>kwargs</code> <p>Other arguments to be passed through to pandas csv reader function. See the pandas docs for details: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\dataset\\csv_dataset.py</code> <pre><code>class CSVDataset(InMemoryDataset):\n\"\"\"A dataset from a CSV file.\n    CSVDataset reads entries from a CSV file, where the first row is the header. The root directory of the csv file\n    may be accessed using dataset.parent_path. This may be useful if the csv contains relative path information\n    that you want to feed into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the CSV file.\n        delimiter: What delimiter is used by the file.\n        kwargs: Other arguments to be passed through to pandas csv reader function. See the pandas docs for details:\n            https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.\n    \"\"\"\ndef __init__(self, file_path: str, delimiter: str = \",\", **kwargs) -&gt; None:\ndf = pd.read_csv(file_path, delimiter=delimiter, **kwargs)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html", "title": "dataset", "text": ""}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.DatasetSummary", "title": "<code>DatasetSummary</code>", "text": "<p>This class contains information summarizing a dataset object.</p> <p>Parameters:</p> Name Type Description Default <code>num_instances</code> <code>int</code> <p>The number of data instances within the dataset (influences the size of an epoch).</p> required <code>num_classes</code> <code>Optional[int]</code> <p>How many different classes are present.</p> <code>None</code> <code>keys</code> <code>Dict[str, KeySummary]</code> <p>What keys does the dataset provide, along with summary information about each key.</p> required <code>class_key</code> <code>Optional[str]</code> <p>Which key corresponds to class information (if known).</p> <code>None</code> <code>class_key_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A mapping of the original class string values to the values which are output to the pipeline.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class DatasetSummary:\n\"\"\"This class contains information summarizing a dataset object.\n    Args:\n        num_instances: The number of data instances within the dataset (influences the size of an epoch).\n        num_classes: How many different classes are present.\n        keys: What keys does the dataset provide, along with summary information about each key.\n        class_key: Which key corresponds to class information (if known).\n        class_key_mapping: A mapping of the original class string values to the values which are output to the pipeline.\n    \"\"\"\nnum_instances: int\nnum_classes: Optional[int]\nclass_key: Optional[str]\nclass_key_mapping: Optional[Dict[str, Any]]\nkeys: Dict[str, KeySummary]\ndef __init__(self,\nnum_instances: int,\nkeys: Dict[str, KeySummary],\nnum_classes: Optional[int] = None,\nclass_key_mapping: Optional[Dict[str, Any]] = None,\nclass_key: Optional[str] = None):\nself.num_instances = num_instances\nself.class_key = class_key\nself.num_classes = num_classes\nself.class_key_mapping = class_key_mapping\nself.keys = keys\ndef __repr__(self):\nreturn \"&lt;DatasetSummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\ndef __str__(self):\nreturn jsonpickle.dumps(self, unpicklable=False)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset", "title": "<code>FEDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class FEDataset(Dataset):\ndef __len__(self) -&gt; int:\n\"\"\"Defines how many datapoints the dataset contains.\n        This is used for computing the number of datapoints available per epoch.\n        Returns:\n            The number of datapoints within the dataset.\n        \"\"\"\nraise NotImplementedError\ndef __getitem__(self, index: int) -&gt; Dict[str, Any]:\n\"\"\"Fetch a data instance at a specified index.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index.\n        \"\"\"\nraise NotImplementedError\ndef split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n        This function enables several types of splitting:\n        1. Splitting by fractions.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        2. Splitting by counts.\n            ```python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n            ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n            ```\n        3. Splitting by indices.\n            ``python\n            ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n            ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n            ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n            ```\n        Args:\n            *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n                datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n                to create the new dataset.\n        Returns:\n            One or more new datasets which are created by removing elements from the current dataset. The number of\n            datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n            then the return will be a single dataset rather than a list of datasets.\n        \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n            \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nsplits = []\nif method == 'number':\n# TODO - convert to a linear congruential generator for large datasets?\n# https://stackoverflow.com/questions/9755538/how-do-i-create-a-list-of-random-numbers-without-duplicates\nindices = random.sample(range(original_size), int_sum)\nstart = 0\nfor stop in n_samples:\nsplits.append((indices[i] for i in range(start, start + stop)))\nstart += stop\nelif method == 'indices':\nsplits = fractions\nsplits = self._do_split(splits)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        Useful if sub-classes want to split by something other than indices (see SiameseDirDataset for example).\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['FEDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nraise NotImplementedError\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nraise NotImplementedError\ndef __str__(self):\nreturn str(self.summary())\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.split", "title": "<code>split</code>", "text": "<p>Split this dataset into multiple smaller datasets.</p> <p>This function enables several types of splitting: 1. Splitting by fractions.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 2. Splitting by counts.     <pre><code>ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\nds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\nds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n</code></pre> 3. Splitting by indices.     ``python     ds = fe.dataset.FEDataset(...)  # len(ds) == 1000     ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6     ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100     ```</p> <p>Args:     *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of         datapoints, and an iterable of integers as the exact indices of the data that should be removed in order         to create the new dataset.</p> <p>Returns:     One or more new datasets which are created by removing elements from the current dataset. The number of     datasets returned will be equal to the number of <code>fractions</code> provided. If only a single value is provided     then the return will be a single dataset rather than a list of datasets.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def split(self, *fractions: Union[float, int, Iterable[int]]) -&gt; Union['FEDataset', List['FEDataset']]:\n\"\"\"Split this dataset into multiple smaller datasets.\n    This function enables several types of splitting:\n    1. Splitting by fractions.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(0.1)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(0.1, 0.2)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    2. Splitting by counts.\n        ```python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split(100)  # len(ds) == 900, len(ds2) == 100\n        ds3, ds4 = ds.split(90, 180)  # len(ds) == 630, len(ds3) == 90, len(ds4) == 180\n        ```\n    3. Splitting by indices.\n        ``python\n        ds = fe.dataset.FEDataset(...)  # len(ds) == 1000\n        ds2 = ds.split([87,2,3,100,121,158])  # len(ds) == 994, len(ds2) == 6\n        ds3 = ds.split(range(100))  # len(ds) == 894, len(ds3) == 100\n        ```\n    Args:\n        *fractions: Floating point values will be interpreted as percentages, integers as an absolute number of\n            datapoints, and an iterable of integers as the exact indices of the data that should be removed in order\n            to create the new dataset.\n    Returns:\n        One or more new datasets which are created by removing elements from the current dataset. The number of\n        datasets returned will be equal to the number of `fractions` provided. If only a single value is provided\n        then the return will be a single dataset rather than a list of datasets.\n    \"\"\"\nassert len(fractions) &gt; 0, \"split requires at least one fraction argument\"\noriginal_size = self._split_length()\nmethod = None\nfrac_sum = 0\nint_sum = 0\nn_samples = []\nfor frac in fractions:\nif isinstance(frac, float):\nfrac_sum += frac\nfrac = math.ceil(original_size * frac)\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, int):\nint_sum += frac\nn_samples.append(frac)\nif method is None:\nmethod = 'number'\nassert method == 'number', \"Split supports either numeric splits or lists of indices but not both\"\nelif isinstance(frac, Iterable):\nif method is None:\nmethod = 'indices'\nassert method == 'indices', \"Split supports either numeric splits or lists of indices but not both\"\nelse:\nraise ValueError(\n\"split only accepts float, int, or iter[int] type splits, but {} was given\".format(frac))\nassert frac_sum &lt; 1, \"total split fraction should sum to less than 1.0, but got: {}\".format(frac_sum)\nassert int_sum &lt; original_size, \\\n        \"total split requirements ({}) should sum to less than dataset size ({})\".format(int_sum, original_size)\nsplits = []\nif method == 'number':\n# TODO - convert to a linear congruential generator for large datasets?\n# https://stackoverflow.com/questions/9755538/how-do-i-create-a-list-of-random-numbers-without-duplicates\nindices = random.sample(range(original_size), int_sum)\nstart = 0\nfor stop in n_samples:\nsplits.append((indices[i] for i in range(start, start + stop)))\nstart += stop\nelif method == 'indices':\nsplits = fractions\nsplits = self._do_split(splits)\nif len(fractions) == 1:\nreturn splits[0]\nreturn splits\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.FEDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset", "title": "<code>InMemoryDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset abstraction to simplify the implementation of datasets which hold their data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[int, Dict[str, Any]]</code> <p>A dictionary like {data_index: {}}. required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class InMemoryDataset(FEDataset):\n\"\"\"A dataset abstraction to simplify the implementation of datasets which hold their data in memory.\n    Args:\n        data: A dictionary like {data_index: {&lt;instance dictionary&gt;}}.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]  # Index-based data dictionary\nsummary: lru_cache\ndef __init__(self, data: Dict[int, Dict[str, Any]]) -&gt; None:\nself.data = data\n# Normally lru cache annotation is shared over all class instances, so calling cache_clear would reset all\n# caches (for example when calling .split()). Instead we make the lru cache per-instance\nself.summary = lru_cache(maxsize=1)(self.summary)\ndef __len__(self) -&gt; int:\nreturn len(self.data)\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        element = data[0]  # {\"x\": &lt;100&gt;}\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        ```\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nif isinstance(index, int):\nreturn self.data[index]\nelse:\nresult = [elem[index] for elem in self.data.values()]\nif isinstance(result[0], np.ndarray):\nreturn np.array(result)\nreturn result\ndef __setitem__(self, key: Union[int, str], value: Union[Dict[str, Any], Sequence[Any]]) -&gt; None:\n\"\"\"Modify data in the dataset.\n        ```python\n        data = fe.dataset.InMemoryDataset(...)  # {\"x\": &lt;100&gt;}, len(data) == 1000\n        column = data[\"x\"]  # &lt;1000x100&gt;\n        column = column - np.mean(column)\n        data[\"x\"] = column\n        ```\n        Args:\n            key: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be updated.\n            value: The value to be inserted for the given `key`. Must be a dictionary if `key` is an integer. Otherwise\n                must be a sequence with the same length as the current length of the dataset.\n        Raises:\n            AssertionError: If the `value` is inappropriate given the type of the `key`.\n        \"\"\"\nif isinstance(key, int):\nassert isinstance(value, Dict), \"if setting a value using an integer index, must provide a dictionary\"\nself.data[key] = value\nelse:\nassert len(value) == len(self.data), \\\n                \"input value must be of length {}, but had length {}\".format(len(self.data), len(value))\nfor i in range(len(self.data)):\nself.data[i][key] = value[i]\nself.summary.cache_clear()\ndef _skip_init(self, data: Dict[int, Dict[str, Any]], **kwargs) -&gt; 'InMemoryDataset':\n\"\"\"A helper method to create new dataset instances without invoking their __init__ methods.\n        Args:\n            data: The data dictionary to be used in the new dataset.\n            **kwargs: Any other member variables to be assigned in the new dataset.\n        Returns:\n            A new dataset based on the given inputs.\n        \"\"\"\nobj = self.__class__.__new__(self.__class__)\nobj.data = data\nfor k, v in kwargs.items():\nif k == 'summary':\ncontinue  # Ignore summary object since we're going to re-initialize it\nelse:\nobj.__setattr__(k, v)\nobj.summary = lru_cache(maxsize=1)(obj.summary)\nreturn obj\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['InMemoryDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New Datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nresults = []\nfor split in splits:\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nresults.append(self._skip_init(data, **{k: v for k, v in self.__dict__.items() if k not in {'data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself.summary.cache_clear()\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nreturn DatasetSummary(num_instances=len(self), keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.InMemoryDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\n# We will check whether the dataset is doing additional pre-processing on top of the self.data keys. If not we\n# can extract extra information about the data without incurring a large computational time cost\nfinal_example = self[0]\noriginal_example = self.data[0]\nkeys = final_example.keys()\nshapes = {}\ndtypes = {}\nn_unique_vals = defaultdict(lambda: 0)\nfor key in keys:\nfinal_val = final_example[key]\n# TODO - if val is empty list, should find a sample which has entries\ndtypes[key] = get_type(final_val)\nshapes[key] = get_shape(final_val)\n# Check whether type and shape have changed by get_item\nif key in original_example:\noriginal_val = original_example[key]\noriginal_dtype = get_type(original_val)\noriginal_shape = get_shape(original_val)\n# If no changes, then we can relatively quickly count the unique values using self.data\nif dtypes[key] == original_dtype and shapes[key] == original_shape and isinstance(\noriginal_val, Hashable):\nn_unique_vals[key] = len({self.data[i][key] for i in range(len(self.data))})\nkey_summary = {\nkey: KeySummary(dtype=dtypes[key], num_unique_values=n_unique_vals[key] or None, shape=shapes[key])\nfor key in keys\n}\nreturn DatasetSummary(num_instances=len(self), keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/dataset.html#fastestimator.fastestimator.dataset.dataset.KeySummary", "title": "<code>KeySummary</code>", "text": "<p>A summary of the dataset attributes corresponding to a particular key.</p> <p>Parameters:</p> Name Type Description Default <code>num_unique_values</code> <code>Optional[int]</code> <p>The number of unique values corresponding to a particular key (if known).</p> <code>None</code> <code>shape</code> <code>List[Optional[int]]</code> <p>The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is ragged.</p> <code>()</code> <code>dtype</code> <code>str</code> <p>The data type of instances corresponding to the given key.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\dataset.py</code> <pre><code>class KeySummary:\n\"\"\"A summary of the dataset attributes corresponding to a particular key.\n    Args:\n        num_unique_values: The number of unique values corresponding to a particular key (if known).\n        shape: The shape of the vectors corresponding to the key. None is used in a list to indicate that a dimension is\n            ragged.\n        dtype: The data type of instances corresponding to the given key.\n    \"\"\"\nnum_unique_values: Optional[int]\nshape: List[Optional[int]]\ndtype: str\ndef __init__(self, dtype: str, num_unique_values: Optional[int] = None, shape: List[Optional[int]] = ()) -&gt; None:\nself.num_unique_values = num_unique_values\nself.shape = shape\nself.dtype = dtype\ndef __repr__(self):\nreturn \"&lt;KeySummary {}&gt;\".format(self.__getstate__())\ndef __getstate__(self):\nreturn {k: v for k, v in self.__dict__.items() if v is not None}\n</code></pre>"}, {"location": "fastestimator/dataset/dir_dataset.html", "title": "dir_dataset", "text": ""}, {"location": "fastestimator/dataset/dir_dataset.html#fastestimator.fastestimator.dataset.dir_dataset.DirDataset", "title": "<code>DirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> <code>recursive_search</code> <code>bool</code> <p>Whether to search within subdirectories for files.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\dataset\\dir_dataset.py</code> <pre><code>class DirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/data.file.\n    Args:\n        root_dir: The path to the directory containing data.\n        data_key: What key to assign to the data values in the data dictionary.\n        file_extension: If provided then only files ending with the file_extension will be included.\n        recursive_search: Whether to search within subdirectories for files.\n    \"\"\"\ndata: Dict[int, Dict[str, str]]\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nfile_extension: Optional[str] = None,\nrecursive_search: bool = True) -&gt; None:\ndata = []\nroot_dir = os.path.normpath(root_dir)\nif not os.path.isdir(root_dir):\nraise AssertionError(\"Provided path is not a directory\")\ntry:\nfor root, dirs, files in os.walk(root_dir):\nfor file_name in files:\nif file_name.startswith(\".\") or (file_extension is not None\nand not file_name.endswith(file_extension)):\ncontinue\ndata.append((os.path.join(root, file_name), os.path.basename(root)))\nif not recursive_search:\nbreak\nexcept StopIteration:\nraise ValueError(\"Invalid directory structure for DirDataset at root: {}\".format(root_dir))\nsuper().__init__({i: {data_key: data[i][0]} for i in range(len(data))})\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html", "title": "generator_dataset", "text": ""}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset", "title": "<code>GeneratorDataset</code>", "text": "<p>         Bases: <code>FEDataset</code></p> <p>A dataset from a generator function.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Generator[Dict[str, Any], int, None]</code> <p>The generator function to invoke in order to get a data sample.</p> required <code>samples_per_epoch</code> <code>int</code> <p>How many samples should be drawn from the generator during an epoch. Note that the generator function will actually be invoke more times than the number specified here due to backend validation routines.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>class GeneratorDataset(FEDataset):\n\"\"\"A dataset from a generator function.\n    Args:\n        generator: The generator function to invoke in order to get a data sample.\n        samples_per_epoch: How many samples should be drawn from the generator during an epoch. Note that the generator\n            function will actually be invoke more times than the number specified here due to backend validation\n            routines.\n    \"\"\"\ndef __init__(self, generator: Generator[Dict[str, Any], int, None], samples_per_epoch: int) -&gt; None:\nself.generator = generator\nself.samples_per_epoch = samples_per_epoch\nnext(self.generator)  # Can't send non-none values to a new generator, so need to run a 'warm-up' first\nself.summary = lru_cache(maxsize=1)(self.summary)\ndef __len__(self):\nreturn self.samples_per_epoch\ndef __getitem__(self, index: int):\nreturn self.generator.send(index)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['GeneratorDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which indices to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing data at the indices specified by `splits` from the current dataset.\n        \"\"\"\nwarnings.warn(\"You probably don't actually want to split a generator dataset\")\nresults = []\nfor split in splits:\nif isinstance(split, Sized):\nsize = len(split)\nelse:\n# TODO - make this efficient somehow\nsize = sum(1 for _ in split)\nresults.append(GeneratorDataset(self.generator, size))\nself.samples_per_epoch -= size\nreturn results\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nreturn DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/generator_dataset.html#fastestimator.fastestimator.dataset.generator_dataset.GeneratorDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\generator_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsample = self[0]\nkey_summary = {}\nfor key in sample.keys():\nval = sample[key]\n# TODO - if val is empty list, should find a sample which has entries\nshape = get_shape(val)\ndtype = get_type(val)\nkey_summary[key] = KeySummary(num_unique_values=None, shape=shape, dtype=dtype)\nreturn DatasetSummary(num_instances=self.samples_per_epoch, keys=key_summary)\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html", "title": "labeled_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset", "title": "<code>LabeledDirDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key</code> <code>str</code> <p>What key to assign to the data values in the data dictionary.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>class LabeledDirDataset(InMemoryDataset):\n\"\"\"A dataset which reads files from a folder hierarchy like root/class(/es)/data.file.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key: What key to assign to the data values in the data dictionary.\n        label_key: What key to assign to the label values in the data dictionary.\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\ndata: Dict[int, Dict[str, Any]]\nmapping: Dict[str, Any]\nlabel_key: str\ndef __init__(self,\nroot_dir: str,\ndata_key: str = \"x\",\nlabel_key: str = \"y\",\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None) -&gt; None:\n# Recursively find all the data\nroot_dir = os.path.normpath(root_dir)\ndata = {}\nkeys = deque([\"\"])\nfor _, dirs, entries in os.walk(root_dir):\nkey = keys.popleft()\ndirs = [os.path.join(key, d) for d in dirs]\ndirs.reverse()\nkeys.extendleft(dirs)\nentries = [\nos.path.join(key, e) for e in entries if not e.startswith(\".\") and e.endswith(file_extension or \"\")\n]\nif entries:\ndata[key] = entries\n# Compute label mappings\nself.mapping = label_mapping or {label: idx for idx, label in enumerate(sorted(data.keys()))}\nassert self.mapping.keys() &gt;= data.keys(), \\\n            \"Mapping provided to LabeledDirDataset is missing key(s): {}\".format(\ndata.keys() - self.mapping.keys())\n# Store the data by index\nparsed_data = {}\nidx = 0\nfor key, values in data.items():\nlabel = self.mapping[key]\nfor value in values:\nparsed_data[idx] = {data_key: os.path.join(root_dir, value), label_key: label}\nidx += 1\nself.label_key = label_key\nsuper().__init__(parsed_data)\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/labeled_dir_dataset.html#fastestimator.fastestimator.dataset.labeled_dir_dataset.LabeledDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\labeled_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\nsummary.class_key = self.label_key\nsummary.class_key_mapping = self.mapping\nsummary.num_classes = len(self.mapping)\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/numpy_dataset.html", "title": "numpy_dataset", "text": ""}, {"location": "fastestimator/dataset/numpy_dataset.html#fastestimator.fastestimator.dataset.numpy_dataset.NumpyDataset", "title": "<code>NumpyDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset constructed from a dictionary of Numpy data or list of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Union[np.ndarray, List]]</code> <p>A dictionary of data like {\"key1\": , \"key2\": [list]}. required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the Numpy arrays or lists have differing numbers of elements.</p> <code>ValueError</code> <p>If any dictionary value is not instance of Numpy array or list.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\numpy_dataset.py</code> <pre><code>class NumpyDataset(InMemoryDataset):\n\"\"\"A dataset constructed from a dictionary of Numpy data or list of data.\n    Args:\n        data: A dictionary of data like {\"key1\": &lt;numpy array&gt;, \"key2\": [list]}.\n    Raises:\n        AssertionError: If any of the Numpy arrays or lists have differing numbers of elements.\n        ValueError: If any dictionary value is not instance of Numpy array or list.\n    \"\"\"\ndef __init__(self, data: Dict[str, Union[np.ndarray, List]]) -&gt; None:\nsize = None\nfor val in data.values():\nif isinstance(val, np.ndarray):\ncurrent_size = val.shape[0]\nelif isinstance(val, list):\ncurrent_size = len(val)\nelse:\nraise ValueError(\"Please ensure you are passing numpy array or list in the data dictionary.\")\nif size is not None:\nassert size == current_size, \"All data arrays must have the same number of elements\"\nelse:\nsize = current_size\nsuper().__init__({i: {k: v[i] for k, v in data.items()} for i in range(size)})\n</code></pre>"}, {"location": "fastestimator/dataset/op_dataset.html", "title": "op_dataset", "text": ""}, {"location": "fastestimator/dataset/op_dataset.html#fastestimator.fastestimator.dataset.op_dataset.OpDataset", "title": "<code>OpDataset</code>", "text": "<p>         Bases: <code>Dataset</code></p> <p>A wrapper for datasets which allows operators to be applied to them in a pipeline.</p> <p>This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets within an Op dataset as needed.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The base dataset to wrap.</p> required <code>ops</code> <code>List[NumpyOp]</code> <p>A list of ops to be applied after the base <code>dataset</code> <code>__getitem__</code> is invoked.</p> required <code>mode</code> <code>str</code> <p>What mode the system is currently running in ('train', 'eval', 'test', or 'infer').</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\op_dataset.py</code> <pre><code>class OpDataset(Dataset):\n\"\"\"A wrapper for datasets which allows operators to be applied to them in a pipeline.\n    This class should not be directly instantiated by the end user. The fe.Pipeline will automatically wrap datasets\n    within an Op dataset as needed.\n    Args:\n        dataset: The base dataset to wrap.\n        ops: A list of ops to be applied after the base `dataset` `__getitem__` is invoked.\n        mode: What mode the system is currently running in ('train', 'eval', 'test', or 'infer').\n    \"\"\"\ndef __init__(self, dataset: Dataset, ops: List[NumpyOp], mode: str) -&gt; None:\nself.dataset = dataset\nif isinstance(self.dataset, BatchDataset):\nself.dataset.reset_index_maps()\nself.ops = ops\nself.mode = mode\ndef __getitem__(self, index: int) -&gt; Mapping[str, Any]:\n\"\"\"Fetch a data instance at a specified index, and apply transformations to it.\n        Args:\n            index: Which datapoint to retrieve.\n        Returns:\n            The data dictionary from the specified index, with transformations applied.\n        \"\"\"\nitems = deepcopy(self.dataset[index])  # Deepcopy to prevent ops from overwriting values in datasets\nif isinstance(self.dataset, BatchDataset):\nunique_list = []\nfor item in items:\nif id(item) not in unique_list:\nforward_numpyop(self.ops, item, self.mode)\nunique_list.append(id(item))\nif self.dataset.pad_value is not None:\npad_batch(items, self.dataset.pad_value)\nitems = {key: np.array([item[key] for item in items]) for key in items[0]}\nelse:\nforward_numpyop(self.ops, items, self.mode)\nreturn items\ndef __len__(self):\nreturn len(self.dataset)\n</code></pre>"}, {"location": "fastestimator/dataset/pickle_dataset.html", "title": "pickle_dataset", "text": ""}, {"location": "fastestimator/dataset/pickle_dataset.html#fastestimator.fastestimator.dataset.pickle_dataset.PickleDataset", "title": "<code>PickleDataset</code>", "text": "<p>         Bases: <code>InMemoryDataset</code></p> <p>A dataset from a pickle file.</p> <p>PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed into, say, an ImageReader Op.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The (absolute) path to the pickle file.</p> required Source code in <code>fastestimator\\fastestimator\\dataset\\pickle_dataset.py</code> <pre><code>class PickleDataset(InMemoryDataset):\n\"\"\"A dataset from a pickle file.\n    PickleDataset reads entries from pickled pandas data-frames. The root directory of the pickle file may be accessed\n    using dataset.parent_path. This may be useful if the file contains relative path information that you want to feed\n    into, say, an ImageReader Op.\n    Args:\n        file_path: The (absolute) path to the pickle file.\n    \"\"\"\ndef __init__(self, file_path: str) -&gt; None:\ndf = pd.read_pickle(file_path)\nself.parent_path = os.path.dirname(file_path)\nsuper().__init__(df.to_dict(orient='index'))\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html", "title": "siamese_dir_dataset", "text": ""}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset", "title": "<code>SiameseDirDataset</code>", "text": "<p>         Bases: <code>LabeledDirDataset</code></p> <p>A dataset which returns pairs of data.</p> <p>This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs, where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index rather than by data instance index.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>str</code> <p>The path to the directory containing data sorted by folders.</p> required <code>data_key_left</code> <code>str</code> <p>What key to assign to the first data element in the pair.</p> <code>'x_a'</code> <code>data_key_right</code> <code>str</code> <p>What key to assign to the second data element in the pair.</p> <code>'x_b'</code> <code>label_key</code> <code>str</code> <p>What key to assign to the label values in the data dictionary.</p> <code>'y'</code> <code>percent_matching_data</code> <code>float</code> <p>What percentage of the time should data be paired by class (label value = 1).</p> <code>0.5</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary defining the mapping to use. If not provided will map classes to int labels.</p> <code>None</code> <code>file_extension</code> <code>Optional[str]</code> <p>If provided then only files ending with the file_extension will be included.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>class SiameseDirDataset(LabeledDirDataset):\n\"\"\"A dataset which returns pairs of data.\n    This dataset reads files from a folder hierarchy like root/class(/es)/data.file. Data is returned in pairs,\n    where the label value is 1 if the data are drawn from the same class, and 0 otherwise. One epoch is defined as\n    the time it takes to visit every data point exactly once as the 'data_key_left'. Each data point may occur zero\n    or many times as 'data_key_right' within the same epoch. SiameseDirDataset.split() will split by class index\n    rather than by data instance index.\n    Args:\n        root_dir: The path to the directory containing data sorted by folders.\n        data_key_left: What key to assign to the first data element in the pair.\n        data_key_right: What key to assign to the second data element in the pair.\n        label_key: What key to assign to the label values in the data dictionary.\n        percent_matching_data: What percentage of the time should data be paired by class (label value = 1).\n        label_mapping: A dictionary defining the mapping to use. If not provided will map classes to int labels.\n        file_extension: If provided then only files ending with the file_extension will be included.\n    \"\"\"\nclass_data: Dict[Any, Set[int]]\ndef __init__(self,\nroot_dir: str,\ndata_key_left: str = \"x_a\",\ndata_key_right: str = \"x_b\",\nlabel_key: str = \"y\",\npercent_matching_data: float = 0.5,\nlabel_mapping: Optional[Dict[str, Any]] = None,\nfile_extension: Optional[str] = None):\nsuper().__init__(root_dir, data_key_left, label_key, label_mapping, file_extension)\nself.class_data = self._data_to_class(self.data, label_key)\nself.percent_matching_data = percent_matching_data\nself.data_key_left = data_key_left\nself.data_key_right = data_key_right\nself.label_key = label_key\n@staticmethod\ndef _data_to_class(data: Dict[int, Dict[str, Any]], label_key: str) -&gt; Dict[Any, Set[int]]:\n\"\"\"A helper method to build a mapping from classes to their corresponding data indices.\n        Args:\n            data: A data dictionary like {&lt;index&gt;: {\"data_key\": &lt;data value&gt;}}.\n            label_key: Which key inside `data` corresponds to the label value for a given index entry.\n        Returns:\n            A mapping like {\"label1\": {&lt;indices with label1&gt;}, \"label2\": {&lt;indices with label2&gt;}}.\n        \"\"\"\nclass_data = {}\nfor idx, elem in data.items():\nclass_data.setdefault(elem[label_key], set()).add(idx)\nreturn class_data\ndef _split_length(self) -&gt; int:\n\"\"\"The length of a dataset to be used for the purpose of computing splits.\n        In this case, splits are computed based on the number of classes rather than the number of instances per class.\n        Returns:\n            The apparent length of the dataset for the purpose of the .split() function\n        \"\"\"\nreturn len(self.class_data)\ndef _do_split(self, splits: Sequence[Iterable[int]]) -&gt; List['SiameseDirDataset']:\n\"\"\"Split the current dataset apart into several smaller datasets.\n        Args:\n            splits: Which classes to remove from the current dataset in order to create new dataset(s). One dataset will\n                be generated for every iterable within the `splits` sequence.\n        Returns:\n            New datasets generated by removing classes at the indices specified by `splits` from the current dataset.\n        \"\"\"\n# Splits in this context refer to class indices rather than the typical data indices\nresults = []\nfor split in splits:\n# Convert class indices to data indices\nint_class_keys = list(sorted(self.class_data.keys()))\nsplit = [item for i in split for item in self.class_data[int_class_keys[i]]]\ndata = {new_idx: self.data.pop(old_idx) for new_idx, old_idx in enumerate(split)}\nclass_data = self._data_to_class(data, self.label_key)\nresults.append(\nself._skip_init(data,\nclass_data=class_data,\n**{k: v\nfor k, v in self.__dict__.items() if k not in {'data', 'class_data'}}))\n# Re-key the remaining data to be contiguous from 0 to new max index\nself.data = {new_idx: v for new_idx, (old_idx, v) in enumerate(self.data.items())}\nself.class_data = self._data_to_class(self.data, self.label_key)\n# The summary function is being cached by a base class, so reset our cache here\n# noinspection PyUnresolvedReferences\nself.summary.cache_clear()\nreturn results\ndef __getitem__(self, index: int):\n\"\"\"Extract items from the dataset based on the given `batch_idx`.\n        Args:\n            index: Which data instance to use as the 'left' element.\n        Returns:\n            A datapoint for the given index.\n        \"\"\"\nbase_item = deepcopy(self.data[index])\nif np.random.uniform(0, 1) &lt; self.percent_matching_data:\n# Generate matching data\nclazz_items = self.class_data[base_item[self.label_key]]\nother = np.random.choice(list(clazz_items - {index}))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 1\nelse:\n# Generate non-matching data\nother_classes = self.class_data.keys() - {base_item[self.label_key]}\nother_class = np.random.choice(list(other_classes))\nother = np.random.choice(list(self.class_data[other_class]))\nbase_item[self.data_key_right] = self.data[other][self.data_key_left]\nbase_item[self.label_key] = 0\nreturn base_item\ndef one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n        The similarity should be highest between the index 0 elements of the arrays.\n        Args:\n            n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n        Returns:\n            ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n            [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n        \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n            \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\ndef summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n        Returns:\n            A summary representation of this dataset.\n        \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.one_shot_trial", "title": "<code>one_shot_trial</code>", "text": "<p>Generate one-shot trial data.</p> <p>The similarity should be highest between the index 0 elements of the arrays.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],</p> <code>List[str]</code> <p>[class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def one_shot_trial(self, n: int) -&gt; Tuple[List[str], List[str]]:\n\"\"\"Generate one-shot trial data.\n    The similarity should be highest between the index 0 elements of the arrays.\n    Args:\n        n: The number of samples to draw for computing one shot accuracy. Should be &lt;= the number of total classes.\n    Returns:\n        ([class_a_instance_x, class_a_instance_x, class_a_instance_x, ...],\n        [class_a_instance_w, class_b_instance_y, class_c_instance_z, ...])\n    \"\"\"\nassert n &gt; 1, \"one_shot_trial requires an n-value of at least 2\"\nassert n &lt;= len(self.class_data.keys()), \\\n        \"one_shot_trial only supports up to {} comparisons, but an n-value of {} was given\".format(\nlen(self.class_data.keys()), n)\nclasses = np.random.choice(list(self.class_data.keys()), size=n, replace=False)\nbase_image_indices = np.random.choice(list(self.class_data[classes[0]]), size=2, replace=False)\nl1 = [self.data[base_image_indices[0]][self.data_key_left]] * n\nl2 = [self.data[base_image_indices[1]][self.data_key_left]]\nfor clazz in classes[1:]:\nindex = np.random.choice(list(self.class_data[clazz]))\nl2.append(self.data[index][self.data_key_left])\nreturn l1, l2\n</code></pre>"}, {"location": "fastestimator/dataset/siamese_dir_dataset.html#fastestimator.fastestimator.dataset.siamese_dir_dataset.SiameseDirDataset.summary", "title": "<code>summary</code>", "text": "<p>Generate a summary representation of this dataset.</p> <p>Returns:</p> Type Description <code>DatasetSummary</code> <p>A summary representation of this dataset.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\siamese_dir_dataset.py</code> <pre><code>def summary(self) -&gt; DatasetSummary:\n\"\"\"Generate a summary representation of this dataset.\n    Returns:\n        A summary representation of this dataset.\n    \"\"\"\nsummary = super().summary()\n# since class key is re-mapped, remove class key mapping to reduce confusion\nsummary.class_key_mapping = None\nreturn summary\n</code></pre>"}, {"location": "fastestimator/dataset/data/breast_cancer.html", "title": "breast_cancer", "text": ""}, {"location": "fastestimator/dataset/data/breast_cancer.html#fastestimator.fastestimator.dataset.data.breast_cancer.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.</p> <p>For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.</p> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\breast_cancer.py</code> <pre><code>def load_data() -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset.\n    For more information about this dataset and the meaning of the features it contains, see the sklearn documentation.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x, y) = load_breast_cancer(return_X_y=True)\nx_train, x_eval, y_train, y_eval = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train, x_eval = np.float32(x_train), np.float32(x_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cifar10.html", "title": "cifar10", "text": ""}, {"location": "fastestimator/dataset/data/cifar10.html#fastestimator.fastestimator.dataset.data.cifar10.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the CIFAR10 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cifar10.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the CIFAR10 dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.cifar10.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/cub200.html", "title": "cub200", "text": ""}, {"location": "fastestimator/dataset/data/cub200.html#fastestimator.fastestimator.dataset.data.cub200.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.</p> <p>Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local     storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\cub200.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the Caltech-UCSD Birds 200 (CUB200) dataset.\n    Sourced from http://www.vision.caltech.edu/visipedia/CUB-200.html. This method will download the data to local\n        storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'CUB200')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'CUB200')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, 'cub200.csv')\nimage_compressed_path = os.path.join(root_dir, 'images.tgz')\nannotation_compressed_path = os.path.join(root_dir, 'annotations.tgz')\nimage_extracted_path = os.path.join(root_dir, 'images')\nannotation_extracted_path = os.path.join(root_dir, 'annotations-mat')\nif not (os.path.exists(image_extracted_path) and os.path.exists(annotation_extracted_path)):\n# download\nif not (os.path.exists(image_compressed_path) and os.path.exists(annotation_compressed_path)):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://www.vision.caltech.edu/visipedia-data/CUB-200/images.tgz', root_dir, bar=bar_custom)\nwget.download('http://www.vision.caltech.edu/visipedia-data/CUB-200/annotations.tgz',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(image_compressed_path) as img_tar:\nimg_tar.extractall(root_dir)\nwith tarfile.open(annotation_compressed_path) as anno_tar:\nanno_tar.extractall(root_dir)\n# glob and generate csv\nif not os.path.exists(csv_path):\nimage_folder = os.path.join(root_dir, \"images\")\nclass_names = os.listdir(image_folder)\nlabel_map = {}\nimages = []\nlabels = []\nidx = 0\nfor class_name in class_names:\nif not class_name.startswith(\"._\"):\nimage_folder_class = os.path.join(image_folder, class_name)\nlabel_map[class_name] = idx\nidx += 1\nimage_names = os.listdir(image_folder_class)\nfor image_name in image_names:\nif not image_name.startswith(\"._\"):\nimages.append(os.path.join(image_folder_class, image_name))\nlabels.append(label_map[class_name])\nzipped_list = list(zip(images, labels))\nrandom.shuffle(zipped_list)\ndf = pd.DataFrame(zipped_list, columns=[\"image\", \"label\"])\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['annotation'] = df['image'].str.replace('images', 'annotations-mat').str.replace('jpg', 'mat')\ndf.to_csv(csv_path, index=False)\nprint(\"Data summary is saved at {}\".format(csv_path))\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/german_ner.html", "title": "german_ner", "text": ""}, {"location": "fastestimator/dataset/data/german_ner.html#fastestimator.fastestimator.dataset.data.german_ner.get_sentences_and_labels", "title": "<code>get_sentences_and_labels</code>", "text": "<p>Combines tokens into sentences and create vocab set for train data and labels.</p> <p>For simplicity tokens with 'O' entity are omitted.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the downloaded dataset file.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str], Set[str], Set[str]]</code> <p>(sentences, labels, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\german_ner.py</code> <pre><code>def get_sentences_and_labels(path: str) -&gt; Tuple[List[str], List[str], Set[str], Set[str]]:\n\"\"\"Combines tokens into sentences and create vocab set for train data and labels.\n    For simplicity tokens with 'O' entity are omitted.\n    Args:\n        path: Path to the downloaded dataset file.\n    Returns:\n        (sentences, labels, train_vocab, label_vocab)\n    \"\"\"\nwords, tags = [], []\nword_vocab, label_vocab = set(), set()\nsentences, labels = [], []\ndata = open(path)\nfor line in data:\nif line[0] != '#':\nline = line.split()\nif len(line) &gt; 2 and line[2] != 'O':\nwords.append(line[1])\ntags.append(line[2])\nword_vocab.add(line[1])\nlabel_vocab.add(line[2])\nelse:\nsentences.append(\" \".join([s for s in words]))\nlabels.append([t for t in tags])\nwords.clear()\ntags.clear()\nsentences = list(filter(None, sentences))\nlabels = list(filter(None, labels))\nreturn sentences[:10000], labels[:10000], word_vocab, label_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/german_ner.html#fastestimator.fastestimator.dataset.data.german_ner.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the GermEval dataset.</p> <p>Dataset from GermEval 2014 contains 31,000 sentences corresponding to over 590,000 tokens from German wikipedia and News corpora. The sentence is encoded as one token per line with information provided in tab-seprated columns. Sourced from https://sites.google.com/site/germeval2014ner/data</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]</code> <p>(train_data, eval_data, train_vocab, label_vocab)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\german_ner.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[NumpyDataset, NumpyDataset, Set[str], Set[str]]:\n\"\"\"Load and return the GermEval dataset.\n    Dataset from GermEval 2014 contains 31,000 sentences corresponding to over 590,000 tokens from German wikipedia\n    and News corpora. The sentence is encoded as one token per line with information provided in tab-seprated columns.\n    Sourced from https://sites.google.com/site/germeval2014ner/data\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data, train_vocab, label_vocab)\n    \"\"\"\nurl = 'https://sites.google.com/site/germeval2014ner/data/NER-de-train.tsv?attredirects=0&amp;d=1'\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'GermEval')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'GermEval')\nos.makedirs(root_dir, exist_ok=True)\ndata_path = os.path.join(root_dir, 'de_ner.tsv')\ndata_folder_path = os.path.join(root_dir, 'germeval')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_path):\nprint(\"Downloading data to {}\".format(root_dir))\nstream = requests.get(url, stream=True)  # python wget does not work\ntotal_size = int(stream.headers.get('content-length', 0))\nblock_size = 128  # 1 MB\nprogress = tqdm(total=total_size, unit='B', unit_scale=True)\nwith open(data_path, 'wb') as outfile:\nfor data in stream.iter_content(block_size):\nprogress.update(len(data))\noutfile.write(data)\nprogress.close()\nx, y, x_vocab, y_vocab = get_sentences_and_labels(data_path)\nx_train, x_eval, y_train, y_eval = train_test_split(x, y, test_size=0.2, random_state=42)\nx_train = np.array(x_train)\nx_eval = np.array(x_eval)\ny_train = np.array(y_train)\ny_eval = np.array(y_eval)\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data, x_vocab, y_vocab\n</code></pre>"}, {"location": "fastestimator/dataset/data/horse2zebra.html", "title": "horse2zebra", "text": ""}, {"location": "fastestimator/dataset/data/horse2zebra.html#fastestimator.fastestimator.dataset.data.horse2zebra.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the horse2zebra dataset.</p> <p>Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will     download the data to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The desired batch size.</p> required <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[BatchDataset, BatchDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\horse2zebra.py</code> <pre><code>def load_data(batch_size: int, root_dir: Optional[str] = None) -&gt; Tuple[BatchDataset, BatchDataset]:\n\"\"\"Load and return the horse2zebra dataset.\n    Sourced from https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip. This method will\n        download the data to local storage if the data has not been previously downloaded.\n    Args:\n        batch_size: The desired batch size.\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'horse2zebra')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'horse2zebra')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'horse2zebra.zip')\ndata_folder_path = os.path.join(root_dir, 'images')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\nzip_file.extractall(root_dir)\nos.rename(os.path.join(root_dir, 'horse2zebra'), data_folder_path)\ntest_a = DirDataset(root_dir=os.path.join(data_folder_path, 'testA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntest_b = DirDataset(root_dir=os.path.join(data_folder_path, 'testB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_a = DirDataset(root_dir=os.path.join(data_folder_path, 'trainA'),\ndata_key=\"A\",\nfile_extension='.jpg',\nrecursive_search=False)\ntrain_b = DirDataset(root_dir=os.path.join(data_folder_path, 'trainB'),\ndata_key=\"B\",\nfile_extension='.jpg',\nrecursive_search=False)\noutputs = (BatchDataset(datasets=[train_a, train_b], num_samples=[batch_size, batch_size]),\nBatchDataset(datasets=[test_a, test_b], num_samples=[batch_size, batch_size]))\nreturn outputs\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html", "title": "imdb_review", "text": ""}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the IMDB Movie review dataset.</p> <p>This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).</p> <p>Parameters:</p> Name Type Description Default <code>max_len</code> <code>int</code> <p>Maximum desired length of an input sequence.</p> required <code>vocab_size</code> <code>int</code> <p>Vocabulary size to learn word embeddings.</p> required <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def load_data(max_len: int, vocab_size: int) -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the IMDB Movie review dataset.\n    This dataset contains 25,000 reviews labeled by sentiments (either positive or negative).\n    Args:\n        max_len: Maximum desired length of an input sequence.\n        vocab_size: Vocabulary size to learn word embeddings.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.imdb.load_data(maxlen=max_len, num_words=vocab_size)\n# pad the sequences to max length\nx_train = np.array([pad(x, max_len, 0) for x in x_train])\nx_eval = np.array([pad(x, max_len, 0) for x in x_eval])\ntrain_data = NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = NumpyDataset({\"x\": x_eval, \"y\": y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/imdb_review.html#fastestimator.fastestimator.dataset.data.imdb_review.pad", "title": "<code>pad</code>", "text": "<p>Pad an input_list to a given size.</p> <p>Parameters:</p> Name Type Description Default <code>input_list</code> <code>List[int]</code> <p>The list to be padded.</p> required <code>padding_size</code> <code>int</code> <p>The desired length of the returned list.</p> required <code>padding_value</code> <code>int</code> <p>The value to be inserted for padding.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p><code>input_list</code> with <code>padding_value</code>s appended until the <code>padding_size</code> is reached.</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\imdb_review.py</code> <pre><code>def pad(input_list: List[int], padding_size: int, padding_value: int) -&gt; List[int]:\n\"\"\"Pad an input_list to a given size.\n    Args:\n        input_list: The list to be padded.\n        padding_size: The desired length of the returned list.\n        padding_value: The value to be inserted for padding.\n    Returns:\n        `input_list` with `padding_value`s appended until the `padding_size` is reached.\n    \"\"\"\nreturn input_list + [padding_value] * abs((len(input_list) - padding_size))\n</code></pre>"}, {"location": "fastestimator/dataset/data/mendeley.html", "title": "mendeley", "text": ""}, {"location": "fastestimator/dataset/data/mendeley.html#fastestimator.fastestimator.dataset.data.mendeley.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Mendeley dataset.</p> <p>Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2</p> <p>CC BY 4.0 licence: https://creativecommons.org/licenses/by/4.0/</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mendeley.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the Mendeley dataset.\n    Kermany, Daniel; Zhang, Kang; Goldbaum, Michael (2018), \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray\n    Images for Classification\", Mendeley Data, v2 http://dx.doi.org/10.17632/rscbjbr9sj.2\n    CC BY 4.0 licence:\n    https://creativecommons.org/licenses/by/4.0/\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nurl = 'https://data.mendeley.com/datasets/rscbjbr9sj/2/files/41d542e7-7f91-47f6-9ff2-dd8e5a5a7861/' \\\n          'ChestXRay2017.zip?dl=1'\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Mendeley')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Mendeley')\nos.makedirs(root_dir, exist_ok=True)\ndata_compressed_path = os.path.join(root_dir, 'ChestXRay2017.zip')\ndata_folder_path = os.path.join(root_dir, 'chest_xray')\nif not os.path.exists(data_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nstream = requests.get(url, stream=True)  # python wget does not work\ntotal_size = int(stream.headers.get('content-length', 0))\nblock_size = int(1e6)  # 1 MB\nprogress = tqdm(total=total_size, unit='B', unit_scale=True)\nwith open(data_compressed_path, 'wb') as outfile:\nfor data in stream.iter_content(block_size):\nprogress.update(len(data))\noutfile.write(data)\nprogress.close()\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"chest_xray/\"), zip_file.namelist()))\nlabel_mapping = {'NORMAL': 0, 'PNEUMONIA': 1}\nreturn LabeledDirDataset(os.path.join(data_folder_path, \"train\"), label_mapping=label_mapping,\nfile_extension=\".jpeg\"), LabeledDirDataset(os.path.join(data_folder_path, \"test\"),\nlabel_mapping=label_mapping,\nfile_extension=\".jpeg\")\n</code></pre>"}, {"location": "fastestimator/dataset/data/mnist.html", "title": "mnist", "text": ""}, {"location": "fastestimator/dataset/data/mnist.html#fastestimator.fastestimator.dataset.data.mnist.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the MNIST dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_key</code> <code>str</code> <p>The key for image.</p> <code>'x'</code> <code>label_key</code> <code>str</code> <p>The key for label.</p> <code>'y'</code> <p>Returns:</p> Type Description <code>Tuple[NumpyDataset, NumpyDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mnist.py</code> <pre><code>def load_data(image_key: str = \"x\", label_key: str = \"y\") -&gt; Tuple[NumpyDataset, NumpyDataset]:\n\"\"\"Load and return the MNIST dataset.\n    Args:\n        image_key: The key for image.\n        label_key: The key for label.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = NumpyDataset({image_key: x_train, label_key: y_train})\neval_data = NumpyDataset({image_key: x_eval, label_key: y_eval})\nreturn train_data, eval_data\n</code></pre>"}, {"location": "fastestimator/dataset/data/montgomery.html", "title": "montgomery", "text": ""}, {"location": "fastestimator/dataset/data/montgomery.html#fastestimator.fastestimator.dataset.data.montgomery.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the montgomery dataset.</p> <p>Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data     to local storage if the data has not been previously downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\montgomery.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; CSVDataset:\n\"\"\"Load and return the montgomery dataset.\n    Sourced from http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip. This method will download the data\n        to local storage if the data has not been previously downloaded.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'Montgomery')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Montgomery')\nos.makedirs(root_dir, exist_ok=True)\ncsv_path = os.path.join(root_dir, \"montgomery.csv\")\ndata_compressed_path = os.path.join(root_dir, 'NLM-MontgomeryCXRSet.zip')\nextract_folder_path = os.path.join(root_dir, 'MontgomerySet')\nif not os.path.exists(extract_folder_path):\n# download\nif not os.path.exists(data_compressed_path):\nprint(\"Downloading data to {}\".format(root_dir))\nwget.download('http://openi.nlm.nih.gov/imgs/collections/NLM-MontgomeryCXRSet.zip',\nroot_dir,\nbar=bar_custom)\n# extract\nprint(\"\\nExtracting file ...\")\nwith zipfile.ZipFile(data_compressed_path, 'r') as zip_file:\n# There's some garbage data from macOS in the zip file that gets filtered out here\nzip_file.extractall(root_dir, filter(lambda x: x.startswith(\"MontgomerySet/\"), zip_file.namelist()))\n# glob and generate csv\nif not os.path.exists(csv_path):\nimg_list = glob(os.path.join(extract_folder_path, 'CXR_png', '*.png'))\ndf = pd.DataFrame(data={'image': img_list})\ndf['image'] = df['image'].apply(lambda x: os.path.relpath(x, root_dir))\ndf['image'] = df['image'].apply(os.path.normpath)\ndf['mask_left'] = df['image'].str.replace('CXR_png', os.path.join('ManualMask', 'leftMask'))\ndf['mask_right'] = df['image'].str.replace('CXR_png', os.path.join('ManualMask', 'rightMask'))\ndf.to_csv(csv_path, index=False)\nreturn CSVDataset(csv_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html", "title": "mscoco", "text": ""}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.MSCOCODataset", "title": "<code>MSCOCODataset</code>", "text": "<p>         Bases: <code>DirDataset</code></p> <p>A specialized DirDataset to handle MSCOCO data.</p> <p>This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.</p> <p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>str</code> <p>The path the directory containing MSOCO images.</p> required <code>annotation_file</code> <code>str</code> <p>The path to the file containing annotation data.</p> required <code>caption_file</code> <code>str</code> <p>The path the file containing caption data.</p> required <code>include_bboxes</code> <code>bool</code> <p>Whether images should be paired with their associated bounding boxes. If true, images without bounding boxes will be ignored and other images may be oversampled in order to take their place.</p> <code>True</code> <code>include_masks</code> <code>bool</code> <p>Whether images should be paired with their associated masks. If true, images without masks will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>include_captions</code> <code>bool</code> <p>Whether images should be paired with their associated captions. If true, images without captions will be ignored and other images may be oversampled in order to take their place.</p> <code>False</code> <code>min_bbox_area</code> <p>Bounding boxes with a total area less than <code>min_bbox_area</code> will be discarded.</p> <code>1.0</code> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>class MSCOCODataset(DirDataset):\n\"\"\"A specialized DirDataset to handle MSCOCO data.\n    This dataset combines images from the MSCOCO data directory with their corresponding bboxes, masks, and captions.\n    Args:\n        image_dir: The path the directory containing MSOCO images.\n        annotation_file: The path to the file containing annotation data.\n        caption_file: The path the file containing caption data.\n        include_bboxes: Whether images should be paired with their associated bounding boxes. If true, images without\n            bounding boxes will be ignored and other images may be oversampled in order to take their place.\n        include_masks: Whether images should be paired with their associated masks. If true, images without masks will\n            be ignored and other images may be oversampled in order to take their place.\n        include_captions: Whether images should be paired with their associated captions. If true, images without\n            captions will be ignored and other images may be oversampled in order to take their place.\n        min_bbox_area: Bounding boxes with a total area less than `min_bbox_area` will be discarded.\n    \"\"\"\ninstances: Optional[COCO]\ncaptions: Optional[COCO]\ndef __init__(self,\nimage_dir: str,\nannotation_file: str,\ncaption_file: str,\ninclude_bboxes: bool = True,\ninclude_masks: bool = False,\ninclude_captions: bool = False,\nmin_bbox_area=1.0) -&gt; None:\nsuper().__init__(root_dir=image_dir, data_key=\"image\", recursive_search=False)\nif include_masks:\nassert include_bboxes, \"must include bboxes with mask data\"\nself.include_bboxes = include_bboxes\nself.include_masks = include_masks\nself.min_bbox_area = min_bbox_area\nwith Suppressor():\nself.instances = COCO(annotation_file)\nself.captions = COCO(caption_file) if include_captions else None\ndef __getitem__(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned. If bboxes, masks, or captions are required and the data\n                at the desired index does not have one or more of these features, then data from a random index which\n                does have all necessary features will be fetched instead.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nhas_data = False\nresponse = {}\nwhile not has_data:\nhas_box, has_mask, has_caption = True, True, True\nresponse = self._get_single_item(index)\nif isinstance(index, str):\nreturn response\nif self.include_bboxes and not response[\"bbox\"]:\nhas_box = False\nif self.include_masks and not response[\"mask\"]:\nhas_mask = False\nif self.captions and not response[\"caption\"]:\nhas_caption = False\nhas_data = has_box and has_mask and has_caption\nindex = np.random.randint(len(self))\nreturn response\ndef _get_single_item(self, index: Union[int, str]) -&gt; Union[Dict[str, Any], np.ndarray, List[Any]]:\n\"\"\"Look up data from the dataset.\n        Args:\n            index: Either an int corresponding to a particular element of data, or a string in which case the\n                corresponding column of data will be returned.\n        Returns:\n            A data dictionary if the index was an int, otherwise a column of data in list format.\n        \"\"\"\nresponse = super().__getitem__(index)\nif isinstance(index, str):\nreturn response\nelse:\nresponse = deepcopy(response)\nimage = response[\"image\"]\nimage_id = int(os.path.splitext(os.path.basename(image))[0])\nresponse[\"image_id\"] = image_id\nif self.include_bboxes:\nself._populate_instance_data(response, image_id)\nif self.captions:\nself._populate_caption_data(response, image_id)\nreturn response\ndef _populate_instance_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add instance data to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find data.\n        \"\"\"\ndata[\"bbox\"] = []\nif self.include_masks:\ndata[\"mask\"] = []\nannotation_ids = self.instances.getAnnIds(imgIds=image_id, iscrowd=False)\nif annotation_ids:\nannotations = self.instances.loadAnns(annotation_ids)\nfor annotation in annotations:\nif annotation[\"bbox\"][2] * annotation[\"bbox\"][3] &gt; self.min_bbox_area:\ndata[\"bbox\"].append(tuple(annotation['bbox'] + [annotation['category_id']]))\nif self.include_masks:\ndata[\"mask\"].append(self.instances.annToMask(annotation))\ndef _populate_caption_data(self, data: Dict[str, Any], image_id: int) -&gt; None:\n\"\"\"Add captions to a data dictionary.\n        Args:\n            data: The dictionary to be augmented.\n            image_id: The id of the image for which to find captions.\n        \"\"\"\ndata[\"caption\"] = []\nannotation_ids = self.captions.getAnnIds(imgIds=image_id)\nif annotation_ids:\nannotations = self.captions.loadAnns(annotation_ids)\nfor annotation in annotations:\ndata[\"caption\"].append(annotation['caption'])\n</code></pre>"}, {"location": "fastestimator/dataset/data/mscoco.html#fastestimator.fastestimator.dataset.data.mscoco.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the COCO dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <code>load_bboxes</code> <code>bool</code> <p>Whether to load bbox-related data.</p> <code>True</code> <code>load_masks</code> <code>bool</code> <p>Whether to load mask data (in the form of an array of 1-hot images).</p> <code>False</code> <code>load_captions</code> <code>bool</code> <p>Whether to load caption-related data.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[MSCOCODataset, MSCOCODataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\mscoco.py</code> <pre><code>def load_data(root_dir: Optional[str] = None,\nload_bboxes: bool = True,\nload_masks: bool = False,\nload_captions: bool = False) -&gt; Tuple[MSCOCODataset, MSCOCODataset]:\n\"\"\"Load and return the COCO dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n        load_bboxes: Whether to load bbox-related data.\n        load_masks: Whether to load mask data (in the form of an array of 1-hot images).\n        load_captions: Whether to load caption-related data.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'MSCOCO2017')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'MSCOCO2017')\nos.makedirs(root_dir, exist_ok=True)\ntrain_data = os.path.join(root_dir, \"train2017\")\neval_data = os.path.join(root_dir, \"val2017\")\nannotation_data = os.path.join(root_dir, \"annotations\")\nfiles = [(train_data, \"train2017.zip\", 'http://images.cocodataset.org/zips/train2017.zip'),\n(eval_data, \"val2017.zip\", 'http://images.cocodataset.org/zips/val2017.zip'),\n(annotation_data,\n\"annotations_trainval2017.zip\",\n'http://images.cocodataset.org/annotations/annotations_trainval2017.zip')]\nfor data_dir, zip_name, download_url in files:\nif not os.path.exists(data_dir):\nzip_path = os.path.join(root_dir, zip_name)\n# Download\nif not os.path.exists(zip_path):\nprint(\"Downloading {} to {}\".format(zip_name, root_dir))\nwget.download(download_url, zip_path, bar=bar_custom)\n# Extract\nprint(\"Extracting {}\".format(zip_name))\nwith zipfile.ZipFile(zip_path, 'r') as zip_file:\nzip_file.extractall(os.path.dirname(zip_path))\ntrain_annotation = os.path.join(annotation_data, \"instances_train2017.json\")\neval_annotation = os.path.join(annotation_data, \"instances_val2017.json\")\ntrain_captions = os.path.join(annotation_data, \"captions_train2017.json\")\neval_captions = os.path.join(annotation_data, \"captions_val2017.json\")\ntrain_ds = MSCOCODataset(train_data,\ntrain_annotation,\ntrain_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\neval_ds = MSCOCODataset(eval_data,\neval_annotation,\neval_captions,\ninclude_bboxes=load_bboxes,\ninclude_masks=load_masks,\ninclude_captions=load_captions)\nreturn train_ds, eval_ds\n</code></pre>"}, {"location": "fastestimator/dataset/data/nih_chestxray.html", "title": "nih_chestxray", "text": ""}, {"location": "fastestimator/dataset/data/nih_chestxray.html#fastestimator.fastestimator.dataset.data.nih_chestxray.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the NIH Chest X-ray dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>DirDataset</code> <p>train_data</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\nih_chestxray.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; DirDataset:\n\"\"\"Load and return the NIH Chest X-ray dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        train_data\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'NIH_Chestxray')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'NIH_Chestxray')\nos.makedirs(root_dir, exist_ok=True)\nimage_extracted_path = os.path.join(root_dir, 'images')\nif not os.path.exists(image_extracted_path):\n# download data\nlinks = [\n'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n]\ndata_paths = [os.path.join(root_dir, \"images_{}.tar.gz\".format(x)) for x in range(len(links))]\nfor idx, (link, data_path) in enumerate(zip(links, data_paths)):\n_download_data(link, data_path, idx, len(links))\n# extract data\nfor idx, data_path in enumerate(data_paths):\nprint(\"Extracting {}, file {} / {}\".format(data_path, idx + 1, len(links)))\nwith tarfile.open(data_path) as img_tar:\nimg_tar.extractall(root_dir)\nreturn DirDataset(image_extracted_path, file_extension='.png', recursive_search=False)\n</code></pre>"}, {"location": "fastestimator/dataset/data/omniglot.html", "title": "omniglot", "text": ""}, {"location": "fastestimator/dataset/data/omniglot.html#fastestimator.fastestimator.dataset.data.omniglot.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Omniglot dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[SiameseDirDataset, SiameseDirDataset]</code> <p>(train_data, eval_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\omniglot.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[SiameseDirDataset, SiameseDirDataset]:\n\"\"\"Load and return the Omniglot dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, eval_data)\n    \"\"\"\nif root_dir is None:\nroot_dir = os.path.join(str(Path.home()), 'fastestimator_data', 'Omniglot')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'Omniglot')\nos.makedirs(root_dir, exist_ok=True)\ntrain_path = os.path.join(root_dir, 'images_background')\neval_path = os.path.join(root_dir, 'images_evaluation')\ntrain_zip = os.path.join(root_dir, 'images_background.zip')\neval_zip = os.path.join(root_dir, 'images_evaluation.zip')\nfiles = [(train_path, train_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip'),\n(eval_path, eval_zip, 'https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip')]\nfor data_path, data_zip, download_link in files:\nif not os.path.exists(data_path):\n# Download\nif not os.path.exists(data_zip):\nprint(\"Downloading data: {}\".format(data_zip))\nwget.download(download_link, data_zip, bar=bar_custom)\n# Extract\nprint(\"Extracting data: {}\".format(data_path))\nwith zipfile.ZipFile(data_zip, 'r') as zip_file:\nzip_file.extractall(root_dir)\nreturn SiameseDirDataset(train_path), SiameseDirDataset(eval_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/svhn.html", "title": "svhn", "text": ""}, {"location": "fastestimator/dataset/data/svhn.html#fastestimator.fastestimator.dataset.data.svhn.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the Street View House Numbers (SVHN) dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[PickleDataset, PickleDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\svhn.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[PickleDataset, PickleDataset]:\n\"\"\"Load and return the Street View House Numbers (SVHN) dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'SVHN')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'SVHN')\nos.makedirs(root_dir, exist_ok=True)\ntrain_file_path = os.path.join(root_dir, 'train.pickle')\ntest_file_path = os.path.join(root_dir, 'test.pickle')\ntrain_compressed_path = os.path.join(root_dir, \"train.tar.gz\")\ntest_compressed_path = os.path.join(root_dir, \"test.tar.gz\")\ntrain_folder_path = os.path.join(root_dir, \"train\")\ntest_folder_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_folder_path):\n# download\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/train.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(train_compressed_path) as tar:\ntar.extractall(root_dir)\nif not os.path.exists(test_folder_path):\n# download\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading eval data to {}\".format(root_dir))\nwget.download('http://ufldl.stanford.edu/housenumbers/test.tar.gz', root_dir, bar=bar_custom)\n# extract\nprint(\"\\nExtracting files ...\")\nwith tarfile.open(test_compressed_path) as tar:\ntar.extractall(root_dir)\n# glob and generate bbox files\nif not os.path.exists(train_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(train_folder_path, \"train\", train_file_path)\nif not os.path.exists(test_file_path):\nprint(\"\\nConstructing bounding box data ...\")\n_extract_metadata(test_folder_path, \"test\", test_file_path)\nreturn PickleDataset(train_file_path), PickleDataset(test_file_path)\n</code></pre>"}, {"location": "fastestimator/dataset/data/usps.html", "title": "usps", "text": ""}, {"location": "fastestimator/dataset/data/usps.html#fastestimator.fastestimator.dataset.data.usps.load_data", "title": "<code>load_data</code>", "text": "<p>Load and return the USPS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_dir</code> <code>Optional[str]</code> <p>The path to store the downloaded data. When <code>path</code> is not provided, the data will be saved into <code>fastestimator_data</code> under the user's home directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[LabeledDirDataset, LabeledDirDataset]</code> <p>(train_data, test_data)</p> Source code in <code>fastestimator\\fastestimator\\dataset\\data\\usps.py</code> <pre><code>def load_data(root_dir: Optional[str] = None) -&gt; Tuple[LabeledDirDataset, LabeledDirDataset]:\n\"\"\"Load and return the USPS dataset.\n    Args:\n        root_dir: The path to store the downloaded data. When `path` is not provided, the data will be saved into\n            `fastestimator_data` under the user's home directory.\n    Returns:\n        (train_data, test_data)\n    \"\"\"\nhome = str(Path.home())\nif root_dir is None:\nroot_dir = os.path.join(home, 'fastestimator_data', 'USPS')\nelse:\nroot_dir = os.path.join(os.path.abspath(root_dir), 'USPS')\nos.makedirs(root_dir, exist_ok=True)\n# download data to memory\ntrain_compressed_path = os.path.join(root_dir, \"zip.train.gz\")\ntest_compressed_path = os.path.join(root_dir, \"zip.test.gz\")\ntrain_base_path = os.path.join(root_dir, \"train\")\ntest_base_path = os.path.join(root_dir, \"test\")\nif not os.path.exists(train_base_path):\nif not os.path.exists(train_compressed_path):\nprint(\"Downloading train data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz',\nroot_dir,\nbar=bar_custom)\ntrain_images, train_labels = _extract_images_labels(train_compressed_path)\n_write_data(train_images, train_labels, train_base_path, \"train\")\nif not os.path.exists(test_base_path):\nif not os.path.exists(test_compressed_path):\nprint(\"Downloading test data to {}\".format(root_dir))\nwget.download('http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz',\nroot_dir,\nbar=bar_custom)\ntest_images, test_labels = _extract_images_labels(test_compressed_path)\n_write_data(test_images, test_labels, test_base_path, \"test\")\n# make datasets\nreturn LabeledDirDataset(train_base_path, file_extension=\".png\"), LabeledDirDataset(test_base_path,\nfile_extension=\".png\")\n</code></pre>"}, {"location": "fastestimator/layers/pytorch/cropping_2d.html", "title": "cropping_2d", "text": ""}, {"location": "fastestimator/layers/pytorch/cropping_2d.html#fastestimator.fastestimator.layers.pytorch.cropping_2d.Cropping2D", "title": "<code>Cropping2D</code>", "text": "<p>         Bases: <code>nn.Module</code></p> <p>A layer for cropping along height and width dimensions</p> <pre><code>x = torch.tensor(list(range(100))).view((1,1,10,10))\nm = fe.layers.pytorch.Cropping2D(3)\ny = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\nm = fe.layers.pytorch.Cropping2D((3, 4))\ny = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\nm = fe.layers.pytorch.Cropping2D(((1, 4), 4))\ny = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cropping</code> <code>Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int, int]]]]</code> <p>Height and width cropping parameters. If a single int 'n' is specified, then the width and height of the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w') is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h' and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then 'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right (assuming the top left corner as the 0,0 origin).</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cropping</code> has an unacceptable data type.</p> Source code in <code>fastestimator\\fastestimator\\layers\\pytorch\\cropping_2d.py</code> <pre><code>class Cropping2D(nn.Module):\n\"\"\"A layer for cropping along height and width dimensions\n    ```python\n    x = torch.tensor(list(range(100))).view((1,1,10,10))\n    m = fe.layers.pytorch.Cropping2D(3)\n    y = m.forward(x)  # [[[[33, 34, 35, 36], [43, 44, 45, 46], [53, 54, 55, 56], [63, 64, 65, 66]]]]\n    m = fe.layers.pytorch.Cropping2D((3, 4))\n    y = m.forward(x)  # [[[[34, 35], [44, 45], [54, 55], [64, 65]]]]\n    m = fe.layers.pytorch.Cropping2D(((1, 4), 4))\n    y = m.forward(x)  # [[[[14, 15], [24, 25], [34, 35], [44, 45], [54, 55]]]]\n    ```\n    Args:\n        cropping: Height and width cropping parameters. If a single int 'n' is specified, then the width and height of\n            the input will both be reduced by '2n', with 'n' coming off of each side of the input. If a tuple ('h', 'w')\n            is provided, then the height and width of the input will be reduced by '2h' and '2w' respectively, with 'h'\n            and 'w' coming off of each side of the input. If a tuple like (('h1', 'h2'), ('w1', 'w2')) is provided, then\n            'h1' will be removed from the top, 'h2' from the bottom, 'w1' from the left, and 'w2' from the right\n            (assuming the top left corner as the 0,0 origin).\n    Raises:\n        ValueError: If `cropping` has an unacceptable data type.\n    \"\"\"\ndef __init__(self, cropping: Union[int, Tuple[Union[int, Tuple[int, int]], Union[int, Tuple[int,\nint]]]] = 0) -&gt; None:\nsuper().__init__()\nif isinstance(cropping, int):\nself.cropping = ((cropping, cropping), (cropping, cropping))\nelif hasattr(cropping, '__len__'):\nif len(cropping) != 2:\nraise ValueError(f\"'cropping' should have two elements, but found {len(cropping)}\")\nif isinstance(cropping[0], int):\nheight_cropping = (cropping[0], cropping[0])\nelif hasattr(cropping[0], '__len__') and len(cropping[0]) == 2:\nheight_cropping = (cropping[0][0], cropping[0][1])\nelse:\nraise ValueError(f\"'cropping' height should be an int or tuple of ints, but found {cropping[0]}\")\nif isinstance(cropping[1], int):\nwidth_cropping = (cropping[1], cropping[1])\nelif hasattr(cropping[1], '__len__') and len(cropping[1]) == 2:\nwidth_cropping = (cropping[1][0], cropping[1][1])\nelse:\nraise ValueError(f\"'cropping' width should be an int or tuple of ints, but found {cropping[1]}\")\nself.cropping = (height_cropping, width_cropping)\nelse:\nraise ValueError(\n\"cropping` should be either an int, a tuple of 2 ints or a tuple of two tuple of 2 ints. Found: \" +\nstr(cropping))\ndef forward(self, x: torch.Tensor) -&gt; torch.Tensor:\nreturn x[:, :, self.cropping[0][0]:-self.cropping[0][1], self.cropping[1][0]:-self.cropping[1][1]]\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/instance_norm.html", "title": "instance_norm", "text": ""}, {"location": "fastestimator/layers/tensorflow/instance_norm.html#fastestimator.fastestimator.layers.tensorflow.instance_norm.InstanceNormalization", "title": "<code>InstanceNormalization</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing instance normalization.</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.</p> <pre><code>n = tfp.distributions.Normal(loc=10, scale=2)\nx = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\nm = fe.layers.tensorflow.InstanceNormalization()\ny = m(x)  # mean ~= 0, stddev ~= 0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>A numerical stability constant added to the variance.</p> <code>1e-05</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\instance_norm.py</code> <pre><code>class InstanceNormalization(layers.Layer):\n\"\"\"A layer for performing instance normalization.\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). See\n    https://arxiv.org/abs/1607.08022 for details about this layer. The implementation here is borrowed from\n    https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py.\n    ```python\n    n = tfp.distributions.Normal(loc=10, scale=2)\n    x = n.sample(sample_shape=(1, 100, 100, 1))  # mean ~= 10, stddev ~= 2\n    m = fe.layers.tensorflow.InstanceNormalization()\n    y = m(x)  # mean ~= 0, stddev ~= 0\n    ```\n    Args:\n        epsilon: A numerical stability constant added to the variance.\n    \"\"\"\ndef __init__(self, epsilon: float = 1e-5) -&gt; None:\nsuper().__init__()\nself.epsilon = epsilon\nself.scale = None\nself.offset = None\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'epsilon': self.epsilon}\ndef build(self, input_shape: Tuple[int, int, int, int]) -&gt; None:\nself.scale = self.add_weight(name='scale',\nshape=input_shape[-1:],\ninitializer=tf.random_normal_initializer(0., 0.02),\ntrainable=True)\nself.offset = self.add_weight(name='offset', shape=input_shape[-1:], initializer='zeros', trainable=True)\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nmean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\ninv = tf.math.rsqrt(variance + self.epsilon)\nnormalized = (x - mean) * inv\nreturn self.scale * normalized + self.offset\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html", "title": "reflection_padding_2d", "text": ""}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D", "title": "<code>ReflectionPadding2D</code>", "text": "<p>         Bases: <code>layers.Layer</code></p> <p>A layer for performing Reflection Padding on 2D arrays.</p> <p>This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels). The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.</p> <pre><code>x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\ny = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\nm = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\ny = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>Tuple[int, int]</code> <p>padding size (Width, Height). The padding size must be less than the size of the corresponding dimension in the input tensor.</p> <code>(1, 1)</code> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>class ReflectionPadding2D(layers.Layer):\n\"\"\"A layer for performing Reflection Padding on 2D arrays.\n    This layer assumes that you are using the a tensor shaped like (Batch, Height, Width, Channels).\n    The implementation here is borrowed from https://stackoverflow.com/questions/50677544/reflection-padding-conv2d.\n    ```python\n    x = tf.reshape(tf.convert_to_tensor(list(range(9))), (1,3,3,1))  # ~ [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 1))\n    y = m(x)  # ~ [[4, 3, 4, 5, 4], [1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7], [4, 3, 4, 5, 4]]\n    m = fe.layers.tensorflow.ReflectionPadding2D((1, 0))\n    y = m(x)  # ~ [[1, 0, 1, 2, 1], [4, 3, 4, 5, 4], [7, 6, 7, 8, 7]]\n    ```\n    Args:\n        padding: padding size (Width, Height). The padding size must be less than the size of the corresponding\n            dimension in the input tensor.\n    \"\"\"\ndef __init__(self, padding: Tuple[int, int] = (1, 1)) -&gt; None:\nsuper().__init__()\nself.padding = tuple(padding)\nself.input_spec = [layers.InputSpec(ndim=4)]\ndef get_config(self) -&gt; Dict[str, Any]:\nreturn {'padding': self.padding}\ndef compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\ndef call(self, x: tf.Tensor) -&gt; tf.Tensor:\nw_pad, h_pad = self.padding\nreturn tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n</code></pre>"}, {"location": "fastestimator/layers/tensorflow/reflection_padding_2d.html#fastestimator.fastestimator.layers.tensorflow.reflection_padding_2d.ReflectionPadding2D.compute_output_shape", "title": "<code>compute_output_shape</code>", "text": "<p>If you are using \"channels_last\" configuration</p> Source code in <code>fastestimator\\fastestimator\\layers\\tensorflow\\reflection_padding_2d.py</code> <pre><code>def compute_output_shape(self, s: Tuple[int, int, int, int]) -&gt; Tuple[int, int, int, int]:\n\"\"\"If you are using \"channels_last\" configuration\"\"\"\nreturn s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n</code></pre>"}, {"location": "fastestimator/op/op.html", "title": "op", "text": ""}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.Op", "title": "<code>Op</code>", "text": "<p>A base class for FastEstimator Operators.</p> <p>Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three main variables: <code>inputs</code>, <code>outputs</code>, and <code>mode</code>. When FastEstimator executes, it holds all of its available data behind the scenes in a data dictionary. If an <code>Op</code> wants to interact with a piece of data from this dictionary, it lists the data's key as one of it's <code>inputs</code>. That data will then be passed to the <code>Op</code> when the <code>Op</code>s forward function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an <code>Op</code> wants to write data into the data dictionary, it can return values from its forward function. These values are then written into the data dictionary under the keys specified by the <code>Op</code>s <code>outputs</code>. An <code>Op</code> will only be run if its associated <code>mode</code> matches the current execution mode. For example, if an <code>Op</code> has a mode of 'eval' but FastEstimator is currently running in the 'train' mode, then the <code>Op</code>s forward function will not be called.</p> <p>Normally, if a single string \"key\" is passed as <code>inputs</code> then the value that is passed to the forward function will be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as <code>inputs</code> then the value passed to the forward function will be the element stored in the data dictionary, but wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an <code>Op</code> is anticipated to take one or more inputs and treat them all in the same way. In such cases the <code>in_list</code> member variable may be manually overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of whether <code>inputs</code> was a single string or a list of strings. For an example of when this is useful, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Similarly, by default, if an <code>Op</code> has a single <code>output</code> string \"key\" then that output R will be written into the data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as <code>outputs</code> then the return value for R from the <code>Op</code> is expected to be a list [X], where the inner value will be written to the data dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an <code>Op</code> wants to always return data in a list format without worrying about whether it had one input or more than one input. In such cases the <code>out_list</code> member variable may be manually overridden to True. This will cause the system to always assume that the response is in list format and unwrap the values before storing them into the data dictionary. For an example, see: fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str], Callable]</code> <p>Key(s) from which to retrieve data from the data dictionary.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>class Op:\n\"\"\"A base class for FastEstimator Operators.\n    Operators are modular pieces of code which can be used to build complex execution graphs. They are based on three\n    main variables: `inputs`, `outputs`, and `mode`. When FastEstimator executes, it holds all of its available data\n    behind the scenes in a data dictionary. If an `Op` wants to interact with a piece of data from this dictionary, it\n    lists the data's key as one of it's `inputs`. That data will then be passed to the `Op` when the `Op`s forward\n    function is invoked (see NumpyOp and TensorOp for more information about the forward function). If an `Op` wants to\n    write data into the data dictionary, it can return values from its forward function. These values are then written\n    into the data dictionary under the keys specified by the `Op`s `outputs`. An `Op` will only be run if its associated\n    `mode` matches the current execution mode. For example, if an `Op` has a mode of 'eval' but FastEstimator is\n    currently running in the 'train' mode, then the `Op`s forward function will not be called.\n    Normally, if a single string \"key\" is passed as `inputs` then the value that is passed to the forward function will\n    be the value exactly as it is stored in the data dictionary: dict[\"key\"]. On the other hand, if [\"key\"] is passed as\n    `inputs` then the value passed to the forward function will be the element stored in the data dictionary, but\n    wrapped within a list: [dict[\"key\"]]. This can be inconvenient in some cases where an `Op` is anticipated to take\n    one or more inputs and treat them all in the same way. In such cases the `in_list` member variable may be manually\n    overridden to True. This will cause data to always be sent to the forward function like [dict[\"key\"]] regardless of\n    whether `inputs` was a single string or a list of strings. For an example of when this is useful, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Similarly, by default, if an `Op` has a single `output` string \"key\" then that output R will be written into the\n    data dictionary exactly as it is presented: dict[\"key\"] = R. If, however, [\"key\"] is given as `outputs` then the\n    return value for R from the `Op` is expected to be a list [X], where the inner value will be written to the data\n    dictionary: dict[\"key\"] = X. This can be inconvenient in some cases where an `Op` wants to always return data in a\n    list format without worrying about whether it had one input or more than one input. In such cases the `out_list`\n    member variable may be manually overridden to True. This will cause the system to always assume that the response is\n    in list format and unwrap the values before storing them into the data dictionary. For an example, see:\n    fe.op.numpyop.univariate.univariate.ImageOnlyAlbumentation.\n    Args:\n        inputs: Key(s) from which to retrieve data from the data dictionary.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ninputs: List[Union[str, Callable]]\noutputs: List[str]\nmode: Set[str]\nin_list: bool  # Whether inputs should be presented as a list or an individual value\nout_list: bool  # Whether outputs will be returned as a list or an individual value\ndef __init__(self,\ninputs: Union[None, str, Iterable[str], Callable] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = to_list(inputs)\nself.outputs = to_list(outputs)\nself.mode = parse_modes(to_set(mode))\nself.in_list = not isinstance(inputs, (str, Callable))\nself.out_list = not isinstance(outputs, str)\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.get_inputs_by_op", "title": "<code>get_inputs_by_op</code>", "text": "<p>Retrieve the necessary input data from the data dictionary in order to run an <code>op</code>.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The op to run.</p> required <code>store</code> <code>Mapping[str, Any]</code> <p>The system's data dictionary to draw inputs out of.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Input data to be fed to the <code>op</code> forward function.</p> Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def get_inputs_by_op(op: Op, store: Mapping[str, Any]) -&gt; Any:\n\"\"\"Retrieve the necessary input data from the data dictionary in order to run an `op`.\n    Args:\n        op: The op to run.\n        store: The system's data dictionary to draw inputs out of.\n    Returns:\n        Input data to be fed to the `op` forward function.\n    \"\"\"\ndata = None\nif op.inputs:\ndata = [store[key] if not isinstance(key, Callable) else key() for key in op.inputs]\nif not op.in_list:\ndata = data[0]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/op.html#fastestimator.fastestimator.op.op.write_outputs_by_op", "title": "<code>write_outputs_by_op</code>", "text": "<p>Write <code>outputs</code> from an <code>op</code> forward function into the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>Op</code> <p>The Op which generated <code>outputs</code>.</p> required <code>store</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary into which to write the <code>outputs</code>.</p> required <code>outputs</code> <code>Any</code> <p>The value(s) generated by the <code>op</code>s forward function.</p> required Source code in <code>fastestimator\\fastestimator\\op\\op.py</code> <pre><code>def write_outputs_by_op(op: Op, store: MutableMapping[str, Any], outputs: Any) -&gt; None:\n\"\"\"Write `outputs` from an `op` forward function into the data dictionary.\n    Args:\n        op: The Op which generated `outputs`.\n        store: The data dictionary into which to write the `outputs`.\n        outputs: The value(s) generated by the `op`s forward function.\n    \"\"\"\nif not op.out_list:\noutputs = [outputs]\nfor key, data in zip(op.outputs, outputs):\nstore[key] = data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html", "title": "numpyop", "text": ""}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.Delete", "title": "<code>Delete</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Delete key(s) and their associated values from the data dictionary.</p> <p>The system has special logic to detect instances of this Op and delete its <code>inputs</code> from the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[str, List[str]]</code> <p>Existing key(s) to be deleted from the data dictionary.</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>class Delete(NumpyOp):\n\"\"\"Delete key(s) and their associated values from the data dictionary.\n    The system has special logic to detect instances of this Op and delete its `inputs` from the data dictionary.\n    Args:\n        keys: Existing key(s) to be deleted from the data dictionary.\n    \"\"\"\ndef __init__(self, keys: Union[str, List[str]], mode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=keys, mode=mode)\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]], state: Dict[str, Any]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp", "title": "<code>NumpyOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns numpy data.</p> <p>These Operators are used in fe.Pipeline to perform data pre-processing / augmentation.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>class NumpyOp(Op):\n\"\"\"An Operator class which takes and returns numpy data.\n    These Operators are used in fe.Pipeline to perform data pre-processing / augmentation.\n    \"\"\"\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n        Args:\n            data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.NumpyOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on individual elements of data before any batching / axis expansion is performed.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The arrays from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on individual elements of data before any batching / axis expansion is performed.\n    Args:\n        data: The arrays from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/numpyop.html#fastestimator.fastestimator.op.numpyop.numpyop.forward_numpyop", "title": "<code>forward_numpyop</code>", "text": "<p>Call the forward function for list of NumpyOps, and modify the data dictionary in place.</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>List[NumpyOp]</code> <p>A list of NumpyOps to execute.</p> required <code>data</code> <code>MutableMapping[str, Any]</code> <p>The data dictionary.</p> required <code>mode</code> <code>str</code> <p>The current execution mode (\"train\", \"eval\", \"test\", or \"infer\").</p> required Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\numpyop.py</code> <pre><code>def forward_numpyop(ops: List[NumpyOp], data: MutableMapping[str, Any], mode: str) -&gt; None:\n\"\"\"Call the forward function for list of NumpyOps, and modify the data dictionary in place.\n    Args:\n        ops: A list of NumpyOps to execute.\n        data: The data dictionary.\n        mode: The current execution mode (\"train\", \"eval\", \"test\", or \"infer\").\n    \"\"\"\nfor op in ops:\nop_data = get_inputs_by_op(op, data)\nop_data = op.forward(op_data, {\"mode\": mode})\nif isinstance(op, Delete):\nfor key in op.inputs:\ndel data[key]\nif op.outputs:\nwrite_outputs_by_op(op, data, op_data)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html", "title": "one_of", "text": ""}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf", "title": "<code>OneOf</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform one of several possible NumpyOps.</p> <p>Parameters:</p> Name Type Description Default <code>numpy_ops</code> <code>NumpyOp</code> <p>A list of ops to choose between with uniform probability.</p> <code>()</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>class OneOf(NumpyOp):\n\"\"\"Perform one of several possible NumpyOps.\n    Args:\n        numpy_ops: A list of ops to choose between with uniform probability.\n    \"\"\"\ndef __init__(self, *numpy_ops: NumpyOp) -&gt; None:\ninputs = numpy_ops[0].inputs\noutputs = numpy_ops[0].outputs\nmode = numpy_ops[0].mode\nfor op in numpy_ops[1:]:\nassert inputs == op.inputs, \"All ops within a OneOf must share the same inputs\"\nassert outputs == op.outputs, \"All ops within a OneOf must share the same outputs\"\nassert mode == op.mode, \"All ops within a OneOf must share the same mode\"\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.numpy_ops = numpy_ops\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n        Args:\n            data: The information to be passed to one of the wrapped operators.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after application of one of the available numpyOps.\n        \"\"\"\nreturn random.choice(self.numpy_ops).forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/one_of.html#fastestimator.fastestimator.op.numpyop.meta.one_of.OneOf.forward", "title": "<code>forward</code>", "text": "<p>Execute a randomly selected op from the list of <code>numpy_ops</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The information to be passed to one of the wrapped operators.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The <code>data</code> after application of one of the available numpyOps.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\one_of.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute a randomly selected op from the list of `numpy_ops`.\n    Args:\n        data: The information to be passed to one of the wrapped operators.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after application of one of the available numpyOps.\n    \"\"\"\nreturn random.choice(self.numpy_ops).forward(data, state)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html", "title": "sometimes", "text": ""}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes", "title": "<code>Sometimes</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Perform a NumpyOp with a given probability.</p> <p>Parameters:</p> Name Type Description Default <code>numpy_op</code> <code>NumpyOp</code> <p>The operator to be performed.</p> required <code>prob</code> <code>float</code> <p>The probability of execution, which should be in the range: [0-1).</p> <code>0.5</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>class Sometimes(NumpyOp):\n\"\"\"Perform a NumpyOp with a given probability.\n    Args:\n        numpy_op: The operator to be performed.\n        prob: The probability of execution, which should be in the range: [0-1).\n    \"\"\"\ndef __init__(self, numpy_op: NumpyOp, prob: float = 0.5) -&gt; None:\nsuper().__init__(inputs=numpy_op.inputs, outputs=numpy_op.outputs, mode=numpy_op.mode)\nself.numpy_op = numpy_op\nself.prob = prob\ndef forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n        Args:\n            data: The information to be passed to the wrapped operator.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The original `data`, or the `data` after running it through the wrapped operator.\n        \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = self.numpy_op.forward(data, state)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/meta/sometimes.html#fastestimator.fastestimator.op.numpyop.meta.sometimes.Sometimes.forward", "title": "<code>forward</code>", "text": "<p>Execute the wrapped operator a certain fraction of the time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The information to be passed to the wrapped operator.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, List[np.ndarray]]</code> <p>The original <code>data</code>, or the <code>data</code> after running it through the wrapped operator.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\meta\\sometimes.py</code> <pre><code>def forward(self, data: Union[np.ndarray, List[np.ndarray]],\nstate: Dict[str, Any]) -&gt; Union[np.ndarray, List[np.ndarray]]:\n\"\"\"Execute the wrapped operator a certain fraction of the time.\n    Args:\n        data: The information to be passed to the wrapped operator.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The original `data`, or the `data` after running it through the wrapped operator.\n    \"\"\"\nif self.prob &gt; np.random.uniform():\ndata = self.numpy_op.forward(data, state)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/affine.html", "title": "affine", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/affine.html#fastestimator.fastestimator.op.numpyop.multivariate.affine.Affine", "title": "<code>Affine</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Perform affine transformations on an image.</p> <p>Parameters:</p> Name Type Description Default <code>rotate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to rotate an image (in degrees). If a single value is given then images will be rotated by     a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated     by a value sampled from the range [a, b].</p> <code>0</code> <code>scale</code> <code>Union[float, Tuple[float, float]]</code> <p>How much to scale an image (in percentage). If a single value is given then all images will be scaled     by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled     based on a value drawn from the range [a,b].</p> <code>1.0</code> <code>shear</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to shear an image (in degrees). If a single value is given then all images will be sheared     on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will     be sheared on X and Y by two values randomly sampled from the range [a, b].</p> <code>0</code> <code>translate</code> <code>Union[Number, Tuple[Number, Number]]</code> <p>How much to translate an image. If a single value is given then the translation extent will be     sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from     the range [a,b]. If integers are given then the translation will be in pixels. If a float then     it will be as a fraction of the image size.</p> <code>0</code> <code>border_handling</code> <code>Union[str, List[str]]</code> <p>What to do in order to fill newly created pixels. Options are 'constant', 'edge',     'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly     selected from the options in the list.</p> <code>'reflect'</code> <code>fill_value</code> <code>Number</code> <p>What pixel value to insert when border_handling is 'constant'.</p> <code>0</code> <code>interpolation</code> <code>str</code> <p>What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',     'bilinear', 'bicubic', 'biquartic', and 'biquintic'.</p> <code>'bilinear'</code> <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\affine.py</code> <pre><code>class Affine(MultiVariateAlbumentation):\n\"\"\"Perform affine transformations on an image.\n    Args:\n        rotate: How much to rotate an image (in degrees). If a single value is given then images will be rotated by\n                a value sampled from the range [-n, n]. If a tuple (a, b) is given then each image will be rotated\n                by a value sampled from the range [a, b].\n        scale: How much to scale an image (in percentage). If a single value is given then all images will be scaled\n                by a value drawn from the range [1.0, n]. If a tuple (a,b) is given then each image will be scaled\n                based on a value drawn from the range [a,b].\n        shear: How much to shear an image (in degrees). If a single value is given then all images will be sheared\n                on X and Y by two values sampled from the range [-n, n]. If a tuple (a, b) is given then images will\n                be sheared on X and Y by two values randomly sampled from the range [a, b].\n        translate: How much to translate an image. If a single value is given then the translation extent will be\n                sampled from the range [0,n]. If a tuple (a,b) is given then the extent will be sampled from\n                the range [a,b]. If integers are given then the translation will be in pixels. If a float then\n                it will be as a fraction of the image size.\n        border_handling: What to do in order to fill newly created pixels. Options are 'constant', 'edge',\n                'symmetric', 'reflect', and 'wrap'. If a list is given, then the method will be randomly\n                selected from the options in the list.\n        fill_value: What pixel value to insert when border_handling is 'constant'.\n        interpolation: What interpolation method to use. Options (from fast to slow) are 'nearest_neighbor',\n                'bilinear', 'bicubic', 'biquartic', and 'biquintic'.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nrotate: Union[Number, Tuple[Number, Number]] = 0,\nscale: Union[float, Tuple[float, float]] = 1.0,\nshear: Union[Number, Tuple[Number, Number]] = 0,\ntranslate: Union[Number, Tuple[Number, Number]] = 0,\nborder_handling: Union[str, List[str]] = \"reflect\",\nfill_value: Number = 0,\ninterpolation: str = \"bilinear\",\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\norder = {'nearest_neighbor': 0, 'bilinear': 1, 'bicubic': 3, 'biquartic': 4, 'biquintic': 5}[interpolation]\nif isinstance(translate, int) or (isinstance(translate, Tuple) and isinstance(translate[0], int)):\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_px=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nelse:\nfunc = IAAAffine(rotate=rotate,\nscale=scale,\nshear=shear,\ntranslate_percent=translate,\norder=order,\ncval=fill_value,\nmode=border_handling,\nalways_apply=True)\nsuper().__init__(func,\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html", "title": "center_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/center_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.center_crop.CenterCrop", "title": "<code>CenterCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop the center of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32 (but uint8 is more efficient)</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\center_crop.py</code> <pre><code>class CenterCrop(MultiVariateAlbumentation):\n\"\"\"Crop the center of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32 (but uint8 is more efficient)\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CenterCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop.html", "title": "crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop.html#fastestimator.fastestimator.op.numpyop.multivariate.crop.Crop", "title": "<code>Crop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a region from the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>x_min</code> <code>int</code> <p>Minimum upper left x coordinate.</p> <code>0</code> <code>y_min</code> <code>int</code> <p>Minimum upper left y coordinate.</p> <code>0</code> <code>x_max</code> <code>int</code> <p>Maximum lower right x coordinate.</p> <code>1024</code> <code>y_max</code> <code>int</code> <p>Maximum lower right y coordinate.</p> <code>1024</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop.py</code> <pre><code>class Crop(MultiVariateAlbumentation):\n\"\"\"Crop a region from the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        x_min: Minimum upper left x coordinate.\n        y_min: Minimum upper left y coordinate.\n        x_max: Maximum lower right x coordinate.\n        y_max: Maximum lower right y coordinate.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nx_min: int = 0,\ny_min: int = 0,\nx_max: int = 1024,\ny_max: int = 1024,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(CropAlb(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html", "title": "crop_non_empty_mask_if_exists", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/crop_non_empty_mask_if_exists.html#fastestimator.fastestimator.op.numpyop.multivariate.crop_non_empty_mask_if_exists.CropNonEmptyMaskIfExists", "title": "<code>CropNonEmptyMaskIfExists</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop an area with mask if mask is non-empty, otherwise crop randomly.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Vertical size of crop in pixels.</p> required <code>width</code> <code>int</code> <p>Horizontal size of crop in pixels.</p> required <code>ignore_values</code> <code>Optional[List[int]]</code> <p>Values to ignore in mask, <code>0</code> values are always ignored (e.g. if background value is 5 set  <code>ignore_values=[5]</code> to ignore).</p> <code>None</code> <code>ignore_channels</code> <code>Optional[List[int]]</code> <p>Channels to ignore in mask (e.g. if background is a first channel set <code>ignore_channels=[0]</code> to ignore).</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\crop_non_empty_mask_if_exists.py</code> <pre><code>class CropNonEmptyMaskIfExists(MultiVariateAlbumentation):\n\"\"\"Crop an area with mask if mask is non-empty, otherwise crop randomly.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Vertical size of crop in pixels.\n        width: Horizontal size of crop in pixels.\n        ignore_values: Values to ignore in mask, `0` values are always ignored (e.g. if background value is 5 set \n            `ignore_values=[5]` to ignore).\n        ignore_channels: Channels to ignore in mask (e.g. if background is a first channel set `ignore_channels=[0]` to\n            ignore).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nignore_values: Optional[List[int]] = None,\nignore_channels: Optional[List[int]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nCropNonEmptyMaskIfExistsAlb(height=height,\nwidth=width,\nignore_values=ignore_values,\nignore_channels=ignore_channels,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html", "title": "elastic_transform", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/elastic_transform.html#fastestimator.fastestimator.op.numpyop.multivariate.elastic_transform.ElasticTransform", "title": "<code>ElasticTransform</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Elastic deformation of images.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Scaling factor during point translation.</p> <code>34.0</code> <code>sigma</code> <code>float</code> <p>Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.</p> <code>4.0</code> <code>alpha_affine</code> <code>float</code> <p>The range will be (-alpha_affine, alpha_affine).</p> <code>50.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> <code>approximate</code> <code>bool</code> <p>Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X speedup on large (512x512) images.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\elastic_transform.py</code> <pre><code>class ElasticTransform(MultiVariateAlbumentation):\n\"\"\"Elastic deformation of images.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        alpha: Scaling factor during point translation.\n        sigma: Gaussian filter parameter. The effect (small to large) is: random -&gt; elastic -&gt; affine -&gt; translation.\n        alpha_affine: The range will be (-alpha_affine, alpha_affine).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n        approximate: Whether to smooth displacement map with fixed kernel size. Enabling this option gives ~2X\n            speedup on large (512x512) images.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nalpha: float = 34.0,\nsigma: float = 4.0,\nalpha_affine: float = 50.0,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\napproximate: bool = False,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nElasticTransformAlb(alpha=alpha,\nsigma=sigma,\nalpha_affine=alpha_affine,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\napproximate=approximate,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/flip.html", "title": "flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/flip.html#fastestimator.fastestimator.op.numpyop.multivariate.flip.Flip", "title": "<code>Flip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image either horizontally, vertically, or both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\flip.py</code> <pre><code>class Flip(MultiVariateAlbumentation):\n\"\"\"Flip an image either horizontally, vertically, or both.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html", "title": "grid_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/grid_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.grid_distortion.GridDistortion", "title": "<code>GridDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Distort an image within a grid sub-division</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>num_steps</code> <code>int</code> <p>count of grid cells on each side.</p> <code>5</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.3</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\grid_distortion.py</code> <pre><code>class GridDistortion(MultiVariateAlbumentation):\n\"\"\"Distort an image within a grid sub-division\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        num_steps: count of grid cells on each side.\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nnum_steps: int = 5,\ndistort_limit: Union[float, Tuple[float, float]] = 0.3,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nGridDistortionAlb(num_steps=num_steps,\ndistort_limit=distort_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html", "title": "horizontal_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/horizontal_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.horizontal_flip.HorizontalFlip", "title": "<code>HorizontalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image horizontally.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\horizontal_flip.py</code> <pre><code>class HorizontalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image horizontally.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(FlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html", "title": "longest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/longest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.longest_max_size.LongestMaxSize", "title": "<code>LongestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\longest_max_size.py</code> <pre><code>class LongestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that maximum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(LongestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html", "title": "mask_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/mask_dropout.html#fastestimator.fastestimator.op.numpyop.multivariate.mask_dropout.MaskDropout", "title": "<code>MaskDropout</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Zero out objects from an image + mask pair.</p> <p>An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance from mask. The mask must be single-channel image, with zero values treated as background. The image can be any number of channels.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).     image_out: The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>max_objects</code> <code>Union[int, Tuple[int, int]]</code> <p>Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]</p> <code>1</code> <code>image_fill_value</code> <code>Union[int, float, str]</code> <p>Fill value to use when filling image. Can be 'inpaint' to apply in-painting (works only  for 3-channel images)</p> <code>0</code> <code>mask_fill_value</code> <code>Union[int, float]</code> <p>Fill value to use when filling mask.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\mask_dropout.py</code> <pre><code>class MaskDropout(MultiVariateAlbumentation):\n\"\"\"Zero out objects from an image + mask pair.\n    An image &amp; mask augmentation that zero out mask and image regions corresponding to randomly chosen object instance\n    from mask. The mask must be single-channel image, with zero values treated as background. The image can be any\n    number of channels.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n                image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        max_objects: Maximum number of labels that can be zeroed out. Can be tuple, in this case it's [min, max]\n        image_fill_value: Fill value to use when filling image.\n            Can be 'inpaint' to apply in-painting (works only  for 3-channel images)\n        mask_fill_value: Fill value to use when filling mask.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_objects: Union[int, Tuple[int, int]] = 1,\nimage_fill_value: Union[int, float, str] = 0,\nmask_fill_value: Union[int, float] = 0,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nMaskDropoutAlb(max_objects=max_objects,\nimage_fill_value=image_fill_value,\nmask_fill_value=mask_fill_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html", "title": "multivariate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/multivariate.html#fastestimator.fastestimator.op.numpyop.multivariate.multivariate.MultiVariateAlbumentation", "title": "<code>MultiVariateAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A base class for the DualTransform albumentation functions.</p> <p>DualTransforms are functions which apply simultaneously to images and corresponding information such as masks  and/or bounding boxes.</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>DualTransform</code> <p>An Albumentation function to be invoked.</p> required <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If none of the various inputs such as <code>image_in</code> or <code>mask_in</code> are provided.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\multivariate.py</code> <pre><code>class MultiVariateAlbumentation(NumpyOp):\n\"\"\"A base class for the DualTransform albumentation functions.\n     DualTransforms are functions which apply simultaneously to images and corresponding information such as masks\n     and/or bounding boxes.\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Raises:\n        AssertionError: If none of the various inputs such as `image_in` or `mask_in` are provided.\n    \"\"\"\ndef __init__(self,\nfunc: DualTransform,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nassert any((image_in, mask_in, masks_in, bbox_in, keypoints_in)), \"At least one input must be non-None\"\nimage_out = image_out or image_in\nmask_out = mask_out or mask_in\nmasks_out = masks_out or masks_in\nbbox_out = bbox_out or bbox_in\nkeypoints_out = keypoints_out or keypoints_in\nkeys = OrderedDict([(\"image\", image_in), (\"mask\", mask_in), (\"masks\", masks_in), (\"bboxes\", bbox_in),\n(\"keypoints\", keypoints_in)])\nself.keys_in = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nkeys = OrderedDict([(\"image\", image_out), (\"mask\", mask_out), (\"masks\", masks_out), (\"bboxes\", bbox_out),\n(\"keypoints\", keypoints_out)])\nself.keys_out = OrderedDict([(k, v) for k, v in keys.items() if v is not None])\nsuper().__init__(inputs=list(self.keys_in.values()), outputs=list(self.keys_out.values()), mode=mode)\nif isinstance(bbox_params, str):\nbbox_params = BboxParams(bbox_params)\nif isinstance(keypoint_params, str):\nkeypoint_params = KeypointParams(keypoint_params)\nself.func = Compose(transforms=[func], bbox_params=bbox_params, keypoint_params=keypoint_params)\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresult = self.func(**{k: v for k, v in zip(self.keys_in.keys(), data)})\nreturn [result[k] for k in self.keys_out.keys()]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html", "title": "optical_distortion", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/optical_distortion.html#fastestimator.fastestimator.op.numpyop.multivariate.optical_distortion.OpticalDistortion", "title": "<code>OpticalDistortion</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Apply optical distortion to an image / mask.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>distort_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If distort_limit is a single float, the range will be (-distort_limit, distort_limit).</p> <code>0.05</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If shift_limit is a single float, the range will be (-shift_limit, shift_limit). </p> <code>0.05</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\optical_distortion.py</code> <pre><code>class OpticalDistortion(MultiVariateAlbumentation):\n\"\"\"Apply optical distortion to an image / mask.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        distort_limit: If distort_limit is a single float, the range will be (-distort_limit, distort_limit).\n        shift_limit: If shift_limit is a single float, the range will be (-shift_limit, shift_limit). \n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ndistort_limit: Union[float, Tuple[float, float]] = 0.05,\nshift_limit: Union[float, Tuple[float, float]] = 0.05,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(\nOpticalDistortionAlb(distort_limit=distort_limit,\nshift_limit=shift_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html", "title": "pad_if_needed", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/pad_if_needed.html#fastestimator.fastestimator.op.numpyop.multivariate.pad_if_needed.PadIfNeeded", "title": "<code>PadIfNeeded</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Pad the sides of an image / mask if size is less than a desired number.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_height</code> <code>int</code> <p>Minimal result image height.</p> <code>1024</code> <code>min_width</code> <code>int</code> <p>Minimal result image width.</p> <code>1024</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>cv2.BORDER_REFLECT_101</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value for mask if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\pad_if_needed.py</code> <pre><code>class PadIfNeeded(MultiVariateAlbumentation):\n\"\"\"Pad the sides of an image / mask if size is less than a desired number.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_height: Minimal result image height.\n        min_width: Minimal result image width.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value for mask if border_mode is cv2.BORDER_CONSTANT.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_height: int = 1024,\nmin_width: int = 1024,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nPadIfNeededAlb(min_height=min_height,\nmin_width=min_width,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html", "title": "random_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop.RandomCrop", "title": "<code>RandomCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height of the crop.</p> required <code>width</code> <code>int</code> <p>Width of the crop.</p> required Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop.py</code> <pre><code>class RandomCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height of the crop.\n        width: Width of the crop.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropAlb(height=height, width=width, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html", "title": "random_crop_near_bbox", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_crop_near_bbox.html#fastestimator.fastestimator.op.numpyop.multivariate.random_crop_near_bbox.RandomCropNearBBox", "title": "<code>RandomCropNearBBox</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop bbox from an image with random shift by x,y coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_part_shift</code> <code>float</code> <p>Float value in the range (0.0, 1.0).</p> <code>0.3</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_crop_near_bbox.py</code> <pre><code>class RandomCropNearBBox(MultiVariateAlbumentation):\n\"\"\"Crop bbox from an image with random shift by x,y coordinates.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_part_shift: Float value in the range (0.0, 1.0).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_part_shift: float = 0.3,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomCropNearBBoxAlb(max_part_shift=max_part_shift, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html", "title": "random_grid_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_grid_shuffle.html#fastestimator.fastestimator.op.numpyop.multivariate.random_grid_shuffle.RandomGridShuffle", "title": "<code>RandomGridShuffle</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Divide an image into a grid and randomly shuffle the grid's cells.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>grid</code> <code>Tuple[int, int]</code> <p>size of grid for splitting image (height, width).</p> <code>(3, 3)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_grid_shuffle.py</code> <pre><code>class RandomGridShuffle(MultiVariateAlbumentation):\n\"\"\"Divide an image into a grid and randomly shuffle the grid's cells.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        grid: size of grid for splitting image (height, width).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ngrid: Tuple[int, int] = (3, 3),\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None):\nsuper().__init__(RandomGridShuffleAlb(grid=grid, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=None,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=None,\nkeypoints_out=None,\nbbox_params=None,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html", "title": "random_resized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_resized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_resized_crop.RandomResizedCrop", "title": "<code>RandomResizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>scale</code> <code>Tuple[float, float]</code> <p>Range of size of the origin size cropped.</p> <code>(0.08, 1.0)</code> <code>ratio</code> <code>Tuple[float, float]</code> <p>Range of aspect ratio of the origin aspect ratio cropped.</p> <code>(0.75, 4 / 3)</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_resized_crop.py</code> <pre><code>class RandomResizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        scale: Range of size of the origin size cropped.\n        ratio: Range of aspect ratio of the origin aspect ratio cropped.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nscale: Tuple[float, float] = (0.08, 1.0),\nratio: Tuple[float, float] = (0.75, 4 / 3),\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomResizedCropAlb(height=height,\nwidth=width,\nscale=scale,\nratio=ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html", "title": "random_rotate_90", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_rotate_90.html#fastestimator.fastestimator.op.numpyop.multivariate.random_rotate_90.RandomRotate90", "title": "<code>RandomRotate90</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate a given input randomly by 90 degrees.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_rotate_90.py</code> <pre><code>class RandomRotate90(MultiVariateAlbumentation):\n\"\"\"Rotate a given input randomly by 90 degrees.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RotateAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html", "title": "random_scale", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_scale.html#fastestimator.fastestimator.op.numpyop.multivariate.random_scale.RandomScale", "title": "<code>RandomScale</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly resize the input. Output image size is different from the input image size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit).</p> <code>0.1</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_scale.py</code> <pre><code>class RandomScale(MultiVariateAlbumentation):\n\"\"\"Randomly resize the input. Output image size is different from the input image size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (1 - scale_limit, 1 + scale_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(RandomScaleAlb(scale_limit=scale_limit, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html", "title": "random_sized_bbox_safe_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_bbox_safe_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_bbox_safe_crop.RandomSizedBBoxSafeCrop", "title": "<code>RandomSizedBBoxSafeCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size without loss of bboxes.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>erosion_rate</code> <code>float</code> <p>Erosion rate applied on input image height before crop.</p> <code>0.0</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_bbox_safe_crop.py</code> <pre><code>class RandomSizedBBoxSafeCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size without loss of bboxes.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        erosion_rate: Erosion rate applied on input image height before crop.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\nerosion_rate: float = 0.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None):\nsuper().__init__(\nRandomSizedBBoxSafeCropAlb(height=height,\nwidth=width,\nerosion_rate=erosion_rate,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=None,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=None,\nbbox_params=bbox_params,\nkeypoint_params=None,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html", "title": "random_sized_crop", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/random_sized_crop.html#fastestimator.fastestimator.op.numpyop.multivariate.random_sized_crop.RandomSizedCrop", "title": "<code>RandomSizedCrop</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Crop a random part of the input and rescale it to some size.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>min_max_height</code> <code>Tuple[int, int]</code> <p>Crop size limits.</p> required <code>height</code> <code>int</code> <p>Height after crop and resize.</p> required <code>width</code> <code>int</code> <p>Width after crop and resize.</p> required <code>w2h_ratio</code> <code>float</code> <p>Aspect ratio of crop.</p> <code>1.0</code> <code>interpolation</code> <code>int</code> <p>flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\random_sized_crop.py</code> <pre><code>class RandomSizedCrop(MultiVariateAlbumentation):\n\"\"\"Crop a random part of the input and rescale it to some size.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        min_max_height: Crop size limits.\n        height: Height after crop and resize.\n        width: Width after crop and resize.\n        w2h_ratio: Aspect ratio of crop.\n        interpolation: flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmin_max_height: Tuple[int, int],\nheight: int,\nwidth: int,\nw2h_ratio: float = 1.0,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRandomSizedCropAlb(min_max_height=min_max_height,\nheight=height,\nwidth=width,\nw2h_ratio=w2h_ratio,\ninterpolation=interpolation,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html", "title": "read_mat", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/read_mat.html#fastestimator.fastestimator.op.numpyop.multivariate.read_mat.ReadMat", "title": "<code>ReadMat</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading .mat files from disk.</p> <p>This expects every sample to have a separate .mat file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Dictionary key that contains the .mat path.</p> required <code>keys</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) to read from the .mat file.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given filepath.</p> <code>''</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\read_mat.py</code> <pre><code>class ReadMat(NumpyOp):\n\"\"\"A class for reading .mat files from disk.\n    This expects every sample to have a separate .mat file.\n    Args:\n        file: Dictionary key that contains the .mat path.\n        keys: Key(s) to read from the .mat file.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        parent_path: Parent path that will be prepended to a given filepath.\n    \"\"\"\ndef __init__(self,\nfile: str,\nkeys: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\"):\nsuper().__init__(inputs=file, outputs=keys, mode=mode)\nself.parent_path = parent_path\nself.out_list = True\ndef forward(self, data: str, state: Dict[str, Any]) -&gt; List[Dict[str, Any]]:\ndata = loadmat(os.path.normpath(os.path.join(self.parent_path, data)))\nresults = [data[key] for key in self.outputs]\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/resize.html", "title": "resize", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/resize.html#fastestimator.fastestimator.op.numpyop.multivariate.resize.Resize", "title": "<code>Resize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Resize the input to the given height and width.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>height</code> <code>int</code> <p>Desired height of the output.</p> required <code>width</code> <code>int</code> <p>Desired width of the output.</p> required <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\resize.py</code> <pre><code>class Resize(MultiVariateAlbumentation):\n\"\"\"Resize the input to the given height and width.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        height: Desired height of the output.\n        width: Desired width of the output.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nheight: int,\nwidth: int,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(ResizeAlb(height=height, width=width, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html", "title": "rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.rotate.Rotate", "title": "<code>Rotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rotate the input by an angle selected randomly from the uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range from which a random angle is picked. If limit is a single int an angle is picked from (-limit, limit).</p> <code>90</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\rotate.py</code> <pre><code>class Rotate(MultiVariateAlbumentation):\n\"\"\"Rotate the input by an angle selected randomly from the uniform distribution.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        limit: Range from which a random angle is picked. If limit is a single int an angle is picked from\n            (-limit, limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nlimit: Union[int, Tuple[int, int]] = 90,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nRotateAlb(limit=limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html", "title": "shift_scale_rotate", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/shift_scale_rotate.html#fastestimator.fastestimator.op.numpyop.multivariate.shift_scale_rotate.ShiftScaleRotate", "title": "<code>ShiftScaleRotate</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Randomly apply affine transforms: translate, scale and rotate the input.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>shift_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Shift factor range for both height and width. If shift_limit is a single float value, the range will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].</p> <code>0.0625</code> <code>scale_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Scaling factor range. If scale_limit is a single float value, the range will be (-scale_limit, scale_limit).</p> <code>0.1</code> <code>rotate_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Rotation range. If rotate_limit is a single int value, the range will be (-rotate_limit, rotate_limit).</p> <code>45</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> <code>border_mode</code> <code>int</code> <p>Flag that is used to specify the pixel extrapolation method. Should be one of: cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.</p> <code>cv2.BORDER_REFLECT_101</code> <code>value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT.</p> <code>None</code> <code>mask_value</code> <code>Union[None, int, float, List[int], List[float]]</code> <p>Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\shift_scale_rotate.py</code> <pre><code>class ShiftScaleRotate(MultiVariateAlbumentation):\n\"\"\"Randomly apply affine transforms: translate, scale and rotate the input.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        shift_limit: Shift factor range for both height and width. If shift_limit is a single float value, the range\n            will be (-shift_limit, shift_limit). Absolute values for lower and upper bounds should lie in range [0, 1].\n        scale_limit: Scaling factor range. If scale_limit is a single float value, the range will be\n            (-scale_limit, scale_limit).\n        rotate_limit: Rotation range. If rotate_limit is a single int value, the range will be\n            (-rotate_limit, rotate_limit).\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n        border_mode: Flag that is used to specify the pixel extrapolation method. Should be one of:\n            cv2.BORDER_CONSTANT, cv2.BORDER_REPLICATE, cv2.BORDER_REFLECT, cv2.BORDER_WRAP, cv2.BORDER_REFLECT_101.\n        value: Padding value if border_mode is cv2.BORDER_CONSTANT.\n        mask_value: Padding value if border_mode is cv2.BORDER_CONSTANT applied for masks.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nshift_limit: Union[float, Tuple[float, float]] = 0.0625,\nscale_limit: Union[float, Tuple[float, float]] = 0.1,\nrotate_limit: Union[int, Tuple[int, int]] = 45,\ninterpolation: int = cv2.INTER_LINEAR,\nborder_mode: int = cv2.BORDER_REFLECT_101,\nvalue: Union[None, int, float, List[int], List[float]] = None,\nmask_value: Union[None, int, float, List[int], List[float]] = None,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(\nShiftScaleRotateAlb(shift_limit=shift_limit,\nscale_limit=scale_limit,\nrotate_limit=rotate_limit,\ninterpolation=interpolation,\nborder_mode=border_mode,\nvalue=value,\nmask_value=mask_value,\nalways_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html", "title": "smallest_max_size", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/smallest_max_size.html#fastestimator.fastestimator.op.numpyop.multivariate.smallest_max_size.SmallestMaxSize", "title": "<code>SmallestMaxSize</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> <code>max_size</code> <code>int</code> <p>Maximum size of smallest side of the image after the transformation.</p> <code>1024</code> <code>interpolation</code> <code>int</code> <p>Flag that is used to specify the interpolation algorithm. Should be one of: cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.</p> <code>cv2.INTER_LINEAR</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\smallest_max_size.py</code> <pre><code>class SmallestMaxSize(MultiVariateAlbumentation):\n\"\"\"Rescale an image so that minimum side is equal to max_size, keeping the aspect ratio of the initial image.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n        max_size: Maximum size of smallest side of the image after the transformation.\n        interpolation: Flag that is used to specify the interpolation algorithm. Should be one of:\n            cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS4.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmax_size: int = 1024,\ninterpolation: int = cv2.INTER_LINEAR,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(SmallestMaxSizeAlb(max_size=max_size, interpolation=interpolation, always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html", "title": "transpose", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/transpose.html#fastestimator.fastestimator.op.numpyop.multivariate.transpose.Transpose", "title": "<code>Transpose</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Transpose the input by swapping rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\transpose.py</code> <pre><code>class Transpose(MultiVariateAlbumentation):\n\"\"\"Transpose the input by swapping rows and columns.\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(TransposeAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html", "title": "vertical_flip", "text": ""}, {"location": "fastestimator/op/numpyop/multivariate/vertical_flip.html#fastestimator.fastestimator.op.numpyop.multivariate.vertical_flip.VerticalFlip", "title": "<code>VerticalFlip</code>", "text": "<p>         Bases: <code>MultiVariateAlbumentation</code></p> <p>Flip an image vertically (over x-axis).</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[str]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>image_in</code> <code>Optional[str]</code> <p>The key of an image to be modified.</p> <code>None</code> <code>mask_in</code> <code>Optional[str]</code> <p>The key of a mask to be modified (with the same random factors as the image).</p> <code>None</code> <code>masks_in</code> <code>Optional[str]</code> <p>The key of masks to be modified (with the same random factors as the image).</p> <code>None</code> <code>bbox_in</code> <code>Optional[str]</code> <p>The key of a bounding box(es) to be modified (with the same random factors as the image).</p> <code>None</code> <code>keypoints_in</code> <code>Optional[str]</code> <p>The key of keypoints to be modified (with the same random factors as the image).</p> <code>None</code> <code>image_out</code> <code>Optional[str]</code> <p>The key to write the modified image (defaults to <code>image_in</code> if None).</p> <code>None</code> <code>mask_out</code> <code>Optional[str]</code> <p>The key to write the modified mask (defaults to <code>mask_in</code> if None).</p> <code>None</code> <code>masks_out</code> <code>Optional[str]</code> <p>The key to write the modified masks (defaults to <code>masks_in</code> if None).</p> <code>None</code> <code>bbox_out</code> <code>Optional[str]</code> <p>The key to write the modified bounding box(es) (defaults to <code>bbox_in</code> if None).</p> <code>None</code> <code>keypoints_out</code> <code>Optional[str]</code> <p>The key to write the modified keypoints (defaults to <code>keypoints_in</code> if None).</p> <code>None</code> <code>bbox_params</code> <code>Union[BboxParams, str, None]</code> <p>Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').</p> <code>None</code> <code>keypoint_params</code> <code>Union[KeypointParams, str, None]</code> <p>Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\multivariate\\vertical_flip.py</code> <pre><code>class VerticalFlip(MultiVariateAlbumentation):\n\"\"\"Flip an image vertically (over x-axis).\n    Args:\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        image_in: The key of an image to be modified.\n        mask_in: The key of a mask to be modified (with the same random factors as the image).\n        masks_in: The key of masks to be modified (with the same random factors as the image).\n        bbox_in: The key of a bounding box(es) to be modified (with the same random factors as the image).\n        keypoints_in: The key of keypoints to be modified (with the same random factors as the image).\n        image_out: The key to write the modified image (defaults to `image_in` if None).\n        mask_out: The key to write the modified mask (defaults to `mask_in` if None).\n        masks_out: The key to write the modified masks (defaults to `masks_in` if None).\n        bbox_out: The key to write the modified bounding box(es) (defaults to `bbox_in` if None).\n        keypoints_out: The key to write the modified keypoints (defaults to `keypoints_in` if None).\n        bbox_params: Parameters defining the type of bounding box ('coco', 'pascal_voc', 'albumentations' or 'yolo').\n        keypoint_params: Parameters defining the type of keypoints ('xy', 'yx', 'xya', 'xys', 'xyas', 'xysa').\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\nmode: Optional[str] = None,\nimage_in: Optional[str] = None,\nmask_in: Optional[str] = None,\nmasks_in: Optional[str] = None,\nbbox_in: Optional[str] = None,\nkeypoints_in: Optional[str] = None,\nimage_out: Optional[str] = None,\nmask_out: Optional[str] = None,\nmasks_out: Optional[str] = None,\nbbox_out: Optional[str] = None,\nkeypoints_out: Optional[str] = None,\nbbox_params: Union[BboxParams, str, None] = None,\nkeypoint_params: Union[KeypointParams, str, None] = None):\nsuper().__init__(VerticalFlipAlb(always_apply=True),\nimage_in=image_in,\nmask_in=mask_in,\nmasks_in=masks_in,\nbbox_in=bbox_in,\nkeypoints_in=keypoints_in,\nimage_out=image_out,\nmask_out=mask_out,\nmasks_out=masks_out,\nbbox_out=bbox_out,\nkeypoints_out=keypoints_out,\nbbox_params=bbox_params,\nkeypoint_params=keypoint_params,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/binarize.html", "title": "binarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/binarize.html#fastestimator.fastestimator.op.numpyop.univariate.binarize.Binarize", "title": "<code>Binarize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Binarization threshold.</p> required <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\binarize.py</code> <pre><code>class Binarize(NumpyOp):\n\"\"\"Binarize the input data such that all elements &gt;= threshold become 1 otherwise 0.\n    Args:\n        threshold: Binarization threshold.\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nthreshold: float,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.threshold = threshold\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [(dat &gt;= self.threshold).astype(np.float32) for dat in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/blur.html", "title": "blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/blur.html#fastestimator.fastestimator.op.numpyop.univariate.blur.Blur", "title": "<code>Blur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a randomly-sized kernel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\blur.py</code> <pre><code>class Blur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a randomly-sized kernel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(BlurAlb(blur_limit=blur_limit, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html", "title": "channel_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.channel_dropout.ChannelDropout", "title": "<code>ChannelDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly drop channels from the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>channel_drop_range</code> <code>Tuple[int, int]</code> <p>Range from which we choose the number of channels to drop.</p> <code>(1, 1)</code> <code>fill_value</code> <code>Union[int, float]</code> <p>Pixel values for the dropped channel.</p> <code>0</code> Image types <p>int8, uint16, unit32, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_dropout.py</code> <pre><code>class ChannelDropout(ImageOnlyAlbumentation):\n\"\"\"Randomly drop channels from the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        channel_drop_range: Range from which we choose the number of channels to drop.\n        fill_value: Pixel values for the dropped channel.\n    Image types:\n        int8, uint16, unit32, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nchannel_drop_range: Tuple[int, int] = (1, 1),\nfill_value: Union[int, float] = 0):\nsuper().__init__(\nChannelDropoutAlb(channel_drop_range=channel_drop_range, fill_value=fill_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html", "title": "channel_shuffle", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_shuffle.html#fastestimator.fastestimator.op.numpyop.univariate.channel_shuffle.ChannelShuffle", "title": "<code>ChannelShuffle</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly rearrange channels of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>int8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_shuffle.py</code> <pre><code>class ChannelShuffle(ImageOnlyAlbumentation):\n\"\"\"Randomly rearrange channels of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        int8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ChannelShuffleAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html", "title": "channel_transpose", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/channel_transpose.html#fastestimator.fastestimator.op.numpyop.univariate.channel_transpose.ChannelTranspose", "title": "<code>ChannelTranspose</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transpose the data (for example to make it channel-width-height instead of width-height-channel).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>axes</code> <code>List[int]</code> <p>The permutation order.</p> <code>(2, 0, 1)</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\channel_transpose.py</code> <pre><code>class ChannelTranspose(NumpyOp):\n\"\"\"Transpose the data (for example to make it channel-width-height instead of width-height-channel).\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        axes: The permutation order.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\naxes: List[int] = (2, 0, 1)):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axes = axes\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.transpose(elem, self.axes) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/clahe.html", "title": "clahe", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/clahe.html#fastestimator.fastestimator.op.numpyop.univariate.clahe.CLAHE", "title": "<code>CLAHE</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply contrast limited adaptive histogram equalization to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>clip_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit).</p> <code>4.0</code> <code>tile_grid_size</code> <code>Tuple[int, int]</code> <p>size of grid for histogram equalization.</p> <code>(8, 8)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\clahe.py</code> <pre><code>class CLAHE(ImageOnlyAlbumentation):\n\"\"\"Apply contrast limited adaptive histogram equalization to the image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        clip_limit: upper threshold value for contrast limiting. If clip_limit is a single float value, the range will\n            be (1, clip_limit).\n        tile_grid_size: size of grid for histogram equalization.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nclip_limit: Union[float, Tuple[float, float]] = 4.0,\ntile_grid_size: Tuple[int, int] = (8, 8)):\nsuper().__init__(CLAHEAlb(clip_limit=clip_limit, tile_grid_size=tile_grid_size, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html", "title": "coarse_dropout", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/coarse_dropout.html#fastestimator.fastestimator.op.numpyop.univariate.coarse_dropout.CoarseDropout", "title": "<code>CoarseDropout</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Drop rectangular regions from an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_holes</code> <code>int</code> <p>Maximum number of regions to zero out.</p> <code>8</code> <code>max_height</code> <code>int</code> <p>Maximum height of the hole.</p> <code>8</code> <code>max_width</code> <code>int</code> <p>Maximum width of the hole.</p> <code>8</code> <code>min_holes</code> <code>Optional[int]</code> <p>Minimum number of regions to zero out. If <code>None</code>, <code>min_holes</code> is set to <code>max_holes</code>.</p> <code>None</code> <code>min_height</code> <code>Optional[int]</code> <p>Minimum height of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_height</code>.</p> <code>None</code> <code>min_width</code> <code>Optional[int]</code> <p>Minimum width of the hole. If <code>None</code>, <code>min_height</code> is set to <code>max_width</code>.</p> <code>None</code> <code>fill_value</code> <code>Union[int, float, List[int], List[float]]</code> <p>value for dropped pixels.</p> <code>0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\coarse_dropout.py</code> <pre><code>class CoarseDropout(ImageOnlyAlbumentation):\n\"\"\"Drop rectangular regions from an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_holes: Maximum number of regions to zero out.\n        max_height: Maximum height of the hole.\n        max_width: Maximum width of the hole.\n        min_holes: Minimum number of regions to zero out. If `None`, `min_holes` is set to `max_holes`.\n        min_height: Minimum height of the hole. If `None`, `min_height` is set to `max_height`.\n        min_width: Minimum width of the hole. If `None`, `min_height` is set to `max_width`.\n        fill_value: value for dropped pixels.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_holes: int = 8,\nmax_height: int = 8,\nmax_width: int = 8,\nmin_holes: Optional[int] = None,\nmin_height: Optional[int] = None,\nmin_width: Optional[int] = None,\nfill_value: Union[int, float, List[int], List[float]] = 0):\nsuper().__init__(\nCoarseDropoutAlb(max_holes=max_holes,\nmax_height=max_height,\nmax_width=max_width,\nmin_holes=min_holes,\nmin_height=min_height,\nmin_width=min_width,\nfill_value=fill_value,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/downscale.html", "title": "downscale", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/downscale.html#fastestimator.fastestimator.op.numpyop.univariate.downscale.Downscale", "title": "<code>Downscale</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease image quality by downscaling and then upscaling.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>scale_min</code> <code>float</code> <p>Lower bound on the image scale. Should be &lt; 1.</p> <code>0.25</code> <code>scale_max</code> <code>float</code> <p>Upper bound on the image scale. Should be &gt;= scale_min.</p> <code>0.25</code> <code>interpolation</code> <code>int</code> <p>cv2 interpolation method.</p> <code>cv2.INTER_NEAREST</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\downscale.py</code> <pre><code>class Downscale(ImageOnlyAlbumentation):\n\"\"\"Decrease image quality by downscaling and then upscaling.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        scale_min: Lower bound on the image scale. Should be &lt; 1.\n        scale_max:  Upper bound on the image scale. Should be &gt;= scale_min.\n        interpolation: cv2 interpolation method.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nscale_min: float = 0.25,\nscale_max: float = 0.25,\ninterpolation: int = cv2.INTER_NEAREST):\nsuper().__init__(\nDownscaleAlb(scale_min=scale_min, scale_max=scale_max, interpolation=interpolation, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/equalize.html", "title": "equalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/equalize.html#fastestimator.fastestimator.op.numpyop.univariate.equalize.Equalize", "title": "<code>Equalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Equalize the image histogram.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>eq_mode</code> <code>str</code> <p>{'cv', 'pil'}. Use OpenCV or Pillow equalization method.</p> <code>'cv'</code> <code>by_channels</code> <code>bool</code> <p>If True, use equalization by channels separately, else convert image to YCbCr representation and use equalization by <code>Y</code> channel.</p> <code>True</code> <code>mask</code> <code>Union[None, np.ndarray, Callable]</code> <p>If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel array or callable. Function signature must include <code>image</code> argument.</p> <code>None</code> <code>mask_params</code> <code>List[str]</code> <p>Params for mask function.</p> <code>()</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\equalize.py</code> <pre><code>class Equalize(ImageOnlyAlbumentation):\n\"\"\"Equalize the image histogram.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        eq_mode: {'cv', 'pil'}. Use OpenCV or Pillow equalization method.\n        by_channels: If True, use equalization by channels separately, else convert image to YCbCr representation and\n            use equalization by `Y` channel.\n        mask: If given, only the pixels selected by the mask are included in the analysis. May be 1 channel or 3 channel\n            array or callable. Function signature must include `image` argument.\n        mask_params: Params for mask function.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\neq_mode: str = \"cv\",\nby_channels: bool = True,\nmask: Union[None, np.ndarray, Callable] = None,\nmask_params: List[str] = ()):\nsuper().__init__(\nEqualizeAlb(mode=eq_mode, by_channels=by_channels, mask=mask, mask_params=mask_params, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html", "title": "expand_dims", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/expand_dims.html#fastestimator.fastestimator.op.numpyop.univariate.expand_dims.ExpandDims", "title": "<code>ExpandDims</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transpose the data (for example to make it channel-width-height instead of width-height-channel)</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of inputs to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified inputs.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>axis</code> <code>int</code> <p>The axis to expand.</p> <code>-1</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\expand_dims.py</code> <pre><code>class ExpandDims(NumpyOp):\n\"\"\"Transpose the data (for example to make it channel-width-height instead of width-height-channel)\n    Args:\n        inputs: Key(s) of inputs to be modified.\n        outputs: Key(s) into which to write the modified inputs.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        axis: The axis to expand.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\naxis: int = -1):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [np.expand_dims(elem, self.axis) for elem in data]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/from_float.html", "title": "from_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/from_float.html#fastestimator.fastestimator.op.numpyop.univariate.from_float.FromFloat", "title": "<code>FromFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Takes an input float image in range [0, 1.0] and then multiplies by <code>max_value</code> to get an int image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the multiplier. If None it will be inferred by dtype.</p> <code>None</code> <code>dtype</code> <code>Union[str, np.dtype]</code> <p>The data type to cast the output as.</p> <code>'uint16'</code> Image types <p>float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\from_float.py</code> <pre><code>class FromFloat(ImageOnlyAlbumentation):\n\"\"\"Takes an input float image in range [0, 1.0] and then multiplies by `max_value` to get an int image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_value: The maximum value to serve as the multiplier. If None it will be inferred by dtype.\n        dtype: The data type to cast the output as.\n    Image types:\n        float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None,\ndtype: Union[str, np.dtype] = \"uint16\"):\nsuper().__init__(FromFloatAlb(max_value=max_value, dtype=dtype, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html", "title": "gaussian_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_blur.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_blur.GaussianBlur", "title": "<code>GaussianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with a Gaussian filter of random kernel size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_blur.py</code> <pre><code>class GaussianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with a Gaussian filter of random kernel size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum Gaussian kernel size for blurring the input image. Should be odd and in range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(GaussianBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html", "title": "gaussian_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/gaussian_noise.html#fastestimator.fastestimator.op.numpyop.univariate.gaussian_noise.GaussianNoise", "title": "<code>GaussianNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply gaussian noise to the image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>var_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).</p> <code>(10.0, 50.0)</code> <code>mean</code> <code>float</code> <p>Mean of the noise.</p> <code>0.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\gaussian_noise.py</code> <pre><code>class GaussianNoise(ImageOnlyAlbumentation):\n\"\"\"Apply gaussian noise to the image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        var_limit: Variance range for noise. If var_limit is a single float, the range will be (0, var_limit).\n        mean: Mean of the noise.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nvar_limit: Union[float, Tuple[float, float]] = (10.0, 50.0),\nmean: float = 0.0):\nsuper().__init__(GaussNoiseAlb(var_limit=var_limit, mean=mean, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html", "title": "hue_saturation_value", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/hue_saturation_value.html#fastestimator.fastestimator.op.numpyop.univariate.hue_saturation_value.HueSaturationValue", "title": "<code>HueSaturationValue</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly modify the hue, saturation, and value of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>hue_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing hue. If hue_shift_limit is a single int, the range will be (-hue_shift_limit, hue_shift_limit).</p> <code>20</code> <code>sat_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>Range for changing saturation. If sat_shift_limit is a single int, the range will be (-sat_shift_limit, sat_shift_limit).</p> <code>30</code> <code>val_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing value. If val_shift_limit is a single int, the range will be (-val_shift_limit, val_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\hue_saturation_value.py</code> <pre><code>class HueSaturationValue(ImageOnlyAlbumentation):\n\"\"\"Randomly modify the hue, saturation, and value of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        hue_shift_limit: Range for changing hue. If hue_shift_limit is a single int, the range will be\n            (-hue_shift_limit, hue_shift_limit).\n        sat_shift_limit: Range for changing saturation. If sat_shift_limit is a single int, the range will be\n            (-sat_shift_limit, sat_shift_limit).\n        val_shift_limit: range for changing value. If val_shift_limit is a single int, the range will be\n            (-val_shift_limit, val_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nhue_shift_limit: Union[int, Tuple[int, int]] = 20,\nsat_shift_limit: Union[int, Tuple[int, int]] = 30,\nval_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nHueSaturationValueAlb(hue_shift_limit=hue_shift_limit,\nsat_shift_limit=sat_shift_limit,\nval_shift_limit=val_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html", "title": "image_compression", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/image_compression.html#fastestimator.fastestimator.op.numpyop.univariate.image_compression.ImageCompression", "title": "<code>ImageCompression</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Decrease compression of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>quality_lower</code> <code>float</code> <p>Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>99</code> <code>quality_upper</code> <code>float</code> <p>Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.</p> <code>100</code> <code>compression_type</code> <code>ImgCmpAlb.ImageCompressionType</code> <p>should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.</p> <code>ImgCmpAlb.ImageCompressionType.JPEG</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\image_compression.py</code> <pre><code>class ImageCompression(ImageOnlyAlbumentation):\n\"\"\"Decrease compression of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        quality_lower: Lower bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        quality_upper: Upper bound on the image quality. Should be in [0, 100] range for jpeg and [1, 100] for webp.\n        compression_type: should be ImageCompressionType.JPEG or ImageCompressionType.WEBP.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nquality_lower: float = 99,\nquality_upper: float = 100,\ncompression_type: ImgCmpAlb.ImageCompressionType = ImgCmpAlb.ImageCompressionType.JPEG):\nsuper().__init__(\nImgCmpAlb(quality_lower=quality_lower,\nquality_upper=quality_upper,\ncompression_type=compression_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html", "title": "invert_img", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/invert_img.html#fastestimator.fastestimator.op.numpyop.univariate.invert_img.InvertImg", "title": "<code>InvertImg</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert an image by subtracting its pixel values from 255.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>int8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\invert_img.py</code> <pre><code>class InvertImg(ImageOnlyAlbumentation):\n\"\"\"Invert an image by subtracting its pixel values from 255.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        int8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(InvertImgAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html", "title": "iso_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/iso_noise.html#fastestimator.fastestimator.op.numpyop.univariate.iso_noise.ISONoise", "title": "<code>ISONoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply camera sensor noise.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>color_shift</code> <code>Tuple[float, float]</code> <p>Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS colorspace.</p> <code>(0.01, 0.05)</code> <code>intensity</code> <code>Tuple[float, float]</code> <p>Multiplicative factor that controls the strength of color and luminace noise.</p> <code>(0.1, 0.5)</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\iso_noise.py</code> <pre><code>class ISONoise(ImageOnlyAlbumentation):\n\"\"\"Apply camera sensor noise.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        color_shift: Variance range for color hue change. Measured as a fraction of 360 degree Hue angle in the HLS\n            colorspace.\n        intensity: Multiplicative factor that controls the strength of color and luminace noise.\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ncolor_shift: Tuple[float, float] = (0.01, 0.05),\nintensity: Tuple[float, float] = (0.1, 0.5)):\nsuper().__init__(ISONoiseAlb(color_shift=color_shift, intensity=intensity, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html", "title": "median_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/median_blur.html#fastestimator.fastestimator.op.numpyop.univariate.median_blur.MedianBlur", "title": "<code>MedianBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Blur the image with median filter of random aperture size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf). If image is a float type then only 3 and 5 are valid sizes.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\median_blur.py</code> <pre><code>class MedianBlur(ImageOnlyAlbumentation):\n\"\"\"Blur the image with median filter of random aperture size.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum aperture linear size for blurring the input image. Should be odd and in range [3, inf).\n            If image is a float type then only 3 and 5 are valid sizes.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 5):\nsuper().__init__(MedianBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/minmax.html", "title": "minmax", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/minmax.html#fastestimator.fastestimator.op.numpyop.univariate.minmax.Minmax", "title": "<code>Minmax</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Normalize data using the minmax method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>A small value to prevent numeric instability in the division.</p> <code>1e-07</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\minmax.py</code> <pre><code>class Minmax(NumpyOp):\n\"\"\"Normalize data using the minmax method.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        epsilon: A small value to prevent numeric instability in the division.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nepsilon: float = 1e-7):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.epsilon = epsilon\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_minmax(elem) for elem in data]\ndef _apply_minmax(self, data: np.ndarray) -&gt; np.ndarray:\ndata_max = np.max(data)\ndata_min = np.min(data)\ndata = (data - data_min) / max((data_max - data_min), self.epsilon)\nreturn data.astype(np.float32)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html", "title": "motion_blur", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/motion_blur.html#fastestimator.fastestimator.op.numpyop.univariate.motion_blur.MotionBlur", "title": "<code>MotionBlur</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Motion Blur the image with a randomly-sized kernel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>blur_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>maximum kernel size for blurring the input image. Should be in the range [3, inf).</p> <code>7</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\motion_blur.py</code> <pre><code>class MotionBlur(ImageOnlyAlbumentation):\n\"\"\"Motion Blur the image with a randomly-sized kernel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        blur_limit: maximum kernel size for blurring the input image. Should be in the range [3, inf).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nblur_limit: Union[int, Tuple[int, int]] = 7):\nsuper().__init__(MotionBlurAlb(blur_limit=blur_limit, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html", "title": "multiplicative_noise", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/multiplicative_noise.html#fastestimator.fastestimator.op.numpyop.univariate.multiplicative_noise.MultiplicativeNoise", "title": "<code>MultiplicativeNoise</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Multiply an image with random perturbations.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>multiplier</code> <code>Union[float, Tuple[float, float]]</code> <p>If a single float, the image will be multiplied by this number. If tuple of floats then <code>multiplier</code> will be in the range [multiplier[0], multiplier[1]).</p> <code>(0.9, 1.1)</code> <code>per_channel</code> <code>bool</code> <p>Whether to sample different multipliers for each channel of the image.</p> <code>False</code> <code>elementwise</code> <code>bool</code> <p>If <code>False</code> multiply multiply all pixels in an image with a random value sampled once. If <code>True</code> Multiply image pixels with values that are pixelwise randomly sampled.</p> <code>False</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\multiplicative_noise.py</code> <pre><code>class MultiplicativeNoise(ImageOnlyAlbumentation):\n\"\"\"Multiply an image with random perturbations.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        multiplier: If a single float, the image will be multiplied by this number. If tuple of floats then `multiplier`\n            will be in the range [multiplier[0], multiplier[1]).\n        per_channel: Whether to sample different multipliers for each channel of the image.\n        elementwise: If `False` multiply multiply all pixels in an image with a random value sampled once.\n            If `True` Multiply image pixels with values that are pixelwise randomly sampled.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmultiplier: Union[float, Tuple[float, float]] = (0.9, 1.1),\nper_channel: bool = False,\nelementwise: bool = False):\nsuper().__init__(\nMultiplicativeNoiseAlb(multiplier=multiplier,\nper_channel=per_channel,\nelementwise=elementwise,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/normalize.html", "title": "normalize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/normalize.html#fastestimator.fastestimator.op.numpyop.univariate.normalize.Normalize", "title": "<code>Normalize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>mean</code> <code>Union[float, Tuple[float, ...]]</code> <p>Mean values to subtract.</p> <code>(0.485, 0.456, 0.406)</code> <code>std</code> <code>Union[float, Tuple[float, ...]]</code> <p>The divisor.</p> <code>(0.229, 0.224, 0.225)</code> <code>max_pixel_value</code> <code>float</code> <p>Maximum possible pixel value.</p> <code>255.0</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\normalize.py</code> <pre><code>class Normalize(ImageOnlyAlbumentation):\n\"\"\"Divide pixel values by a maximum value, subtract mean per channel and divide by std per channel.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        mean: Mean values to subtract.\n        std: The divisor.\n        max_pixel_value: Maximum possible pixel value.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmean: Union[float, Tuple[float, ...]] = (0.485, 0.456, 0.406),\nstd: Union[float, Tuple[float, ...]] = (0.229, 0.224, 0.225),\nmax_pixel_value: float = 255.0):\nsuper().__init__(NormalizeAlb(mean=mean, std=std, max_pixel_value=max_pixel_value, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/onehot.html", "title": "onehot", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/onehot.html#fastestimator.fastestimator.op.numpyop.univariate.onehot.Onehot", "title": "<code>Onehot</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Transform an integer label to one-hot-encoding.</p> <p>This can be desirable for increasing robustness against incorrect labels: https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Input key(s) of labels to be onehot encoded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Output key(s) of labels.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes.</p> required <code>label_smoothing</code> <code>float</code> <p>Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing / num_classes, the other class output is: label_smoothing / num_classes.</p> <code>0.0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\onehot.py</code> <pre><code>class Onehot(NumpyOp):\n\"\"\"Transform an integer label to one-hot-encoding.\n    This can be desirable for increasing robustness against incorrect labels:\n    https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0\n    Args:\n        inputs: Input key(s) of labels to be onehot encoded.\n        outputs: Output key(s) of labels.\n        num_classes: Total number of classes.\n        label_smoothing: Smoothing factor, after smoothing class output is: 1 - label_smoothing + label_smoothing\n            / num_classes, the other class output is: label_smoothing / num_classes.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nnum_classes: int,\nlabel_smoothing: float = 0.0,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.num_classes = num_classes\nself.label_smoothing = label_smoothing\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Union[int, np.ndarray]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_onehot(elem) for elem in data]\ndef _apply_onehot(self, data: Union[int, np.ndarray]) -&gt; np.ndarray:\nclass_index = np.array(data)\nassert \"int\" in str(class_index.dtype)\nassert class_index.size == 1, \"data must have only one item\"\nclass_index = class_index.item()\nassert class_index &lt; self.num_classes, \"label value should be smaller than num_classes\"\noutput = np.full((self.num_classes), fill_value=self.label_smoothing / self.num_classes)\noutput[class_index] = 1.0 - self.label_smoothing + self.label_smoothing / self.num_classes\nreturn output\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html", "title": "pad_sequence", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/pad_sequence.html#fastestimator.fastestimator.op.numpyop.univariate.pad_sequence.PadSequence", "title": "<code>PadSequence</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Pad sequences to the same length with provided value.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of sequences to be padded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are padded.</p> required <code>max_len</code> <code>int</code> <p>Maximum length of all sequences.</p> required <code>value</code> <code>Union[str, int]</code> <p>Padding value.</p> <code>0</code> <code>append</code> <code>bool</code> <p>Pad before or after the sequences. True for padding the values after the sequence, False otherwise.</p> <code>True</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\pad_sequence.py</code> <pre><code>class PadSequence(NumpyOp):\n\"\"\"Pad sequences to the same length with provided value.\n    Args:\n        inputs: Key(s) of sequences to be padded.\n        outputs: Key(s) of sequences that are padded.\n        max_len: Maximum length of all sequences.\n        value: Padding value.\n        append: Pad before or after the sequences. True for padding the values after the sequence, False otherwise.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmax_len: int,\nvalue: Union[str, int] = 0,\nappend: bool = True,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.max_len = max_len\nself.value = value\nself.append = append\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._pad_sequence(elem) for elem in data]\ndef _pad_sequence(self, data: np.ndarray) -&gt; np.ndarray:\n\"\"\"Pad the input sequence to the maximum length. Sequences longer than `max_len` are truncated.\n        Args:\n            data: input sequence in the data.\n        Returns:\n            Padded sequence\n        \"\"\"\nif len(data) &lt; self.max_len:\npad_len = self.max_len - len(data)\npad_arr = np.full(pad_len, self.value)\nif self.append:\ndata = np.append(data, pad_arr)\nelse:\ndata = np.append(pad_arr, data)\nelse:\ndata = data[:self.max_len]\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/posterize.html", "title": "posterize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/posterize.html#fastestimator.fastestimator.op.numpyop.univariate.posterize.Posterize", "title": "<code>Posterize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Reduce the number of bits for each color channel</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>num_bits</code> <code>Union[int, Tuple[int, int], Tuple[int, int, int], Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]]</code> <p>Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be in the range [0, 8].</p> <code>4</code> Image types <p>uint8</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\posterize.py</code> <pre><code>class Posterize(ImageOnlyAlbumentation):\n\"\"\"Reduce the number of bits for each color channel\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        num_bits: Number of high bits. If num_bits is a single value, the range will be [num_bits, num_bits]. A triplet\n            of ints will be interpreted as [r, g, b], and a triplet of pairs as [[r1, r1], [g1, g2], [b1, b2]]. Must be\n            in the range [0, 8].\n    Image types:\n        uint8\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nnum_bits: Union[int,\nTuple[int, int],\nTuple[int, int, int],\nTuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]] = 4):\nsuper().__init__(PosterizeAlb(num_bits=num_bits, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html", "title": "random_brightness_contrast", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_brightness_contrast.html#fastestimator.fastestimator.op.numpyop.univariate.random_brightness_contrast.RandomBrightnessContrast", "title": "<code>RandomBrightnessContrast</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly change the brightness and contrast of an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>brightness_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing brightness. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>contrast_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>Factor range for changing contrast. If limit is a single float, the range will be (-limit, limit).</p> <code>0.2</code> <code>brightness_by_max</code> <code>bool</code> <p>If True adjust contrast by image dtype maximum, else adjust contrast by image mean.</p> <code>True</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_brightness_contrast.py</code> <pre><code>class RandomBrightnessContrast(ImageOnlyAlbumentation):\n\"\"\"Randomly change the brightness and contrast of an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        brightness_limit: Factor range for changing brightness.\n            If limit is a single float, the range will be (-limit, limit).\n        contrast_limit: Factor range for changing contrast.\n            If limit is a single float, the range will be (-limit, limit).\n        brightness_by_max: If True adjust contrast by image dtype maximum, else adjust contrast by image mean.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nbrightness_limit: Union[float, Tuple[float, float]] = 0.2,\ncontrast_limit: Union[float, Tuple[float, float]] = 0.2,\nbrightness_by_max: bool = True):\nsuper().__init__(\nRandomBrightnessContrastAlb(brightness_limit=brightness_limit,\ncontrast_limit=contrast_limit,\nbrightness_by_max=brightness_by_max,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html", "title": "random_fog", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_fog.html#fastestimator.fastestimator.op.numpyop.univariate.random_fog.RandomFog", "title": "<code>RandomFog</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add fog to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>fog_coef_lower</code> <code>float</code> <p>Lower limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>0.3</code> <code>fog_coef_upper</code> <code>float</code> <p>Upper limit for fog intensity coefficient. Should be in the range [0, 1].</p> <code>1.0</code> <code>alpha_coef</code> <code>float</code> <p>Transparency of the fog circles. Should be in the range [0, 1].</p> <code>0.08</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_fog.py</code> <pre><code>class RandomFog(ImageOnlyAlbumentation):\n\"\"\"Add fog to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        fog_coef_lower: Lower limit for fog intensity coefficient. Should be in the range [0, 1].\n        fog_coef_upper: Upper limit for fog intensity coefficient. Should be in the range [0, 1].\n        alpha_coef: Transparency of the fog circles. Should be in the range [0, 1].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nfog_coef_lower: float = 0.3,\nfog_coef_upper: float = 1.0,\nalpha_coef: float = 0.08):\nsuper().__init__(\nRandomFogAlb(fog_coef_lower=fog_coef_lower,\nfog_coef_upper=fog_coef_upper,\nalpha_coef=alpha_coef,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html", "title": "random_gamma", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_gamma.html#fastestimator.fastestimator.op.numpyop.univariate.random_gamma.RandomGamma", "title": "<code>RandomGamma</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Apply a gamma transform to an image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>gamma_limit</code> <code>Union[float, Tuple[float, float]]</code> <p>If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).</p> <code>(80, 120)</code> <code>eps</code> <code>float</code> <p>A numerical stability constant to avoid division by zero.</p> <code>1e-07</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_gamma.py</code> <pre><code>class RandomGamma(ImageOnlyAlbumentation):\n\"\"\"Apply a gamma transform to an image.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        gamma_limit: If gamma_limit is a single float value, the range will be (-gamma_limit, gamma_limit).\n        eps: A numerical stability constant to avoid division by zero.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ngamma_limit: Union[float, Tuple[float, float]] = (80, 120),\neps: float = 1e-7):\nsuper().__init__(RandomGammaAlb(gamma_limit=gamma_limit, eps=eps, always_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html", "title": "random_rain", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_rain.html#fastestimator.fastestimator.op.numpyop.univariate.random_rain.RandomRain", "title": "<code>RandomRain</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add rain to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>slant_lower</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>-10</code> <code>slant_upper</code> <code>int</code> <p>Should be in range [-20, 20].</p> <code>10</code> <code>drop_length</code> <code>int</code> <p>Should be in range [0, 100].</p> <code>20</code> <code>drop_width</code> <code>int</code> <p>Should be in range [1, 5].</p> <code>1</code> <code>drop_color</code> <code>Tuple[int, int, int]</code> <p>Rain lines color (r, g, b).</p> <code>(200, 200, 200)</code> <code>blur_value</code> <code>int</code> <p>How blurry to make the rain.</p> <code>7</code> <code>brightness_coefficient</code> <code>float</code> <p>Rainy days are usually shady. Should be in range [0, 1].</p> <code>0.7</code> <code>rain_type</code> <code>Optional[str]</code> <p>One of [None, \"drizzle\", \"heavy\", \"torrential\"].</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_rain.py</code> <pre><code>class RandomRain(ImageOnlyAlbumentation):\n\"\"\"Add rain to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        slant_lower: Should be in range [-20, 20].\n        slant_upper: Should be in range [-20, 20].\n        drop_length: Should be in range [0, 100].\n        drop_width: Should be in range [1, 5].\n        drop_color: Rain lines color (r, g, b).\n        blur_value: How blurry to make the rain.\n        brightness_coefficient: Rainy days are usually shady. Should be in range [0, 1].\n        rain_type: One of [None, \"drizzle\", \"heavy\", \"torrential\"].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nslant_lower: int = -10,\nslant_upper: int = 10,\ndrop_length: int = 20,\ndrop_width: int = 1,\ndrop_color: Tuple[int, int, int] = (200, 200, 200),\nblur_value: int = 7,\nbrightness_coefficient: float = 0.7,\nrain_type: Optional[str] = None):\nsuper().__init__(\nRandomRainAlb(slant_lower=slant_lower,\nslant_upper=slant_upper,\ndrop_length=drop_length,\ndrop_width=drop_width,\ndrop_color=drop_color,  # Their docstring type hint doesn't match the real code\nblur_value=blur_value,\nbrightness_coefficient=brightness_coefficient,\nrain_type=rain_type,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html", "title": "random_shadow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_shadow.html#fastestimator.fastestimator.op.numpyop.univariate.random_shadow.RandomShadow", "title": "<code>RandomShadow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add shadows to an image</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>shadow_roi</code> <code>Tuple[float, float, float, float]</code> <p>Region of the image where shadows will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0.0, 0.5, 1.0, 1.0)</code> <code>num_shadows_lower</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [0, <code>num_shadows_upper</code>].</p> <code>1</code> <code>num_shadows_upper</code> <code>int</code> <p>Lower limit for the possible number of shadows. Should be in range [<code>num_shadows_lower</code>, inf].</p> <code>2</code> <code>shadow_dimension</code> <code>int</code> <p>Number of edges in the shadow polygons.</p> <code>5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_shadow.py</code> <pre><code>class RandomShadow(ImageOnlyAlbumentation):\n\"\"\"Add shadows to an image\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        shadow_roi: Region of the image where shadows will appear (x_min, y_min, x_max, y_max).\n            All values should be in range [0, 1].\n        num_shadows_lower: Lower limit for the possible number of shadows. Should be in range [0, `num_shadows_upper`].\n        num_shadows_upper: Lower limit for the possible number of shadows.\n            Should be in range [`num_shadows_lower`, inf].\n        shadow_dimension: Number of edges in the shadow polygons.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nshadow_roi: Tuple[float, float, float, float] = (0.0, 0.5, 1.0, 1.0),\nnum_shadows_lower: int = 1,\nnum_shadows_upper: int = 2,\nshadow_dimension: int = 5):\nprint(\"\\033[93m {}\\033[00m\".format(\n\"Warning! RandomShadow does not work with multi-threaded Pipelines. Either do not use this Op or else \" +\n\"set your Pipeline num_process=0\"))\n# TODO - Have pipeline look for bad ops and auto-magically set num_process correctly\nsuper().__init__(\nRandomShadowAlb(shadow_roi=shadow_roi,\nnum_shadows_lower=num_shadows_lower,\nnum_shadows_upper=num_shadows_upper,\nshadow_dimension=shadow_dimension,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html", "title": "random_snow", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_snow.html#fastestimator.fastestimator.op.numpyop.univariate.random_snow.RandomSnow", "title": "<code>RandomSnow</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Bleach out some pixels to simulate snow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be modified.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>snow_point_lower</code> <code>float</code> <p>Lower bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.1</code> <code>snow_point_upper</code> <code>float</code> <p>Upper bound of the amount of snow. Should be in the range [0, 1].</p> <code>0.3</code> <code>brightness_coeff</code> <code>float</code> <p>A larger number will lead to a more snow on the image. Should be &gt;= 0.</p> <code>2.5</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_snow.py</code> <pre><code>class RandomSnow(ImageOnlyAlbumentation):\n\"\"\"Bleach out some pixels to simulate snow.\n    Args:\n        inputs: Key(s) of images to be modified.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        snow_point_lower: Lower bound of the amount of snow. Should be in the range [0, 1].\n        snow_point_upper: Upper bound of the amount of snow. Should be in the range [0, 1].\n        brightness_coeff: A larger number will lead to a more snow on the image. Should be &gt;= 0.\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nsnow_point_lower: float = 0.1,\nsnow_point_upper: float = 0.3,\nbrightness_coeff: float = 2.5):\nsuper().__init__(\nRandomSnowAlb(snow_point_lower=snow_point_lower,\nsnow_point_upper=snow_point_upper,\nbrightness_coeff=brightness_coeff,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html", "title": "random_sun_flare", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/random_sun_flare.html#fastestimator.fastestimator.op.numpyop.univariate.random_sun_flare.RandomSunFlare", "title": "<code>RandomSunFlare</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Add a sun flare to the image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>flare_roi</code> <code>Tuple[float, float, float, float]</code> <p>region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be in range [0, 1].</p> <code>(0, 0, 1, 0.5)</code> <code>angle_lower</code> <code>float</code> <p>should be in range [0, <code>angle_upper</code>].</p> <code>0.0</code> <code>angle_upper</code> <code>float</code> <p>should be in range [<code>angle_lower</code>, 1].</p> <code>1.0</code> <code>num_flare_circles_lower</code> <code>int</code> <p>lower limit for the number of flare circles. Should be in range [0, <code>num_flare_circles_upper</code>].</p> <code>6</code> <code>num_flare_circles_upper</code> <code>int</code> <p>upper limit for the number of flare circles. Should be in range [<code>num_flare_circles_lower</code>, inf].</p> <code>10</code> <code>src_radius</code> <code>int</code> <p>Radius of the flare.</p> <code>400</code> <code>src_color</code> <code>Tuple[int, int, int]</code> <p>Color of the flare (R,G,B).</p> <code>(255, 255, 255)</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\random_sun_flare.py</code> <pre><code>class RandomSunFlare(ImageOnlyAlbumentation):\n\"\"\"Add a sun flare to the image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        flare_roi: region of the image where flare will appear (x_min, y_min, x_max, y_max). All values should be\n            in range [0, 1].\n        angle_lower: should be in range [0, `angle_upper`].\n        angle_upper: should be in range [`angle_lower`, 1].\n        num_flare_circles_lower: lower limit for the number of flare circles.\n            Should be in range [0, `num_flare_circles_upper`].\n        num_flare_circles_upper: upper limit for the number of flare circles.\n            Should be in range [`num_flare_circles_lower`, inf].\n        src_radius: Radius of the flare.\n        src_color: Color of the flare (R,G,B).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nflare_roi: Tuple[float, float, float, float] = (0, 0, 1, 0.5),\nangle_lower: float = 0.0,\nangle_upper: float = 1.0,\nnum_flare_circles_lower: int = 6,\nnum_flare_circles_upper: int = 10,\nsrc_radius: int = 400,\nsrc_color: Tuple[int, int, int] = (255, 255, 255)):\nsuper().__init__(\nRandomSunFlareAlb(flare_roi=flare_roi,\nangle_lower=angle_lower,\nangle_upper=angle_upper,\nnum_flare_circles_lower=num_flare_circles_lower,\nnum_flare_circles_upper=num_flare_circles_upper,\nsrc_radius=src_radius,\nsrc_color=src_color,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/read_image.html", "title": "read_image", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/read_image.html#fastestimator.fastestimator.op.numpyop.univariate.read_image.ReadImage", "title": "<code>ReadImage</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>A class for reading png or jpg images from disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of paths to images to be loaded.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of images to be output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>parent_path</code> <code>str</code> <p>Parent path that will be prepended to a given path.</p> <code>''</code> <code>color_flag</code> <code>Union[str, int]</code> <p>Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.</p> <code>cv2.IMREAD_COLOR</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>inputs</code> and <code>outputs</code> have mismatched lengths, or the <code>color_flag</code> is unacceptable.</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\read_image.py</code> <pre><code>class ReadImage(NumpyOp):\n\"\"\"A class for reading png or jpg images from disk.\n    Args:\n        inputs: Key(s) of paths to images to be loaded.\n        outputs: Key(s) of images to be output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        parent_path: Parent path that will be prepended to a given path.\n        color_flag: Whether to read the image as 'color', 'grey', or one of the cv2.IMREAD flags.\n    Raises:\n        AssertionError: If `inputs` and `outputs` have mismatched lengths, or the `color_flag` is unacceptable.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nparent_path: str = \"\",\ncolor_flag: Union[str, int] = cv2.IMREAD_COLOR):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nif isinstance(self.inputs, List) and isinstance(self.outputs, List):\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.parent_path = parent_path\nassert isinstance(color_flag, int) or color_flag in {'color', 'gray', 'grey'}, \\\n            f\"Unacceptable color_flag value: {color_flag}\"\nself.color_flag = color_flag\nif self.color_flag == \"color\":\nself.color_flag = cv2.IMREAD_COLOR\nelif self.color_flag in {\"gray\", \"grey\"}:\nself.color_flag = cv2.IMREAD_GRAYSCALE\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresults = []\nfor path in data:\npath = os.path.normpath(os.path.join(self.parent_path, path))\nimg = cv2.imread(path, self.color_flag)\nif self.color_flag in {cv2.IMREAD_COLOR, cv2.IMREAD_REDUCED_COLOR_2, cv2.IMREAD_REDUCED_COLOR_4,\ncv2.IMREAD_REDUCED_COLOR_8}:\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nif not isinstance(img, np.ndarray):\nraise ValueError('cv2 did not read correctly for file \"{}\"'.format(path))\nif self.color_flag in {cv2.IMREAD_GRAYSCALE, cv2.IMREAD_REDUCED_GRAYSCALE_2, cv2.IMREAD_REDUCED_GRAYSCALE_4,\ncv2.IMREAD_REDUCED_GRAYSCALE_8}:\nimg = np.expand_dims(img, -1)\nresults.append(img)\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/reshape.html#fastestimator.fastestimator.op.numpyop.univariate.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>An Op which re-shapes data to a target shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.</p> required <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of the data to be reshaped.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\reshape.py</code> <pre><code>class Reshape(NumpyOp):\n\"\"\"An Op which re-shapes data to a target shape.\n    Args:\n        shape: The desired output shape. At most one value may be -1 to put all of the leftover elements into that axis.\n        inputs: Key(s) of the data to be reshaped.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nshape: Union[int, Tuple[int, ...]],\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.shape = shape\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_reshape(elem) for elem in data]\ndef _apply_reshape(self, data):\ndata = np.reshape(data, self.shape)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html", "title": "rgb_shift", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/rgb_shift.html#fastestimator.fastestimator.op.numpyop.univariate.rgb_shift.RGBShift", "title": "<code>RGBShift</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Randomly shift the channel values for an input RGB image.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be normalized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the normalized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>r_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the red channel. If r_shift_limit is a single int, the range will be (-r_shift_limit, r_shift_limit).</p> <code>20</code> <code>g_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the green channel. If g_shift_limit is a single int, the range will be (-g_shift_limit, g_shift_limit).</p> <code>20</code> <code>b_shift_limit</code> <code>Union[int, Tuple[int, int]]</code> <p>range for changing values for the blue channel. If b_shift_limit is a single int, the range will be (-b_shift_limit, b_shift_limit).</p> <code>20</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\rgb_shift.py</code> <pre><code>class RGBShift(ImageOnlyAlbumentation):\n\"\"\"Randomly shift the channel values for an input RGB image.\n    Args:\n        inputs: Key(s) of images to be normalized.\n        outputs: Key(s) into which to write the normalized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        r_shift_limit: range for changing values for the red channel. If r_shift_limit is a single int, the range\n            will be (-r_shift_limit, r_shift_limit).\n        g_shift_limit: range for changing values for the green channel. If g_shift_limit is a single int, the range\n            will be (-g_shift_limit, g_shift_limit).\n        b_shift_limit: range for changing values for the blue channel. If b_shift_limit is a single int, the range\n            will be (-b_shift_limit, b_shift_limit).\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nr_shift_limit: Union[int, Tuple[int, int]] = 20,\ng_shift_limit: Union[int, Tuple[int, int]] = 20,\nb_shift_limit: Union[int, Tuple[int, int]] = 20):\nsuper().__init__(\nRGBShiftAlb(r_shift_limit=r_shift_limit,\ng_shift_limit=g_shift_limit,\nb_shift_limit=b_shift_limit,\nalways_apply=True),\ninputs=inputs,\noutputs=outputs,\nmode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/solarize.html", "title": "solarize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/solarize.html#fastestimator.fastestimator.op.numpyop.univariate.solarize.Solarize", "title": "<code>Solarize</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Invert all pixel values above a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be solarized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the solarized images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>threshold</code> <code>Union[int, Tuple[int, int], float, Tuple[float, float]]</code> <p>Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].</p> <code>128</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\solarize.py</code> <pre><code>class Solarize(ImageOnlyAlbumentation):\n\"\"\"Invert all pixel values above a threshold.\n    Args:\n        inputs: Key(s) of images to be solarized.\n        outputs: Key(s) into which to write the solarized images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        threshold: Range for the solarizing threshold. If threshold is a single value 't', the range will be [t, t].\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nthreshold: Union[int, Tuple[int, int], float, Tuple[float, float]] = 128):\nsuper().__init__(SolarizeAlb(threshold=threshold, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_array.html", "title": "to_array", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_array.html#fastestimator.fastestimator.op.numpyop.univariate.to_array.ToArray", "title": "<code>ToArray</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Convert data to a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of the data to be converted.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the converted data.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype to apply to the output array, or None to infer the type.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_array.py</code> <pre><code>class ToArray(NumpyOp):\n\"\"\"Convert data to a numpy array.\n    Args:\n        inputs: Key(s) of the data to be converted.\n        outputs: Key(s) into which to write the converted data.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        dtype: The dtype to apply to the output array, or None to infer the type.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ndtype: Optional[str] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.dtype = dtype\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Any], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._apply_transform(elem) for elem in data]\ndef _apply_transform(self, data: Any) -&gt; np.ndarray:\nreturn np.array(data, dtype=self.dtype)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_float.html", "title": "to_float", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_float.html#fastestimator.fastestimator.op.numpyop.univariate.to_float.ToFloat", "title": "<code>ToFloat</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Divides an input by max_value to give a float image in range [0,1].</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be converted to floating point representation.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>max_value</code> <code>Optional[float]</code> <p>The maximum value to serve as the divisor. If None it will be inferred by dtype.</p> <code>None</code> Image types <p>Any</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_float.py</code> <pre><code>class ToFloat(ImageOnlyAlbumentation):\n\"\"\"Divides an input by max_value to give a float image in range [0,1].\n    Args:\n        inputs: Key(s) of images to be converted to floating point representation.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        max_value: The maximum value to serve as the divisor. If None it will be inferred by dtype.\n    Image types:\n        Any\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\nmax_value: Optional[float] = None):\nsuper().__init__(ToFloatAlb(max_value=max_value, always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html", "title": "to_gray", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_gray.html#fastestimator.fastestimator.op.numpyop.univariate.to_gray.ToGray", "title": "<code>ToGray</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be converted to grayscale.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the modified images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_gray.py</code> <pre><code>class ToGray(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to grayscale. If the mean pixel value of the result is &gt; 127, the image is inverted.\n    Args:\n        inputs: Key(s) of images to be converted to grayscale.\n        outputs: Key(s) into which to write the modified images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToGrayAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html", "title": "to_sepia", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/to_sepia.html#fastestimator.fastestimator.op.numpyop.univariate.to_sepia.ToSepia", "title": "<code>ToSepia</code>", "text": "<p>         Bases: <code>ImageOnlyAlbumentation</code></p> <p>Convert an RGB image to sepia.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of images to be converted to sepia.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) into which to write the sepia images.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Image types <p>uint8, float32</p> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\to_sepia.py</code> <pre><code>class ToSepia(ImageOnlyAlbumentation):\n\"\"\"Convert an RGB image to sepia.\n    Args:\n        inputs: Key(s) of images to be converted to sepia.\n        outputs: Key(s) into which to write the sepia images.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Image types:\n        uint8, float32\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(ToSepiaAlb(always_apply=True), inputs=inputs, outputs=outputs, mode=mode)\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html", "title": "tokenize", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/tokenize.html#fastestimator.fastestimator.op.numpyop.univariate.tokenize.Tokenize", "title": "<code>Tokenize</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Split the sequences into tokens.</p> <p>Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if defined in the passed function object. By default, tokenize only splits the sequences into tokens.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of sequences to be tokenized.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences that are tokenized.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>tokenize_fn</code> <code>Union[None, Callable[[str], List[str]]]</code> <p>Tokenization function object.</p> <code>None</code> <code>to_lower_case</code> <code>bool</code> <p>Whether to convert tokens to lowercase.</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\tokenize.py</code> <pre><code>class Tokenize(NumpyOp):\n\"\"\"Split the sequences into tokens.\n    Tokenize split the document/sequence into tokens and at the same time perform additional operations on tokens if\n    defined in the passed function object. By default, tokenize only splits the sequences into tokens.\n    Args:\n        inputs: Key(s) of sequences to be tokenized.\n        outputs: Key(s) of sequences that are tokenized.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        tokenize_fn: Tokenization function object.\n        to_lower_case: Whether to convert tokens to lowercase.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\ntokenize_fn: Union[None, Callable[[str], List[str]]] = None,\nto_lower_case: bool = False) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nself.tokenize_fn = tokenize_fn\nself.to_lower_case = to_lower_case\ndef forward(self, data: List[str], state: Dict[str, Any]) -&gt; List[List[str]]:\nreturn [self._apply_tokenization(seq) for seq in data]\ndef _apply_tokenization(self, data: str) -&gt; List[str]:\n\"\"\"Split the sequence into tokens and apply lowercase if `do_lower_case` is set.\n        Args:\n            data: Input sequence.\n        Returns:\n            A list of tokens.\n        \"\"\"\nif self.tokenize_fn:\ndata = self.tokenize_fn(data)\nelse:\ndata = data.split()\nif self.to_lower_case:\ndata = list(map(lambda x: x.lower(), data))\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/univariate.html", "title": "univariate", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/univariate.html#fastestimator.fastestimator.op.numpyop.univariate.univariate.ImageOnlyAlbumentation", "title": "<code>ImageOnlyAlbumentation</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Operators which apply to single images (as opposed to images + masks or images + bounding boxes).</p> <p>This is a wrapper for functionality provided by the Albumentations library: https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects it provides is available at https://albumentations-demo.herokuapp.com.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>ImageOnlyTransform</code> <p>An Albumentation function to be invoked.</p> required <code>inputs</code> <code>Union[str, List[str], Callable]</code> <p>Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the <code>func</code> will be run in replay mode so that the exact same augmentation is applied to each value.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key(s) under which to write the outputs of this Op back to the data dictionary.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\univariate.py</code> <pre><code>class ImageOnlyAlbumentation(NumpyOp):\n\"\"\"Operators which apply to single images (as opposed to images + masks or images + bounding boxes).\n    This is a wrapper for functionality provided by the Albumentations library:\n    https://github.com/albumentations-team/albumentations. A useful visualization tool for many of the possible effects\n    it provides is available at https://albumentations-demo.herokuapp.com.\n    Args:\n        func: An Albumentation function to be invoked.\n        inputs: Key(s) from which to retrieve data from the data dictionary. If more than one key is provided, the\n            `func` will be run in replay mode so that the exact same augmentation is applied to each value.\n        outputs: Key(s) under which to write the outputs of this Op back to the data dictionary.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfunc: ImageOnlyTransform,\ninputs: Union[str, List[str], Callable],\noutputs: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert len(self.inputs) == len(self.outputs), \"Input and Output lengths must match\"\nself.func = Compose(transforms=[func])\nself.replay_func = ReplayCompose(transforms=[deepcopy(func)])\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[np.ndarray], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nresults = [self.replay_func(image=data[0]) if len(data) &gt; 1 else self.func(image=data[0])]\nfor i in range(1, len(data)):\nresults.append(self.replay_func.replay(results[0]['replay'], image=data[i]))\nreturn [result[\"image\"] for result in results]\n</code></pre>"}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html", "title": "word_to_id", "text": ""}, {"location": "fastestimator/op/numpyop/univariate/word_to_id.html#fastestimator.fastestimator.op.numpyop.univariate.word_to_id.WordtoId", "title": "<code>WordtoId</code>", "text": "<p>         Bases: <code>NumpyOp</code></p> <p>Converts words to their corresponding id using mapper function or dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>Union[Dict[str, int], Callable[[List[str]], List[int]]]</code> <p>Mapper function or dictionary</p> required <code>inputs</code> <code>Union[str, Iterable[str], Callable]</code> <p>Key(s) of sequences to be converted to ids.</p> required <code>outputs</code> <code>Union[str, Iterable[str]]</code> <p>Key(s) of sequences are converted to ids.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\numpyop\\univariate\\word_to_id.py</code> <pre><code>class WordtoId(NumpyOp):\n\"\"\"Converts words to their corresponding id using mapper function or dictionary.\n    Args:\n        mapping: Mapper function or dictionary\n        inputs: Key(s) of sequences to be converted to ids.\n        outputs: Key(s) of sequences are converted to ids.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(\nself,\nmapping: Union[Dict[str, int], Callable[[List[str]], List[int]]],\ninputs: Union[str, Iterable[str], Callable],\noutputs: Union[str, Iterable[str]],\nmode: Union[None, str, Iterable[str]] = None,\n) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\nassert callable(mapping) or isinstance(mapping, dict), \\\n            \"Incorrect data type provided for `mapping`. Please provide a function or a dictionary.\"\nself.mapping = mapping\ndef forward(self, data: List[List[str]], state: Dict[str, Any]) -&gt; List[np.ndarray]:\nreturn [self._convert_to_id(elem) for elem in data]\ndef _convert_to_id(self, data: List[str]) -&gt; np.ndarray:\n\"\"\"Flatten the input list and map the token to ids using mapper function or lookup table.\n        Args:\n            data: Input array of tokens\n        Raises:\n            Exception: If neither of the mapper function or dictionary object is passed\n        Returns:\n            Array of token ids\n        \"\"\"\nif callable(self.mapping):\ndata = self.mapping(data)\nelse:\ndata = [self.mapping.get(token) for token in data]\nreturn np.array(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/argmax.html", "title": "argmax", "text": ""}, {"location": "fastestimator/op/tensorop/argmax.html#fastestimator.fastestimator.op.tensorop.argmax.Argmax", "title": "<code>Argmax</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Get the argmax from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>axis</code> <code>int</code> <p>The axis along which to collect the argmax.</p> <code>0</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\argmax.py</code> <pre><code>class Argmax(TensorOp):\n\"\"\"Get the argmax from a tensor.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        outputs: The key(s) under which to save the output.\n        axis: The axis along which to collect the argmax.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\naxis: int = 0,\nmode: Union[None, str, Iterable[str]] = \"eval\"):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.axis = axis\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nreturn [argmax(tensor=tensor, axis=self.axis) for tensor in data]\n</code></pre>"}, {"location": "fastestimator/op/tensorop/average.html", "title": "average", "text": ""}, {"location": "fastestimator/op/tensorop/average.html#fastestimator.fastestimator.op.tensorop.average.Average", "title": "<code>Average</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Compute the average across tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Iterable[str]]</code> <p>Keys of tensors to be averaged.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\average.py</code> <pre><code>class Average(TensorOp):\n\"\"\"Compute the average across tensors.\n    Args:\n        inputs: Keys of tensors to be averaged.\n        outputs: The key under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self, inputs: Union[str, Iterable[str]], outputs: str, mode: Union[None, str,\nIterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, False\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\nresult = zeros_like(data[0])\nfor tensor in data:\nresult += tensor\nreturn result / len(data)\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gather.html", "title": "gather", "text": ""}, {"location": "fastestimator/op/tensorop/gather.html#fastestimator.fastestimator.op.tensorop.gather.Gather", "title": "<code>Gather</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Gather values from an input tensor.</p> <p>If indices are not provided, the maximum values along the batch dimension will be collected.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to gather values from.</p> required <code>indices</code> <code>Union[None, str, List[str]]</code> <p>A tensor containing target indices to gather.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the output.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gather.py</code> <pre><code>class Gather(TensorOp):\n\"\"\"Gather values from an input tensor.\n    If indices are not provided, the maximum values along the batch dimension will be collected.\n    Args:\n        inputs: The tensor(s) to gather values from.\n        indices: A tensor containing target indices to gather.\n        outputs: The key(s) under which to save the output.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nindices: Union[None, str, List[str]] = None,\nmode: Union[None, str, Iterable[str]] = \"eval\"):\nindices = to_list(indices)\nself.num_indices = len(indices)\ncombined_inputs = indices\ncombined_inputs.extend(to_list(inputs))\nsuper().__init__(inputs=combined_inputs, outputs=outputs, mode=mode)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nindices = data[:self.num_indices]\ninputs = data[self.num_indices:]\nresults = []\nfor idx, tensor in enumerate(inputs):\n# Check len(indices[0]) since an empty indices element is used to trigger the else\nif isinstance(indices[0], tf.Tensor) or isinstance(indices[0], torch.Tensor):\nelem_len = indices[0].shape[0]\nelse:\nelem_len = len(indices[0])\nif len(indices) &gt; idx and elem_len &gt; 0:\nresults.append(gather_from_batch(tensor, indices=indices[idx]))\nelif len(indices) == 1 and elem_len &gt; 0:\n# One set of indices for all outputs\nresults.append(gather_from_batch(tensor, indices=indices[0]))\nelse:\nresults.append(reduce_max(tensor, 1))  # The maximum value within each batch element\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/reshape.html", "title": "reshape", "text": ""}, {"location": "fastestimator/op/tensorop/reshape.html#fastestimator.fastestimator.op.tensorop.reshape.Reshape", "title": "<code>Reshape</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Reshape a input tensor to conform to a given shape.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>Key of the input tensor that is to be reshaped.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>Key of the output tensor that has been reshaped.</p> required <code>shape</code> <code>Union[int, Tuple[int, ...]]</code> <p>Target shape.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\reshape.py</code> <pre><code>class Reshape(TensorOp):\n\"\"\"Reshape a input tensor to conform to a given shape.\n    Args:\n        inputs: Key of the input tensor that is to be reshaped.\n        outputs: Key of the output tensor that has been reshaped.\n        shape: Target shape.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\noutputs: Union[str, List[str]],\nshape: Union[int, Tuple[int, ...]],\nmode: Union[None, str, Iterable[str]] = \"!infer\") -&gt; None:\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.shape = shape\nself.in_list, self.out_list = False, False\ndef forward(self, data: Tensor, state: Dict[str, Any]) -&gt; Tensor:\nif isinstance(data, tf.Tensor):\nreturn tf.reshape(data, self.shape)\nelif isinstance(data, torch.Tensor):\nreturn data.view(self.shape)\nelse:\nraise ValueError(\"unrecognized data format: {}\".format(type(data)))\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html", "title": "tensorop", "text": ""}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp", "title": "<code>TensorOp</code>", "text": "<p>         Bases: <code>Op</code></p> <p>An Operator class which takes and returns tensor data.</p> <p>These Operators are used in fe.Network to perform graph-based operations like neural network training.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>class TensorOp(Op):\n\"\"\"An Operator class which takes and returns tensor data.\n    These Operators are used in fe.Network to perform graph-based operations like neural network training.\n    \"\"\"\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n        This method will be invoked on batches of data.\n        Args:\n            data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n            state: Information about the current execution context, for example {\"mode\": \"train\"}.\n        Returns:\n            The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n            dictionary based on whatever keys this Op declares as its `outputs`.\n        \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/tensorop.html#fastestimator.fastestimator.op.tensorop.tensorop.TensorOp.forward", "title": "<code>forward</code>", "text": "<p>A method which will be invoked in order to transform data.</p> <p>This method will be invoked on batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[Tensor]]</code> <p>The batch from the data dictionary corresponding to whatever keys this Op declares as its <code>inputs</code>.</p> required <code>state</code> <code>Dict[str, Any]</code> <p>Information about the current execution context, for example {\"mode\": \"train\"}.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, List[Tensor]]</code> <p>The <code>data</code> after applying whatever transform this Op is responsible for. It will be written into the data</p> <code>Union[Tensor, List[Tensor]]</code> <p>dictionary based on whatever keys this Op declares as its <code>outputs</code>.</p> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\tensorop.py</code> <pre><code>def forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\n\"\"\"A method which will be invoked in order to transform data.\n    This method will be invoked on batches of data.\n    Args:\n        data: The batch from the data dictionary corresponding to whatever keys this Op declares as its `inputs`.\n        state: Information about the current execution context, for example {\"mode\": \"train\"}.\n    Returns:\n        The `data` after applying whatever transform this Op is responsible for. It will be written into the data\n        dictionary based on whatever keys this Op declares as its `outputs`.\n    \"\"\"\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html", "title": "fgsm", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/fgsm.html#fastestimator.fastestimator.op.tensorop.gradient.fgsm.FGSM", "title": "<code>FGSM</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Create an adversarial sample from input data using the Fast Gradient Sign Method.</p> <p>See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Key of the input to be attacked.</p> required <code>loss</code> <code>str</code> <p>Key of the loss value to use for gradient computation.</p> required <code>outputs</code> <code>str</code> <p>The key under which to save the output.</p> required <code>epsilon</code> <code>float</code> <p>The strength of the perturbation to use in the attack.</p> <code>0.01</code> <code>clip_low</code> <code>Optional[float]</code> <p>a minimum value to clip the output by (defaults to min value of data when set to None).</p> <code>None</code> <code>clip_high</code> <code>Optional[float]</code> <p>a maximum value to clip the output by (defaults to max value of data when set to None).</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\fgsm.py</code> <pre><code>class FGSM(TensorOp):\n\"\"\"Create an adversarial sample from input data using the Fast Gradient Sign Method.\n    See https://arxiv.org/abs/1412.6572 for an explanation of adversarial attacks.\n    Args:\n        data: Key of the input to be attacked.\n        loss: Key of the loss value to use for gradient computation.\n        outputs: The key under which to save the output.\n        epsilon: The strength of the perturbation to use in the attack.\n        clip_low: a minimum value to clip the output by (defaults to min value of data when set to None).\n        clip_high: a maximum value to clip the output by (defaults to max value of data when set to None).\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ndata: str,\nloss: str,\noutputs: str,\nepsilon: float = 0.01,\nclip_low: Optional[float] = None,\nclip_high: Optional[float] = None,\nmode: Union[None, str, Iterable[str]] = None):\nsuper().__init__(inputs=[data, loss], outputs=outputs, mode=mode)\nself.epsilon = epsilon\nself.clip_low = clip_low\nself.clip_high = clip_high\nself.retain_graph = True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ndata, loss = data\ngrad = get_gradient(target=loss, sources=data, tape=state['tape'], retain_graph=self.retain_graph)\nadverse_data = clip_by_value(data + self.epsilon * sign(grad),\nmin_value=self.clip_low or reduce_min(data),\nmax_value=self.clip_high or reduce_max(data))\nreturn adverse_data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/gradient.html", "title": "gradient", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/gradient.html#fastestimator.fastestimator.op.tensorop.gradient.gradient.GradientOp", "title": "<code>GradientOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Return the gradients of finals w.r.t. inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to compute gradients with respect to.</p> required <code>finals</code> <code>Union[str, List[str]]</code> <p>The tensor(s) to compute gradients from.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The key(s) under which to save the gradients.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\gradient.py</code> <pre><code>class GradientOp(TensorOp):\n\"\"\"Return the gradients of finals w.r.t. inputs.\n    Args:\n        inputs: The tensor(s) to compute gradients with respect to.\n        finals: The tensor(s) to compute gradients from.\n        outputs: The key(s) under which to save the gradients.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, List[str]],\nfinals: Union[str, List[str]],\noutputs: Union[str, List[str]],\nmode: Union[None, str, Iterable[str]] = \"eval\"):\ninputs = to_list(inputs)\nfinals = to_list(finals)\noutputs = to_list(outputs)\nassert len(inputs) == len(finals) == len(outputs), \\\n            \"GradientOp requires the same number of inputs, finals, and outputs\"\ninputs.extend(finals)\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.retain_graph = True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\ninitials = data[:len(data) // 2]\nfinals = data[len(data) // 2:]\nresults = []\nfor initial, final in zip(initials, finals):\nresults.append(get_gradient(final, initial, tape=state['tape'], retain_graph=self.retain_graph))\nreturn results\n</code></pre>"}, {"location": "fastestimator/op/tensorop/gradient/watch.html", "title": "watch", "text": ""}, {"location": "fastestimator/op/tensorop/gradient/watch.html#fastestimator.fastestimator.op.tensorop.gradient.watch.Watch", "title": "<code>Watch</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Watch one or more tensors for later gradient computation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>which tensors to watch during future computation.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\gradient\\watch.py</code> <pre><code>class Watch(TensorOp):\n\"\"\"Watch one or more tensors for later gradient computation.\n    Args:\n        inputs: which tensors to watch during future computation.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self, inputs: Union[None, str, Iterable[str]], mode: Union[None, str, Iterable[str]] = None) -&gt; None:\nsuper().__init__(inputs=inputs, outputs=inputs, mode=mode)\nself.in_list, self.out_list = True, True\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; List[Tensor]:\nfor idx, tensor in enumerate(data):\ndata[idx] = watch(tensor=tensor, tape=state['tape'])\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html", "title": "cross_entropy", "text": ""}, {"location": "fastestimator/op/tensorop/loss/cross_entropy.html#fastestimator.fastestimator.op.tensorop.loss.cross_entropy.CrossEntropy", "title": "<code>CrossEntropy</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str], Callable]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss value.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'!infer'</code> <code>from_logits</code> <code>bool</code> <p>Whether y_pred is logits (without softmax).</p> <code>False</code> <code>average_loss</code> <code>bool</code> <p>Whether to average the element-wise loss after the Loss Op.</p> <code>True</code> <code>form</code> <code>Optional[str]</code> <p>What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will automatically infer the correct form based on tensor shape.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\cross_entropy.py</code> <pre><code>class CrossEntropy(TensorOp):\n\"\"\"Calculate Element-Wise CrossEntropy (binary, categorical or sparse categorical).\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss value.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        from_logits: Whether y_pred is logits (without softmax).\n        average_loss: Whether to average the element-wise loss after the Loss Op.\n        form: What form of cross entropy should be performed ('binary', 'categorical', 'sparse', or None). None will\n            automatically infer the correct form based on tensor shape.\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str], Callable] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = \"!infer\",\nfrom_logits: bool = False,\naverage_loss: bool = True,\nform: Optional[str] = None):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nself.from_logits = from_logits\nself.average_loss = average_loss\nself.form = form\nself.cross_entropy_fn = {\n\"binary\": binary_crossentropy,\n\"categorical\": categorical_crossentropy,\n\"sparse\": sparse_categorical_crossentropy\n}\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nform = self.form\nif form is None:\nif len(y_pred.shape) == 2 and y_pred.shape[-1] &gt; 1:\nif len(y_true.shape) == 2 and y_true.shape[-1] &gt; 1:\nform = \"categorical\"\nelse:\nform = \"sparse\"\nelse:\nform = \"binary\"\nloss = self.cross_entropy_fn[form](y_pred, y_true, from_logits=self.from_logits, average_loss=self.average_loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html", "title": "mean_squared_error", "text": ""}, {"location": "fastestimator/op/tensorop/loss/mean_squared_error.html#fastestimator.fastestimator.op.tensorop.loss.mean_squared_error.MeanSquaredError", "title": "<code>MeanSquaredError</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>Calculate the mean squared error loss between two tensors.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str], Callable]</code> <p>A tuple or list like: [, ]. <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store the computed loss.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\loss\\mean_squared_error.py</code> <pre><code>class MeanSquaredError(TensorOp):\n\"\"\"Calculate the mean squared error loss between two tensors.\n    Args:\n        inputs: A tuple or list like: [&lt;y_pred&gt;, &lt;y_true&gt;].\n        outputs: String key under which to store the computed loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[None, str, Iterable[str], Callable] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\naverage_loss: bool = True):\nself.average_loss = average_loss\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\ndef forward(self, data: List[Tensor], state: Dict[str, Any]) -&gt; Tensor:\ny_pred, y_true = data\nloss = mean_squared_error(y_true=y_true, y_pred=y_pred)\nif self.average_loss:\nloss = reduce_mean(loss)\nreturn loss\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/model.html", "title": "model", "text": ""}, {"location": "fastestimator/op/tensorop/model/model.html#fastestimator.fastestimator.op.tensorop.model.model.ModelOp", "title": "<code>ModelOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs forward passes of a neural network over batch data to generate predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model compiled by fe.build.</p> required <code>inputs</code> <code>Union[None, str, Iterable[str], Callable]</code> <p>String key of input training data.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>String key under which to store predictions.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> <code>trainable</code> <code>bool</code> <p>Indicates whether the model should have its weights tracked for update.</p> <code>True</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\model.py</code> <pre><code>class ModelOp(TensorOp):\n\"\"\"This class performs forward passes of a neural network over batch data to generate predictions.\n    Args:\n        model: A model compiled by fe.build.\n        inputs: String key of input training data.\n        outputs: String key under which to store predictions.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        trainable: Indicates whether the model should have its weights tracked for update.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\ninputs: Union[None, str, Iterable[str], Callable] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None,\ntrainable: bool = True):\nsuper().__init__(inputs=inputs, outputs=outputs, mode=mode)\nassert hasattr(model, \"fe_compiled\"), \"must use fe.build to compile the model before use\"\nself.model = model\nself.trainable = trainable\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]) -&gt; Union[Tensor, List[Tensor]]:\ntraining = state['mode'] == \"train\" and self.trainable\ndata = feed_forward(self.model, data, training=training)\nreturn data\n</code></pre>"}, {"location": "fastestimator/op/tensorop/model/update.html", "title": "update", "text": ""}, {"location": "fastestimator/op/tensorop/model/update.html#fastestimator.fastestimator.op.tensorop.model.update.UpdateOp", "title": "<code>UpdateOp</code>", "text": "<p>         Bases: <code>TensorOp</code></p> <p>This class performs updates to a model's weights based on the loss.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>Model instance compiled by fe.build.</p> required <code>loss_name</code> <code>str</code> <p>The name of loss.</p> required <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'train'</code> Source code in <code>fastestimator\\fastestimator\\op\\tensorop\\model\\update.py</code> <pre><code>class UpdateOp(TensorOp):\n\"\"\"This class performs updates to a model's weights based on the loss.\n    Args:\n        model: Model instance compiled by fe.build.\n        loss_name: The name of loss.\n        mode: What mode(s) to execute this Op in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nloss_name: str,\nmode: Union[None, str, Iterable[str]] = \"train\"):\nsuper().__init__(inputs=loss_name, outputs=None, mode=mode)\nself.model = model\nself.retain_graph = False\nself.weight_decay = isinstance(self.model, tf.keras.Model) and self.model.losses\nif not hasattr(self.model, \"loss_name\"):\nself.model.loss_name = {loss_name}\nelse:\nself.model.loss_name.add(loss_name)\ndef forward(self, data: Union[Tensor, List[Tensor]], state: Dict[str, Any]):\nif not state[\"warmup\"]:\nif self.weight_decay:\ndata = data + tf.reduce_sum(self.model.losses)\nupdate_model(self.model, data, tape=state['tape'], retain_graph=self.retain_graph)\n</code></pre>"}, {"location": "fastestimator/schedule/lr_shedule.html", "title": "lr_shedule", "text": ""}, {"location": "fastestimator/schedule/lr_shedule.html#fastestimator.fastestimator.schedule.lr_shedule.cosine_decay", "title": "<code>cosine_decay</code>", "text": "<p>Learning rate cosine decay function (using half of cosine curve).</p> <p>This method is useful for scheduling learning rates which oscillate over time: <pre><code>s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>int</code> <p>The current step or epoch during training starting from 1.</p> required <code>cycle_length</code> <code>int</code> <p>The decay cycle length.</p> required <code>init_lr</code> <code>float</code> <p>Initial learning rate to decay from.</p> required <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <code>start</code> <code>int</code> <p>The step or epoch to start the decay schedule.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>lr</code> <p>learning rate given current step or epoch.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\lr_shedule.py</code> <pre><code>def cosine_decay(time: int, cycle_length: int, init_lr: float, min_lr: float = 1e-6, start: int = 1):\n\"\"\"Learning rate cosine decay function (using half of cosine curve).\n    This method is useful for scheduling learning rates which oscillate over time:\n    ```python\n    s = fe.schedule.LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])\n    ```\n    Args:\n        time: The current step or epoch during training starting from 1.\n        cycle_length: The decay cycle length.\n        init_lr: Initial learning rate to decay from.\n        min_lr: Minimum learning rate.\n        start: The step or epoch to start the decay schedule.\n    Returns:\n        lr: learning rate given current step or epoch.\n    \"\"\"\nif time &lt; start:\nlr = init_lr\nelse:\nstep_in_cycle = (time - start) % cycle_length / cycle_length\nlr = (init_lr - min_lr) / 2 * math.cos(step_in_cycle * math.pi) + (init_lr + min_lr) / 2\nreturn lr\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html", "title": "schedule", "text": ""}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.EpochScheduler", "title": "<code>EpochScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which selects entries based on a specified epoch mapping.</p> <p>This can be useful for making networks grow over time, or to use more challenging data augmentation as training progresses.</p> <pre><code>s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"a\"\ns.get_current_value(epoch=3)  # \"b\"\ns.get_current_value(epoch=4)  # None\ns.get_current_value(epoch=99)  # None\ns.get_current_value(epoch=100)  # \"c\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>epoch_dict</code> <code>Dict[int, T]</code> <p>A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key will be used to determine which element to return. None values may be used to cause nothing to happen for a particular epoch.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>epoch_dict</code> is of the wrong type, or contains invalid keys.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>class EpochScheduler(Scheduler[T]):\n\"\"\"A scheduler which selects entries based on a specified epoch mapping.\n    This can be useful for making networks grow over time, or to use more challenging data augmentation as training\n    progresses.\n    ```python\n    s = fe.schedule.EpochScheduler({1:\"a\", 3:\"b\", 4:None, 100: \"c\"})\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"a\"\n    s.get_current_value(epoch=3)  # \"b\"\n    s.get_current_value(epoch=4)  # None\n    s.get_current_value(epoch=99)  # None\n    s.get_current_value(epoch=100)  # \"c\"\n    ```\n    Args:\n        epoch_dict: A mapping from epoch -&gt; element. For epochs in between keys in the dictionary, the closest prior key\n            will be used to determine which element to return. None values may be used to cause nothing to happen for a\n            particular epoch.\n    Raises:\n        AssertionError: If the `epoch_dict` is of the wrong type, or contains invalid keys.\n    \"\"\"\ndef __init__(self, epoch_dict: Dict[int, T]) -&gt; None:\nassert isinstance(epoch_dict, dict), \"must provide dictionary as epoch_dict\"\nself.epoch_dict = epoch_dict\nself.keys = sorted(self.epoch_dict)\nfor key in self.keys:\nassert isinstance(key, int), \"found non-integer key: {}\".format(key)\nassert key &gt;= 1, \"found non-positive key: {}\".format(key)\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\nif epoch in self.keys:\nvalue = self.epoch_dict[epoch]\nelse:\nlast_key = self._get_last_key(epoch)\nif last_key is None:\nvalue = None\nelse:\nvalue = self.epoch_dict[last_key]\nreturn value\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn list(self.epoch_dict.values())\ndef _get_last_key(self, epoch: int) -&gt; Union[int, None]:\n\"\"\"Find the nearest prior key to the given epoch.\n        Args:\n            epoch: The current target epoch.\n        Returns:\n            The largest epoch number &lt;= the given `epoch` that is in the `epoch_dict`.\n        \"\"\"\nlast_key = None\nfor key in self.keys:\nif key &gt; epoch:\nbreak\nlast_key = key\nreturn last_key\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.RepeatScheduler", "title": "<code>RepeatScheduler</code>", "text": "<p>         Bases: <code>Scheduler[T]</code></p> <p>A scheduler which repeats a collection of entries one after another every epoch.</p> <p>One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.</p> <pre><code>s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\ns.get_current_value(epoch=1)  # \"a\"\ns.get_current_value(epoch=2)  # \"b\"\ns.get_current_value(epoch=3)  # \"c\"\ns.get_current_value(epoch=4)  # \"a\"\ns.get_current_value(epoch=5)  # \"b\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>repeat_list</code> <code>List[Optional[T]]</code> <p>What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>repeat_list</code> is not a List.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>class RepeatScheduler(Scheduler[T]):\n\"\"\"A scheduler which repeats a collection of entries one after another every epoch.\n    One case where this class would be useful is if you want to perform one version of an Op on even epochs, and a\n    different version on odd epochs. None values can be used to achieve an end result of skipping an Op every so often.\n    ```python\n    s = fe.schedule.RepeatScheduler([\"a\", \"b\", \"c\"])\n    s.get_current_value(epoch=1)  # \"a\"\n    s.get_current_value(epoch=2)  # \"b\"\n    s.get_current_value(epoch=3)  # \"c\"\n    s.get_current_value(epoch=4)  # \"a\"\n    s.get_current_value(epoch=5)  # \"b\"\n    ```\n    Args:\n        repeat_list: What elements to cycle between every epoch. Note that epochs start counting from 1. To have nothing\n        happen for a particular epoch, None values may be used.\n    Raises:\n        AssertionError: If `repeat_list` is not a List.\n    \"\"\"\ndef __init__(self, repeat_list: List[Optional[T]]) -&gt; None:\nassert isinstance(repeat_list, List), \"must provide a list as input of RepeatSchedule\"\nself.repeat_list = repeat_list\nself.cycle_length = len(repeat_list)\nassert self.cycle_length &gt; 1, \"list length must be greater than 1\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n# epoch-1 since the training epoch is 1-indexed rather than 0-indexed.\nreturn self.repeat_list[(epoch - 1) % self.cycle_length]\ndef get_all_values(self) -&gt; List[Optional[T]]:\nreturn self.repeat_list\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler", "title": "<code>Scheduler</code>", "text": "<p>         Bases: <code>Generic[T]</code></p> <p>A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>class Scheduler(Generic[T]):\n\"\"\"A class which can wrap things like Datasets and Ops to make their behavior epoch-dependent.\n    \"\"\"\ndef get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n        Args:\n            epoch: The current epoch.\n        Returns:\n            The element from the Scheduler to be used at the given `epoch`. This value might be None.\n        \"\"\"\nraise NotImplementedError\ndef get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n        Returns:\n            A list of all the values stored in the `Scheduler`. This may contain None values.\n        \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_all_values", "title": "<code>get_all_values</code>", "text": "<p>Get a list of all the possible values stored in the <code>Scheduler</code>.</p> <p>Returns:</p> Type Description <code>List[Optional[T]]</code> <p>A list of all the values stored in the <code>Scheduler</code>. This may contain None values.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_all_values(self) -&gt; List[Optional[T]]:\n\"\"\"Get a list of all the possible values stored in the `Scheduler`.\n    Returns:\n        A list of all the values stored in the `Scheduler`. This may contain None values.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.Scheduler.get_current_value", "title": "<code>get_current_value</code>", "text": "<p>Fetch whichever of the <code>Scheduler</code>s elements is appropriate based on the current epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch.</p> required <p>Returns:</p> Type Description <code>Optional[T]</code> <p>The element from the Scheduler to be used at the given <code>epoch</code>. This value might be None.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_value(self, epoch: int) -&gt; Optional[T]:\n\"\"\"Fetch whichever of the `Scheduler`s elements is appropriate based on the current epoch.\n    Args:\n        epoch: The current epoch.\n    Returns:\n        The element from the Scheduler to be used at the given `epoch`. This value might be None.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_current_items", "title": "<code>get_current_items</code>", "text": "<p>Select items which should be executed for given mode and epoch.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Iterable[Union[T, Scheduler[T]]]</code> <p>A list of possible items or Schedulers of items to choose from.</p> required <code>run_modes</code> <code>Optional[Union[str, Iterable[str]]]</code> <p>The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of all modes will be returned.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>The desired execution epoch. If None, items across all epochs will be returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[T]</code> <p>The items which should be executed.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_current_items(items: Iterable[Union[T, Scheduler[T]]],\nrun_modes: Optional[Union[str, Iterable[str]]] = None,\nepoch: Optional[int] = None) -&gt; List[T]:\n\"\"\"Select items which should be executed for given mode and epoch.\n    Args:\n        items: A list of possible items or Schedulers of items to choose from.\n        run_modes: The desired execution mode. One or more of \"train\", \"eval\", \"test\", or \"infer\". If None, items of\n            all modes will be returned.\n        epoch: The desired execution epoch. If None, items across all epochs will be returned.\n    Returns:\n        The items which should be executed.\n    \"\"\"\nselected_items = []\nrun_modes = to_set(run_modes)\nfor item in items:\nif isinstance(item, Scheduler):\nif epoch is None:\nitem = item.get_all_values()\nelse:\nitem = [item.get_current_value(epoch)]\nelse:\nitem = [item]\nfor item_ in item:\nif item_ and (not run_modes or not hasattr(item_, \"mode\") or not item_.mode\nor item_.mode.intersection(run_modes)):\nselected_items.append(item_)\nreturn selected_items\n</code></pre>"}, {"location": "fastestimator/schedule/schedule.html#fastestimator.fastestimator.schedule.schedule.get_signature_epochs", "title": "<code>get_signature_epochs</code>", "text": "<p>Find all epochs of changes due to schedulers.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>List[Any]</code> <p>List of items to scan from.</p> required <code>total_epochs</code> <code>int</code> <p>The maximum epoch number to consider when searching for signature epochs.</p> required <code>mode</code> <code>Optional[str]</code> <p>Current execution mode. If None, all execution modes will be considered.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[int]</code> <p>The epoch numbers of changes.</p> Source code in <code>fastestimator\\fastestimator\\schedule\\schedule.py</code> <pre><code>def get_signature_epochs(items: List[Any], total_epochs: int, mode: Optional[str] = None) -&gt; List[int]:\n\"\"\"Find all epochs of changes due to schedulers.\n    Args:\n        items: List of items to scan from.\n        total_epochs: The maximum epoch number to consider when searching for signature epochs.\n        mode: Current execution mode. If None, all execution modes will be considered.\n    Returns:\n        The epoch numbers of changes.\n    \"\"\"\nunique_configs = []\nsignature_epochs = []\nfor epoch in range(1, total_epochs + 1):\nepoch_config = get_current_items(items, run_modes=mode, epoch=epoch)\nif epoch_config not in unique_configs:\nunique_configs.append(epoch_config)\nsignature_epochs.append(epoch)\nreturn signature_epochs\n</code></pre>"}, {"location": "fastestimator/summary/summary.html", "title": "summary", "text": ""}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary", "title": "<code>Summary</code>", "text": "<p>A summary object that records training history.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the experiment. If None then experiment results will be ignored.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>class Summary:\n\"\"\"A summary object that records training history.\n    Args:\n        name: Name of the experiment. If None then experiment results will be ignored.\n    \"\"\"\ndef __init__(self, name: Optional[str]) -&gt; None:\nself.name = name\nself.history = defaultdict(lambda: defaultdict(dict))  # {mode: {key: {step: value}}}\ndef merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n        Args:\n            other: Other `summary` object to be merged.\n        \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\ndef __bool__(self) -&gt; bool:\n\"\"\"Whether training history should be recorded.\n        Returns:\n            True iff this `Summary` has a non-None name.\n        \"\"\"\nreturn bool(self.name)\n</code></pre>"}, {"location": "fastestimator/summary/summary.html#fastestimator.fastestimator.summary.summary.Summary.merge", "title": "<code>merge</code>", "text": "<p>Merge another <code>Summary</code> into this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Summary</code> <p>Other <code>summary</code> object to be merged.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\summary.py</code> <pre><code>def merge(self, other: 'Summary'):\n\"\"\"Merge another `Summary` into this one.\n    Args:\n        other: Other `summary` object to be merged.\n    \"\"\"\nfor mode, sub in other.history.items():\nfor key, val in sub.items():\nself.history[mode][key].update(val)\n</code></pre>"}, {"location": "fastestimator/summary/system.html", "title": "system", "text": ""}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System", "title": "<code>System</code>", "text": "<p>A class which tracks state information while the fe.Estimator is running.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>BaseNetwork</code> <p>The network instance being used by the current fe.Estimator.</p> required <code>mode</code> <code>Optional[str]</code> <p>The current execution mode (or None for warmup).</p> <code>None</code> <code>num_devices</code> <code>int</code> <p>How many GPUs are available for training.</p> <code>torch.cuda.device_count()</code> <code>log_steps</code> <code>Optional[int]</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>None</code> <code>total_epochs</code> <code>int</code> <p>How many epochs training is expected to run for.</p> <code>0</code> <code>max_train_steps_per_epoch</code> <code>Optional[int]</code> <p>Whether training epochs will be cut short after N steps (or use None if they will run to completion)</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>mode</code> <p>What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.</p> <code>global_step</code> <code>Optional[int]</code> <p>How many training steps have elapsed.</p> <code>num_devices</code> <p>How many GPUs are available for training.</p> <code>log_steps</code> <p>Log every n steps (0 to disable train logging, None to disable all logging).</p> <code>total_epochs</code> <p>How many epochs training is expected to run for.</p> <code>epoch_idx</code> <code>Optional[int]</code> <p>The current epoch index for the training (starting from 1).</p> <code>batch_idx</code> <p>The current batch index within an epoch (starting from 1).</p> <code>stop_training</code> <p>A flag to signal that training should abort.</p> <code>network</code> <p>A reference to the network being used this epoch</p> <code>max_train_steps_per_epoch</code> <p>Training will complete after n steps even if loader is not yet exhausted.</p> <code>max_eval_steps_per_epoch</code> <p>Evaluation will complete after n steps even if loader is not yet exhausted.</p> <code>summary</code> <p>An object to write experiment results to.</p> <code>experiment_time</code> <p>A timestamp indicating when this model was trained.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>class System:\n\"\"\"A class which tracks state information while the fe.Estimator is running.\n    Args:\n        network: The network instance being used by the current fe.Estimator.\n        mode: The current execution mode (or None for warmup).\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        max_train_steps_per_epoch: Whether training epochs will be cut short after N steps (or use None if they will run to\n            completion)\n    Attributes:\n        mode: What is the current execution mode of the estimator ('train', 'eval', 'test'), None if warmup.\n        global_step: How many training steps have elapsed.\n        num_devices: How many GPUs are available for training.\n        log_steps: Log every n steps (0 to disable train logging, None to disable all logging).\n        total_epochs: How many epochs training is expected to run for.\n        epoch_idx: The current epoch index for the training (starting from 1).\n        batch_idx: The current batch index within an epoch (starting from 1).\n        stop_training: A flag to signal that training should abort.\n        network: A reference to the network being used this epoch\n        max_train_steps_per_epoch: Training will complete after n steps even if loader is not yet exhausted.\n        max_eval_steps_per_epoch: Evaluation will complete after n steps even if loader is not yet exhausted.\n        summary: An object to write experiment results to.\n        experiment_time: A timestamp indicating when this model was trained.\n    \"\"\"\nmode: Optional[str]\nglobal_step: Optional[int]\nnum_devices: int\nlog_steps: Optional[int]\ntotal_epochs: int\nepoch_idx: Optional[int]\nbatch_idx: Optional[int]\nstop_training: bool\nnetwork: BaseNetwork\nmax_train_steps_per_epoch: Optional[int]\nmax_eval_steps_per_epoch: Optional[int]\nsummary: Summary\nexperiment_time: str\ndef __init__(self,\nnetwork: BaseNetwork,\nmode: Optional[str] = None,\nnum_devices: int = torch.cuda.device_count(),\nlog_steps: Optional[int] = None,\ntotal_epochs: int = 0,\nmax_train_steps_per_epoch: Optional[int] = None,\nmax_eval_steps_per_epoch: Optional[int] = None) -&gt; None:\nself.network = network\nself.mode = mode\nself.num_devices = num_devices\nself.log_steps = log_steps\nself.total_epochs = total_epochs\nself.batch_idx = None\nself.max_train_steps_per_epoch = max_train_steps_per_epoch\nself.max_eval_steps_per_epoch = max_eval_steps_per_epoch\nself.stop_training = False\nself.summary = Summary(None)\nself.experiment_time = \"\"\nself._initialize_state()\ndef _initialize_state(self) -&gt; None:\n\"\"\"Initialize the training state.\n        \"\"\"\nself.global_step = None\nself.epoch_idx = 0\ndef load_state(self, json_path) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            json_path: The json file path to load from.\n        \"\"\"\nwith open(json_path, 'r') as fp:\nstate = json.load(fp)\nself.epoch_idx = state[\"epoch_idx\"]\nself.global_step = state[\"global_step\"]\ndef save_state(self, json_path) -&gt; None:\n\"\"\"Load training state.\n        Args:\n            json_path: The json file path to save to.\n        \"\"\"\n# TODO \"summary\" and \"experiment_time\" needs to be saved in the future\nstate = {\"epoch_idx\": self.epoch_idx, \"global_step\": self.global_step}\nwith open(json_path, 'w') as fp:\njson.dump(state, fp, indent=4)\ndef update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n        \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\ndef update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n        \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\ndef reset(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n        Args:\n            summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n        \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name)\ndef reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n        Args:\n            summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n        \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\ndef write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n        Args:\n            key: The key to write into the summary object.\n            value: The value to write into the summary object.\n        \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.load_state", "title": "<code>load_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <p>The json file path to load from.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def load_state(self, json_path) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        json_path: The json file path to load from.\n    \"\"\"\nwith open(json_path, 'r') as fp:\nstate = json.load(fp)\nself.epoch_idx = state[\"epoch_idx\"]\nself.global_step = state[\"global_step\"]\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset", "title": "<code>reset</code>", "text": "<p>Reset the current <code>System</code> for a new round of training, including a new <code>Summary</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. The <code>Summary</code> object will store information iff name is not None.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Reset the current `System` for a new round of training, including a new `Summary` object.\n    Args:\n        summary_name: The name of the experiment. The `Summary` object will store information iff name is not None.\n    \"\"\"\nself.experiment_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"train\"\nself._initialize_state()\nself.batch_idx = None\nself.stop_training = False\nself.summary = Summary(summary_name)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.reset_for_test", "title": "<code>reset_for_test</code>", "text": "<p>Partially reset the current <code>System</code> object for a new round of testing.</p> <p>Parameters:</p> Name Type Description Default <code>summary_name</code> <code>Optional[str]</code> <p>The name of the experiment. If not provided, the system will re-use the previous summary name.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def reset_for_test(self, summary_name: Optional[str] = None) -&gt; None:\n\"\"\"Partially reset the current `System` object for a new round of testing.\n    Args:\n        summary_name: The name of the experiment. If not provided, the system will re-use the previous summary name.\n    \"\"\"\nself.experiment_time = self.experiment_time or datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\nself.mode = \"test\"\nif not self.stop_training:\nself.epoch_idx = self.total_epochs\nself.stop_training = False\nself.summary.name = summary_name or self.summary.name  # Keep old experiment name if new one not provided\nself.summary.history.pop('test', None)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.save_state", "title": "<code>save_state</code>", "text": "<p>Load training state.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <p>The json file path to save to.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def save_state(self, json_path) -&gt; None:\n\"\"\"Load training state.\n    Args:\n        json_path: The json file path to save to.\n    \"\"\"\n# TODO \"summary\" and \"experiment_time\" needs to be saved in the future\nstate = {\"epoch_idx\": self.epoch_idx, \"global_step\": self.global_step}\nwith open(json_path, 'w') as fp:\njson.dump(state, fp, indent=4)\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_batch_idx", "title": "<code>update_batch_idx</code>", "text": "<p>Increment the current <code>batch_idx</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_batch_idx(self) -&gt; None:\n\"\"\"Increment the current `batch_idx`.\n    \"\"\"\nif self.batch_idx is None:\nself.batch_idx = 1\nelse:\nself.batch_idx += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.update_global_step", "title": "<code>update_global_step</code>", "text": "<p>Increment the current <code>global_step</code>.</p> Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def update_global_step(self) -&gt; None:\n\"\"\"Increment the current `global_step`.\n    \"\"\"\nif self.global_step is None:\nself.global_step = 1\nelse:\nself.global_step += 1\n</code></pre>"}, {"location": "fastestimator/summary/system.html#fastestimator.fastestimator.summary.system.System.write_summary", "title": "<code>write_summary</code>", "text": "<p>Write an entry into the <code>Summary</code> object (iff the experiment was named).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to write into the summary object.</p> required <code>value</code> <code>Any</code> <p>The value to write into the summary object.</p> required Source code in <code>fastestimator\\fastestimator\\summary\\system.py</code> <pre><code>def write_summary(self, key: str, value: Any) -&gt; None:\n\"\"\"Write an entry into the `Summary` object (iff the experiment was named).\n    Args:\n        key: The key to write into the summary object.\n        value: The value to write into the summary object.\n    \"\"\"\nif self.summary:\nself.summary.history[self.mode][key][self.global_step or 0] = value\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html", "title": "log_parse", "text": ""}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_dir", "title": "<code>parse_log_dir</code>", "text": "<p>A function which will gather all log files within a given folder and pass them along for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>The path to a directory containing log files.</p> required <code>log_extension</code> <code>str</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>recursive_search</code> <code>bool</code> <p>Whether to recursively search sub-directories for log files.</p> <code>False</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>1</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_dir(dir_path: str,\nlog_extension: str = '.txt',\nrecursive_search: bool = False,\nsmooth_factor: float = 1,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\nshare_legend: bool = True,\npretty_names: bool = False) -&gt; None:\n\"\"\"A function which will gather all log files within a given folder and pass them along for visualization.\n    Args:\n        dir_path: The path to a directory containing log files.\n        log_extension: The extension of the log files.\n        recursive_search: Whether to recursively search sub-directories for log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n    \"\"\"\nloader = DirDataset(root_dir=dir_path, file_extension=log_extension, recursive_search=recursive_search)\nfile_paths = list(map(lambda d: d['x'], loader.data.values()))\nparse_log_files(file_paths,\nlog_extension,\nsmooth_factor,\nsave,\nsave_path,\nignore_metrics,\nshare_legend,\npretty_names)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_file", "title": "<code>parse_log_file</code>", "text": "<p>A function which will parse log files into a dictionary of metrics.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to a log file.</p> required <code>file_extension</code> <code>str</code> <p>The extension of the log file.</p> required <p>Returns:</p> Type Description <code>Summary</code> <p>An experiment summarizing the given log file.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_file(file_path: str, file_extension: str) -&gt; Summary:\n\"\"\"A function which will parse log files into a dictionary of metrics.\n    Args:\n        file_path: The path to a log file.\n        file_extension: The extension of the log file.\n    Returns:\n        An experiment summarizing the given log file.\n    \"\"\"\n# TODO: need to handle multi-line output like confusion matrix\nexperiment = Summary(strip_suffix(os.path.split(file_path)[1].strip(), file_extension))\nwith open(file_path) as file:\nfor line in file:\nmode = None\nif line.startswith(\"FastEstimator-Train\") or line.startswith(\"FastEstimator-Finish\"):\nmode = \"train\"\nelif line.startswith(\"FastEstimator-Eval\"):\nmode = \"eval\"\nelif line.startswith(\"FastEstimator-Test\"):\nmode = \"test\"\nif mode is None:\ncontinue\nparsed_line = re.findall(r\"([^:^;\\s]+):[\\s]*([-]?[0-9]+[.]?[0-9]*);\", line)\nstep = parsed_line[0]\nassert step[0] == \"step\", \\\n                \"Log file (%s) seems to be missing step information, or step is not listed first\" % file\nfor metric in parsed_line[1:]:\nexperiment.history[mode][metric[0]].update({int(step[1]): float(metric[1])})\nreturn experiment\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_parse.html#fastestimator.fastestimator.summary.logs.log_parse.parse_log_files", "title": "<code>parse_log_files</code>", "text": "<p>Parse one or more log files for graphing.</p> <p>This function which will iterate through the given log file paths, parse them to extract metrics, remove any metrics which are blacklisted, and then pass the necessary information on the graphing function.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>A list of paths to various log files.</p> required <code>log_extension</code> <code>Optional[str]</code> <p>The extension of the log files.</p> <code>'.txt'</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>save</code> <code>bool</code> <p>Whether to save (True) or display (False) the generated graph.</p> <code>False</code> <code>save_path</code> <code>Optional[str]</code> <p>Where to save the image if save is true. Defaults to dir_path if not provided.</p> <code>None</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics within the log files which will not be visualized.</p> <code>None</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no log files are provided.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_parse.py</code> <pre><code>def parse_log_files(file_paths: List[str],\nlog_extension: Optional[str] = '.txt',\nsmooth_factor: float = 0,\nsave: bool = False,\nsave_path: Optional[str] = None,\nignore_metrics: Optional[Set[str]] = None,\nshare_legend: bool = True,\npretty_names: bool = False) -&gt; None:\n\"\"\"Parse one or more log files for graphing.\n    This function which will iterate through the given log file paths, parse them to extract metrics, remove any\n    metrics which are blacklisted, and then pass the necessary information on the graphing function.\n    Args:\n        file_paths: A list of paths to various log files.\n        log_extension: The extension of the log files.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        save: Whether to save (True) or display (False) the generated graph.\n        save_path: Where to save the image if save is true. Defaults to dir_path if not provided.\n        ignore_metrics: Any metrics within the log files which will not be visualized.\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n    Raises:\n        AssertionError: If no log files are provided.\n    \"\"\"\nif file_paths is None or len(file_paths) &lt; 1:\nraise AssertionError(\"must provide at least one log file\")\nif save and save_path is None:\nsave_path = file_paths[0]\nexperiments = []\nfor file_path in file_paths:\nexperiments.append(parse_log_file(file_path, log_extension))\nvisualize_logs(experiments,\nsave_path=save_path,\nsmooth_factor=smooth_factor,\nshare_legend=share_legend,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics)\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html", "title": "log_plot", "text": ""}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.plot_logs", "title": "<code>plot_logs</code>", "text": "<p>A function which will plot experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any keys to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of keys to include during plotting. If None then all will be included.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>The handle of the pyplot figure.</p> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def plot_logs(experiments: List[Summary],\nsmooth_factor: float = 0,\nshare_legend: bool = True,\nignore_metrics: Optional[Set[str]] = None,\npretty_names: bool = False,\ninclude_metrics: Optional[Set[str]] = None) -&gt; plt.Figure:\n\"\"\"A function which will plot experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any keys to ignore during plotting.\n        include_metrics: A whitelist of keys to include during plotting. If None then all will be included.\n    Returns:\n        The handle of the pyplot figure.\n    \"\"\"\nexperiments = to_list(experiments)\nignore_keys = ignore_metrics or set()\nignore_keys = to_set(ignore_keys)\nignore_keys |= {'epoch', 'progress', 'total_train_steps'}\ninclude_keys = to_set(include_metrics)\n# TODO: epoch should be indicated on the axis (top x axis?)\n# TODO: figure out how ignore_metrics should interact with mode\nmax_time = 0\nmetric_keys = set()\nfor experiment in experiments:\nhistory = experiment.history\nfor mode, metrics in history.items():\nfor key, value in metrics.items():\nif key in ignore_keys:\ncontinue\nif include_keys and key not in include_keys:\nignore_keys.add(key)\ncontinue\nif any(map(lambda x: isinstance(x[1], np.ndarray) and x[1].ndim &gt; 0, value.items())):\nignore_keys.add(key)\ncontinue  # TODO: nd array not currently supported. maybe in future visualize as heat map?\nif value.keys():\nmax_time = max(max_time, max(value.keys()))\nmetric_keys.add(\"{}: {}\".format(mode, key))\nmetric_list = sorted(list(metric_keys))  # Sort the metrics alphabetically for consistency\nnum_metrics = len(metric_list)\nnum_experiments = len(experiments)\nif num_metrics == 0:\nreturn plt.subplots(111)[0]\n# map the metrics into an n x n grid, then remove any extra rows. Final grid will be m x n with m &lt;= n\nnum_cols = math.ceil(math.sqrt(num_metrics))\nmetric_grid_location = {key: (idx // num_cols, idx % num_cols) for (idx, key) in enumerate(metric_list)}\nnum_rows = math.ceil(num_metrics / num_cols)\nsns.set_context('paper')\nfig, axs = plt.subplots(num_rows, num_cols, sharex='all', figsize=(4 * num_cols, 2.8 * num_rows))\n# If only one row, need to re-format the axs object for consistency. Likewise for columns\nif num_rows == 1:\naxs = [axs]\nif num_cols == 1:\naxs = [axs]\nfor metric in metric_grid_location.keys():\naxis = axs[metric_grid_location[metric][0]][metric_grid_location[metric][1]]\naxis.set_title(metric if not pretty_names else prettify_metric_name(metric))\naxis.ticklabel_format(axis='y', style='sci', scilimits=(-2, 3))\naxis.grid(linestyle='--')\naxis.spines['top'].set_visible(False)\naxis.spines['right'].set_visible(False)\naxis.spines['bottom'].set_visible(False)\naxis.spines['left'].set_visible(False)\naxis.tick_params(bottom=False, left=False)\nfor i in range(num_cols):\naxs[num_rows - 1][i].set_xlabel('Steps')\n# some of the columns in the last row might be unused, so disable them\nlast_column_idx = num_cols - (num_rows * num_cols - num_metrics) - 1\nfor i in range(last_column_idx + 1, num_cols):\naxs[num_rows - 1][i].axis('off')\naxs[num_rows - 2][i].set_xlabel('Steps')\naxs[num_rows - 2][i].xaxis.set_tick_params(which='both', labelbottom=True)\ncolors = sns.hls_palette(n_colors=num_experiments,\ns=0.95) if num_experiments &gt; 10 else sns.color_palette(\"colorblind\")\nhandles = []\nlabels = []\nbar_counter = defaultdict(lambda: 0)\nfor (color_idx, experiment) in enumerate(experiments):\nlabels.append(experiment.name)\nmetrics = {\n\"{}: {}\".format(mode, key): val\nfor mode,\nsub in experiment.history.items() for key,\nval in sub.items() if key not in ignore_keys\n}\nfor (idx, (metric, value)) in enumerate(metrics.items()):\ndata = np.array(list(value.items()))\nif len(data) == 1:\ny = data[0][1]\nif isinstance(y, str):\nvals = [float(x) for x in re.findall(r'\\d+\\.?\\d+', y)]\nif len(vals) == 1:\ny = vals[0]\nwidth = max(10, max_time // 10)\nx = max_time // 2 + (2 * (bar_counter[metric] % 2) - 1) * width * math.ceil(bar_counter[metric] / 2)\nln = axs[metric_grid_location[metric][0]][metric_grid_location[metric][1]].bar(\nx=x, height=y, color=colors[color_idx], label=experiment.name, width=width)\nbar_counter[metric] += 1\nelse:\ny = data[:, 1] if smooth_factor == 0 else gaussian_filter1d(data[:, 1], sigma=smooth_factor)\nln = axs[metric_grid_location[metric][0]][metric_grid_location[metric][1]].plot(\ndata[:, 0], y, color=colors[color_idx], label=experiment.name, linewidth=1.5)\nif idx == 0:\nhandles.append(ln[0])\nplt.tight_layout()\nif len(labels) &gt; 1 or labels[0]:\nif share_legend and num_rows &gt; 1:\nif last_column_idx == num_cols - 1:\nfig.subplots_adjust(bottom=0.15)\nfig.legend(handles, labels, loc='lower center', ncol=num_cols + 1)\nelse:\naxs[num_rows - 1][last_column_idx + 1].legend(handles, labels, loc='center', fontsize='large')\nelse:\nfor i in range(num_rows):\nfor j in range(num_cols):\nif i == num_rows - 1 and j &gt; last_column_idx:\nbreak\naxs[i][j].legend(loc='best', fontsize='small')\nreturn fig\n</code></pre>"}, {"location": "fastestimator/summary/logs/log_plot.html#fastestimator.fastestimator.summary.logs.log_plot.visualize_logs", "title": "<code>visualize_logs</code>", "text": "<p>A function which will save or display experiment histories for comparison viewing / analysis.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>List[Summary]</code> <p>Experiment(s) to plot.</p> required <code>save_path</code> <code>str</code> <p>The path where the figure should be saved, or None to display the figure to the screen.</p> <code>None</code> <code>smooth_factor</code> <code>float</code> <p>A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).</p> <code>0</code> <code>share_legend</code> <code>bool</code> <p>Whether to have one legend across all graphs (True) or one legend per graph (False).</p> <code>True</code> <code>pretty_names</code> <code>bool</code> <p>Whether to modify the metric names in graph titles (True) or leave them alone (False).</p> <code>False</code> <code>ignore_metrics</code> <code>Optional[Set[str]]</code> <p>Any metrics to ignore during plotting.</p> <code>None</code> <code>include_metrics</code> <code>Optional[Set[str]]</code> <p>A whitelist of metric keys (None whitelists all keys).</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\summary\\logs\\log_plot.py</code> <pre><code>def visualize_logs(experiments: List[Summary],\nsave_path: str = None,\nsmooth_factor: float = 0,\nshare_legend: bool = True,\npretty_names: bool = False,\nignore_metrics: Optional[Set[str]] = None,\ninclude_metrics: Optional[Set[str]] = None):\n\"\"\"A function which will save or display experiment histories for comparison viewing / analysis.\n    Args:\n        experiments: Experiment(s) to plot.\n        save_path: The path where the figure should be saved, or None to display the figure to the screen.\n        smooth_factor: A non-negative float representing the magnitude of gaussian smoothing to apply (zero for none).\n        share_legend: Whether to have one legend across all graphs (True) or one legend per graph (False).\n        pretty_names: Whether to modify the metric names in graph titles (True) or leave them alone (False).\n        ignore_metrics: Any metrics to ignore during plotting.\n        include_metrics: A whitelist of metric keys (None whitelists all keys).\n    \"\"\"\nplot_logs(experiments,\nsmooth_factor=smooth_factor,\nshare_legend=share_legend,\npretty_names=pretty_names,\nignore_metrics=ignore_metrics,\ninclude_metrics=include_metrics)\nif save_path is None:\nplt.show()\nelse:\nsave_path = os.path.dirname(save_path)\nif save_path == \"\":\nsave_path = \".\"\nos.makedirs(save_path, exist_ok=True)\nsave_file = os.path.join(save_path, 'parse_logs.png')\nprint(\"Saving to {}\".format(save_file))\nplt.savefig(save_file, dpi=300, bbox_inches=\"tight\")\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html", "title": "nightly_util", "text": ""}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_apphub_source_dir_path", "title": "<code>get_apphub_source_dir_path</code>", "text": "<p>Get the absolute path to the apphub folder containing the files to be tested by the <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the corresponding apphub directory.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_apphub_source_dir_path(working_file: str) -&gt; str:\n\"\"\"Get the absolute path to the apphub folder containing the files to be tested by the `working_file`.\n    Args:\n        working_file: The absolute path to a test file.\n    Returns:\n        The absolute path to the corresponding apphub directory.\n    \"\"\"\napphub_path = get_uncle_path(\"apphub\", working_file)\nrelative_dir_path = get_relative_path(\"apphub_scripts\", working_file)\nsource_dir_path = os.path.join(apphub_path, relative_dir_path)\nreturn source_dir_path\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_relative_path", "title": "<code>get_relative_path</code>", "text": "<p>Convert an absolute path into a relative path within the parent folder.</p> <p>Parameters:</p> Name Type Description Default <code>parent_dir</code> <code>str</code> <p>A parent folder</p> required <code>working_file</code> <code>str</code> <p>The absolute path to a test file.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The relative path to the test file within the parent_dir folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> is not located within the parent_dir folder.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_relative_path(parent_dir: str, working_file: str) -&gt; str:\n\"\"\"Convert an absolute path into a relative path within the parent folder.\n    Args:\n        parent_dir: A parent folder\n        working_file: The absolute path to a test file.\n    Returns:\n        The relative path to the test file within the parent_dir folder.\n    Raises:\n        OSError: If the `working_file` is not located within the parent_dir folder.\n    \"\"\"\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nsplit = current_dir.split(\"{}/\".format(parent_dir))\nif len(split) == 1:\nraise OSError(\"This file need to be put inside {} directory\".format(parent_dir))\nreturn split[-1]\n</code></pre>"}, {"location": "fastestimator/test/nightly_util.html#fastestimator.fastestimator.test.nightly_util.get_uncle_path", "title": "<code>get_uncle_path</code>", "text": "<p>Find the path to the uncle folder of <code>working_file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>uncle_dir</code> <code>str</code> <p>A target uncle folder</p> required <code>working_file</code> <code>str</code> <p>A file within the same FastEstimator repository as apphub examples.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The root path to the apphub folder.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the <code>working_file</code> does not correspond to any of the uncle paths.</p> Source code in <code>fastestimator\\fastestimator\\test\\nightly_util.py</code> <pre><code>def get_uncle_path(uncle_dir: str, working_file: str) -&gt; str:\n\"\"\"Find the path to the uncle folder of `working_file`.\n    Args:\n        uncle_dir: A target uncle folder\n        working_file: A file within the same FastEstimator repository as apphub examples.\n    Returns:\n        The root path to the apphub folder.\n    Raises:\n        OSError: If the `working_file` does not correspond to any of the uncle paths.\n    \"\"\"\nuncle_path = None\ncurrent_dir = os.path.abspath(os.path.join(working_file, \"..\"))\nwhile current_dir != \"/\":\ncurrent_dir = os.path.abspath(os.path.join(current_dir, \"..\"))\nif uncle_dir in os.listdir(current_dir):\nuncle_path = os.path.abspath(os.path.join(current_dir, uncle_dir))\nbreak\nif uncle_path is None:\nraise OSError(\"Could not find the {} directory\".format(uncle_dir))\nreturn uncle_path\n</code></pre>"}, {"location": "fastestimator/trace/trace.html", "title": "trace", "text": ""}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.EvalEssential", "title": "<code>EvalEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during evaluation.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Any keys which should be collected over the course of an eval epoch.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>class EvalEssential(Trace):\n\"\"\"A trace to collect important information during evaluation.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Any keys which should be collected over the course of an eval epoch.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(mode=\"eval\", inputs=monitor_names)\nself.eval_results = None\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.eval_results = None\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.eval_results is None:\nself.eval_results = {key: [data[key]] for key in self.inputs if key in data}\nelse:\nfor key in self.inputs:\nif key in data:\nself.eval_results[key].append(data[key])\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key, value_list in self.eval_results.items():\ndata.write_with_log(key, np.mean(np.array(value_list), axis=0))\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Logger", "title": "<code>Logger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace that prints log messages.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>class Logger(Trace):\n\"\"\"A Trace that prints log messages.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    \"\"\"\ndef __init__(self) -&gt; None:\nsuper().__init__(inputs=\"*\")\ndef on_begin(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nstart_step = 1 if not self.system.global_step else self.system.global_step\nself._print_message(\"FastEstimator-Start: step: {}; \".format(start_step), data)\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.system.log_steps and (\nself.system.global_step % self.system.log_steps == 0 or self.system.global_step == 1):\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data)\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == \"train\" and self.system.log_steps:\nself._print_message(\"FastEstimator-Train: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == \"eval\":\nself._print_message(\"FastEstimator-Eval: step: {}; \".format(self.system.global_step), data, True)\nelif self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Test: step: {}; \".format(self.system.global_step), data, True)\ndef on_end(self, data: Data) -&gt; None:\nif not self.system.mode == \"test\":\nself._print_message(\"FastEstimator-Finish: step: {}; \".format(self.system.global_step), data)\ndef _print_message(self, header: str, data: Data, log_epoch: bool = False) -&gt; None:\n\"\"\"Print a log message to the screen, and record the `data` into the `system` summary.\n        Args:\n            header: The prefix for the log message.\n            data: A collection of data to be recorded.\n            log_epoch: Whether epoch information should be included in the log message.\n        \"\"\"\nlog_message = header\nif log_epoch:\nlog_message += \"epoch: {}; \".format(self.system.epoch_idx)\nself.system.write_summary('epoch', self.system.epoch_idx)\nfor key, val in data.read_logs().items():\nval = to_number(val)\nself.system.write_summary(key, val)\nif val.size &gt; 1:\nlog_message += \"\\n{}:\\n{};\".format(key, np.array2string(val, separator=','))\nelse:\nlog_message += \"{}: {}; \".format(key, str(val))\nprint(log_message)\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace", "title": "<code>Trace</code>", "text": "<p>Trace controls the training loop. Users can use the <code>Trace</code> base class to customize their own functionality.</p> <p>Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are also given a pointer to the current <code>System</code> instance which allows access to more information as well as giving the ability to modify or even cancel training. The order of function invocations is as follows:</p> <pre><code>        Training:                                       Testing:\n\n    on_begin                                            on_begin\n        |                                                   |\n    on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n        |                          |                        |                         |\n    on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n        |                       |  |                        |                      |  |\n    on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n        |                          ^                        |                         |\n    on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n        |                          |                        |\n    on_epoch_begin (eval)          |                    on_end\n        |                          ^\n    on_batch_begin (eval) &lt;----&lt;   |\n        |                      |   |\n    on_batch_end (eval) &gt;-----^    |\n        |                          |\n    on_epoch_end (eval) &gt;----------^\n        |\n    on_end\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to read from the state dictionary as inputs.</p> <code>None</code> <code>outputs</code> <code>Union[None, str, Iterable[str]]</code> <p>A set of keys that this trace intends to write into the system buffer.</p> <code>None</code> <code>mode</code> <code>Union[None, str, Iterable[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>class Trace:\n\"\"\"Trace controls the training loop. Users can use the `Trace` base class to customize their own functionality.\n    Traces are invoked by the fe.Estimator periodically as it runs. In addition to the current data dictionary, they are\n    also given a pointer to the current `System` instance which allows access to more information as well as giving the\n    ability to modify or even cancel training. The order of function invocations is as follows:\n    ``` plot\n            Training:                                       Testing:\n        on_begin                                            on_begin\n            |                                                   |\n        on_epoch_begin (train)  &lt;------&lt;                    on_epoch_begin (test)  &lt;------&lt;\n            |                          |                        |                         |\n        on_batch_begin (train) &lt;----&lt;  |                    on_batch_begin (test) &lt;----&lt;  |\n            |                       |  |                        |                      |  |\n        on_batch_end (train) &gt;-----^   |                    on_batch_end (test) &gt;------^  |\n            |                          ^                        |                         |\n        on_epoch_end (train)           |                    on_epoch_end (test) &gt;---------^\n            |                          |                        |\n        on_epoch_begin (eval)          |                    on_end\n            |                          ^\n        on_batch_begin (eval) &lt;----&lt;   |\n            |                      |   |\n        on_batch_end (eval) &gt;-----^    |\n            |                          |\n        on_epoch_end (eval) &gt;----------^\n            |\n        on_end\n    ```\n    Args:\n        inputs: A set of keys that this trace intends to read from the state dictionary as inputs.\n        outputs: A set of keys that this trace intends to write into the system buffer.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\nsystem: System\ninputs: List[str]\noutputs: List[str]\nmode: Set[str]\ndef __init__(self,\ninputs: Union[None, str, Iterable[str]] = None,\noutputs: Union[None, str, Iterable[str]] = None,\nmode: Union[None, str, Iterable[str]] = None) -&gt; None:\nself.inputs = to_list(inputs)\nself.outputs = to_list(outputs)\nself.mode = parse_modes(to_set(mode))\ndef on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n        Args:\n            data: The current batch and prediction data, as well as any information written by prior `Traces`.\n        \"\"\"\npass\ndef on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\ndef on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n        Args:\n            data: A dictionary through which traces can communicate with each other or write values for logging.\n        \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Runs at the beginning of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each batch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_batch_end", "title": "<code>on_batch_end</code>", "text": "<p>Runs at the end of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>The current batch and prediction data, as well as any information written by prior <code>Traces</code>.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_batch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each batch.\n    Args:\n        data: The current batch and prediction data, as well as any information written by prior `Traces`.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_begin", "title": "<code>on_begin</code>", "text": "<p>Runs once at the beginning of training or testing.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_begin(self, data: Data) -&gt; None:\n\"\"\"Runs once at the beginning of training or testing.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_end", "title": "<code>on_end</code>", "text": "<p>Runs once at the end training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_end(self, data: Data) -&gt; None:\n\"\"\"Runs once at the end training.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Runs at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_begin(self, data: Data) -&gt; None:\n\"\"\"Runs at the beginning of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.Trace.on_epoch_end", "title": "<code>on_epoch_end</code>", "text": "<p>Runs at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>A dictionary through which traces can communicate with each other or write values for logging.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>def on_epoch_end(self, data: Data) -&gt; None:\n\"\"\"Runs at the end of each epoch.\n    Args:\n        data: A dictionary through which traces can communicate with each other or write values for logging.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/trace/trace.html#fastestimator.fastestimator.trace.trace.TrainEssential", "title": "<code>TrainEssential</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace to collect important information during training.</p> <p>Please don't add this trace into an estimator manually. FastEstimator will add it automatically.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_names</code> <code>Set[str]</code> <p>Which keys from the data dictionary to monitor during training.</p> required Source code in <code>fastestimator\\fastestimator\\trace\\trace.py</code> <pre><code>class TrainEssential(Trace):\n\"\"\"A trace to collect important information during training.\n    Please don't add this trace into an estimator manually. FastEstimator will add it automatically.\n    Args:\n        monitor_names: Which keys from the data dictionary to monitor during training.\n    \"\"\"\ndef __init__(self, monitor_names: Set[str]) -&gt; None:\nsuper().__init__(inputs=monitor_names, mode=\"train\", outputs=[\"steps/sec\", \"epoch_time\", \"total_time\"])\nself.elapse_times = []\nself.train_start = None\nself.epoch_start = None\nself.step_start = None\ndef on_begin(self, data: Data) -&gt; None:\nself.train_start = time.perf_counter()\ndata.write_with_log(\"num_device\", self.system.num_devices)\ndata.write_with_log(\"logging_interval\", self.system.log_steps)\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.epoch_start = time.perf_counter()\nself.step_start = time.perf_counter()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.log_steps and (self.system.global_step % self.system.log_steps == 0\nor self.system.global_step == 1):\nfor key in self.inputs:\nif key in data:\ndata.write_with_log(key, data[key])\nif self.system.global_step &gt; 1:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"steps/sec\", round(self.system.log_steps / np.sum(self.elapse_times), 2))\nself.elapse_times = []\nself.step_start = time.perf_counter()\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.log_steps:\nself.elapse_times.append(time.perf_counter() - self.step_start)\ndata.write_with_log(\"epoch_time\", \"{} sec\".format(round(time.perf_counter() - self.epoch_start, 2)))\ndef on_end(self, data: Data) -&gt; None:\ndata.write_with_log(\"total_time\", \"{} sec\".format(round(time.perf_counter() - self.train_start, 2)))\nfor model in self.system.network.models:\nif hasattr(model, \"current_optimizer\"):\ndata.write_with_log(model.model_name + \"_lr\", get_lr(model))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/early_stopping.html", "title": "early_stopping", "text": ""}, {"location": "fastestimator/trace/adapt/early_stopping.html#fastestimator.fastestimator.trace.adapt.early_stopping.EarlyStopping", "title": "<code>EarlyStopping</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Stop training when a monitored quantity has stopped improving.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>str</code> <p>Quantity to be monitored.</p> <code>'loss'</code> <code>min_delta</code> <code>float</code> <p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta will count as no improvement.</p> <code>0.0</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will be stopped.</p> <code>0</code> <code>compare</code> <code>str</code> <p>One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored has stopped decreasing; in <code>max</code> mode it will stop when the quantity monitored has stopped increasing.</p> <code>'min'</code> <code>baseline</code> <code>Optional[float]</code> <p>Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.</p> <code>None</code> <code>mode</code> <code>str</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>'eval'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>compare</code> is an invalid value or more than one <code>monitor</code> is provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\early_stopping.py</code> <pre><code>class EarlyStopping(Trace):\n\"\"\"Stop training when a monitored quantity has stopped improving.\n    Args:\n        monitor: Quantity to be monitored.\n        min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an\n            absolute change of less than min_delta will count as no improvement.\n        patience: Number of epochs with no improvement after which training will be stopped.\n        compare: One of {\"min\", \"max\"}. In \"min\" mode, training will stop when the quantity monitored\n            has stopped decreasing; in `max` mode it will stop when the quantity monitored has stopped increasing.\n        baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't\n            show improvement over the baseline.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    Raises:\n        ValueError: If `compare` is an invalid value or more than one `monitor` is provided.\n    \"\"\"\ndef __init__(self,\nmonitor: str = \"loss\",\nmin_delta: float = 0.0,\npatience: int = 0,\ncompare: str = 'min',\nbaseline: Optional[float] = None,\nmode: str = 'eval') -&gt; None:\nsuper().__init__(inputs=monitor, mode=mode)\nif len(self.inputs) != 1:\nraise ValueError(\"EarlyStopping supports only one monitor key\")\nif compare not in ['min', 'max']:\nraise ValueError(\"compare_mode can only be `min` or `max`\")\nself.monitored_key = monitor\nself.min_delta = abs(min_delta)\nself.wait = 0\nself.best = 0\nself.patience = patience\nself.baseline = baseline\nif compare == 'min':\nself.monitor_op = np.less\nself.min_delta *= -1\nelse:\nself.monitor_op = np.greater\ndef on_begin(self, data: Data) -&gt; None:\nself.wait = 0\nif self.baseline is not None:\nself.best = self.baseline\nelse:\nself.best = np.Inf if self.monitor_op == np.less else -np.Inf\ndef on_epoch_end(self, data: Data) -&gt; None:\ncurrent = data[self.monitored_key]\nif current is None:\nreturn\nif self.monitor_op(current - self.min_delta, self.best):\nself.best = current\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nself.system.stop_training = True\nprint(\"FastEstimator-EarlyStopping: '{}' triggered an early stop. Its best value was {} at epoch {}\\\n                      \".format(self.monitored_key, self.best, self.system.epoch_idx - self.wait))\n</code></pre>"}, {"location": "fastestimator/trace/adapt/lr_scheduler.html", "title": "lr_scheduler", "text": ""}, {"location": "fastestimator/trace/adapt/lr_scheduler.html#fastestimator.fastestimator.trace.adapt.lr_scheduler.LRScheduler", "title": "<code>LRScheduler</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Learning rate scheduler trace that changes the learning rate while training.</p> <p>This class requires an input function which takes either 'epoch' or 'step' as input: <pre><code>s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on step\ns = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\nfe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>lr_fn</code> <code>Callable[[int], float]</code> <p>A lr scheduling function that takes either 'epoch' or 'step' as input.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the <code>lr_fn</code> is not configured properly.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\lr_scheduler.py</code> <pre><code>class LRScheduler(Trace):\n\"\"\"Learning rate scheduler trace that changes the learning rate while training.\n    This class requires an input function which takes either 'epoch' or 'step' as input:\n    ```python\n    s = LRScheduler(model=model, lr_fn=lambda step: fe.schedule.cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on step\n    s = LRScheduler(model=model, lr_fn=lambda epoch: fe.schedule.cosine_decay(epoch, cycle_length=3750, init_lr=1e-3))\n    fe.Estimator(..., traces=[s])  # Learning rate will change based on epoch\n    ```\n    Args:\n        model: A model instance compiled with fe.build.\n        lr_fn: A lr scheduling function that takes either 'epoch' or 'step' as input.\n    Raises:\n        AssertionError: If the `lr_fn` is not configured properly.\n    \"\"\"\nsystem: System\ndef __init__(self, model: Union[tf.keras.Model, torch.nn.Module], lr_fn: Callable[[int], float]) -&gt; None:\nself.model = model\nself.lr_fn = lr_fn\nassert hasattr(lr_fn, \"__call__\"), \"lr_fn must be a function\"\narg = list(inspect.signature(lr_fn).parameters.keys())\nassert len(arg) == 1 and arg[0] in {\"step\", \"epoch\"}, \"the lr_fn input arg must be either 'step' or 'epoch'\"\nself.schedule_mode = arg[0]\nsuper().__init__(mode=\"train\", outputs=self.model.model_name + \"_lr\")\ndef on_epoch_begin(self, data: Data) -&gt; None:\nif self.schedule_mode == \"epoch\":\nnew_lr = np.float32(self.lr_fn(self.system.epoch_idx))\nset_lr(self.model, new_lr)\ndef on_batch_begin(self, data: Data) -&gt; None:\nif self.schedule_mode == \"step\":\nnew_lr = np.float32(self.lr_fn(self.system.global_step))\nset_lr(self.model, new_lr)\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.system.log_steps and (self.system.global_step % self.system.log_steps == 0\nor self.system.global_step == 1):\ncurrent_lr = np.float32(get_lr(self.model))\ndata.write_with_log(self.outputs[0], current_lr)\n</code></pre>"}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html", "title": "reduce_lr_on_plateau", "text": ""}, {"location": "fastestimator/trace/adapt/reduce_lr_on_plateau.html#fastestimator.fastestimator.trace.adapt.reduce_lr_on_plateau.ReduceLROnPlateau", "title": "<code>ReduceLROnPlateau</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Reduce learning rate based on evaluation results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>metric</code> <code>Optional[str]</code> <p>The metric name to be monitored. If None, the model's validation loss will be used as the metric.</p> <code>None</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait before reducing LR again.</p> <code>10</code> <code>factor</code> <code>float</code> <p>Reduce factor for the learning rate.</p> <code>0.1</code> <code>best_mode</code> <code>str</code> <p>Higher is better (\"max\") or lower is better (\"min\").</p> <code>'min'</code> <code>min_lr</code> <code>float</code> <p>Minimum learning rate.</p> <code>1e-06</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the loss cannot be inferred from the <code>model</code> and a <code>metric</code> was not provided.</p> Source code in <code>fastestimator\\fastestimator\\trace\\adapt\\reduce_lr_on_plateau.py</code> <pre><code>class ReduceLROnPlateau(Trace):\n\"\"\"Reduce learning rate based on evaluation results.\n    Args:\n        model: A model instance compiled with fe.build.\n        metric: The metric name to be monitored. If None, the model's validation loss will be used as the metric.\n        patience: Number of epochs to wait before reducing LR again.\n        factor: Reduce factor for the learning rate.\n        best_mode: Higher is better (\"max\") or lower is better (\"min\").\n        min_lr: Minimum learning rate.\n    Raises:\n        AssertionError: If the loss cannot be inferred from the `model` and a `metric` was not provided.\n    \"\"\"\nsystem: System\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nmetric: Optional[str] = None,\npatience: int = 10,\nfactor: float = 0.1,\nbest_mode: str = \"min\",\nmin_lr: float = 1e-6) -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \"cannot infer model loss name, please put the model to UpdateOp first\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\", inputs=metric, outputs=model.model_name + \"_lr\")\nself.model = model\nself.patience = patience\nself.factor = factor\nself.best_mode = best_mode\nself.min_lr = min_lr\nself.wait = 0\nif self.best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"best_mode must be either 'min' or 'max'\")\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.inputs[0]], self.best):\nself.best = data[self.inputs[0]]\nself.wait = 0\nelse:\nself.wait += 1\nif self.wait &gt;= self.patience:\nnew_lr = max(self.min_lr, np.float32(self.factor * get_lr(self.model)))\nset_lr(self.model, new_lr)\nself.wait = 0\ndata.write_with_log(self.outputs[0], new_lr)\nprint(\"FastEstimator-ReduceLROnPlateau: learning rate reduced to {}\".format(new_lr))\n</code></pre>"}, {"location": "fastestimator/trace/io/best_model_saver.html", "title": "best_model_saver", "text": ""}, {"location": "fastestimator/trace/io/best_model_saver.html#fastestimator.fastestimator.trace.io.best_model_saver.BestModelSaver", "title": "<code>BestModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save the weights of best model based on a given evaluation metric.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the model.</p> required <code>metric</code> <code>Optional[str]</code> <p>Eval metric name to monitor. If None, the model's loss will be used.</p> <code>None</code> <code>save_best_mode</code> <code>str</code> <p>Can be 'min' or 'max'.</p> <code>'min'</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a <code>metric</code> is not provided and it cannot be inferred from the <code>model</code>.</p> <code>ValueError</code> <p>If <code>save_best_mode</code> is an unacceptable string.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\best_model_saver.py</code> <pre><code>class BestModelSaver(Trace):\n\"\"\"Save the weights of best model based on a given evaluation metric.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the model.\n        metric: Eval metric name to monitor. If None, the model's loss will be used.\n        save_best_mode: Can be 'min' or 'max'.\n    Raises:\n        AssertionError: If a `metric` is not provided and it cannot be inferred from the `model`.\n        ValueError: If `save_best_mode` is an unacceptable string.\n    \"\"\"\ndef __init__(self,\nmodel: Union[tf.keras.Model, torch.nn.Module],\nsave_dir: str,\nmetric: Optional[str] = None,\nsave_best_mode: str = \"min\") -&gt; None:\nif not metric:\nassert hasattr(model, \"loss_name\"), \"cannot infer model loss name, please put the model to UpdateOp first\"\nassert len(model.loss_name) == 1, \"the model has more than one losses, please provide the metric explicitly\"\nmetric = next(iter(model.loss_name))\nsuper().__init__(mode=\"eval\",\ninputs=metric,\noutputs=[\"since_best_{}\".format(metric), \"{}_{}\".format(save_best_mode, metric)])\nself.model = model\nself.save_dir = save_dir\nself.save_best_mode = save_best_mode\nself.since_best = 0\nif self.save_best_mode == \"min\":\nself.best = np.Inf\nself.monitor_op = np.less\nelif self.save_best_mode == \"max\":\nself.best = -np.Inf\nself.monitor_op = np.greater\nelse:\nraise ValueError(\"save_best_mode must be either 'min' or 'max'\")\n@property\ndef metric(self) -&gt; str:\nreturn self.inputs[0]\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.monitor_op(data[self.metric], self.best):\nself.best = data[self.metric]\nself.since_best = 0\nif self.save_dir:\nmodel_name = \"{}_best_{}\".format(self.model.model_name, self.metric)\nmodel_path = save_model(self.model, self.save_dir, model_name)\nprint(\"FastEstimator-BestModelSaver: Saved model to {}\".format(model_path))\nelse:\nself.since_best += 1\ndata.write_with_log(self.outputs[0], self.since_best)\ndata.write_with_log(self.outputs[1], self.best)\n</code></pre>"}, {"location": "fastestimator/trace/io/csv_logger.html", "title": "csv_logger", "text": ""}, {"location": "fastestimator/trace/io/csv_logger.html#fastestimator.fastestimator.trace.io.csv_logger.CSVLogger", "title": "<code>CSVLogger</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Log monitored quantities in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Output filename.</p> required <code>monitor_names</code> <code>Optional[Union[List[str], str]]</code> <p>List of keys to monitor. If None then all metrics will be recorded.</p> <code>None</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\csv_logger.py</code> <pre><code>class CSVLogger(Trace):\n\"\"\"Log monitored quantities in a CSV file.\n    Args:\n        filename: Output filename.\n        monitor_names: List of keys to monitor. If None then all metrics will be recorded.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\nfilename: str,\nmonitor_names: Optional[Union[List[str], str]] = None,\nmode: Union[str, Set[str]] = (\"eval\", \"test\")) -&gt; None:\nsuper().__init__(inputs=\"*\" if monitor_names is None else monitor_names, mode=mode)\nself.filename = filename\nself.data = None\ndef on_begin(self, data: Data) -&gt; None:\nself.data = defaultdict(list)\ndef on_epoch_end(self, data: Data) -&gt; None:\nself.data[\"mode\"].append(self.system.mode)\nself.data[\"epoch\"].append(self.system.epoch_idx)\nif \"*\" in self.inputs:\nfor key, value in data.read_logs().items():\nself.data[key].append(value)\nelse:\nfor key in self.inputs:\nself.data[key].append(data[key])\ndef on_end(self, data: Data) -&gt; None:\ndf = pd.DataFrame(data=self.data)\nif os.path.exists(self.filename):\ndf.to_csv(self.filename, mode='a', index=False)\nelse:\ndf.to_csv(self.filename, index=False)\n</code></pre>"}, {"location": "fastestimator/trace/io/image_saver.html", "title": "image_saver", "text": ""}, {"location": "fastestimator/trace/io/image_saver.html#fastestimator.fastestimator.trace.io.image_saver.ImageSaver", "title": "<code>ImageSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that saves images to the disk.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be saved.</p> required <code>save_dir</code> <code>str</code> <p>The directory into which to write the images.</p> <code>os.getcwd()</code> <code>dpi</code> <code>int</code> <p>How many dots per inch to save.</p> <code>300</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_saver.py</code> <pre><code>class ImageSaver(Trace):\n\"\"\"A trace that saves images to the disk.\n    Args:\n        inputs: Key(s) of images to be saved.\n        save_dir: The directory into which to write the images.\n        dpi: How many dots per inch to save.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nsave_dir: str = os.getcwd(),\ndpi: int = 300,\nmode: Union[str, Set[str]] = (\"eval\", \"test\")) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nself.save_dir = save_dir\nself.dpi = dpi\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nif isinstance(imgs, ImgData):\nf = imgs.paint_figure()\nim_path = os.path.join(self.save_dir,\n\"{}_{}_epoch_{}.png\".format(key, self.system.mode, self.system.epoch_idx))\nplt.savefig(im_path, dpi=self.dpi, bbox_inches=\"tight\")\nplt.close(f)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\nelse:\nfor idx, img in enumerate(imgs):\nf = show_image(img, title=key)\nim_path = os.path.join(\nself.save_dir,\n\"{}_{}_epoch_{}_elem_{}.png\".format(key, self.system.mode, self.system.epoch_idx, idx))\nplt.savefig(im_path, dpi=self.dpi, bbox_inches=\"tight\")\nplt.close(f)\nprint(\"FastEstimator-ImageSaver: saved image to {}\".format(im_path))\n</code></pre>"}, {"location": "fastestimator/trace/io/image_viewer.html", "title": "image_viewer", "text": ""}, {"location": "fastestimator/trace/io/image_viewer.html#fastestimator.fastestimator.trace.io.image_viewer.ImageViewer", "title": "<code>ImageViewer</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that interrupts your training in order to display images on the screen.</p> <p>This class is useful primarily for Jupyter Notebook, or for debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, Sequence[str]]</code> <p>Key(s) of images to be displayed.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>width</code> <code>int</code> <p>The width in inches of the figure.</p> <code>12</code> <code>height</code> <code>int</code> <p>The height in inches of the figure.</p> <code>6</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\image_viewer.py</code> <pre><code>class ImageViewer(Trace):\n\"\"\"A trace that interrupts your training in order to display images on the screen.\n    This class is useful primarily for Jupyter Notebook, or for debugging purposes.\n    Args:\n        inputs: Key(s) of images to be displayed.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        width: The width in inches of the figure.\n        height: The height in inches of the figure.\n    \"\"\"\ndef __init__(self,\ninputs: Union[str, Sequence[str]],\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\nwidth: int = 12,\nheight: int = 6) -&gt; None:\nsuper().__init__(inputs=inputs, mode=mode)\nplt.rcParams['figure.figsize'] = [width, height]\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor key in self.inputs:\nif key in data:\nimgs = data[key]\nif isinstance(imgs, ImgData):\nfig = imgs.paint_numpy(dpi=96)\nplt.imshow(fig[0])\nplt.axis('off')\nplt.tight_layout()\nplt.show()\nelse:\nfor idx, img in enumerate(imgs):\nshow_image(img, title=\"{}_{}\".format(key, idx))\nplt.show()\n</code></pre>"}, {"location": "fastestimator/trace/io/model_saver.html", "title": "model_saver", "text": ""}, {"location": "fastestimator/trace/io/model_saver.html#fastestimator.fastestimator.trace.io.model_saver.ModelSaver", "title": "<code>ModelSaver</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Save model weights based on epoch frequency during training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[tf.keras.Model, torch.nn.Module]</code> <p>A model instance compiled with fe.build.</p> required <code>save_dir</code> <code>str</code> <p>Folder path into which to save the <code>model</code>.</p> required <code>frequency</code> <code>int</code> <p>Model saving frequency in epoch(s).</p> <code>1</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\model_saver.py</code> <pre><code>class ModelSaver(Trace):\n\"\"\"Save model weights based on epoch frequency during training.\n    Args:\n        model: A model instance compiled with fe.build.\n        save_dir: Folder path into which to save the `model`.\n        frequency: Model saving frequency in epoch(s).\n    \"\"\"\ndef __init__(self, model: Union[tf.keras.Model, torch.nn.Module], save_dir: str, frequency: int = 1) -&gt; None:\nsuper().__init__(mode=\"train\")\nself.model = model\nself.save_dir = save_dir\nself.frequency = frequency\ndef on_epoch_end(self, data: Data) -&gt; None:\n# No model will be saved when save_dir is None, which makes smoke test easier.\nif self.save_dir and self.system.epoch_idx % self.frequency == 0:\nmodel_name = \"{}_epoch_{}\".format(self.model.model_name, self.system.epoch_idx)\nmodel_path = save_model(self.model, self.save_dir, model_name)\nprint(\"FastEstimator-ModelSaver: Saved model to {}\".format(model_path))\n</code></pre>"}, {"location": "fastestimator/trace/io/qms.html", "title": "qms", "text": ""}, {"location": "fastestimator/trace/io/qms.html#fastestimator.fastestimator.trace.io.qms.QMSTest", "title": "<code>QMSTest</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Automate QMS testing and report generation.</p> <p>Parameters:</p> Name Type Description Default <code>test_descriptions</code> <code>Union[str, List[str]]</code> <p>List of text-based descriptions.</p> required <code>test_criterias</code> <code>Union[List[Callable], Callable]</code> <p>List of test functions. Function input argument names needs to match keys from the data dictionary.</p> required <code>test_title</code> <code>str</code> <p>Title of the test.</p> <code>'QMSTest'</code> <code>json_output</code> <code>str</code> <p>Path into which to write the output results JSON.</p> <code>''</code> <code>doc_output</code> <code>str</code> <p>Path into which to write the output QMS summary report (docx).</p> <code>''</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the number of <code>test_descriptions</code> and <code>test_criteria</code> do not match.</p> Source code in <code>fastestimator\\fastestimator\\trace\\io\\qms.py</code> <pre><code>class QMSTest(Trace):\n\"\"\"Automate QMS testing and report generation.\n    Args:\n        test_descriptions: List of text-based descriptions.\n        test_criterias: List of test functions. Function input argument names needs to match keys from the data\n            dictionary.\n        test_title: Title of the test.\n        json_output: Path into which to write the output results JSON.\n        doc_output: Path into which to write the output QMS summary report (docx).\n    Raises:\n        AssertionError: If the number of `test_descriptions` and `test_criteria` do not match.\n    \"\"\"\ndef __init__(self,\ntest_descriptions: Union[str, List[str]],\ntest_criterias: Union[List[Callable], Callable],\ntest_title: str = \"QMSTest\",\njson_output: str = \"\",\ndoc_output: str = \"\") -&gt; None:\nself.json_output = json_output\nself.doc_output = doc_output\nself.test_title = test_title\nself.test_descriptions = to_list(test_descriptions)\nself.test_criterias = to_list(test_criterias)\nassert len(self.test_descriptions) == len(self.test_criterias), \"inconsistent input length found\"\nall_inputs = set()\nfor criteria in self.test_criterias:\nall_inputs.update(inspect.signature(criteria).parameters.keys())\nsuper().__init__(inputs=all_inputs, mode=\"test\")\nself.total_pass, self.total_fail = 0, 0\ndef _initialize_json_summary(self) -&gt; None:\n\"\"\"Initialize json summary\n        \"\"\"\nself.json_summary = {\"title\": self.test_title, \"stories\": []}\ndef on_begin(self, data: Data) -&gt; None:\nself._initialize_json_summary()\nself.total_pass, self.total_fail = 0, 0\ndef on_epoch_end(self, data: Data) -&gt; None:\nfor criteria, description in zip(self.test_criterias, self.test_descriptions):\nstory = {\"description\": description}\nis_passed = criteria(*[data[var_name] for var_name in list(inspect.signature(criteria).parameters.keys())])\nstory[\"passed\"] = str(is_passed)\nif is_passed:\nself.total_pass += 1\nelse:\nself.total_fail += 1\nself.json_summary[\"stories\"].append(story)\ndef on_end(self, data: Data) -&gt; None:\nif self.json_output.endswith(\".json\"):\njson_path = self.json_output\nelse:\njson_path = os.path.join(self.json_output, \"QMS.json\")\nwith open(json_path, 'w') as fp:\njson.dump(self.json_summary, fp, indent=4)\nprint(\"Saved QMS JSON report to {}\".format(json_path))\nif self.doc_output.endswith(\".docx\"):\ndoc_path = self.doc_output\nelse:\ndoc_path = os.path.join(self.doc_output, \"QMS_summary.docx\")\ndoc_summary = _QMSDocx(self.total_pass, self.total_fail)\ndoc_summary.save(doc_path)\nprint(\"Saved QMS summary report to {}\".format(doc_path))\n</code></pre>"}, {"location": "fastestimator/trace/io/restore_wizard.html", "title": "restore_wizard", "text": ""}, {"location": "fastestimator/trace/io/restore_wizard.html#fastestimator.fastestimator.trace.io.restore_wizard.RestoreWizard", "title": "<code>RestoreWizard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace that can backup and load your entire training status.</p> <p>System includes model weights, optimizer state, global step and epoch index.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Directory to save and load system.</p> required <code>frequency</code> <code>int</code> <p>Saving frequency in epoch(s).</p> <code>1</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\restore_wizard.py</code> <pre><code>class RestoreWizard(Trace):\n\"\"\"A trace that can backup and load your entire training status.\n    System includes model weights, optimizer state, global step and epoch index.\n    Args:\n        directory: Directory to save and load system.\n        frequency: Saving frequency in epoch(s).\n    \"\"\"\ndef __init__(self, directory: str, frequency: int = 1) -&gt; None:\nsuper().__init__(mode=\"train\")\nself.directory = directory\nself.frequency = frequency\nself.model_extension = {\"tf\": \"h5\", \"torch\": \"pt\"}\nself.optimizer_extension = {\"tf\": \"pkl\", \"torch\": \"pt\"}\nself.system_file = \"system.json\"\ndef on_begin(self, data: Data) -&gt; None:\nif not os.path.exists(self.directory) or not os.listdir(self.directory):\nprint(\"FastEstimator-RestoreWizard: Backing up in {}\".format(self.directory))\nelse:\nself._scan_files()\nself._load_files()\ndata.write_with_log(\"epoch\", self.system.epoch_idx)\nprint(\"FastEstimator-RestoreWizard: Restoring from {}, resume training\".format(self.directory))\ndef _load_files(self) -&gt; None:\n\"\"\"Restore from files.\n        \"\"\"\nsystem_path = os.path.join(self.directory, self.system_file)\nself.system.load_state(json_path=system_path)\nfor model in self.system.network.models:\nif isinstance(model, tf.keras.Model):\nframework = \"tf\"\nelif isinstance(model, torch.nn.Module):\nframework = \"torch\"\nelse:\nraise ValueError(\"Unknown model type {}\".format(type(model)))\nweights_path = os.path.join(self.directory,\n\"{}.{}\".format(model.model_name, self.model_extension[framework]))\nload_model(model, weights_path=weights_path, load_optimizer=True)\ndef _scan_files(self) -&gt; None:\n\"\"\"Scan necessary files to load.\n        \"\"\"\nsystem_path = os.path.join(self.directory, self.system_file)\nassert os.path.exists(system_path), \"cannot find system file at {}\".format(system_path)\nfor model in self.system.network.models:\nif isinstance(model, tf.keras.Model):\nframework = \"tf\"\nelif isinstance(model, torch.nn.Module):\nframework = \"torch\"\nelse:\nraise ValueError(\"Unknown model type {}\".format(type(model)))\nweights_path = os.path.join(self.directory,\n\"{}.{}\".format(model.model_name, self.model_extension[framework]))\nassert os.path.exists(weights_path), \"cannot find model weights file at {}\".format(weights_path)\noptimizer_path = os.path.join(self.directory,\n\"{}_opt.{}\".format(model.model_name, self.optimizer_extension[framework]))\nassert os.path.exists(optimizer_path), \"cannot find model optimizer file at {}\".format(optimizer_path)\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.epoch_idx % self.frequency == 0:\n# Save all models and optimizer state\nfor model in self.system.network.models:\nsave_model(model, save_dir=self.directory, save_optimizer=True)\n# Save system state\nself.system.save_state(json_path=os.path.join(self.directory, self.system_file))\nprint(\"FastEstimator-RestoreWizard: Saved milestones to {}\".format(self.directory))\n</code></pre>"}, {"location": "fastestimator/trace/io/tensorboard.html", "title": "tensorboard", "text": ""}, {"location": "fastestimator/trace/io/tensorboard.html#fastestimator.fastestimator.trace.io.tensorboard.TensorBoard", "title": "<code>TensorBoard</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Output data for use in TensorBoard.</p> <p>Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the --reload_multifile=true flag until their multi-writer use case is finished: https://github.com/tensorflow/tensorboard/issues/1063</p> <p>Parameters:</p> Name Type Description Default <code>log_dir</code> <code>str</code> <p>Path of the directory where the log files to be parsed by TensorBoard should be saved.</p> <code>'logs'</code> <code>update_freq</code> <code>Union[None, int, str]</code> <p>'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000, the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace mostly useless.</p> <code>100</code> <code>write_graph</code> <code>bool</code> <p>Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.</p> <code>True</code> <code>write_images</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard images.</p> <code>None</code> <code>weight_histogram_freq</code> <code>Union[None, int, str]</code> <p>Frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. Same argument format as <code>update_freq</code>.</p> <code>None</code> <code>paint_weights</code> <code>bool</code> <p>If True the system will attempt to visualize model weights as an image.</p> <code>False</code> <code>write_embeddings</code> <code>Union[None, str, List[str]]</code> <p>If a string or list of strings is provided, the corresponding keys will be written to TensorBoard embeddings.</p> <code>None</code> <code>embedding_labels</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to label information for the <code>write_embeddings</code>.</p> <code>None</code> <code>embedding_images</code> <code>Union[None, str, List[str]]</code> <p>Keys corresponding to raw images to be associated with the <code>write_embeddings</code>.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\trace\\io\\tensorboard.py</code> <pre><code>class TensorBoard(Trace):\n\"\"\"Output data for use in TensorBoard.\n    Note that if you plan to run a tensorboard server simultaneous to training, you may want to consider using the\n    --reload_multifile=true flag until their multi-writer use case is finished:\n    https://github.com/tensorflow/tensorboard/issues/1063\n    Args:\n        log_dir: Path of the directory where the log files to be parsed by TensorBoard should be saved.\n        update_freq: 'batch', 'epoch', integer, or strings like '10s', '15e'. When using 'batch', writes the losses and\n            metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 1000,\n            the callback will write the metrics and losses to TensorBoard every 1000 samples. You can also use strings\n            like '8s' to indicate every 8 steps or '5e' to indicate every 5 epochs. Note that writing too frequently to\n            TensorBoard can slow down your training. You can use None to disable updating, but this will make the trace\n            mostly useless.\n        write_graph: Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph\n            is set to True.\n        write_images: If a string or list of strings is provided, the corresponding keys will be written to TensorBoard\n            images.\n        weight_histogram_freq: Frequency (in epochs) at which to compute activation and weight histograms for the layers\n            of the model. Same argument format as `update_freq`.\n        paint_weights: If True the system will attempt to visualize model weights as an image.\n        write_embeddings: If a string or list of strings is provided, the corresponding keys will be written to\n            TensorBoard embeddings.\n        embedding_labels: Keys corresponding to label information for the `write_embeddings`.\n        embedding_images: Keys corresponding to raw images to be associated with the `write_embeddings`.\n    \"\"\"\nFreq = namedtuple('Freq', ['is_step', 'freq'])\nwriter: _BaseWriter\ndef __init__(self,\nlog_dir: str = 'logs',\nupdate_freq: Union[None, int, str] = 100,\nwrite_graph: bool = True,\nwrite_images: Union[None, str, List[str]] = None,\nweight_histogram_freq: Union[None, int, str] = None,\npaint_weights: bool = False,\nwrite_embeddings: Union[None, str, List[str]] = None,\nembedding_labels: Union[None, str, List[str]] = None,\nembedding_images: Union[None, str, List[str]] = None) -&gt; None:\nsuper().__init__(inputs=\"*\")\nself.root_log_dir = log_dir\nself.update_freq = self._parse_freq(update_freq)\nself.write_graph = write_graph\nself.painted_graphs = set()\nself.write_images = to_set(write_images)\nself.histogram_freq = self._parse_freq(weight_histogram_freq)\nif paint_weights and self.histogram_freq.freq == 0:\nself.histogram_freq.is_step = False\nself.histogram_freq.freq = 1\nself.paint_weights = paint_weights\nwrite_embeddings = to_list(write_embeddings)\nembedding_labels = to_list(embedding_labels)\nif embedding_labels:\nassert len(embedding_labels) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_labels keys, but recieved {len(embedding_labels)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_labels = [None for _ in range(len(write_embeddings))]\nembedding_images = to_list(embedding_images)\nif embedding_images:\nassert len(embedding_images) == len(write_embeddings), \\\n                f\"Expected {len(write_embeddings)} embedding_images keys, but recieved {len(embedding_images)}. Use \\\n                None to pad out the list if you have labels for only a subset of all embeddings.\"\nelse:\nembedding_images = [None for _ in range(len(write_embeddings))]\nself.write_embeddings = [(feature, label, img_label) for feature,\nlabel,\nimg_label in zip(write_embeddings, embedding_labels, embedding_images)]\ndef _parse_freq(self, freq: Union[None, str, int]) -&gt; Freq:\n\"\"\"A helper function to convert string based frequency inputs into epochs or steps\n        Args:\n            freq: One of either None, \"step\", \"epoch\", \"#s\", \"#e\", or #, where # is an integer.\n        Returns:\n            A `Freq` object recording whether the trace should run on an epoch basis or a step basis, as well as the\n            frequency with which it should run.\n        \"\"\"\nif freq is None:\nreturn self.Freq(False, 0)\nif isinstance(freq, int):\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(True, freq)\nif isinstance(freq, str):\nif freq in {'step', 's'}:\nreturn self.Freq(True, 1)\nif freq in {'epoch', 'e'}:\nreturn self.Freq(False, 1)\nparts = re.match(r\"^([0-9]+)([se])$\", freq)\nif parts is None:\nraise ValueError(f\"Tensorboard frequency argument must be formatted like &lt;int&gt;&lt;s|e&gt; but got {freq}\")\nfreq = int(parts[1])\nif freq &lt; 1:\nraise ValueError(f\"Tensorboard frequency argument must be a positive integer but got {freq}\")\nreturn self.Freq(parts[2] == 's', freq)\nelse:\nraise ValueError(f\"Unrecognized type passed as Tensorboard frequency: {type(freq)}\")\ndef on_begin(self, data: Data) -&gt; None:\nprint(\"FastEstimator-Tensorboard: writing logs to {}\".format(\nos.path.abspath(os.path.join(self.root_log_dir, self.system.experiment_time))))\nself.writer = _TfWriter(self.root_log_dir, self.system.experiment_time, self.system.network) if isinstance(\nself.system.network, TFNetwork) else _TorchWriter(\nself.root_log_dir, self.system.experiment_time, self.system.network)\nif self.write_graph and self.system.global_step == 1:\nself.painted_graphs = set()\ndef on_batch_end(self, data: Data) -&gt; None:\nif self.write_graph and self.system.network.epoch_models.symmetric_difference(self.painted_graphs):\nself.writer.write_epoch_models(mode=self.system.mode, data=data)\nself.painted_graphs = self.system.network.epoch_models\nif self.system.mode != 'train':\nreturn\nif self.histogram_freq.freq and self.histogram_freq.is_step and \\\n                self.system.global_step % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\nif self.update_freq.freq and self.update_freq.is_step and self.system.global_step % self.update_freq.freq == 0:\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.system.mode == 'train' and self.histogram_freq.freq and not self.histogram_freq.is_step and \\\n                self.system.epoch_idx % self.histogram_freq.freq == 0:\nself.writer.write_weights(mode=self.system.mode,\nmodels=self.system.network.models,\nstep=self.system.global_step,\nvisualize=self.paint_weights)\nif self.update_freq.freq and (self.update_freq.is_step or self.system.epoch_idx % self.update_freq.freq == 0):\nself.writer.write_scalars(mode=self.system.mode,\nstep=self.system.global_step,\nscalars=filter(lambda x: is_number(x[1]), data.items()))\nself.writer.write_images(\nmode=self.system.mode,\nstep=self.system.global_step,\nimages=filter(lambda x: x[1] is not None, map(lambda y: (y, data.get(y)), self.write_images)))\nself.writer.write_embeddings(\nmode=self.system.mode,\nstep=self.system.global_step,\nembeddings=filter(\nlambda x: x[1] is not None,\nmap(lambda t: (t[0], data.get(t[0]), data.get(t[1]), data.get(t[2])), self.write_embeddings)))\ndef on_end(self, data: Data) -&gt; None:\nself.writer.close()\n</code></pre>"}, {"location": "fastestimator/trace/metric/accuracy.html", "title": "accuracy", "text": ""}, {"location": "fastestimator/trace/metric/accuracy.html#fastestimator.fastestimator.trace.metric.accuracy.Accuracy", "title": "<code>Accuracy</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the accuracy for a given set of predictions.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'accuracy'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\accuracy.py</code> <pre><code>class Accuracy(Trace):\n\"\"\"A trace which computes the accuracy for a given set of predictions.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"accuracy\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.total = 0\nself.correct = 0\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.total = 0\nself.correct = 0\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.correct += np.sum(y_pred.ravel() == y_true.ravel())\nself.total += len(y_pred.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.correct / self.total)\n</code></pre>"}, {"location": "fastestimator/trace/metric/confusion_matrix.html", "title": "confusion_matrix", "text": ""}, {"location": "fastestimator/trace/metric/confusion_matrix.html#fastestimator.fastestimator.trace.metric.confusion_matrix.ConfusionMatrix", "title": "<code>ConfusionMatrix</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes the confusion matrix between y_true and y_predicted.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>num_classes</code> <code>int</code> <p>Total number of classes of the confusion matrix.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'confusion_matrix'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\confusion_matrix.py</code> <pre><code>class ConfusionMatrix(Trace):\n\"\"\"Computes the confusion matrix between y_true and y_predicted.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        num_classes: Total number of classes of the confusion matrix.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nnum_classes: int,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"confusion_matrix\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.num_classes = num_classes\nself.matrix = None\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.matrix = None\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nbatch_confusion = confusion_matrix(y_true, y_pred, labels=list(range(0, self.num_classes)))\nif self.matrix is None:\nself.matrix = batch_confusion\nelse:\nself.matrix += batch_confusion\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], self.matrix)\n</code></pre>"}, {"location": "fastestimator/trace/metric/dice.html", "title": "dice", "text": ""}, {"location": "fastestimator/trace/metric/dice.html#fastestimator.fastestimator.trace.metric.dice.Dice", "title": "<code>Dice</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Dice score for binary classification between y_true and y_predicted.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>The key of the ground truth mask.</p> required <code>pred_key</code> <code>str</code> <p>The key of the prediction values.</p> required <code>threshold</code> <code>float</code> <p>The threshold for binarizing the prediction.</p> <code>0.5</code> <code>mode</code> <code>Union[None, str, List[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'Dice'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\dice.py</code> <pre><code>class Dice(Trace):\n\"\"\"Dice score for binary classification between y_true and y_predicted.\n    Args:\n        true_key: The key of the ground truth mask.\n        pred_key: The key of the prediction values.\n        threshold: The threshold for binarizing the prediction.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nthreshold: float = 0.5,\nmode: Union[None, str, List[str]] = (\"eval\", \"test\"),\noutput_name: str = \"Dice\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.threshold = threshold\nself.smooth = 1e-8\nself.dice = np.array([])\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.dice = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nbatch_size = y_true.shape[0]\ny_true, y_pred = y_true.reshape((batch_size, -1)), y_pred.reshape((batch_size, -1))\nprediction_label = (y_pred &gt;= self.threshold).astype(np.int32)\nintersection = np.sum(y_true * prediction_label, axis=-1)\narea_sum = np.sum(y_true, axis=-1) + np.sum(prediction_label, axis=-1)\ndice = (2. * intersection + self.smooth) / (area_sum + self.smooth)\nself.dice.extend(list(dice))\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], np.mean(self.dice))\n</code></pre>"}, {"location": "fastestimator/trace/metric/f1_score.html", "title": "f1_score", "text": ""}, {"location": "fastestimator/trace/metric/f1_score.html#fastestimator.fastestimator.trace.metric.f1_score.F1Score", "title": "<code>F1Score</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate the F1 score for a classification task and report it back to the logger.</p> <p>Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store back to the state.</p> <code>'f1_score'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\f1_score.py</code> <pre><code>class F1Score(Trace):\n\"\"\"Calculate the F1 score for a classification task and report it back to the logger.\n    Consider using MCC instead: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store back to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"f1_score\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = f1_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = f1_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/metric/mcc.html", "title": "mcc", "text": ""}, {"location": "fastestimator/trace/metric/mcc.html#fastestimator.fastestimator.trace.metric.mcc.MCC", "title": "<code>MCC</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A trace which computes the Matthews Correlation Coefficient for a given set of predictions.</p> <p>This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,  a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies anti-correlation (you should invert your classifier predictions in order to do better).</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>What to call the output from this trace (for example in the logger output).</p> <code>'mcc'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mcc.py</code> <pre><code>class MCC(Trace):\n\"\"\"A trace which computes the Matthews Correlation Coefficient for a given set of predictions.\n    This is a preferable metric to accuracy or F1 score since it automatically corrects for class imbalances and does\n    not depend on the choice of target class (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6941312/). Ideal value is 1,\n     a value of 0 means your predictions are completely uncorrelated with the true data. A value less than zero implies\n    anti-correlation (you should invert your classifier predictions in order to do better).\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: What to call the output from this trace (for example in the logger output).\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"mcc\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), mode=mode, outputs=output_name)\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_true.extend(y_true)\nself.y_pred.extend(y_pred)\ndef on_epoch_end(self, data: Data) -&gt; None:\ndata.write_with_log(self.outputs[0], matthews_corrcoef(y_true=self.y_true, y_pred=self.y_pred))\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html", "title": "mean_average_precision", "text": "<p>COCO Mean average precisin (mAP) implementation.</p>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision", "title": "<code>MeanAveragePrecision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Calculate COCO mean average precision.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Maximum <code>int</code> value for your class label. In COCO dataset we only used 80 classes, but the maxium value of the class label is <code>90</code>. In this case <code>num_classes</code> should be <code>90</code>.</p> required <p>Returns:</p> Type Description <p>Mean Average Precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>class MeanAveragePrecision(Trace):\n\"\"\"Calculate COCO mean average precision.\n    Args:\n        num_classes: Maximum `int` value for your class label. In COCO dataset we only used 80 classes, but the maxium\n            value of the class label is `90`. In this case `num_classes` should be `90`.\n    Returns:\n        Mean Average Precision.\n    \"\"\"\ndef __init__(self,\nnum_classes: int,\ntrue_key='bbox',\npred_key: str = 'pred',\nmode: str = \"eval\",\noutput_name=(\"mAP\", \"AP50\", \"AP75\")) -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nassert len(self.outputs) == 3, 'MeanAvgPrecision trace adds 3 fields mAP AP50 AP75 to state dict'\nself.iou_thres = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05).astype(np.int) + 1, endpoint=True)\nself.recall_thres = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01).astype(np.int) + 1, endpoint=True)\nself.categories = range(1, num_classes + 1)  # MSCOCO style class label starts from 1\nself.max_detection = 100\nself.image_ids = []\n# eval\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0  # reset per epoch\n# reset per batch\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\nself.counter = 0  # REMOVE\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef _get_id_in_epoch(self, idx_in_batch: int) -&gt; int:\n\"\"\"Get unique image id in epoch.\n        Id starts from 1.\n        Args:\n            idx_in_batch: Image id within a batch.\n        Returns:\n            Global unique id within one epoch.\n        \"\"\"\n# for this batch\nnum_unique_id_previous = len(np.unique(self.ids_unique))\nself.ids_unique.append(idx_in_batch)\nnum_unique_id = len(np.unique(self.ids_unique))\nif num_unique_id &gt; num_unique_id_previous:\n# for epoch\nself.ids_in_epoch += 1\nself.ids_batch_to_epoch[idx_in_batch] = self.ids_in_epoch\nreturn self.ids_in_epoch\ndef on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\ndef on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n@staticmethod\ndef _reshape_gt(gt_array: np.ndarray) -&gt; np.ndarray:\n\"\"\"Reshape ground truth and add local image id within batch.\n        The input ground truth array has shape (batch_size, num_bbox, 5). The 5 is [x1, y1, w, h, label] for each\n        bounding box.\n        For output we drop all padded bounding boxes (all zeros), and flatten the batch dimension. The output shape is\n        (batch_size * num_bbox, 6). The 6 is [id_in_batch, x1, y1, w, h, label].\n        Args:\n            gt_array: Ground truth with shape (batch_size, num_bbox, 5).\n        Returns:\n            Ground truth with shape (batch_size * num_bbox, 6).\n        \"\"\"\nlocal_ids = np.repeat(range(gt_array.shape[0]), gt_array.shape[1], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\ngt_with_id = np.concatenate([local_ids, gt_array.reshape(-1, 5)], axis=1)\nkeep = gt_with_id[..., -1] &gt; 0\nreturn gt_with_id[keep]\n@staticmethod\ndef _reshape_pred(pred: List[np.ndarray]) -&gt; np.ndarray:\n\"\"\"Reshape predicted bounding boxes and add local image id within batch.\n        The input prediction array is a list of batch_size elements. For each element inside the list, it\n        has shape (num_bbox, 6). The 6 is [x1, y1, w, h, label, score] for each bounding box.\n        For output we flatten the batch dimension. The output shape is (total_num_bbox_in_batch, 7). The 7 is\n        [id_in_batch, x1, y1, w, h, label, score].\n        Args:\n            pred: List of predected bounding boxes for each image. Each element in the list has shape (num_bbox, 6).\n        Returns:\n            Predected bounding boxes with shape (total_num_bbox_in_batch, 7).\n        \"\"\"\npred_with_id = []\nfor index, item in enumerate(pred):\nlocal_ids = np.repeat([index], item.shape[0], axis=None)\nlocal_ids = np.expand_dims(local_ids, axis=-1)\npred_with_id.append(np.concatenate([local_ids, item], axis=1))\npred_with_id = np.concatenate(pred_with_id, axis=0)\nreturn pred_with_id\ndef on_batch_end(self, data: Data):\n# begin of reading det and gt\npred = list(map(to_number, data[self.pred_key]))  # pred is list (batch, ) of np.ndarray (?, 6)\npred = self._reshape_pred(pred)\ngt = to_number(data[self.true_key])  # gt is np.array (batch, box, 5), box dimension is padded\ngt = self._reshape_gt(gt)\nground_truth_bb = []\nfor gt_item in gt:\nidx_in_batch, x1, y1, w, h, label = gt_item\nlabel = int(label)\nid_epoch = self._get_id_in_epoch(idx_in_batch)\nself.batch_image_ids.append(id_epoch)\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label}\nground_truth_bb.append(tmp_dict)\npredicted_bb = []\nfor pred_item in pred:\nidx_in_batch, x1, y1, w, h, label, score = pred_item\nlabel = int(label)\nid_epoch = self.ids_batch_to_epoch[idx_in_batch]\nself.image_ids.append(id_epoch)\ntmp_dict = {'idx': id_epoch, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'label': label, 'score': score}\npredicted_bb.append(tmp_dict)\nfor dict_elem in ground_truth_bb:\nself.gt[dict_elem['idx'], dict_elem['label']].append(dict_elem)\nfor dict_elem in predicted_bb:\nself.det[dict_elem['idx'], dict_elem['label']].append(dict_elem)\n# end of reading det and gt\n# compute iou matrix, matrix index is (img_id, cat_id), each element in matrix has shape (num_det, num_gt)\nself.ious = {(img_id, cat_id): self.compute_iou(self.det[img_id, cat_id], self.gt[img_id, cat_id])\nfor img_id in self.batch_image_ids for cat_id in self.categories}\nfor cat_id in self.categories:\nfor img_id in self.batch_image_ids:\nself.evalimgs[(cat_id, img_id)] = self.evaluate_img(cat_id, img_id)\ndef on_epoch_end(self, data: Data):\nself.accumulate()\nmean_ap = self.summarize()\nap50 = self.summarize(iou=0.5)\nap75 = self.summarize(iou=0.75)\ndata[self.outputs[0]] = mean_ap\ndata[self.outputs[1]] = ap50\ndata[self.outputs[2]] = ap75\ndef evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n        Args:\n            cat_id:\n            img_id:\n        Returns:\n        \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\ndef accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\ndef summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n        Args:\n            iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n                is the mean average precision.\n        Returns:\n            Average precision.\n        \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\ndef compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n        We leverage `maskUtils.iou`.\n        Args:\n            det: Detection array.\n            gt: Ground truth array.\n        Returns:\n            Intersection of union array.\n        \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.accumulate", "title": "<code>accumulate</code>", "text": "<p>Generate precision-recall curve.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def accumulate(self) -&gt; None:\n\"\"\"Generate precision-recall curve.\"\"\"\nkey_list = sorted(self.evalimgs)  # key format (cat_id, img_id)\neval_list = [self.evalimgs[key] for key in key_list]\nself.image_ids = np.unique(self.image_ids)\nnum_iou_thresh = len(self.iou_thres)\nnum_recall_thresh = len(self.recall_thres)\nnum_categories = len(self.categories)\ncat_list_zeroidx = [n for n, cat in enumerate(self.categories)]\nnum_imgs = len(self.image_ids)\nmaxdets = self.max_detection\n# initialize these at -1\nprecision_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\nrecall_matrix = -np.ones((num_iou_thresh, num_categories))\nscores_matrix = -np.ones((num_iou_thresh, num_recall_thresh, num_categories))\n# loop through category\nfor cat_index in cat_list_zeroidx:\nNk = cat_index * num_imgs\n# each element is one image inside this category\neval_by_category = [eval_list[Nk + img_idx] for img_idx in range(num_imgs)]\n# drop None\neval_by_category = [e for e in eval_by_category if not e is None]\n# no image inside this category\nif len(eval_by_category) == 0:\ncontinue\ndet_scores = np.concatenate([e['dtScores'][0:maxdets] for e in eval_by_category])\n# sort from high score to low score, is this necessary?\nsorted_score_inds = np.argsort(-det_scores, kind='mergesort')\ndet_scores_sorted = det_scores[sorted_score_inds]\ndet_match = np.concatenate([e['dtMatches'][:, 0:maxdets] for e in eval_by_category],\naxis=1)[:, sorted_score_inds]  # shape (num_iou_thresh, num_det_all_images)\n# number of all image gts in one category\nnum_all_gt = np.sum([e['num_gt'] for e in eval_by_category])\n# for all images no gt inside this category\nif num_all_gt == 0:\ncontinue\ntps = det_match &gt; 0\nfps = det_match == 0\ntp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\nfp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\nfor index, (true_positives, false_positives) in enumerate(zip(tp_sum, fp_sum)):\ntrue_positives = np.array(true_positives)\nfalse_positives = np.array(false_positives)\nnd = len(true_positives)\nrecall = true_positives / num_all_gt\nprecision = true_positives / (false_positives + true_positives + np.spacing(1))\nprecision_at_recall = np.zeros((num_recall_thresh, ))\nscore = np.zeros((num_recall_thresh, ))\nif nd:\nrecall_matrix[index, cat_index] = recall[-1]\nelse:\nrecall_matrix[index, cat_index] = 0\nprecision = precision.tolist()\nprecision_at_recall = precision_at_recall.tolist()\n# smooth precision along the curve, remove zigzag\nfor i in range(nd - 1, 0, -1):\nif precision[i] &gt; precision[i - 1]:\nprecision[i - 1] = precision[i]\ninds = np.searchsorted(recall, self.recall_thres, side='left')\ntry:\nfor recall_index, precision_index in enumerate(inds):\nprecision_at_recall[recall_index] = precision[precision_index]\nscore[recall_index] = det_scores_sorted[precision_index]\nexcept:\npass\nprecision_matrix[index, :, cat_index] = np.array(precision_at_recall)\nscores_matrix[index, :, cat_index] = np.array(score)\nself.eval = {\n'counts': [num_iou_thresh, num_recall_thresh, num_categories],\n'precision': precision_matrix,\n'recall': recall_matrix,\n'scores': scores_matrix,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.compute_iou", "title": "<code>compute_iou</code>", "text": "<p>Compute intersection over union.</p> <p>We leverage <code>maskUtils.iou</code>.</p> <p>Parameters:</p> Name Type Description Default <code>det</code> <code>np.ndarray</code> <p>Detection array.</p> required <code>gt</code> <code>np.ndarray</code> <p>Ground truth array.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Intersection of union array.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def compute_iou(self, det: np.ndarray, gt: np.ndarray) -&gt; np.ndarray:\n\"\"\"Compute intersection over union.\n    We leverage `maskUtils.iou`.\n    Args:\n        det: Detection array.\n        gt: Ground truth array.\n    Returns:\n        Intersection of union array.\n    \"\"\"\nnum_dt = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_dt == 0:\nreturn []\nboxes_a = np.zeros(shape=(0, 4), dtype=float)\nboxes_b = np.zeros(shape=(0, 4), dtype=float)\ninds = np.argsort([-d['score'] for d in det], kind='mergesort')\ndet = [det[i] for i in inds]\nif len(det) &gt; self.max_detection:\ndet = det[0:self.max_detection]\nboxes_a = [[dt_elem['x1'], dt_elem['y1'], dt_elem['w'], dt_elem['h']] for dt_elem in det]\nboxes_b = [[gt_elem['x1'], gt_elem['y1'], gt_elem['w'], gt_elem['h']] for gt_elem in gt]\niscrowd = [0] * num_gt  # to leverage maskUtils.iou\niou_dt_gt = maskUtils.iou(boxes_a, boxes_b, iscrowd)\nreturn iou_dt_gt\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.evaluate_img", "title": "<code>evaluate_img</code>", "text": "<p>Find gt matches for det given one image and one category.</p> <p>Parameters:</p> Name Type Description Default <code>cat_id</code> <code>int</code> required <code>img_id</code> <code>int</code> required Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def evaluate_img(self, cat_id: int, img_id: int) -&gt; Dict:\n\"\"\"Find gt matches for det given one image and one category.\n    Args:\n        cat_id:\n        img_id:\n    Returns:\n    \"\"\"\ndet = self.det[img_id, cat_id]\ngt = self.gt[img_id, cat_id]\nnum_det = len(det)\nnum_gt = len(gt)\nif num_gt == 0 and num_det == 0:\nreturn None\n# sort detections, is ths necessary?\ndet_index = np.argsort([-d['score'] for d in det], kind='mergesort')\n# cap to max_detection\ndet = [det[i] for i in det_index[0:self.max_detection]]\n# get iou matrix for given (img_id, cat_id), the output has shape (num_det, num_gt)\niou_mat = self.ious[img_id, cat_id]\nnum_iou_thresh = len(self.iou_thres)\ndet_match = np.zeros((num_iou_thresh, num_det))\ngt_match = np.zeros((num_iou_thresh, num_gt))\nif len(iou_mat) != 0:\n# loop through each iou thresh\nfor thres_idx, thres_value in enumerate(self.iou_thres):\n# loop through each detection, for each detection, match only one gt\nfor det_idx, _ in enumerate(det):\nm = -1\niou_threshold = min([thres_value, 1 - 1e-10])\n# loop through each gt, find the gt gives max iou\nfor gt_idx, _ in enumerate(gt):\nif gt_match[thres_idx, gt_idx] &gt; 0:\ncontinue\nif iou_mat[det_idx, gt_idx] &gt;= iou_threshold:\niou_threshold = iou_mat[det_idx, gt_idx]\nm = gt_idx\nif m != -1:\ndet_match[thres_idx, det_idx] = gt[m]['idx']\ngt_match[thres_idx, m] = 1\nreturn {\n'image_id': img_id,\n'category_id': cat_id,\n'gtIds': [g['idx'] for g in gt],\n'dtMatches': det_match,  # shape (num_iou_thresh, num_det), value is zero or GT index\n'gtMatches': gt_match,  # shape (num_iou_thresh, num_gt), value 1 or zero\n'dtScores': [d['score'] for d in det],\n'num_gt': num_gt,\n}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_batch_begin", "title": "<code>on_batch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_batch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.gt = defaultdict(list)  # gt for evaluation\nself.det = defaultdict(list)  # det for evaluation\nself.batch_image_ids = []  # img_ids per batch\nself.ious = defaultdict(list)\nself.ids_unique = []\nself.ids_batch_to_epoch = {}\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.on_epoch_begin", "title": "<code>on_epoch_begin</code>", "text": "<p>Reset instance variables.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def on_epoch_begin(self, data: Data):\n\"\"\"Reset instance variables.\"\"\"\nself.image_ids = []  # append all the image ids coming from each iteration\nself.evalimgs = {}\nself.eval = {}\nself.ids_in_epoch = 0\n</code></pre>"}, {"location": "fastestimator/trace/metric/mean_average_precision.html#fastestimator.fastestimator.trace.metric.mean_average_precision.MeanAveragePrecision.summarize", "title": "<code>summarize</code>", "text": "<p>Compute average precision given one intersection union threshold.</p> <p>Parameters:</p> Name Type Description Default <code>iou</code> <code>float</code> <p>Intersection over union threshold. If this value is <code>None</code>, then average all iou thresholds. The result is the mean average precision.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Average precision.</p> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\mean_average_precision.py</code> <pre><code>def summarize(self, iou: float = None) -&gt; float:\n\"\"\"Compute average precision given one intersection union threshold.\n    Args:\n        iou: Intersection over union threshold. If this value is `None`, then average all iou thresholds. The result\n            is the mean average precision.\n    Returns:\n        Average precision.\n    \"\"\"\nprecision_at_iou = self.eval['precision']  # shape (num_iou_thresh, num_recall_thresh, num_categories)\nif iou is not None:\niou_thresh_index = np.where(iou == self.iou_thres)[0]\nprecision_at_iou = precision_at_iou[iou_thresh_index]\nprecision_at_iou = precision_at_iou[:, :, :]\nif len(precision_at_iou[precision_at_iou &gt; -1]) == 0:\nmean_ap = -1\nelse:\nmean_ap = np.mean(precision_at_iou[precision_at_iou &gt; -1])\nreturn mean_ap\n</code></pre>"}, {"location": "fastestimator/trace/metric/precision.html", "title": "precision", "text": ""}, {"location": "fastestimator/trace/metric/precision.html#fastestimator.fastestimator.trace.metric.precision.Precision", "title": "<code>Precision</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Computes precision for a classification task and reports it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'precision'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\precision.py</code> <pre><code>class Precision(Trace):\n\"\"\"Computes precision for a classification task and reports it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"precision\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = precision_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = precision_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/metric/recall.html", "title": "recall", "text": ""}, {"location": "fastestimator/trace/metric/recall.html#fastestimator.fastestimator.trace.metric.recall.Recall", "title": "<code>Recall</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>Compute recall for a classification task and report it back to the logger.</p> <p>Parameters:</p> Name Type Description Default <code>true_key</code> <code>str</code> <p>Name of the key that corresponds to ground truth in the batch dictionary.</p> required <code>pred_key</code> <code>str</code> <p>Name of the key that corresponds to predicted score in the batch dictionary.</p> required <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>output_name</code> <code>str</code> <p>Name of the key to store to the state.</p> <code>'recall'</code> Source code in <code>fastestimator\\fastestimator\\trace\\metric\\recall.py</code> <pre><code>class Recall(Trace):\n\"\"\"Compute recall for a classification task and report it back to the logger.\n    Args:\n        true_key: Name of the key that corresponds to ground truth in the batch dictionary.\n        pred_key: Name of the key that corresponds to predicted score in the batch dictionary.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        output_name: Name of the key to store to the state.\n    \"\"\"\ndef __init__(self,\ntrue_key: str,\npred_key: str,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\noutput_name: str = \"recall\") -&gt; None:\nsuper().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\nself.binary_classification = None\nself.y_true = []\nself.y_pred = []\n@property\ndef true_key(self) -&gt; str:\nreturn self.inputs[0]\n@property\ndef pred_key(self) -&gt; str:\nreturn self.inputs[1]\ndef on_epoch_begin(self, data: Data) -&gt; None:\nself.y_true = []\nself.y_pred = []\ndef on_batch_end(self, data: Data) -&gt; None:\ny_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\nself.binary_classification = y_pred.shape[-1] == 1\nif y_true.shape[-1] &gt; 1 and y_true.ndim &gt; 1:\ny_true = np.argmax(y_true, axis=-1)\nif y_pred.shape[-1] &gt; 1:\ny_pred = np.argmax(y_pred, axis=-1)\nelse:\ny_pred = np.round(y_pred)\nassert y_pred.size == y_true.size\nself.y_pred.extend(y_pred.ravel())\nself.y_true.extend(y_true.ravel())\ndef on_epoch_end(self, data: Data) -&gt; None:\nif self.binary_classification:\nscore = recall_score(self.y_true, self.y_pred, average='binary')\nelse:\nscore = recall_score(self.y_true, self.y_pred, average=None)\ndata.write_with_log(self.outputs[0], score)\n</code></pre>"}, {"location": "fastestimator/trace/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/trace/xai/saliency.html#fastestimator.fastestimator.trace.xai.saliency.Saliency", "title": "<code>Saliency</code>", "text": "<p>         Bases: <code>Trace</code></p> <p>A Trace which computes saliency maps for a given model throughout training.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>A model compiled with fe.build to be analyzed.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the input values for the model.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>Keys for the output values from a model.</p> required <code>class_key</code> <code>Optional[str]</code> <p>The key of the true labels corresponding to the model inputs (not required).</p> <code>None</code> <code>label_mapping</code> <code>Optional[Dict[str, Any]]</code> <p>{class_string: model_output_value}.</p> <code>None</code> <code>outputs</code> <code>Union[str, List[str]]</code> <p>The name of the output which will be generated by this trace.</p> <code>'saliency'</code> <code>samples</code> <code>Union[None, int, Dict[str, Any]]</code> <p>How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.</p> <code>None</code> <code>mode</code> <code>Union[str, Set[str]]</code> <p>What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument like \"!infer\" or \"!train\".</p> <code>('eval', 'test')</code> <code>smoothing</code> <code>int</code> <p>How many rounds of smoothing should be applied to the saliency mask (0 to disable).</p> <code>25</code> <code>integrating</code> <code>Union[int, Tuple[int, int]]</code> <p>How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was provided by the smoothing variable (useful if you want to compare techniques / save on computation time)/</p> <code>(25, 7)</code> Source code in <code>fastestimator\\fastestimator\\trace\\xai\\saliency.py</code> <pre><code>class Saliency(Trace):\n\"\"\"A Trace which computes saliency maps for a given model throughout training.\n    Args:\n        model: A model compiled with fe.build to be analyzed.\n        model_inputs: Keys for the input values for the model.\n        model_outputs: Keys for the output values from a model.\n        class_key: The key of the true labels corresponding to the model inputs (not required).\n        label_mapping: {class_string: model_output_value}.\n        outputs: The name of the output which will be generated by this trace.\n        samples: How many datapoints to collect in order to perform visualization, or {model_input_key: model_input}.\n        mode: What mode(s) to execute this Trace in. For example, \"train\", \"eval\", \"test\", or \"infer\". To execute\n            regardless of mode, pass None. To execute in all modes except for a particular one, you can pass an argument\n            like \"!infer\" or \"!train\".\n        smoothing: How many rounds of smoothing should be applied to the saliency mask (0 to disable).\n        integrating: How many rounds of integration should be applied to the saliency mask (0 to disable). A tuple may\n            be used to indicate (# integration, # smoothing) if a different amount of smoothing is desired than was\n            provided by the smoothing variable (useful if you want to compare techniques / save on computation time)/\n    \"\"\"\nsamples: Dict[str, Union[None, int, Dict[str, Any]]]  # {mode: val}\nn_found: Dict[str, int]  # {mode: val}\nn_required: Dict[str, int]  # {mode: val}\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\nclass_key: Optional[str] = None,\nlabel_mapping: Optional[Dict[str, Any]] = None,\noutputs: Union[str, List[str]] = \"saliency\",\nsamples: Union[None, int, Dict[str, Any]] = None,\nmode: Union[str, Set[str]] = (\"eval\", \"test\"),\nsmoothing: int = 25,\nintegrating: Union[int, Tuple[int, int]] = (25, 7)) -&gt; None:\n# Model outputs are required due to inability to statically determine the number of outputs from a pytorch model\nself.class_key = class_key\nself.model_outputs = to_list(model_outputs)\nsuper().__init__(inputs=to_list(self.class_key) + to_list(model_inputs), outputs=outputs, mode=mode)\nself.smoothing = smoothing\nself.integrating = integrating\nself.samples = {}\nself.n_found = {}\nself.n_required = {}\n# TODO - handle non-hashable labels\nself.label_mapping = {val: key for key, val in label_mapping.items()} if label_mapping else None\nfor mode in mode or (\"train\", \"eval\", \"test\"):\nself.samples[mode] = samples\nif isinstance(samples, int):\nself.samples[mode] = None\nself.n_found[mode] = 0\nself.n_required[mode] = samples\nelse:\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nif self.samples[mode] is None:\nself.samples[mode] = defaultdict(list)\nself.salnet = SaliencyNet(model=model, model_inputs=model_inputs, model_outputs=model_outputs, outputs=outputs)\ndef on_batch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif not self.samples[mode] or self.n_found[mode] &lt; self.n_required[mode]:\nn_samples = 0\nfor key in self.inputs:\nself.samples[mode][key].append(data[key])\nn_samples = len(data[key])\nself.n_found[mode] += n_samples\ndef on_epoch_end(self, data: Data) -&gt; None:\nmode = self.system.mode\nif self.n_found[mode] &gt; 0:\nif self.n_required[mode] &gt; 0:\n# We are keeping a user-specified number of samples\nself.samples[mode] = {\nkey: concat(val)[:self.n_required[mode]]\nfor key, val in self.samples[mode].items()\n}\nelse:\n# We are keeping one batch of data\nself.samples[mode] = {key: val[0] for key, val in self.samples[mode].items()}\n# even if you haven't found n_required samples, you're at end of epoch so no point trying to collect more\nself.n_found[mode] = 0\nself.n_required[mode] = 0\nmasks = self.salnet.get_masks(self.samples[mode])\nsmoothed, integrated, smint = {}, {}, {}\nif self.smoothing:\nsmoothed = self.salnet.get_smoothed_masks(self.samples[mode], nsamples=self.smoothing)\nif self.integrating:\nif isinstance(self.integrating, Tuple):\nn_integration, n_smoothing = self.integrating\nelse:\nn_integration = self.integrating\nn_smoothing = self.smoothing\nintegrated = self.salnet.get_integrated_masks(self.samples[mode], nsamples=n_integration)\nif n_smoothing:\nsmint = self.salnet.get_smoothed_masks(self.samples[mode],\nnsamples=n_smoothing,\nnintegration=n_integration)\n# Arrange the outputs\nargs = {}\nif self.class_key:\nclasses = self.samples[mode][self.class_key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\nargs[self.class_key] = classes\nfor key in self.model_outputs:\nclasses = masks[key]\nif self.label_mapping:\nclasses = np.array([self.label_mapping[clazz] for clazz in to_number(squeeze(classes))])\nargs[key] = classes\nsal = smint or integrated or smoothed or masks\nfor key, val in self.samples[mode].items():\nif key is not self.class_key:\nargs[key] = val\n# Create a linear combination of the original image, the saliency mask, and the product of the two in\n# order to highlight regions of importance\nmin_val = reduce_min(val)\ndiff = reduce_max(val) - min_val\nfor outkey in self.outputs:\nargs[\"{} {}\".format(key, outkey)] = (0.3 * (sal[outkey] * (val - min_val) + min_val) + 0.3 * val +\n0.4 * sal[outkey] * diff + min_val)\nfor key in self.outputs:\nargs[key] = masks[key]\nif smoothed:\nargs[\"Smoothed {}\".format(key)] = smoothed[key]\nif integrated:\nargs[\"Integrated {}\".format(key)] = integrated[key]\nif smint:\nargs[\"SmInt {}\".format(key)] = smint[key]\nresult = ImgData(colormap=\"inferno\", **args)\ndata.write_without_log(self.outputs[0], result)\n</code></pre>"}, {"location": "fastestimator/util/data.html", "title": "data", "text": ""}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data", "title": "<code>Data</code>", "text": "<p>         Bases: <code>ChainMap[str, Any]</code></p> <p>A class which contains prediction and batch data.</p> <p>Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not. We therefore provide helper methods to write entries into <code>Data</code> which are intended or not intended for logging.</p> <pre><code>d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\na = d[\"a\"]  # 0\nd.write_with_log(\"d\", 3)\nd.write_without_log(\"e\", 5)\nd.write_with_log(\"a\", 4)\na = d[\"a\"]  # 4\nr = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_data</code> <code>Optional[MutableMapping[str, Any]]</code> <p>The batch data dictionary. In practice this is itself often a ChainMap containing separate prediction and batch dictionaries.</p> <code>None</code> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>class Data(ChainMap[str, Any]):\n\"\"\"A class which contains prediction and batch data.\n    Data objects can be interacted with as if they are regular dictionaries. They are however, actually a combination of\n    two dictionaries, a dictionary for trace communication and a dictionary of prediction+batch data. In general, data\n    written into the trace dictionary will be logged by the system, whereas data in the pred+batch dictionary will not.\n    We therefore provide helper methods to write entries into `Data` which are intended or not intended for logging.\n    ```python\n    d = fe.util.Data({\"a\":0, \"b\":1, \"c\":2})\n    a = d[\"a\"]  # 0\n    d.write_with_log(\"d\", 3)\n    d.write_without_log(\"e\", 5)\n    d.write_with_log(\"a\", 4)\n    a = d[\"a\"]  # 4\n    r = d.read_logs(extra_keys={\"c\"})  # {\"c\":2, \"d\":3, \"a\":4}\n    ```\n    Args:\n        batch_data: The batch data dictionary. In practice this is itself often a ChainMap containing separate\n            prediction and batch dictionaries.\n    \"\"\"\nmaps: List[MutableMapping[str, Any]]\ndef __init__(self, batch_data: Optional[MutableMapping[str, Any]] = None) -&gt; None:\nsuper().__init__({}, batch_data or {})\ndef write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n        Args:\n            key: The key to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.__setitem__(key, value)\ndef write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n        Args:\n            key: The ey to associate with the new entry.\n            value: The new entry to be written.\n        \"\"\"\nself.maps[1][key] = value\ndef read_logs(self) -&gt; Dict[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n        Returns:\n            A dictionary of all of the keys and values to be logged.\n        \"\"\"\nreturn self.maps[0]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.read_logs", "title": "<code>read_logs</code>", "text": "<p>Read all values from the <code>Data</code> dictionary which were intended to be logged.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary of all of the keys and values to be logged.</p> Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def read_logs(self) -&gt; Dict[str, Any]:\n\"\"\"Read all values from the `Data` dictionary which were intended to be logged.\n    Returns:\n        A dictionary of all of the keys and values to be logged.\n    \"\"\"\nreturn self.maps[0]\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_with_log", "title": "<code>write_with_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_with_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it be logged.\n    Args:\n        key: The key to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.__setitem__(key, value)\n</code></pre>"}, {"location": "fastestimator/util/data.html#fastestimator.fastestimator.util.data.Data.write_without_log", "title": "<code>write_without_log</code>", "text": "<p>Write a given <code>value</code> into the <code>Data</code> dictionary with the intent that it not be logged.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The ey to associate with the new entry.</p> required <code>value</code> <code>Any</code> <p>The new entry to be written.</p> required Source code in <code>fastestimator\\fastestimator\\util\\data.py</code> <pre><code>def write_without_log(self, key: str, value: Any) -&gt; None:\n\"\"\"Write a given `value` into the `Data` dictionary with the intent that it not be logged.\n    Args:\n        key: The ey to associate with the new entry.\n        value: The new entry to be written.\n    \"\"\"\nself.maps[1][key] = value\n</code></pre>"}, {"location": "fastestimator/util/img_data.html", "title": "img_data", "text": ""}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData", "title": "<code>ImgData</code>", "text": "<p>         Bases: <code>OrderedDict</code></p> <p>A container for image related data.</p> <p>This class is useful for automatically laying out collections of images for comparison and visualization.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nfig = d.paint_figure()\nplt.show()\nimg = 0.5*np.ones((4, 32, 32, 3))\nmask = np.zeros_like(img)\nmask[0, 10:20, 10:30, :] = [1, 0, 0]\nmask[1, 5:15, 5:20, :] = [0, 1, 0]\nbbox = np.array([[[3,7,10,6,'box1'], [20,20,8,8,'box2']]]*4)\nd = fe.util.ImgData(y=tf.ones((4,)), x=[img, mask, bbox])\nfig = d.paint_figure()\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>colormap</code> <code>str</code> <p>What colormap to use when rendering greyscale images. A good colorization option is 'inferno'.</p> <code>'Greys'</code> <code>**kwargs</code> <code>Union[Tensor, List[Tensor]]</code> <p>image_title / image pairs for visualization. Images with the same batch dimensions will be laid out side-by-side, with earlier kwargs entries displayed further to the left. The value part of the key/value pair can be a list of tensors, in which case the elements of the list are overlaid. This can be useful for displaying masks and bounding boxes on top of images. In such cases, the largest image should be put as the first entry in the list. Bounding boxes should be shaped like (batch, n_boxes, box), where each box is formatted like (x0, y0, width, height[, label]).</p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If a list of Tensors is provided as an input, but that list has an inconsistent batch dimension.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>class ImgData(OrderedDict):\n\"\"\"A container for image related data.\n    This class is useful for automatically laying out collections of images for comparison and visualization.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    fig = d.paint_figure()\n    plt.show()\n    img = 0.5*np.ones((4, 32, 32, 3))\n    mask = np.zeros_like(img)\n    mask[0, 10:20, 10:30, :] = [1, 0, 0]\n    mask[1, 5:15, 5:20, :] = [0, 1, 0]\n    bbox = np.array([[[3,7,10,6,'box1'], [20,20,8,8,'box2']]]*4)\n    d = fe.util.ImgData(y=tf.ones((4,)), x=[img, mask, bbox])\n    fig = d.paint_figure()\n    plt.show()\n    ```\n    Args:\n        colormap: What colormap to use when rendering greyscale images. A good colorization option is 'inferno'.\n        **kwargs: image_title / image pairs for visualization. Images with the same batch dimensions will be laid out\n            side-by-side, with earlier kwargs entries displayed further to the left. The value part of the key/value\n            pair can be a list of tensors, in which case the elements of the list are overlaid. This can be useful for\n            displaying masks and bounding boxes on top of images. In such cases, the largest image should be put as the\n            first entry in the list. Bounding boxes should be shaped like (batch, n_boxes, box), where each box is\n            formatted like (x0, y0, width, height[, label]).\n    Raises:\n        AssertionError: If a list of Tensors is provided as an input, but that list has an inconsistent batch dimension.\n    \"\"\"\nn_elements: Dict[int, List[str]]\ndef __init__(self, colormap: str = \"Greys\", **kwargs: Union[Tensor, List[Tensor]]) -&gt; None:\nself.n_elements = {}  # Not a default dict b/c that complicates the computations later\nself.colormap = colormap\nsuper().__init__(**kwargs)\ndef __setitem__(self, key: str, value: Union[Tensor, List[Tensor]]):\n# Convert all values into a list for consistency\nvalue = to_list(value)\nbatch_size = value[0].shape[0]\nfor elem in value[1:]:\nassert elem.shape[0] == batch_size, \"Provided item has an inconsistent batch size\"\nsuper().__setitem__(key, value)\nself.n_elements.setdefault(batch_size, []).append(key)\ndef __delitem__(self, key: str):\nsuper().__delitem__(key)\nfor k, lst in self.n_elements.items():\nlst.remove(key)\nif len(lst) == 0:\ndel self.n_elements[k]\ndef _to_grid(self) -&gt; List[List[Tuple[str, np.ndarray]]]:\n\"\"\"Convert the elements of ImgData into a grid view.\n        One row in the grid is generated for each unique batch dimension present within the ImgData. Each column in the\n        grid is a tensor with batch dimension matching the current row. Columns are given in the order they were input\n        into the ImgData constructor.\n        Returns:\n            The ImgData arranged as a grid, with entries in the grid as (key, value) pairs.\n        \"\"\"\nsorted_sections = sorted(self.n_elements.keys())\nreturn [[(key, self[key]) for key in self.n_elements[n_rows]] for n_rows in sorted_sections]\ndef _n_rows(self) -&gt; int:\n\"\"\"Computes how many rows are present in the ImgData grid.\n        Returns:\n            The number of rows in the ImgData grid.\n        \"\"\"\nreturn len(self.n_elements)\ndef _n_cols(self) -&gt; int:\n\"\"\"Computes how many columns are present in the ImgData grid.\n        Returns:\n            The number of columns in the ImgData grid.\n        \"\"\"\nreturn max((len(elem) for elem in self.n_elements.values()))\n@staticmethod\ndef _shape_to_width(shape: Tuple[int], min_width=200) -&gt; int:\n\"\"\"Decide the width of an image for visualization.\n        Args:\n            shape: The shape of the image.\n            min_width: The minimum desired width for visualization.\n        Returns:\n            The maximum between the width specified by `shape` and the given `min_width` value.\n        \"\"\"\nif len(shape) &lt; 2:\n# text field, use default width\npass\nelif len(shape) == 2:\n# image field: width x height\nmin_width = max(shape[0], min_width)\nelse:\n# image field: batch x width x height\nmin_width = max(shape[1], min_width)\nreturn min_width\n@staticmethod\ndef _shape_to_height(shape: Tuple[int], min_height=200) -&gt; int:\n\"\"\"Decide the height of an image for visualization.\n        Args:\n            shape: The shape of the image.\n            min_height: The minimum desired width for visualization.\n        Returns:\n            The maximum between the height specified by `shape` and the given `min_height` value.\n        \"\"\"\nif len(shape) &lt; 2:\n# text field, use default width\npass\nelif len(shape) == 2:\n# image field: width x height\nmin_height = max(shape[0], min_height)\nelse:\n# image field: batch x width x height\nmin_height = max(shape[1], min_height) * shape[0]\nreturn min_height\ndef _widths(self, row: int, gap: int = 50, min_width: int = 200) -&gt; List[Tuple[int, int]]:\n\"\"\"Get the display widths of a particular row.\n        Args:\n            row: The row to measure.\n            gap: How much space to allow between each column.\n            min_width: The minimum width for a column.\n        Returns:\n            A list of (x1, x2) coordinates marking the beginning and end coordinates of each column in the `row`.\n        \"\"\"\nkeys = list(sorted(self.n_elements.keys()))\n# For overlay values consider the zeroth element for the shape\nrow = [self[key][0] for key in self.n_elements[keys[row]]]\nwidths = [(0, ImgData._shape_to_width(row[0].shape, min_width=min_width))]\nfor img in row[1:]:\nwidths.append(\n(widths[-1][1] + gap, widths[-1][1] + gap + ImgData._shape_to_width(img.shape, min_width=min_width)))\nreturn widths\ndef _total_width(self, gap: int = 50, min_width: int = 200) -&gt; int:\n\"\"\"Get the total width necessary for the image by considering the widths of each row.\n        Args:\n            gap: The horizontal space between each column in the grid.\n            min_width: The minimum width of a column.\n        Returns:\n            The total width of the image.\n        \"\"\"\nreturn max(\n(self._widths(row, gap=gap, min_width=min_width)[-1][-1] for row in range(len(self.n_elements.keys()))))\ndef _heights(self, gap: int = 100, min_height: int = 200) -&gt; List[Tuple[int, int]]:\n\"\"\"Get the display heights of each row.\n        Args:\n            gap: How much space to allow between each row.\n            min_height: The minimum height for a row.\n        Returns:\n            A list of (y1, y2) coordinates marking the top and bottom coordinates of each row in the grid.\n        \"\"\"\nkeys = list(sorted(self.n_elements.keys()))\n# For overlay values consider the zeroth element for the shape\nrows = [[self[key][0] for key in self.n_elements[keys[row]]] for row in range(self._n_rows())]\nheights = [\nmax((ImgData._shape_to_height(elem.shape, min_height=min_height) for elem in rows[i]))\nfor i in range(self._n_rows())\n]\noffset = 10\nresult = [(offset, heights[0] + offset)]\nfor height in heights[1:]:\nresult.append((result[-1][1] + gap, result[-1][1] + gap + height))\nreturn result\ndef _total_height(self, gap: int = 100, min_height: int = 200) -&gt; int:\n\"\"\"Get the total height necessary for the image by considering the heights of each row.\n        Args:\n            gap: The vertical space between each row in the grid.\n            min_height: The minimum height of a row.\n        Returns:\n            The total height of the image.\n        \"\"\"\nheights = self._heights(gap=gap, min_height=min_height)\n# Add some space at the top for the labels\nreturn heights[-1][1] + 30\ndef _batch_size(self, row: int) -&gt; int:\n\"\"\"Get the batch size associated with the given `row`.\n        Args:\n            row: The row for which to report the batch size.\n        Returns:\n            The batch size of all of the entries in the row.\n        \"\"\"\nreturn sorted(self.n_elements.keys())[row]\ndef paint_figure(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96,\nsave_path: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"Visualize the current ImgData entries in a matplotlib figure.\n        ```python\n        d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n        fig = d.paint_figure()\n        plt.show()\n        ```\n        Args:\n            height_gap: How much space to put between each row.\n            min_height: The minimum height of a row.\n            width_gap: How much space to put between each column.\n            min_width: The minimum width of a column.\n            dpi: The resolution of the image to display.\n            save_path: If provided, the figure will be saved to the given path.\n        Returns:\n            The handle to the generated matplotlib figure.\n        \"\"\"\ntotal_width = self._total_width(gap=width_gap, min_width=min_width)\ntotal_height = self._total_height(gap=height_gap, min_height=min_height)\nfig = plt.figure(figsize=(total_width / dpi, total_height / dpi), dpi=dpi)\ngrid = self._to_grid()\n# TODO - elements with batch size = 1 should be laid out in a grid like for plotting\nfor row_idx, (start_height, end_height) in enumerate(self._heights(gap=height_gap, min_height=min_height)):\nrow = grid[row_idx]\nbatch_size = self._batch_size(row_idx)\ngs = GridSpec(nrows=batch_size,\nncols=total_width,\nfigure=fig,\nleft=0.0,\nright=1.0,\nbottom=start_height / total_height,\ntop=end_height / total_height,\nhspace=0.05,\nwspace=0.0)\nfor batch_idx in range(batch_size):\nfor col_idx, width in enumerate(self._widths(row=row_idx, gap=width_gap, min_width=min_width)):\nax = fig.add_subplot(gs[batch_idx, width[0]:width[1]])\nimg_stack = [elem[batch_idx] for elem in row[col_idx][1]]\nfor idx, img in enumerate(img_stack):\nshow_image(img,\naxis=ax,\nfig=fig,\ntitle=row[col_idx][0] if (batch_idx == 0 and idx == 0) else None,\nstack_depth=idx,\ncolor_map=self.colormap)\nif save_path:\nplt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\nreturn fig\ndef paint_numpy(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96) -&gt; np.ndarray:\n\"\"\"Visualize the current ImgData entries into an image stored in a numpy array.\n        ```python\n        d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n        img = d.paint_numpy()\n        plt.imshow(img[0])\n        plt.show()\n        ```\n        Args:\n            height_gap: How much space to put between each row.\n            min_height: The minimum height of a row.\n            width_gap: How much space to put between each column.\n            min_width: The minimum width of a column.\n            dpi: The resolution of the image to display.\n        Returns:\n            A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.\n        \"\"\"\nfig = self.paint_figure(height_gap=height_gap,\nmin_height=min_height,\nwidth_gap=width_gap,\nmin_width=min_width,\ndpi=dpi)\ncanvas = plt_backend_agg.FigureCanvasAgg(fig)\ncanvas.draw()\ndata = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\nw, h = fig.canvas.get_width_height()\ndata = data.reshape([h, w, 4])[:, :, 0:3]\nplt.close(fig)\nreturn np.stack([data])  # Add a batch dimension\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData.paint_figure", "title": "<code>paint_figure</code>", "text": "<p>Visualize the current ImgData entries in a matplotlib figure.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nfig = d.paint_figure()\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>height_gap</code> <code>int</code> <p>How much space to put between each row.</p> <code>100</code> <code>min_height</code> <code>int</code> <p>The minimum height of a row.</p> <code>200</code> <code>width_gap</code> <code>int</code> <p>How much space to put between each column.</p> <code>50</code> <code>min_width</code> <code>int</code> <p>The minimum width of a column.</p> <code>200</code> <code>dpi</code> <code>int</code> <p>The resolution of the image to display.</p> <code>96</code> <code>save_path</code> <code>Optional[str]</code> <p>If provided, the figure will be saved to the given path.</p> <code>None</code> <p>Returns:</p> Type Description <code>plt.Figure</code> <p>The handle to the generated matplotlib figure.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>def paint_figure(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96,\nsave_path: Optional[str] = None) -&gt; plt.Figure:\n\"\"\"Visualize the current ImgData entries in a matplotlib figure.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    fig = d.paint_figure()\n    plt.show()\n    ```\n    Args:\n        height_gap: How much space to put between each row.\n        min_height: The minimum height of a row.\n        width_gap: How much space to put between each column.\n        min_width: The minimum width of a column.\n        dpi: The resolution of the image to display.\n        save_path: If provided, the figure will be saved to the given path.\n    Returns:\n        The handle to the generated matplotlib figure.\n    \"\"\"\ntotal_width = self._total_width(gap=width_gap, min_width=min_width)\ntotal_height = self._total_height(gap=height_gap, min_height=min_height)\nfig = plt.figure(figsize=(total_width / dpi, total_height / dpi), dpi=dpi)\ngrid = self._to_grid()\n# TODO - elements with batch size = 1 should be laid out in a grid like for plotting\nfor row_idx, (start_height, end_height) in enumerate(self._heights(gap=height_gap, min_height=min_height)):\nrow = grid[row_idx]\nbatch_size = self._batch_size(row_idx)\ngs = GridSpec(nrows=batch_size,\nncols=total_width,\nfigure=fig,\nleft=0.0,\nright=1.0,\nbottom=start_height / total_height,\ntop=end_height / total_height,\nhspace=0.05,\nwspace=0.0)\nfor batch_idx in range(batch_size):\nfor col_idx, width in enumerate(self._widths(row=row_idx, gap=width_gap, min_width=min_width)):\nax = fig.add_subplot(gs[batch_idx, width[0]:width[1]])\nimg_stack = [elem[batch_idx] for elem in row[col_idx][1]]\nfor idx, img in enumerate(img_stack):\nshow_image(img,\naxis=ax,\nfig=fig,\ntitle=row[col_idx][0] if (batch_idx == 0 and idx == 0) else None,\nstack_depth=idx,\ncolor_map=self.colormap)\nif save_path:\nplt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/img_data.html#fastestimator.fastestimator.util.img_data.ImgData.paint_numpy", "title": "<code>paint_numpy</code>", "text": "<p>Visualize the current ImgData entries into an image stored in a numpy array.</p> <pre><code>d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\nimg = d.paint_numpy()\nplt.imshow(img[0])\nplt.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>height_gap</code> <code>int</code> <p>How much space to put between each row.</p> <code>100</code> <code>min_height</code> <code>int</code> <p>The minimum height of a row.</p> <code>200</code> <code>width_gap</code> <code>int</code> <p>How much space to put between each column.</p> <code>50</code> <code>min_width</code> <code>int</code> <p>The minimum width of a column.</p> <code>200</code> <code>dpi</code> <code>int</code> <p>The resolution of the image to display.</p> <code>96</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.</p> Source code in <code>fastestimator\\fastestimator\\util\\img_data.py</code> <pre><code>def paint_numpy(self,\nheight_gap: int = 100,\nmin_height: int = 200,\nwidth_gap: int = 50,\nmin_width: int = 200,\ndpi: int = 96) -&gt; np.ndarray:\n\"\"\"Visualize the current ImgData entries into an image stored in a numpy array.\n    ```python\n    d = fe.util.ImgData(y=tf.ones((4,)), x=0.5*tf.ones((4, 32, 32, 3)))\n    img = d.paint_numpy()\n    plt.imshow(img[0])\n    plt.show()\n    ```\n    Args:\n        height_gap: How much space to put between each row.\n        min_height: The minimum height of a row.\n        width_gap: How much space to put between each column.\n        min_width: The minimum width of a column.\n        dpi: The resolution of the image to display.\n    Returns:\n        A numpy array with dimensions (1, height, width, 3) containing an image representation of this ImgData.\n    \"\"\"\nfig = self.paint_figure(height_gap=height_gap,\nmin_height=min_height,\nwidth_gap=width_gap,\nmin_width=min_width,\ndpi=dpi)\ncanvas = plt_backend_agg.FigureCanvasAgg(fig)\ncanvas.draw()\ndata = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\nw, h = fig.canvas.get_width_height()\ndata = data.reshape([h, w, 4])[:, :, 0:3]\nplt.close(fig)\nreturn np.stack([data])  # Add a batch dimension\n</code></pre>"}, {"location": "fastestimator/util/util.html", "title": "util", "text": "<p>Utilities for FastEstimator.</p>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.DefaultKeyDict", "title": "<code>DefaultKeyDict</code>", "text": "<p>         Bases: <code>dict</code></p> <p>Like collections.defaultdict but it passes the key argument to the default function.</p> <pre><code>d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\nprint(d[\"a\"])  # 4\nprint(d[\"c\"])  # \"cc\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>Callable[[Any], Any]</code> <p>A function which takes a key and returns a default value based on the key.</p> required <code>**kwargs</code> <p>Initial key/value pairs for the dictionary.</p> <code>{}</code> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class DefaultKeyDict(dict):\n\"\"\"Like collections.defaultdict but it passes the key argument to the default function.\n    ```python\n    d = fe.util.DefaultKeyDict(default=lambda x: x+x, a=4, b=6)\n    print(d[\"a\"])  # 4\n    print(d[\"c\"])  # \"cc\"\n    ```\n    Args:\n        default: A function which takes a key and returns a default value based on the key.\n        **kwargs: Initial key/value pairs for the dictionary.\n    \"\"\"\ndef __init__(self, default: Callable[[Any], Any], **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.factory = default\ndef __missing__(self, key: Any) -&gt; Any:\nres = self[key] = self.factory(key)\nreturn res\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.NonContext", "title": "<code>NonContext</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which is used to make nothing unusual happen.</p> <pre><code>a = 5\nwith fe.util.NonContext():\na = a + 37\nprint(a)  # 42\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class NonContext(object):\n\"\"\"A class which is used to make nothing unusual happen.\n    ```python\n    a = 5\n    with fe.util.NonContext():\n        a = a + 37\n    print(a)  # 42\n    ```\n    \"\"\"\ndef __enter__(self) -&gt; None:\npass\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Suppressor", "title": "<code>Suppressor</code>", "text": "<p>         Bases: <code>object</code></p> <p>A class which can be used to silence output of function calls.</p> <pre><code>x = lambda: print(\"hello\")\nx()  # \"hello\"\nwith fe.util.Suppressor():\nx()  #\nx()  # \"hello\"\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Suppressor(object):\n\"\"\"A class which can be used to silence output of function calls.\n    ```python\n    x = lambda: print(\"hello\")\n    x()  # \"hello\"\n    with fe.util.Suppressor():\n        x()  #\n    x()  # \"hello\"\n    ```\n    \"\"\"\ndef __enter__(self) -&gt; None:\n# pylint: disable=attribute-defined-outside-init\nself.stdout = sys.stdout\nself.stderr = sys.stderr\n# pylint: enable=attribute-defined-outside-init\nsys.stdout = self\nsys.stderr = self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nsys.stdout = self.stdout\nsys.stderr = self.stderr\ndef write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n        Args:\n            dummy: The string which wanted to be printed.\n        \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Suppressor.write", "title": "<code>write</code>", "text": "<p>A function which is invoked during print calls.</p> <p>Parameters:</p> Name Type Description Default <code>dummy</code> <code>str</code> <p>The string which wanted to be printed.</p> required Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def write(self, dummy: str) -&gt; None:\n\"\"\"A function which is invoked during print calls.\n    Args:\n        dummy: The string which wanted to be printed.\n    \"\"\"\npass\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.Timer", "title": "<code>Timer</code>", "text": "<p>         Bases: <code>ContextDecorator</code></p> <p>A class that can be used to time things.</p> <pre><code>x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\nwith fe.util.Timer():\nx()  # Task took 0.1639 seconds\n@fe.util.Timer(\"T2\")\ndef func():\nreturn x()\nfunc()  # T2 took 0.14819 seconds\n</code></pre> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>class Timer(ContextDecorator):\n\"\"\"A class that can be used to time things.\n    ```python\n    x = lambda: list(map(lambda i: i + i/2, list(range(int(1e6)))))\n    with fe.util.Timer():\n        x()  # Task took 0.1639 seconds\n    @fe.util.Timer(\"T2\")\n    def func():\n        return x()\n    func()  # T2 took 0.14819 seconds\n    ```\n    \"\"\"\ndef __init__(self, name=\"Task\") -&gt; None:\nself.name = name\nself.start = None\nself.end = None\nself.interval = None\ndef __enter__(self) -&gt; 'Timer':\nself.start = time.perf_counter()\nreturn self\ndef __exit__(self, *exc: Tuple[Optional[Type], Optional[Exception], Optional[Any]]) -&gt; None:\nself.end = time.perf_counter()\nself.interval = self.end - self.start\ntf.print(\"{} took {} seconds\".format(self.name, self.interval))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.draw", "title": "<code>draw</code>", "text": "<p>Print our name.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def draw() -&gt; None:\n\"\"\"Print our name.\n    \"\"\"\nprint(Figlet(font=\"slant\").renderText(\"FastEstimator\"))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_batch_size", "title": "<code>get_batch_size</code>", "text": "<p>Infer batch size from a batch dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The batch dictionary.</p> required <p>Returns:</p> Type Description <code>int</code> <p>batch size.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_batch_size(data: Dict[str, Any]) -&gt; int:\n\"\"\"Infer batch size from a batch dictionary.\n    Args:\n        data: The batch dictionary.\n    Returns:\n        batch size.\n    \"\"\"\nassert isinstance(data, dict), \"data input must be a dictionary\"\nbatch_size = set(data[key].shape[0] for key in data if hasattr(data[key], \"shape\") and list(data[key].shape))\nassert len(batch_size) == 1, \"invalid batch size: {}\".format(batch_size)\nreturn batch_size.pop()\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_num_devices", "title": "<code>get_num_devices</code>", "text": "<p>Determine the number of available GPUs.</p> <p>Returns:</p> Type Description <p>The number of available GPUs, or 1 if none are found.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_num_devices():\n\"\"\"Determine the number of available GPUs.\n    Returns:\n        The number of available GPUs, or 1 if none are found.\n    \"\"\"\nreturn max(torch.cuda.device_count(), 1)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_shape", "title": "<code>get_shape</code>", "text": "<p>A function to find the shapes of an object or sequence of objects.</p> <p>Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an attempt will be made to determine which of the interior dimensions are ragged.</p> <pre><code>x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\nx = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\nx = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data to infer the shape of.</p> required <p>Returns:</p> Type Description <code>List[Optional[int]]</code> <p>A list representing the shape of the data.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_shape(obj: Any) -&gt; List[Optional[int]]:\n\"\"\"A function to find the shapes of an object or sequence of objects.\n    Lists or Tuples will assume that the zeroth dimension is ragged (shape==None). If entries in the list have\n    mismatched ranks, then only the list dimension will be considered as part of the shape. If all ranks are equal, an\n    attempt will be made to determine which of the interior dimensions are ragged.\n    ```python\n    x = fe.util.get_shape(np.ones((12,22,11)))  # [12, 22, 11]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5))])  # [None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((18, 5, 4))])  # [None, None, None, None]\n    x = fe.util.get_shape([np.ones((12,22,11)), np.ones((12, 22, 4))])  # [None, 12, 22, None]\n    x = fe.util.get_shape({\"a\": np.ones((12,22,11))})  # []\n    ```\n    Args:\n        obj: Data to infer the shape of.\n    Returns:\n        A list representing the shape of the data.\n    \"\"\"\nif hasattr(obj, \"shape\"):\nresult = list(obj.shape)\nelif isinstance(obj, (List, Tuple)):\nshapes = [get_shape(ob) for ob in obj]\nresult = [None]\nif shapes:\nrank = len(shapes[0])\nif any((len(shape) != rank for shape in shapes)):\nreturn result\nresult.extend(shapes[0])\nfor shape in shapes[1:]:\nfor idx, dim in enumerate(shape):\nif result[idx + 1] != dim:\nresult[idx + 1] = None\nelse:\nresult = []\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.get_type", "title": "<code>get_type</code>", "text": "<p>A function to try and infer the types of data within containers.</p> <pre><code>x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\nx = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\nx = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\nx = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\nx = fe.util.get_type(27)  # \"int\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Data which may be wrapped in some kind of container.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the data type of the <code>obj</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def get_type(obj: Any) -&gt; str:\n\"\"\"A function to try and infer the types of data within containers.\n    ```python\n    x = fe.util.get_type(np.ones((10, 10), dtype='int32'))  # \"int32\"\n    x = fe.util.get_type(tf.ones((10, 10), dtype='float16'))  # \"&lt;dtype: 'float16'&gt;\"\n    x = fe.util.get_type(torch.ones((10, 10)).type(torch.float))  # \"torch.float32\"\n    x = fe.util.get_type([np.ones((10,10)) for i in range(4)])  # \"List[float64]\"\n    x = fe.util.get_type(27)  # \"int\"\n    ```\n    Args:\n        obj: Data which may be wrapped in some kind of container.\n    Returns:\n        A string representation of the data type of the `obj`.\n    \"\"\"\nif hasattr(obj, \"dtype\"):\nresult = str(obj.dtype)\nelif isinstance(obj, (List, Tuple)):\nif len(obj) &gt; 0:\nresult = \"List[{}]\".format(get_type(obj[0]))\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nelse:\nresult = strip_suffix(strip_prefix(str(type(obj)), \"&lt;class '\"), \"'&gt;\")\nreturn result\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.is_number", "title": "<code>is_number</code>", "text": "<p>Check if a given string can be converted into a number.</p> <pre><code>x = fe.util.is_number(\"13.7\")  # True\nx = fe.util.is_number(\"ae13.7\")  # False\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>str</code> <p>A potentially numeric input string.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True iff <code>arg</code> represents a number.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def is_number(arg: str) -&gt; bool:\n\"\"\"Check if a given string can be converted into a number.\n    ```python\n    x = fe.util.is_number(\"13.7\")  # True\n    x = fe.util.is_number(\"ae13.7\")  # False\n    ```\n    Args:\n        arg: A potentially numeric input string.\n    Returns:\n        True iff `arg` represents a number.\n    \"\"\"\ntry:\nfloat(arg)\nreturn True\nexcept (ValueError, TypeError):\nreturn False\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_batch", "title": "<code>pad_batch</code>", "text": "<p>A function to pad a batch of data in-place by appending to the ends of the tensors.</p> <pre><code>data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\nfe.util.pad_batch(data, pad_value=0)\nprint(data)  # [{'x': [[1., 1.], [1., 1.],[0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[MutableMapping[str, Any]]</code> <p>A list of data to be padded.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to pad with.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the data within the batch do not have matching ranks.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_batch(batch: List[MutableMapping[str, Any]], pad_value: Union[float, int]) -&gt; None:\n\"\"\"A function to pad a batch of data in-place by appending to the ends of the tensors.\n    ```python\n    data = [{\"x\": np.ones((2, 2)), \"y\": 8}, {\"x\": np.ones((3, 1)), \"y\": 4}]\n    fe.util.pad_batch(data, pad_value=0)\n    print(data)  # [{'x': [[1., 1.], [1., 1.],[0., 0.]], 'y': 8}, {'x': [[1., 0.], [1., 0.], [1., 0.]]), 'y': 4}]\n    ```\n    Args:\n        batch: A list of data to be padded.\n        pad_value: The value to pad with.\n    Raises:\n        AssertionError: If the data within the batch do not have matching ranks.\n    \"\"\"\nfor key in batch[0].keys():\nshapes = [data[key].shape for data in batch if hasattr(data[key], \"shape\")]\nif len(set(shapes)) &gt; 1:\nassert len(set(len(shape) for shape in shapes)) == 1, \"data within batch must have same rank\"\nmax_shapes = tuple(np.max(np.array(shapes), axis=0))\nfor data in batch:\ndata[key] = pad_data(data[key], max_shapes, pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.pad_data", "title": "<code>pad_data</code>", "text": "<p>Pad <code>data</code> by appending <code>pad_value</code>s along it's dimensions until the <code>target_shape</code> is reached.</p> <pre><code>x = np.ones((1,2))\nx = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>np.ndarray</code> <p>The data to be padded.</p> required <code>target_shape</code> <code>Tuple[int, ...]</code> <p>The desired shape for <code>data</code>. Should have the same rank as <code>data</code>, with each dimension being &gt;= the size of the <code>data</code> dimension.</p> required <code>pad_value</code> <code>Union[float, int]</code> <p>The value to insert into <code>data</code> if padding is required to achieve the <code>target_shape</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The <code>data</code>, padded to the <code>target_shape</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def pad_data(data: np.ndarray, target_shape: Tuple[int, ...], pad_value: Union[float, int]) -&gt; np.ndarray:\n\"\"\"Pad `data` by appending `pad_value`s along it's dimensions until the `target_shape` is reached.\n    ```python\n    x = np.ones((1,2))\n    x = fe.util.pad_data(x, target_shape=(3, 3), pad_value = -2)  # [[1, 1, -2], [-2, -2, -2], [-2, -2, -2]]\n    ```\n    Args:\n        data: The data to be padded.\n        target_shape: The desired shape for `data`. Should have the same rank as `data`, with each dimension being &gt;=\n            the size of the `data` dimension.\n        pad_value: The value to insert into `data` if padding is required to achieve the `target_shape`.\n    Returns:\n        The `data`, padded to the `target_shape`.\n    \"\"\"\nshape_difference = np.array(target_shape) - np.array(data.shape)\npadded_shape = np.array([np.zeros_like(shape_difference), shape_difference]).T\nreturn np.pad(data, padded_shape, 'constant', constant_values=pad_value)\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.parse_modes", "title": "<code>parse_modes</code>", "text": "<p>A function to determine which modes to run on based on a set of modes potentially containing blacklist values.</p> <pre><code>m = fe.util.parse_modes({\"train\"})  # {\"train\"}\nm = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\nm = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\nm = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modes</code> <code>Set[str]</code> <p>The desired modes to run on (possibly containing blacklisted modes).</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p>The modes to run on (converted to a whitelist).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def parse_modes(modes: Set[str]) -&gt; Set[str]:\n\"\"\"A function to determine which modes to run on based on a set of modes potentially containing blacklist values.\n    ```python\n    m = fe.util.parse_modes({\"train\"})  # {\"train\"}\n    m = fe.util.parse_modes({\"!train\"})  # {\"eval\", \"test\", \"infer\"}\n    m = fe.util.parse_modes({\"train\", \"eval\"})  # {\"train\", \"eval\"}\n    m = fe.util.parse_modes({\"!train\", \"!infer\"})  # {\"eval\", \"test\"}\n    ```\n    Args:\n        modes: The desired modes to run on (possibly containing blacklisted modes).\n    Returns:\n        The modes to run on (converted to a whitelist).\n    Raises:\n        AssertionError: If invalid modes are detected, or if blacklisted modes and whitelisted modes are mixed.\n    \"\"\"\nvalid_fields = {\"train\", \"eval\", \"test\", \"infer\", \"!train\", \"!eval\", \"!test\", \"!infer\"}\nassert modes.issubset(valid_fields), \"Invalid modes argument {}\".format(modes - valid_fields)\nnegation = set([mode.startswith(\"!\") for mode in modes])\nassert len(negation) &lt; 2, \"cannot mix !mode with mode, found {}\".format(modes)\nif True in negation:\nnew_modes = {\"train\", \"eval\", \"test\", \"infer\"}\nfor mode in modes:\nnew_modes.discard(mode.strip(\"!\"))\nmodes = new_modes\nreturn modes\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.parse_string_to_python", "title": "<code>parse_string_to_python</code>", "text": "<p>Convert a string into a python object.</p> <pre><code>x = fe.util.parse_string_to_python(\"5\")  # 5\nx = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\nx = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>val</code> <code>str</code> <p>An input string.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A python object version of the input string.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def parse_string_to_python(val: str) -&gt; Any:\n\"\"\"Convert a string into a python object.\n    ```python\n    x = fe.util.parse_string_to_python(\"5\")  # 5\n    x = fe.util.parse_string_to_python(\"[5, 4, 0.3]\")  # [5, 4, 0.3]\n    x = fe.util.parse_string_to_python(\"{'a':5, 'b':7}\")  # {'a':5, 'b':7}\n    ```\n    Args:\n        val: An input string.\n    Returns:\n        A python object version of the input string.\n    \"\"\"\nif val is None or not val:\nreturn \"\"\ntry:\nreturn literal_eval(val)\nexcept (ValueError, SyntaxError):\ntry:\nreturn json.loads(val)\nexcept json.JSONDecodeError:\nreturn val\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.prettify_metric_name", "title": "<code>prettify_metric_name</code>", "text": "<p>Add spaces to camel case words, then swap _ for space, and capitalize each word.</p> <pre><code>x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>A string to be formatted.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The formatted version of 'metric'.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def prettify_metric_name(metric: str) -&gt; str:\n\"\"\"Add spaces to camel case words, then swap _ for space, and capitalize each word.\n    ```python\n    x = fe.util.prettify_metric_name(\"myUgly_loss\")  # \"My Ugly Loss\"\n    ```\n    Args:\n        metric: A string to be formatted.\n    Returns:\n        The formatted version of 'metric'.\n    \"\"\"\nreturn string.capwords(re.sub(\"([a-z])([A-Z])\", r\"\\g&lt;1&gt; \\g&lt;2&gt;\", metric).replace(\"_\", \" \"))\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.show_image", "title": "<code>show_image</code>", "text": "<p>Plots a given image onto an axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>plt.Axes</code> <p>The matplotlib axis to plot on, or None for a new plot.</p> <code>None</code> <code>fig</code> <code>plt.Figure</code> <p>A reference to the figure to plot on, or None if new plot.</p> <code>None</code> <code>im</code> <code>Union[np.ndarray, Tensor]</code> <p>The image to display (width X height).</p> required <code>title</code> <code>Optional[str]</code> <p>A title for the image.</p> <code>None</code> <code>color_map</code> <code>str</code> <p>Which colormap to use for greyscale images.</p> <code>'inferno'</code> <code>stack_depth</code> <code>int</code> <p>Multiple images can be drawn onto the same axis. When stack depth is greater than zero, the <code>im</code> will be alpha blended on top of a given axis.</p> <code>0</code> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def show_image(im: Union[np.ndarray, Tensor],\naxis: plt.Axes = None,\nfig: plt.Figure = None,\ntitle: Optional[str] = None,\ncolor_map: str = \"inferno\",\nstack_depth: int = 0) -&gt; Optional[plt.Figure]:\n\"\"\"Plots a given image onto an axis.\n    Args:\n        axis: The matplotlib axis to plot on, or None for a new plot.\n        fig: A reference to the figure to plot on, or None if new plot.\n        im: The image to display (width X height).\n        title: A title for the image.\n        color_map: Which colormap to use for greyscale images.\n        stack_depth: Multiple images can be drawn onto the same axis. When stack depth is greater than zero, the `im`\n            will be alpha blended on top of a given axis.\n    \"\"\"\nif axis is None:\nfig, axis = plt.subplots(1, 1)\naxis.axis('off')\n# Compute width of axis for text font size\nbbox = axis.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\nwidth, height = bbox.width * fig.dpi, bbox.height * fig.dpi\nspace = min(width, height)\nif not hasattr(im, 'shape') or len(im.shape) &lt; 2:\n# text data\nim = to_number(im)\nif hasattr(im, 'shape') and len(im.shape) == 1:\nim = im[0]\nim = im.item()\nif isinstance(im, bytes):\nim = im.decode('utf8')\ntext = \"{}\".format(im)\naxis.text(0.5,\n0.5,\nim,\nha='center',\ntransform=axis.transAxes,\nva='center',\nwrap=False,\nfamily='monospace',\nfontsize=min(45, space // len(text)))\nelif len(im.shape) == 2 and (im.shape[1] == 4 or im.shape[1] == 5):\n# Bounding Box Data. Should be (x0, y0, w, h, &lt;label&gt;)\nboxes = []\nim = to_number(im)\ncolor = [\"m\", \"r\", \"c\", \"g\", \"y\", \"b\"][stack_depth % 6]\nfor box in im:\n# Unpack the box, which may or may not have a label\nx0 = int(box[0])\ny0 = int(box[1])\nwidth = int(box[2])\nheight = int(box[3])\nlabel = None if len(box) &lt; 5 else str(box[4])\n# Don't draw empty boxes\nif width == 0 and height == 0:\ncontinue\nr = Rectangle((x0, y0), width=width, height=height, fill=False, edgecolor=color, linewidth=3)\nboxes.append(r)\nif label:\naxis.text(r.get_x() + 3,\nr.get_y() + 3,\nlabel,\nha='left',\nva='top',\ncolor=color,\nfontsize=min(14, width // len(label)),\nfontweight='bold',\nfamily='monospace')\npc = PatchCollection(boxes, match_original=True)\naxis.add_collection(pc)\nelse:\nif isinstance(im, torch.Tensor) and len(im.shape) &gt; 2:\n# Move channel first to channel last\nchannels = list(range(len(im.shape)))\nchannels.append(channels.pop(0))\nim = im.permute(*channels)\n# image data\nim = to_number(im)\nif np.issubdtype(im.dtype, np.integer):\n# im is already in int format\nim = im.astype(np.uint8)\nelif np.max(im) &lt;= 1 and np.min(im) &gt;= 0:  # im is [0,1]\nim = (im * 255).astype(np.uint8)\nelif np.min(im) &gt;= -1 and np.max(im) &lt;= 1:  # im is [-1, 1]\nim = ((im + 1) * 127.5).astype(np.uint8)\nelse:  # im is in some arbitrary range, probably due to the Normalize Op\nma = abs(np.max(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nmi = abs(np.min(im, axis=tuple([i for i in range(len(im.shape) - 1)]) if len(im.shape) &gt; 2 else None))\nim = (((im + mi) / (ma + mi)) * 255).astype(np.uint8)\n# matplotlib doesn't support (x,y,1) images, so convert them to (x,y)\nif len(im.shape) == 3 and im.shape[2] == 1:\nim = np.reshape(im, (im.shape[0], im.shape[1]))\nalpha = 1 if stack_depth == 0 else 0.3\nif len(im.shape) == 2:\naxis.imshow(im, cmap=plt.get_cmap(name=color_map), alpha=alpha)\nelse:\naxis.imshow(im, alpha=alpha)\nif title is not None:\naxis.set_title(title, fontsize=min(20, 1 + width // len(title)), family='monospace')\nreturn fig\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.strip_prefix", "title": "<code>strip_prefix</code>", "text": "<p>Remove the given <code>prefix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\nx = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>prefix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def strip_prefix(target: Optional[str], prefix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `prefix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_prefix(\"astring.json\", \"ast\")  # \"ring.json\"\n    x = fe.util.strip_prefix(\"astring.json\", \"asa\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        prefix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif prefix is None or target is None:\nreturn target\ns_len = len(prefix)\nif target[:s_len] == prefix:\nreturn target[s_len:]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.strip_suffix", "title": "<code>strip_suffix</code>", "text": "<p>Remove the given <code>suffix</code> from the <code>target</code> if it is present there.</p> <pre><code>x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\nx = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[str]</code> <p>A string to be formatted.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A string to be removed from <code>target</code>.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The formatted version of <code>target</code>.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def strip_suffix(target: Optional[str], suffix: Optional[str]) -&gt; Optional[str]:\n\"\"\"Remove the given `suffix` from the `target` if it is present there.\n    ```python\n    x = fe.util.strip_suffix(\"astring.json\", \".json\")  # \"astring\"\n    x = fe.util.strip_suffix(\"astring.json\", \".yson\")  # \"astring.json\"\n    ```\n    Args:\n        target: A string to be formatted.\n        suffix: A string to be removed from `target`.\n    Returns:\n        The formatted version of `target`.\n    \"\"\"\nif suffix is None or target is None:\nreturn target\ns_len = len(suffix)\nif target[-s_len:] == suffix:\nreturn target[:-s_len]\nreturn target\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_list", "title": "<code>to_list</code>", "text": "<p>Convert data to a list. A single None value will be converted to the empty list.</p> <pre><code>x = fe.util.to_list(None)  # []\nx = fe.util.to_list([None])  # [None]\nx = fe.util.to_list(7)  # [7]\nx = fe.util.to_list([7, 8])  # [7,8]\nx = fe.util.to_list({7})  # [7]\nx = fe.util.to_list((7))  # [7]\nx = fe.util.to_list({'a': 7})  # [{'a': 7}]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container.</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>The input <code>data</code> but inside a list instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_list(data: Any) -&gt; List[Any]:\n\"\"\"Convert data to a list. A single None value will be converted to the empty list.\n    ```python\n    x = fe.util.to_list(None)  # []\n    x = fe.util.to_list([None])  # [None]\n    x = fe.util.to_list(7)  # [7]\n    x = fe.util.to_list([7, 8])  # [7,8]\n    x = fe.util.to_list({7})  # [7]\n    x = fe.util.to_list((7))  # [7]\n    x = fe.util.to_list({'a': 7})  # [{'a': 7}]\n    ```\n    Args:\n        data: Input data, within or without a python container.\n    Returns:\n        The input `data` but inside a list instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn []\nif not isinstance(data, list):\nif isinstance(data, (tuple, set)):\ndata = list(data)\nelse:\ndata = [data]\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/util.html#fastestimator.fastestimator.util.util.to_set", "title": "<code>to_set</code>", "text": "<p>Convert data to a set. A single None value will be converted to the empty set.</p> <pre><code>x = fe.util.to_set(None)  # {}\nx = fe.util.to_set([None])  # {None}\nx = fe.util.to_set(7)  # {7}\nx = fe.util.to_set([7, 8])  # {7,8}\nx = fe.util.to_set({7})  # {7}\nx = fe.util.to_set((7))  # {7}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Input data, within or without a python container. The <code>data</code> must be hashable.</p> required <p>Returns:</p> Type Description <code>Set[Any]</code> <p>The input <code>data</code> but inside a set instead of whatever other container type used to hold it.</p> Source code in <code>fastestimator\\fastestimator\\util\\util.py</code> <pre><code>def to_set(data: Any) -&gt; Set[Any]:\n\"\"\"Convert data to a set. A single None value will be converted to the empty set.\n    ```python\n    x = fe.util.to_set(None)  # {}\n    x = fe.util.to_set([None])  # {None}\n    x = fe.util.to_set(7)  # {7}\n    x = fe.util.to_set([7, 8])  # {7,8}\n    x = fe.util.to_set({7})  # {7}\n    x = fe.util.to_set((7))  # {7}\n    ```\n    Args:\n        data: Input data, within or without a python container. The `data` must be hashable.\n    Returns:\n        The input `data` but inside a set instead of whatever other container type used to hold it.\n    \"\"\"\nif data is None:\nreturn set()\nif not isinstance(data, set):\nif isinstance(data, (tuple, list, KeysView)):\ndata = set(data)\nelse:\ndata = {data}\nreturn data\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html", "title": "wget_util", "text": ""}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.bar_custom", "title": "<code>bar_custom</code>", "text": "<p>Return progress bar string for given values in one of three styles depending on available width.</p> <p>This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.</p> The bar will be one of the following formats depending on available width <p>[..  ] downloaded / total downloaded / total [.. ]</p> <p>If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:     %s / unknown     %s</p> <p>If there is not enough space on the screen, do not display anything. The returned string doesn't include control characters like   used to place cursor at the beginning of the line to erase previous content.</p> <p>This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.</p> <pre><code>wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>current</code> <code>float</code> <p>The current amount of progress.</p> required <code>total</code> <code>float</code> <p>The total amount of progress required by the task.</p> required <code>width</code> <code>int</code> <p>The available width.</p> <code>80</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string to display the current progress.</p> Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def bar_custom(current: float, total: float, width: int = 80) -&gt; str:\n\"\"\"Return progress bar string for given values in one of three styles depending on available width.\n    This function was modified from wget source code at https://bitbucket.org/techtonik/python-wget/src/default/.\n    The bar will be one of the following formats depending on available width:\n        [..  ] downloaded / total\n        downloaded / total\n        [.. ]\n    If total width is unknown or &lt;= 0, the bar will show a bytes counter using two adaptive styles:\n        %s / unknown\n        %s\n    If there is not enough space on the screen, do not display anything. The returned string doesn't include control\n    characters like \\r used to place cursor at the beginning of the line to erase previous content.\n    This function leaves one free character at the end of the string to avoid automatic linefeed on Windows.\n    ```python\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        current: The current amount of progress.\n        total: The total amount of progress required by the task.\n        width: The available width.\n    Returns:\n        A formatted string to display the current progress.\n    \"\"\"\n# process special case when total size is unknown and return immediately\nif not total or total &lt; 0:\nmsg = \"{} / unknown\".format(current)\nif len(msg) &lt; width:  # leaves one character to avoid linefeed\nreturn msg\nif len(\"{}\".format(current)) &lt; width:\nreturn \"{}\".format(current)\n# --- adaptive layout algorithm ---\n#\n# [x] describe the format of the progress bar\n# [x] describe min width for each data field\n# [x] set priorities for each element\n# [x] select elements to be shown\n#   [x] choose top priority element min_width &lt; avail_width\n#   [x] lessen avail_width by value if min_width\n#   [x] exclude element from priority list and repeat\n#  10% [.. ]  10/100\n# pppp bbbbb sssssss\nmin_width = {\n'percent': 4,  # 100%\n'bar': 3,  # [.]\n'size': len(\"{}\".format(total)) * 2 + 3,  # 'xxxx / yyyy'\n}\npriority = ['percent', 'bar', 'size']\n# select elements to show\nselected = []\navail = width\nfor field in priority:\nif min_width[field] &lt; avail:\nselected.append(field)\navail -= min_width[field] + 1  # +1 is for separator or for reserved space at\n# the end of line to avoid linefeed on Windows\n# render\noutput = ''\nfor field in selected:\nif field == 'percent':\n# fixed size width for percentage\noutput += \"{}%\".format(100 * current // total).rjust(min_width['percent'])\nelif field == 'bar':  # [. ]\n# bar takes its min width + all available space\noutput += wget.bar_thermometer(current, total, min_width['bar'] + avail)\nelif field == 'size':\n# size field has a constant width (min == max)\noutput += \"{:.2f} / {:.2f} MB\".format(current / 1e6, total / 1e6).rjust(min_width['size'])\nselected = selected[1:]\nif selected:\noutput += ' '  # add field separator\nreturn output\n</code></pre>"}, {"location": "fastestimator/util/wget_util.html#fastestimator.fastestimator.util.wget_util.callback_progress", "title": "<code>callback_progress</code>", "text": "<p>Callback function for urlretrieve that is called when a connection is created and then once for each block.</p> <p>Draws adaptive progress bar in terminal/console.</p> <p>Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a linefeed on Windows.</p> <pre><code>import wget\nwget.callback_progress = fe.util.callback_progress\nwget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>blocks</code> <code>int</code> <p>number of blocks transferred so far.</p> required <code>block_size</code> <code>int</code> <p>in bytes.</p> required <code>total_size</code> <code>int</code> <p>in bytes, can be -1 if server doesn't return it.</p> required <code>bar_function</code> <code>Callable[[int, int, int], str]</code> <p>another callback function to visualize progress.</p> required Source code in <code>fastestimator\\fastestimator\\util\\wget_util.py</code> <pre><code>def callback_progress(blocks: int, block_size: int, total_size: int, bar_function: Callable[[int, int, int],\nstr]) -&gt; None:\n\"\"\"Callback function for urlretrieve that is called when a connection is created and then once for each block.\n    Draws adaptive progress bar in terminal/console.\n    Use sys.stdout.write() instead of \"print\", because it allows one more symbols at the line end without triggering a\n    linefeed on Windows.\n    ```python\n    import wget\n    wget.callback_progress = fe.util.callback_progress\n    wget.download('http://url.com', '/save/dir', bar=fe.util.bar_custom)\n    ```\n    Args:\n        blocks: number of blocks transferred so far.\n        block_size: in bytes.\n        total_size: in bytes, can be -1 if server doesn't return it.\n        bar_function: another callback function to visualize progress.\n    \"\"\"\nwidth = min(100, wget.get_console_width())\nif width == 0:  # sys.stdout.fileno() in get_console_width() is not supported in jupyter notebook\nwidth = 80\ncurrent_size = min(blocks * block_size, total_size)\nprogress = bar_function(current_size, total_size, width)\nif progress:\nsys.stdout.write(\"\\r{}\".format(progress))\nif current_size &gt;= total_size:\nsys.stdout.write(\"\\n\")\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html", "title": "saliency", "text": ""}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet", "title": "<code>SaliencyNet</code>", "text": "<p>A class to generate saliency masks from a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model, compiled with fe.build, which is to be inspected.</p> required <code>model_inputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model inputs within the data dictionary.</p> required <code>model_outputs</code> <code>Union[str, Sequence[str]]</code> <p>The key(s) corresponding to the model outputs which are written into the data dictionary.</p> required <code>outputs</code> <code>Union[str, List[str]]</code> <p>The keys(s) under which to write the generated saliency images.</p> <code>'saliency'</code> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>class SaliencyNet:\n\"\"\"A class to generate saliency masks from a given model.\n    Args:\n        model: The model, compiled with fe.build, which is to be inspected.\n        model_inputs: The key(s) corresponding to the model inputs within the data dictionary.\n        model_outputs: The key(s) corresponding to the model outputs which are written into the data dictionary.\n        outputs: The keys(s) under which to write the generated saliency images.\n    \"\"\"\ndef __init__(self,\nmodel: Model,\nmodel_inputs: Union[str, Sequence[str]],\nmodel_outputs: Union[str, Sequence[str]],\noutputs: Union[str, List[str]] = \"saliency\"):\nmode = \"test\"\nself.model_op = ModelOp(model=model, mode=mode, inputs=model_inputs, outputs=model_outputs, trainable=False)\nself.outputs = to_list(outputs)\nself.mode = mode\nself.gather_keys = [\"SaliencyNet_Target_Index_{}\".format(key) for key in self.model_outputs]\nself.network = Network(ops=[\nWatch(inputs=self.model_inputs, mode=mode),\nself.model_op,\nGather(inputs=self.model_outputs,\nindices=self.gather_keys,\noutputs=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\nmode=mode),\nGradientOp(inputs=self.model_inputs,\nfinals=[\"SaliencyNet_Intermediate_{}\".format(key) for key in self.model_outputs],\noutputs=deepcopy(self.outputs),\nmode=mode),\n])\n@property\ndef model_inputs(self):\nreturn deepcopy(self.model_op.inputs)\n@property\ndef model_outputs(self):\nreturn deepcopy(self.model_op.outputs)\n@staticmethod\ndef _convert_for_visualization(tensor: Tensor, tile: int = 99) -&gt; np.ndarray:\n\"\"\"Modify the range of data in a given input `tensor` to be appropriate for visualization.\n        Args:\n            tensor: Input masks, whose channel values are to be reduced by absolute value summation.\n            tile: The percentile [0-100] used to set the max value of the image.\n        Returns:\n            A (batch X width X height) image after visualization clipping is applied.\n        \"\"\"\nif isinstance(tensor, torch.Tensor):\nchannel_axis = 1\nelse:\nchannel_axis = -1\nflattened_mask = reduce_sum(abs(tensor), axis=channel_axis, keepdims=True)\nnon_batch_axes = list(range(len(flattened_mask.shape)))[1:]\nvmax = percentile(flattened_mask, tile, axis=non_batch_axes, keepdims=True)\nvmin = reduce_min(flattened_mask, axis=non_batch_axes, keepdims=True)\nreturn clip_by_value((flattened_mask - vmin) / (vmax - vmin), 0, 1)\ndef get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\ndef _get_mask(self, batch: Dict[str, Any]) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n        Returns:\n            The model outputs and the raw saliency mask(s) for the given `batch` of data. Model predictions are reduced\n            via argmax.\n        \"\"\"\nfor key in self.gather_keys:\n# If there's no target key, use an empty array which will cause the max-likelihood class to be selected\nbatch.setdefault(key, [])\nprediction = self.network.transform(data=batch, mode=self.mode)\nfor key in self.model_outputs:\nprediction[key] = argmax(prediction[key], axis=1)\nreturn prediction\ndef _get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Tensor]:\n\"\"\"Generates raw integrated saliency mask(s) from a given `batch` of data.\n        This method assumes that the Network is already loaded.\n        Args:\n            batch: A batch of input data to be fed to the model.\n            nsamples: How many samples to consider during integration.\n        Returns:\n            The raw integrated saliency mask(s) for the given `batch` of data.\n        \"\"\"\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\ninput_baselines = [zeros_like(ins) + (reduce_max(ins) + reduce_min(ins)) / 2 for ins in model_inputs]\ninput_diffs = [\nmodel_input - input_baseline for model_input, input_baseline in zip(model_inputs, input_baselines)\n]\nresponse = {}\nfor alpha in np.linspace(0.0, 1.0, nsamples):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nx_step = input_baselines[idx] + alpha * input_diffs[idx]\nnoisy_batch[input_name] = x_step\ngrads_and_preds = self._get_mask(noisy_batch)\nfor key in self.outputs:\nif key in response:\nresponse[key] += grads_and_preds[key]\nelse:\nresponse[key] = grads_and_preds[key]\nfor key in self.outputs:\ngrad = response[key]\nfor diff in input_diffs:\ngrad = grad * diff\nresponse[key] = grad\nreturn response\ndef get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n        Args:\n            batch: An input batch of data.\n            stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n            nsamples: Number of samples to average across to get the smooth gradient.\n            nintegration: Number of samples to compute when integrating (None to disable).\n            magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n        Returns:\n            Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nnoisy_batch, nsamples=nintegration)\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\ndef get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n        See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n        Args:\n            batch: An input batch of data.\n            nsamples: Number of samples to average across to get the integrated gradient.\n        Returns:\n            Greyscale saliency masks smoothed via the IntegratedGradient method.\n        \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_integrated_masks", "title": "<code>get_integrated_masks</code>", "text": "<p>Generates integrated greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the integrated gradient.</p> <code>25</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency masks smoothed via the IntegratedGradient method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_integrated_masks(self, batch: Dict[str, Any], nsamples: int = 25) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates integrated greyscale saliency mask(s) from a given `batch` of data.\n    See https://arxiv.org/abs/1703.01365 for background on the IntegratedGradient method.\n    Args:\n        batch: An input batch of data.\n        nsamples: Number of samples to average across to get the integrated gradient.\n    Returns:\n        Greyscale saliency masks smoothed via the IntegratedGradient method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\n# Performing integration might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nresponse.update(self._get_integrated_masks(batch, nsamples=nsamples))\nfor key in self.outputs:\nresponse[key] = self._convert_for_visualization(response[key])\nreturn response\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_masks", "title": "<code>get_masks</code>", "text": "<p>Generates greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>A batch of input data to be fed to the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>The model's classification decisions and greyscale saliency mask(s) for the given <code>batch</code> of data.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_masks(self, batch: Dict[str, Any]) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: A batch of input data to be fed to the model.\n    Returns:\n        The model's classification decisions and greyscale saliency mask(s) for the given `batch` of data.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\ngrads_and_preds = self._get_mask(batch)\nfor key in self.outputs:\ngrads_and_preds[key] = self._convert_for_visualization(grads_and_preds[key])\nreturn grads_and_preds\n</code></pre>"}, {"location": "fastestimator/xai/saliency.html#fastestimator.fastestimator.xai.saliency.SaliencyNet.get_smoothed_masks", "title": "<code>get_smoothed_masks</code>", "text": "<p>Generates smoothed greyscale saliency mask(s) from a given <code>batch</code> of data.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>An input batch of data.</p> required <code>stdev_spread</code> <code>float</code> <p>Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).</p> <code>0.15</code> <code>nsamples</code> <code>int</code> <p>Number of samples to average across to get the smooth gradient.</p> <code>25</code> <code>nintegration</code> <code>Optional[int]</code> <p>Number of samples to compute when integrating (None to disable).</p> <code>None</code> <code>magnitude</code> <code>bool</code> <p>If true, computes the sum of squares of gradients instead of just the sum.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[Tensor, np.ndarray]]</code> <p>Greyscale saliency mask(s) smoothed via the SmoothGrad method.</p> Source code in <code>fastestimator\\fastestimator\\xai\\saliency.py</code> <pre><code>def get_smoothed_masks(self,\nbatch: Dict[str, Any],\nstdev_spread: float = .15,\nnsamples: int = 25,\nnintegration: Optional[int] = None,\nmagnitude: bool = True) -&gt; Dict[str, Union[Tensor, np.ndarray]]:\n\"\"\"Generates smoothed greyscale saliency mask(s) from a given `batch` of data.\n    Args:\n        batch: An input batch of data.\n        stdev_spread: Amount of noise to add to the input, as fraction of the total spread (x_max - x_min).\n        nsamples: Number of samples to average across to get the smooth gradient.\n        nintegration: Number of samples to compute when integrating (None to disable).\n        magnitude: If true, computes the sum of squares of gradients instead of just the sum.\n    Returns:\n        Greyscale saliency mask(s) smoothed via the SmoothGrad method.\n    \"\"\"\n# Shallow copy batch since we're going to modify its contents later\nbatch = {key: val for key, val in batch.items()}\nmodel_inputs = [batch[ins] for ins in self.model_inputs]\nstdevs = [to_number(stdev_spread * (reduce_max(ins) - reduce_min(ins))).item() for ins in model_inputs]\n# Adding noise to the image might cause the max likelihood class value to change, so need to keep track of\n# which class we're comparing to\nresponse = self._get_mask(batch)\nfor gather_key, output_key in zip(self.gather_keys, self.model_outputs):\nbatch[gather_key] = response[output_key]\nif magnitude:\nfor key in self.outputs:\nresponse[key] = response[key] * response[key]\nfor _ in range(nsamples - 1):\nnoisy_batch = {key: batch[key] for key in self.gather_keys}\nfor idx, input_name in enumerate(self.model_inputs):\nnoise = random_normal_like(model_inputs[idx], std=stdevs[idx])\nx_plus_noise = model_inputs[idx] + noise\nnoisy_batch[input_name] = x_plus_noise\ngrads_and_preds = self._get_mask(noisy_batch) if not nintegration else self._get_integrated_masks(\nnoisy_batch, nsamples=nintegration)\nfor name in self.outputs:\ngrad = grads_and_preds[name]\nif magnitude:\nresponse[name] += grad * grad\nelse:\nresponse[name] += grad\nfor key in self.outputs:\ngrad = response[key]\nresponse[key] = self._convert_for_visualization(grad / nsamples)\nreturn response\n</code></pre>"}, {"location": "tutorial/advanced/t01_dataset.html", "title": "Advanced Tutorial 1: Dataset", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.dataset.data.mnist import load_data\ntrain_data, eval_data = load_data()\n</pre> from fastestimator.dataset.data.mnist import load_data train_data, eval_data = load_data() In\u00a0[2]: Copied! <pre>train_data.summary()\n</pre> train_data.summary() Out[2]: <pre>&lt;DatasetSummary {'num_instances': 60000, 'keys': {'x': &lt;KeySummary {'shape': [28, 28], 'dtype': 'uint8'}&gt;, 'y': &lt;KeySummary {'num_unique_values': 10, 'shape': [], 'dtype': 'uint8'}&gt;}}&gt;</pre> <p>Or even more simply, by invoking the print function:</p> In\u00a0[3]: Copied! <pre>print(train_data)\n</pre> print(train_data) <pre>{\"keys\": {\"x\": {\"dtype\": \"uint8\", \"shape\": [28, 28]}, \"y\": {\"dtype\": \"uint8\", \"num_unique_values\": 10, \"shape\": []}}, \"num_instances\": 60000}\n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>test_data = eval_data.split(0.5)\n</pre> test_data = eval_data.split(0.5) <p>Or if I want to split evaluation data into two test datasets with 20% of the evaluation data each:</p> In\u00a0[5]: Copied! <pre>test_data1, test_data2 = eval_data.split(0.2, 0.2)\n</pre> test_data1, test_data2 = eval_data.split(0.2, 0.2) <p></p> In\u00a0[6]: Copied! <pre>test_data3 = eval_data.split(100)\n</pre> test_data3 = eval_data.split(100) <p>And of course, we can generate multiple datasets by providing multiple inputs:</p> In\u00a0[7]: Copied! <pre>test_data4, test_data5 = eval_data.split(100, 100)\n</pre> test_data4, test_data5 = eval_data.split(100, 100) <p></p> In\u00a0[8]: Copied! <pre>test_data6 = eval_data.split([0,1,100])\n</pre> test_data6 = eval_data.split([0,1,100]) <p>If you just want continuous index, here's an easy way to provide index:</p> In\u00a0[9]: Copied! <pre>test_data7 = eval_data.split(range(100))\n</pre> test_data7 = eval_data.split(range(100)) <p>Needless to say, you can provide multiple inputs too:</p> In\u00a0[10]: Copied! <pre>test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5])\n</pre> test_data7, test_data8 = eval_data.split([0, 1 ,2], [3, 4, 5]) <p></p> In\u00a0[11]: Copied! <pre>from fastestimator.dataset.data.breast_cancer import load_data\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_data, eval_data = load_data()\nscaler = StandardScaler()\n\ntrain_data[\"x\"] = scaler.fit_transform(train_data[\"x\"])\neval_data[\"x\"] = scaler.transform(eval_data[\"x\"])\n</pre> from fastestimator.dataset.data.breast_cancer import load_data from sklearn.preprocessing import StandardScaler  train_data, eval_data = load_data() scaler = StandardScaler()  train_data[\"x\"] = scaler.fit_transform(train_data[\"x\"]) eval_data[\"x\"] = scaler.transform(eval_data[\"x\"]) <p></p> <p></p> In\u00a0[12]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_deterministic = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[4,4])\n# ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")  dataset_deterministic = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[4,4]) # ready to use dataset_deterministic in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[13]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\")\ncifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")\n\ndataset_distribution = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=8, probability=[0.5, 0.5])\n# ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x\", label_key=\"y\") cifar_data, _ = cifar10.load_data(image_key=\"x\", label_key=\"y\")  dataset_distribution = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=8, probability=[0.5, 0.5]) # ready to use dataset_distribution in Pipeline, you might need to resize them to have consistent shape <p></p> In\u00a0[14]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.dataset import BatchDataset\n\nmnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\")\ncifar_data, _ = cifar10.load_data(image_key=\"x_cifar\", label_key=\"y_cifar\")\n\ndataset_unpaired = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[1,1])\n# ready to use dataset_unpaired in Pipeline\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.dataset import BatchDataset  mnist_data, _ = mnist.load_data(image_key=\"x_mnist\", label_key=\"y_mnist\") cifar_data, _ = cifar10.load_data(image_key=\"x_cifar\", label_key=\"y_cifar\")  dataset_unpaired = BatchDataset(datasets=[mnist_data, cifar_data], num_samples=[1,1]) # ready to use dataset_unpaired in Pipeline <p></p>"}, {"location": "tutorial/advanced/t01_dataset.html#advanced-tutorial-1-dataset", "title": "Advanced Tutorial 1: Dataset\u00b6", "text": ""}, {"location": "tutorial/advanced/t01_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following topics:</p> <ul> <li>Dataset Summary</li> <li>Dataset Splitting<ul> <li>Random Fraction Split</li> <li>Random Count Split</li> <li>Index Split</li> </ul> </li> <li>Global Dataset Editing</li> <li>BatchDataset<ul> <li>Deterministic Batching</li> <li>Distribution Batching</li> <li>Unpaired Dataset</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>Before going through the tutorial, it is recommended to check beginner tutorial 02 for basic understanding of <code>dataset</code> from PyTorch and FastEstimator. We will talk about more details about <code>fe.dataset</code> API in this tutorial.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-summary", "title": "Dataset summary\u00b6", "text": "<p>As we have mentioned in previous tutorial, users can import our inherited dataset class for easy use in <code>Pipeline</code>. But how do we know what keys are available in the dataset?   Well, obviously one easy way is just call <code>dataset[0]</code> and check the keys. However, there's a more elegant way to check information of dataset: <code>dataset.summary()</code>.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#dataset-splitting", "title": "Dataset Splitting\u00b6", "text": "<p>Dataset splitting is nothing new in machine learning. In FastEstimator, users can easily split their data in different ways.</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-fraction-split", "title": "Random Fraction Split\u00b6", "text": "<p>Let's say we want to randomly split 50% of the evaluation data into test data. This is easily accomplished:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#random-count-split", "title": "Random Count Split\u00b6", "text": "<p>Sometimes instead of fractions, we want an actual number of examples to split; for example, randomly splitting 100 samples from the evaluation dataset:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#index-split", "title": "Index Split\u00b6", "text": "<p>There are times when we need to split the dataset in a specific way. For that, you can provide a list of indexes. For example, if we want to split the 0th, 1st and 100th element of evaluation dataset into new test set:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#global-dataset-editing", "title": "Global Dataset Editing\u00b6", "text": "<p>In deep learning, we usually process the dataset batch by batch. However, when we are handling tabular data, we might need to apply some transformation globally before the training. For example, we may want to standardize the tabular data using <code>sklearn</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#batchdataset", "title": "BatchDataset\u00b6", "text": "<p>There might be scenarios where we need to combine multiple datasets together into one dataset in a specific way. Let's consider three such use-cases now:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#deterministic-batching", "title": "Deterministic Batching\u00b6", "text": "<p>Let's say we have <code>mnist</code> and <code>cifar</code> datasets, and want to combine them with a total batch size of 8. If we always want 4 examples from <code>mnist</code> and the rest from <code>cifar</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#distribution-batching", "title": "Distribution Batching\u00b6", "text": "<p>Some people prefer randomness in a batch. For example, given total batch size of 8, let's say we want 0.5 probability of <code>mnist</code> and the other 0.5 from <code>cifar</code>:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#unpaired-dataset", "title": "Unpaired Dataset\u00b6", "text": "<p>Some deep learning tasks require random unpaired datasets. For example, in image-to-image translation (like Cycle-GAN), the system needs to randomly sample one horse image and one zebra image for every batch. In FastEstimator, <code>BatchDataset</code> can also handle unpaired datasets. The only restriction is that: keys from two different datasets must be unique for unpaired datasets.</p> <p>For example, let's sample one image from <code>mnist</code> and one image from <code>cifar</code> for every batch:</p>"}, {"location": "tutorial/advanced/t01_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>DNN</li> </ul>"}, {"location": "tutorial/advanced/t02_pipeline.html", "title": "Advanced Tutorial 2: Pipeline", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Iterating Through Pipeline<ul> <li>Basic Concept</li> <li>Example</li> </ul> </li> <li>Dropping Last Batch</li> <li>Padding Batch Data</li> <li>Benchmark Pipeline Speed</li> </ul> <p>In the beginner tutorial 4, we learned how to build a data pipeline that handles data loading and preprocessing tasks efficiently. Now that you have understood some basic operations in the <code>Pipeline</code>, we will demonstrate some advanced concepts and how to leverage them to create efficient <code>Pipelines</code> in this tutorial.</p> <p></p> <p></p> <p>In this section, we will see how to iterate through the pipeline in FastEstimator. First we will create a sample NumpyDataset from the data dictionary and load it into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom fastestimator.dataset.data import cifar10\n    \n# sample numpy array to later create datasets from them\nx_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np from fastestimator.dataset.data import cifar10      # sample numpy array to later create datasets from them x_train, y_train = (np.random.sample((10, 2)), np.random.sample((10, 1))) train_data = {\"x\": x_train, \"y\": y_train} In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# create NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3)\n</pre> import fastestimator as fe from fastestimator.dataset.numpy_dataset import NumpyDataset  # create NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3) <p>Let's get the loader object for the <code>Pipeline</code>, then iterate through the loader with a for loop:</p> In\u00a0[3]: Copied! <pre>loader_fe = pipeline_fe.get_loader(mode=\"train\")\n\nfor batch in loader_fe:\n    print(batch)\n</pre> loader_fe = pipeline_fe.get_loader(mode=\"train\")  for batch in loader_fe:     print(batch) <pre>{'x': tensor([[0.1288, 0.2118],\n        [0.9344, 0.5583],\n        [0.0879, 0.5939]], dtype=torch.float64), 'y': tensor([[0.8071],\n        [0.8469],\n        [0.9160]], dtype=torch.float64)}\n{'x': tensor([[0.7866, 0.8248],\n        [0.3285, 0.9311],\n        [0.7637, 0.9474]], dtype=torch.float64), 'y': tensor([[0.5504],\n        [0.8430],\n        [0.7415]], dtype=torch.float64)}\n{'x': tensor([[0.3689, 0.3373],\n        [0.3407, 0.0571],\n        [0.2216, 0.1906]], dtype=torch.float64), 'y': tensor([[0.6517],\n        [0.4824],\n        [0.5171]], dtype=torch.float64)}\n{'x': tensor([[0.6018, 0.4306]], dtype=torch.float64), 'y': tensor([[0.0023]], dtype=torch.float64)}\n</pre> <p></p> <p>Let's say we have CIFAR-10 dataset and we want to find global average pixel value over three channels:</p> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import cifar10\n\ncifar_train, _ = cifar10.load_data()\n</pre> from fastestimator.dataset.data import cifar10  cifar_train, _ = cifar10.load_data() <p>We will take the <code>batch_size</code> 64 and load the data into <code>Pipeline</code></p> In\u00a0[5]: Copied! <pre>pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64)\n</pre> pipeline_cifar = fe.Pipeline(train_data=cifar_train, batch_size=64) <p>Now we will iterate through batch data and compute the mean pixel values for all three channels of the dataset.</p> In\u00a0[6]: Copied! <pre>loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False)\nmean_arr = np.zeros((3))\nfor i, batch in enumerate(loader_fe):\n    mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2))\nmean_arr = mean_arr / (i+1)\n</pre> loader_fe = pipeline_cifar.get_loader(mode=\"train\", shuffle=False) mean_arr = np.zeros((3)) for i, batch in enumerate(loader_fe):     mean_arr = mean_arr + np.mean(batch[\"x\"].numpy(), axis=(0, 1, 2)) mean_arr = mean_arr / (i+1) In\u00a0[7]: Copied! <pre>print(\"Mean pixel value over the channels are: \", mean_arr)\n</pre> print(\"Mean pixel value over the channels are: \", mean_arr) <pre>Mean pixel value over the channels are:  [125.32287898 122.96682199 113.8856495 ]\n</pre> <p></p> <p>If the total number of dataset elements is not divisible by the <code>batch_size</code>, by default, the last batch will have less data than other batches.  To drop the last batch we can set <code>drop_last</code> to <code>True</code>. Therefore, if the last batch is incomplete it will be dropped.</p> In\u00a0[8]: Copied! <pre>pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True)\n</pre> pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, drop_last=True) <p></p> <p>There might be scenario where the input tensors have different dimensions within a batch. For example, in Natural Language Processing, we have input strings with different lengths. For that we need to pad the data to the maximum length within the batch.</p> <p>To further illustrate in code, we will take numpy array that contains different shapes of array elements and load it into the <code>Pipeline</code>.</p> In\u00a0[9]: Copied! <pre># define numpy arrays with different shapes\nelem1 = np.array([4, 5])\nelem2 = np.array([1, 2, 6])\nelem3 = np.array([3])\n\n# create train dataset\nx_train = np.array([elem1, elem2, elem3])\ntrain_data = {\"x\": x_train}\ndataset_fe = NumpyDataset(train_data)\n</pre> # define numpy arrays with different shapes elem1 = np.array([4, 5]) elem2 = np.array([1, 2, 6]) elem3 = np.array([3])  # create train dataset x_train = np.array([elem1, elem2, elem3]) train_data = {\"x\": x_train} dataset_fe = NumpyDataset(train_data) <p>We will set any <code>pad_value</code> that we want to append at the end of the tensor data. <code>pad_value</code> can be either <code>int</code> or <code>float</code>:</p> In\u00a0[10]: Copied! <pre>pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0)\n</pre> pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=3, pad_value=0) <p>Now let's print the batch data after padding:</p> In\u00a0[11]: Copied! <pre>for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):\n    print(elem)\n</pre> for elem in iter(pipeline_fe.get_loader(mode='train', shuffle=False)):     print(elem) <pre>{'x': tensor([[4, 5, 0],\n        [1, 2, 6],\n        [3, 0, 0]])}\n</pre> <p></p> <p>It is often the case that the bottleneck of deep learning training is the data pipeline. As a result, the GPU may be underutilized. FastEstimator provides a method to check the speed of a <code>Pipeline</code> in order to help diagnose any potential problems. The way to benchmark <code>Pipeline</code> speed in FastEstimator is very simple: call <code>Pipeline.benchmark</code>.</p> <p>For illustration, we will create a <code>Pipeline</code> for the CIFAR-10 dataset with list of Numpy operators that expand dimensions, apply <code>Minmax</code> and finally <code>Rotate</code> the input images:</p> In\u00a0[12]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax, ExpandDims\nfrom fastestimator.op.numpyop.multivariate import Rotate\n\npipeline = fe.Pipeline(train_data=cifar_train,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),\n                            Minmax(inputs=\"x\", outputs=\"x_out\"),\n                            Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180)],\n                      batch_size=64)\n</pre> from fastestimator.op.numpyop.univariate import Minmax, ExpandDims from fastestimator.op.numpyop.multivariate import Rotate  pipeline = fe.Pipeline(train_data=cifar_train,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                             Minmax(inputs=\"x\", outputs=\"x_out\"),                             Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180)],                       batch_size=64) <p>Let's benchmark the pre-processing speed for this pipeline in training mode:</p> In\u00a0[13]: Copied! <pre>pipeline_cifar.benchmark(mode=\"train\")\n</pre> pipeline_cifar.benchmark(mode=\"train\") <pre>FastEstimator: Step: 100, Epoch: 1, Steps/sec: 797.9008250605733\nFastEstimator: Step: 200, Epoch: 1, Steps/sec: 2249.3393577839283\nFastEstimator: Step: 300, Epoch: 1, Steps/sec: 2236.913774803168\nFastEstimator: Step: 400, Epoch: 1, Steps/sec: 2244.6406454903963\nFastEstimator: Step: 500, Epoch: 1, Steps/sec: 2303.2515324338206\nFastEstimator: Step: 600, Epoch: 1, Steps/sec: 2250.139806811566\nFastEstimator: Step: 700, Epoch: 1, Steps/sec: 2310.7264336983017\n</pre>"}, {"location": "tutorial/advanced/t02_pipeline.html#advanced-tutorial-2-pipeline", "title": "Advanced Tutorial 2: Pipeline\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#overview", "title": "Overview\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#iterating-through-pipeline", "title": "Iterating Through Pipeline\u00b6", "text": "<p>In many deep learning tasks, the parameters for preprocessing tasks are precomputed by looping through the dataset. For example, in the <code>ImageNet</code> dataset, people usually use a precomputed global pixel average for each channel to normalize the images.</p>"}, {"location": "tutorial/advanced/t02_pipeline.html#basic-concept", "title": "Basic Concept\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#example", "title": "Example\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#dropping-last-batch", "title": "Dropping Last Batch\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#padding-batch-data", "title": "Padding Batch Data\u00b6", "text": ""}, {"location": "tutorial/advanced/t02_pipeline.html#benchmark-pipeline-speed", "title": "Benchmark Pipeline Speed\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html", "title": "Advanced Tutorial 3: Operator", "text": "<p>Here's one simple example of an operator:</p> In\u00a0[1]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddOne(NumpyOp):\n    def __init__(self, inputs, outputs, mode = None):\n        super().__init__(inputs, outputs, mode)\n\n    def forward(self, data, state):\n        x, y = data\n        x = x + 1\n        y = y + 1\n        return x, y\n    \nAddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\"))\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddOne(NumpyOp):     def __init__(self, inputs, outputs, mode = None):         super().__init__(inputs, outputs, mode)      def forward(self, data, state):         x, y = data         x = x + 1         y = y + 1         return x, y      AddOneOp = AddOne(inputs=(\"x\", \"y\"), outputs=(\"x_out\", \"y_out\")) <p>An <code>Op</code> interacts with the required portion of this data using the keys specified through the <code>inputs</code> key, processes the data through the <code>forward</code> function and writes the values returned from the <code>forward</code> function to this data dictionary using the <code>outputs</code> key. The processes are illustrated in the diagram below:</p> <p></p> <p></p> <p></p> <p></p> <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop import Delete\nfrom fastestimator.op.numpyop.meta import OneOf, Sometimes\nfrom fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip\nfrom fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose\n\ntrain_data, eval_data = cifar10.load_data() \n\npipeline1 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])\n\npipeline2 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"), \n                               Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45), \n                               Delete(keys=\"x_mid\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop import Delete from fastestimator.op.numpyop.meta import OneOf, Sometimes from fastestimator.op.numpyop.multivariate import HorizontalFlip, Rotate, VerticalFlip from fastestimator.op.numpyop.univariate import Blur, Minmax, ChannelTranspose  train_data, eval_data = cifar10.load_data()   pipeline1 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45)])  pipeline2 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [HorizontalFlip(image_in=\"x\", image_out=\"x_mid\", mode=\"train\"),                                 Rotate(image_in=\"x_mid\", image_out=\"x\", mode=\"train\", limit=45),                                 Delete(keys=\"x_mid\")]) In\u00a0[3]: Copied! <pre>data1 = pipeline1.get_results()\nprint(\"Keys in pipeline: \", data1.keys())\n\ndata2 = pipeline2.get_results()\nprint(\"Keys in pipeline with Delete Op: \", data2.keys())\n</pre> data1 = pipeline1.get_results() print(\"Keys in pipeline: \", data1.keys())  data2 = pipeline2.get_results() print(\"Keys in pipeline with Delete Op: \", data2.keys()) <pre>Keys in pipeline:  dict_keys(['x', 'y', 'x_mid'])\nKeys in pipeline with Delete Op:  dict_keys(['x', 'y'])\n</pre> <p></p> In\u00a0[4]: Copied! <pre>pipeline3 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=4,\n                        ops = [Sometimes(HorizontalFlip(image_in=\"x\", \n                                                        image_out=\"x_mid\",\n                                                        mode=\"train\"), prob=0.5), \n                               OneOf(Rotate(image_in=\"x_mid\", image_out=\"x_out\", mode=\"train\", limit=45), \n                                     VerticalFlip(image_in=\"x_mid\", image_out=\"x_out\", mode=\"train\"), \n                                     Blur(inputs=\"x_mid\", outputs=\"x_out\", mode=\"train\", blur_limit=7))])\n</pre> pipeline3 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=4,                         ops = [Sometimes(HorizontalFlip(image_in=\"x\",                                                          image_out=\"x_mid\",                                                         mode=\"train\"), prob=0.5),                                 OneOf(Rotate(image_in=\"x_mid\", image_out=\"x_out\", mode=\"train\", limit=45),                                       VerticalFlip(image_in=\"x_mid\", image_out=\"x_out\", mode=\"train\"),                                       Blur(inputs=\"x_mid\", outputs=\"x_out\", mode=\"train\", blur_limit=7))]) <p>Plotting the results of the data pre-processing</p> In\u00a0[5]: Copied! <pre>from fastestimator.backend import to_number\n\ndata3 = pipeline3.get_results()\nimg = fe.util.ImgData(Input_Image=to_number(data3[\"x\"]), Sometimes_Op=to_number(data3[\"x_mid\"]), OneOf_Op=to_number(data3[\"x_out\"]))\nfig = img.paint_figure()\n</pre> from fastestimator.backend import to_number  data3 = pipeline3.get_results() img = fe.util.ImgData(Input_Image=to_number(data3[\"x\"]), Sometimes_Op=to_number(data3[\"x_mid\"]), OneOf_Op=to_number(data3[\"x_out\"])) fig = img.paint_figure() <p>As you can see, Sometimes Op horizontally flips the image with 50% probability and OneOf applies either a vertical flip, rotation, or blur augmentation randomly.</p> <p></p> In\u00a0[6]: Copied! <pre>from albumentations.augmentations.transforms import RandomCrop\nimport numpy as np\n\nclass Patch(NumpyOp):\n    def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):\n        super().__init__(inputs, outputs, mode)\n        self.num_patch = num_patch\n        self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)\n\n    def forward(self, data, state):\n        image, label = data\n        image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)\n        label = np.array([label for _ in range(self.num_patch)])\n        return [image, label]\n    \n    def _gen_patch(self, data):\n        data = self.crop_fn(image=data)\n        return data[\"image\"].astype(np.float32)\n</pre> from albumentations.augmentations.transforms import RandomCrop import numpy as np  class Patch(NumpyOp):     def __init__(self, height, width, inputs, outputs, mode = None, num_patch=2):         super().__init__(inputs, outputs, mode)         self.num_patch = num_patch         self.crop_fn = RandomCrop(height=height, width=width, always_apply=True)      def forward(self, data, state):         image, label = data         image = np.stack([self._gen_patch(image) for _ in range(self.num_patch)], axis=0)         label = np.array([label for _ in range(self.num_patch)])         return [image, label]          def _gen_patch(self, data):         data = self.crop_fn(image=data)         return data[\"image\"].astype(np.float32) <p>Let's create a pipeline and visualize the results.</p> In\u00a0[7]: Copied! <pre>pipeline4 = fe.Pipeline(train_data=train_data,\n                        eval_data=eval_data,\n                        batch_size=8,\n                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"], \n                                   num_patch=4)])\n</pre> pipeline4 = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                         batch_size=8,                         ops=[Minmax(inputs=\"x\", outputs=\"x\"),                              Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x_out\", \"y_out\"],                                     num_patch=4)]) In\u00a0[8]: Copied! <pre>from fastestimator.backend import to_number\n\ndata4 = pipeline4.get_results()\nimg = fe.util.ImgData(Input_Image=to_number(data4[\"x\"]), \n                      Patch_0=to_number(data4[\"x_out\"])[:,0,:,:,:], \n                      Patch_1=to_number(data4[\"x_out\"])[:,1,:,:,:], \n                      Patch_2=to_number(data4[\"x_out\"])[:,2,:,:,:], \n                      Patch_3=to_number(data4[\"x_out\"])[:,3,:,:,:])\nfig = img.paint_figure()\n</pre> from fastestimator.backend import to_number  data4 = pipeline4.get_results() img = fe.util.ImgData(Input_Image=to_number(data4[\"x\"]),                        Patch_0=to_number(data4[\"x_out\"])[:,0,:,:,:],                        Patch_1=to_number(data4[\"x_out\"])[:,1,:,:,:],                        Patch_2=to_number(data4[\"x_out\"])[:,2,:,:,:],                        Patch_3=to_number(data4[\"x_out\"])[:,3,:,:,:]) fig = img.paint_figure() <p></p> <p></p> In\u00a0[9]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass DimensionAdjustment(TensorOp):\n    def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):\n        super().__init__(inputs, outputs, mode)\n        self.reduce_dim = reduce_dim\n    \n    def forward(self, data, state):\n        image, label = data\n        image_out = tf.reshape(image, shape=self._new_shape(image))\n        label_out = tf.reshape(label, shape=self._new_shape(label))\n        return [image_out, label_out]\n    \n    def _new_shape(self, data):\n        return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim]\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class DimensionAdjustment(TensorOp):     def __init__(self, reduce_dim=[0, 1], inputs=None, outputs=None, mode=None):         super().__init__(inputs, outputs, mode)         self.reduce_dim = reduce_dim          def forward(self, data, state):         image, label = data         image_out = tf.reshape(image, shape=self._new_shape(image))         label_out = tf.reshape(label, shape=self._new_shape(label))         return [image_out, label_out]          def _new_shape(self, data):         return [-1] + [data.shape[i] for i in range(len(data.shape)) if i not in self.reduce_dim] In\u00a0[10]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\npipeline5 = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=8,\n                       ops=[Minmax(inputs=\"x\", outputs=\"x\"),\n                            Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"], \n                                  num_patch=4)])\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n</pre> from fastestimator.architecture.tensorflow import LeNet from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  pipeline5 = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=8,                        ops=[Minmax(inputs=\"x\", outputs=\"x\"),                             Patch(height=24, width=24, inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"],                                    num_patch=4)])  model = fe.build(model_fn=lambda: LeNet(input_shape=(24, 24, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     DimensionAdjustment(reduce_dim=[0, 1], inputs=[\"x\", \"y\"], outputs=[\"x\", \"y\"]),     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) <p>Let's check the dimensions the of Pipeline output and DimensionAdjustment TensorOp output.</p> In\u00a0[11]: Copied! <pre>data5 = pipeline5.get_results()\nresult = network.transform(data5, mode=\"infer\")\n\nprint(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\")\nprint(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\")\n</pre> data5 = pipeline5.get_results() result = network.transform(data5, mode=\"infer\")  print(f\"Pipeline Output, Image Shape: {data5['x'].shape}, Label Shape: {data5['y'].shape}\") print(f\"Result Image Shape: {result['x'].shape}, Label Shape: {result['y'].shape}\") <pre>Pipeline Output, Image Shape: torch.Size([8, 4, 24, 24, 3]), Label Shape: torch.Size([8, 4, 1])\nResult Image Shape: (32, 24, 24, 3), Label Shape: (32, 1)\n</pre> <p></p>"}, {"location": "tutorial/advanced/t03_operator.html#advanced-tutorial-3-operator", "title": "Advanced Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/advanced/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Operator Mechanism<ul> <li>data</li> <li>state</li> </ul> </li> <li>NumpyOp<ul> <li>DeleteOp</li> <li>MetaOp</li> <li>Customizing NumpyOps</li> </ul> </li> <li>TensorOp<ul> <li>Customizing TensorOps</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t03_operator.html#operator-mechanism", "title": "Operator Mechanism\u00b6", "text": "<p>We learned about the operator structure in Beginner Tutorial 3. Operators are used to build complex computation graphs in FastEstimator.</p> <p>In FastEstimator, all the available data is held in a data dictionary during execution. An <code>Op</code> runs when it's <code>mode</code> matches the current execution mode. For more information on mode, you can go through Beginner Tutorial 8.</p>"}, {"location": "tutorial/advanced/t03_operator.html#data", "title": "data\u00b6", "text": "<p>The data argument in the <code>forward</code> function passes the portion of data dictionary corresponding to the Operator's <code>inputs</code> into the forward function. If multiple keys are provided as <code>inputs</code>, the data will be a list of corresponding to the values of those keys.</p>"}, {"location": "tutorial/advanced/t03_operator.html#state", "title": "state\u00b6", "text": "<p>The state argument in the <code>forward</code> function stores meta information about training like the current mode, GradientTape for tensorflow, etc. It is very unlikely that you would need to interact with it.</p>"}, {"location": "tutorial/advanced/t03_operator.html#numpyop", "title": "NumpyOp\u00b6", "text": "<p>NumpyOp is used in <code>Pipeline</code> for data pre-processing and augmentation. You can go through Beginner Tutorial 4 to get an overview of NumpyOp and their usage. Here, we will talk about some advanced NumpyOps.</p>"}, {"location": "tutorial/advanced/t03_operator.html#deleteop", "title": "DeleteOp\u00b6", "text": "<p>Delete op is used to delete keys from the data dictionary which are no longer required by the user. This helps in improving processing speed as we are holding only the required data in the memory. Let's see its usage:</p>"}, {"location": "tutorial/advanced/t03_operator.html#metaop", "title": "MetaOp\u00b6", "text": "<p>Meta ops are NumpyOps which operate on other NumpyOps. For example: <code>Sometimes</code> is a meta op which applies a given NumpyOp with the specified probability. <code>OneOf</code> applies only one randomly selected NumpyOp from the given list of NumpyOps.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-numpyops", "title": "Customizing NumpyOps\u00b6", "text": "<p>We can create a custom NumpyOp which suits our needs. Below, we showcase a custom NumpyOp which creates multiple random patches (crops) of images from each image.</p>"}, {"location": "tutorial/advanced/t03_operator.html#tensorop", "title": "TensorOp\u00b6", "text": "<p><code>TensorOps</code> are used to process tensor data. They are used within a <code>Network</code> for graph-based operations. You can go through Beginner Tutorial 6 to get an overview of <code>TensorOps</code> and their usages.</p>"}, {"location": "tutorial/advanced/t03_operator.html#customizing-tensorops", "title": "Customizing TensorOps\u00b6", "text": "<p>We can create a custom <code>TensorOp</code> using TensorFlow or Pytorch library calls according to our requirements. Below, we showcase a custom <code>TensorOp</code> which combines the batch dimension and patch dimension from the output of the above <code>Pipeline</code> to make it compatible to the <code>Network</code>.</p>"}, {"location": "tutorial/advanced/t03_operator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>Convolutional Variational AutoEncoder</li> <li>Semantic Segmentation</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html", "title": "Advanced Tutorial 4: Trace", "text": "<p>Let's create a function to generate a pipeline, model and network to be used for the tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    \n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=batch_size,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\", batch_size=32):     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)          pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=batch_size,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.backend import to_number\nfrom fastestimator.trace import Trace\nfrom sklearn.metrics import fbeta_score\nimport numpy as np\n\nclass FBetaScore(Trace):\n    def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):\n        super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        self.beta = beta\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_epoch_begin(self, data):\n        self.y_true = []\n        self.y_pred = []\n        \n    def on_batch_end(self, data):\n        y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])\n        y_pred = np.argmax(y_pred, axis=-1)\n        self.y_pred.extend(y_pred.ravel())\n        self.y_true.extend(y_true.ravel())\n        \n    def on_epoch_end(self, data):\n        score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")\n        data.write_with_log(self.outputs[0], score)\n</pre> from fastestimator.backend import to_number from fastestimator.trace import Trace from sklearn.metrics import fbeta_score import numpy as np  class FBetaScore(Trace):     def __init__(self, true_key, pred_key, beta=2, output_name=\"f_beta_score\", mode=[\"eval\", \"test\"]):         super().__init__(inputs=(true_key, pred_key), outputs=output_name, mode=mode)         self.true_key = true_key         self.pred_key = pred_key         self.beta = beta         self.y_true = []         self.y_pred = []              def on_epoch_begin(self, data):         self.y_true = []         self.y_pred = []              def on_batch_end(self, data):         y_true, y_pred = to_number(data[self.true_key]), to_number(data[self.pred_key])         y_pred = np.argmax(y_pred, axis=-1)         self.y_pred.extend(y_pred.ravel())         self.y_true.extend(y_true.ravel())              def on_epoch_end(self, data):         score = fbeta_score(self.y_true, self.y_pred, beta=self.beta, average=\"weighted\")         data.write_with_log(self.outputs[0], score) <p>Now let's calculate the f2-score using our custom <code>Trace</code>. f2-score gives more importance to recall.</p> In\u00a0[3]: Copied! <pre>pipeline, model, network = get_pipeline_model_network()\n\ntraces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)\n\nestimator.fit()\n</pre> pipeline, model, network = get_pipeline_model_network()  traces = FBetaScore(true_key=\"y\", pred_key=\"y_pred\", beta=2, output_name=\"f2_score\", mode=\"eval\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=4, traces=traces, log_steps=1000)  estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.3049126; \nFastEstimator-Train: step: 1000; ce: 0.18839744; steps/sec: 121.57; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 19.89 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.04401617; f2_score: 0.9853780424409669; \nFastEstimator-Train: step: 2000; ce: 0.015927518; steps/sec: 95.05; \nFastEstimator-Train: step: 3000; ce: 0.07206129; steps/sec: 186.86; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 10.6 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.04134067; f2_score: 0.9845700637368479; \nFastEstimator-Train: step: 4000; ce: 0.008171058; steps/sec: 169.0; \nFastEstimator-Train: step: 5000; ce: 0.0019764265; steps/sec: 180.37; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 10.88 sec; \nFastEstimator-Eval: step: 5625; epoch: 3; ce: 0.029307945; f2_score: 0.9900004384152095; \nFastEstimator-Train: step: 6000; ce: 0.0135234; steps/sec: 167.19; \nFastEstimator-Train: step: 7000; ce: 0.04989395; steps/sec: 183.41; \nFastEstimator-Train: step: 7500; epoch: 4; epoch_time: 10.4 sec; \nFastEstimator-Eval: step: 7500; epoch: 4; ce: 0.032727916; f2_score: 0.9897883746689528; \nFastEstimator-Finish: step: 7500; total_time: 54.32 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Let's see an example where we utilize the outputs of the <code>Precision</code> and <code>Recall</code> <code>Traces</code> to generate f1-score:</p> In\u00a0[4]: Copied! <pre>from fastestimator.trace.metric import Precision, Recall\n\nclass CustomF1Score(Trace):\n    def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):\n        super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)\n        self.precision_key = precision_key\n        self.recall_key = recall_key\n        \n    def on_epoch_end(self, data):\n        precision = data[self.precision_key]\n        recall = data[self.recall_key]\n        score = 2*(precision*recall)/(precision+recall)\n        data.write_with_log(self.outputs[0], score)\n        \n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = [\n    Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),\n    Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),\n    CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\")\n]\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000)\n</pre> from fastestimator.trace.metric import Precision, Recall  class CustomF1Score(Trace):     def __init__(self, precision_key, recall_key, mode=[\"eval\", \"test\"], output_name=\"f1_score\"):         super().__init__(inputs=(precision_key, recall_key), outputs=output_name, mode=mode)         self.precision_key = precision_key         self.recall_key = recall_key              def on_epoch_end(self, data):         precision = data[self.precision_key]         recall = data[self.recall_key]         score = 2*(precision*recall)/(precision+recall)         data.write_with_log(self.outputs[0], score)           pipeline, model, network = get_pipeline_model_network()  traces = [     Precision(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"precision\"),     Recall(true_key=\"y\", pred_key=\"y_pred\", mode=[\"eval\", \"test\"], output_name=\"recall\"),     CustomF1Score(precision_key=\"precision\", recall_key=\"recall\", mode=[\"eval\", \"test\"], output_name=\"f1_score\") ] estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, log_steps=1000) In\u00a0[5]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.2952752; \nFastEstimator-Train: step: 1000; ce: 0.1313241; steps/sec: 179.84; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 10.96 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.04599155; \nprecision:\n[0.98878505,0.99165275,0.98351648,0.99017682,0.99798793,0.98454746,\n 0.98198198,0.98294243,0.96296296,0.97402597];\nrecall:\n[0.99249531,0.98181818,0.98713235,0.99212598,0.98023715,0.98454746,\n 0.98866213,0.96848739,0.98526316,0.98039216];\nf1_score:\n[0.9906367 ,0.98671096,0.9853211 ,0.99115044,0.9890329 ,0.98454746,\n 0.98531073,0.97566138,0.97398543,0.9771987 ];\nFastEstimator-Train: step: 2000; ce: 0.0038511096; steps/sec: 164.72; \nFastEstimator-Train: step: 3000; ce: 0.004517486; steps/sec: 161.99; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 12.14 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.034655295; \nprecision:\n[0.9906367 ,0.99505766,0.98899083,0.98635478,0.984375  ,0.98675497,\n 0.98868778,0.99353448,0.97717842,0.99107143];\nrecall:\n[0.99249531,0.99834711,0.99080882,0.99606299,0.99604743,0.98675497,\n 0.99092971,0.96848739,0.99157895,0.96732026];\nf1_score:\n[0.99156514,0.99669967,0.98989899,0.99118511,0.99017682,0.98675497,\n 0.98980747,0.98085106,0.98432602,0.97905182];\nFastEstimator-Finish: step: 3750; total_time: 24.53 sec; LeNet_lr: 0.001; \n</pre> <p><code>Note:</code> precision, recall, and f1-score are displayed for each class</p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class MonitorPred(Trace):\n    def __init__(self, true_key, pred_key, mode=\"train\"):\n        super().__init__(inputs=(true_key, pred_key), mode=mode)\n        self.true_key = true_key\n        self.pred_key = pred_key\n        \n    def on_batch_end(self, data):\n        print(\"Global Step Index: \", self.system.global_step)\n        print(\"Batch Index: \", self.system.batch_idx)\n        print(\"Epoch: \", self.system.epoch_idx)\n        print(\"Batch data has following keys: \", list(data.keys()))\n        print(\"Batch true labels: \", data[self.true_key])\n        print(\"Batch predictictions: \", data[self.pred_key])\n\npipeline, model, network = get_pipeline_model_network(batch_size=4)\n\ntraces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\")\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, max_train_steps_per_epoch=2, log_steps=None)\n</pre> class MonitorPred(Trace):     def __init__(self, true_key, pred_key, mode=\"train\"):         super().__init__(inputs=(true_key, pred_key), mode=mode)         self.true_key = true_key         self.pred_key = pred_key              def on_batch_end(self, data):         print(\"Global Step Index: \", self.system.global_step)         print(\"Batch Index: \", self.system.batch_idx)         print(\"Epoch: \", self.system.epoch_idx)         print(\"Batch data has following keys: \", list(data.keys()))         print(\"Batch true labels: \", data[self.true_key])         print(\"Batch predictictions: \", data[self.pred_key])  pipeline, model, network = get_pipeline_model_network(batch_size=4)  traces = MonitorPred(true_key=\"y\", pred_key=\"y_pred\") estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces, max_train_steps_per_epoch=2, log_steps=None) In\u00a0[7]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nGlobal Step Index:  1\nBatch Index:  1\nEpoch:  1\nBatch data has following keys:  ['x', 'y', 'y_pred', 'ce']\nBatch true labels:  tf.Tensor([4 6 6 0], shape=(4,), dtype=uint8)\nBatch predictictions:  tf.Tensor(\n[[0.10117384 0.09088749 0.09792296 0.09737834 0.09084693 0.08700039\n  0.11264212 0.10984743 0.10661378 0.10568672]\n [0.0952943  0.09128962 0.10272249 0.10368769 0.09144977 0.08363624\n  0.1107841  0.11008291 0.10188652 0.10916632]\n [0.10018928 0.08916146 0.10396809 0.10721539 0.0849424  0.08629669\n  0.11222021 0.10986723 0.09851621 0.10762308]\n [0.09981812 0.09000086 0.10369569 0.09561141 0.09411818 0.08580256\n  0.11238981 0.10954484 0.10357945 0.10543918]], shape=(4, 10), dtype=float32)\nGlobal Step Index:  2\nBatch Index:  2\nEpoch:  1\nBatch data has following keys:  ['x', 'y', 'y_pred', 'ce']\nBatch true labels:  tf.Tensor([4 4 9 1], shape=(4,), dtype=uint8)\nBatch predictictions:  tf.Tensor(\n[[0.10240942 0.07996594 0.10190804 0.09579862 0.09545476 0.07724807\n  0.12645632 0.1047412  0.1043587  0.11165903]\n [0.10151558 0.08773842 0.09836152 0.09669358 0.0958946  0.07577368\n  0.12727338 0.10294375 0.10158429 0.11222118]\n [0.10219741 0.08286765 0.10365716 0.09298524 0.09625786 0.06968912\n  0.13070971 0.10312404 0.10423445 0.11427741]\n [0.10077347 0.08387047 0.10196234 0.09324285 0.09473021 0.08261613\n  0.11878415 0.1059215  0.11001182 0.10808703]], shape=(4, 10), dtype=float32)\nGlobal Step Index:  3\nBatch Index:  1\nEpoch:  2\nBatch data has following keys:  ['x', 'y', 'y_pred', 'ce']\nBatch true labels:  tf.Tensor([0 7 7 7], shape=(4,), dtype=uint8)\nBatch predictictions:  tf.Tensor(\n[[0.10566284 0.07728784 0.10565729 0.08178721 0.10713114 0.06507431\n  0.13530098 0.09833021 0.10452496 0.11924319]\n [0.10526433 0.08540256 0.0971095  0.08443997 0.1094939  0.06850007\n  0.12796785 0.09084202 0.10899913 0.12198068]\n [0.10248369 0.0828173  0.10205018 0.0864138  0.10586432 0.07090016\n  0.1273839  0.09568971 0.10854369 0.1178532 ]\n [0.10461577 0.08429881 0.09658652 0.08807645 0.10916384 0.07197928\n  0.12543353 0.09240671 0.10978852 0.11765066]], shape=(4, 10), dtype=float32)\nGlobal Step Index:  4\nBatch Index:  2\nEpoch:  2\nBatch data has following keys:  ['x', 'y', 'y_pred', 'ce']\nBatch true labels:  tf.Tensor([0 5 3 7], shape=(4,), dtype=uint8)\nBatch predictictions:  tf.Tensor(\n[[0.09841534 0.0690296  0.10122424 0.07857155 0.11737346 0.05218776\n  0.13999611 0.10599035 0.11199971 0.12521197]\n [0.10094637 0.07799206 0.10599674 0.08304708 0.11446269 0.060531\n  0.13092558 0.10104699 0.10494769 0.12010376]\n [0.09200194 0.08393346 0.0990442  0.08482413 0.11270893 0.0664842\n  0.12764609 0.10573834 0.11171819 0.11590049]\n [0.10079639 0.08117153 0.10319441 0.08249949 0.11676847 0.06465001\n  0.12598662 0.10077127 0.10564327 0.11851855]], shape=(4, 10), dtype=float32)\n</pre> <p>As you can see, we can visualize information like the global step, batch number, epoch, keys in the data dictionary, true labels, and predictions at batch level using our <code>Trace</code>.</p> <p></p>"}, {"location": "tutorial/advanced/t04_trace.html#advanced-tutorial-4-trace", "title": "Advanced Tutorial 4: Trace\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing Traces<ul> <li>Example</li> </ul> </li> <li>More About Traces<ul> <li>Inputs, Outputs, and Mode</li> <li>Data</li> <li>System</li> </ul> </li> <li>Trace Communication</li> <li>Other Trace Usages<ul> <li>Debugging/Monitoring</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t04_trace.html#customizing-traces", "title": "Customizing Traces\u00b6", "text": "<p>In tutorial 7 in the beginner section, we talked about the basic concept and structure of <code>Traces</code> and used a few <code>Traces</code> provided by FastEstimator. We can also customize a Trace to suit our needs. Let's look at an example of a custom trace implementation:</p>"}, {"location": "tutorial/advanced/t04_trace.html#example", "title": "Example\u00b6", "text": "<p>We can utilize traces to calculate any custom metric needed for monitoring or controlling training. Below, we implement a trace for calculating the F-beta score of our model.</p>"}, {"location": "tutorial/advanced/t04_trace.html#more-about-traces", "title": "More About Traces\u00b6", "text": "<p>As we have now seen a custom Trace implementaion, let's delve deeper into the structure of <code>Traces</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#inputs-outputs-and-mode", "title": "Inputs, Outputs, and Mode\u00b6", "text": "<p>These Trace arguments are similar to the Operator. To recap, the keys from the data dictionary which are required by the Trace can be specified using the <code>inputs</code> argument. The <code>outputs</code> argument is used to specify the keys which the Trace wants to write into the system buffer. Unlike with Ops, the Trace <code>inputs</code> and <code>outputs</code> are essentially on an honor system. FastEstimator will not check whether a Trace is really only reading values listed in its <code>inputs</code> and writing values listed in its <code>outputs</code>. If you are developing a new <code>Trace</code> and want your code to work well with the features provided by FastEstimator, it is important to use these fields correctly. The <code>mode</code> argument is used to specify the mode(s) for trace execution as with <code>Ops</code>.</p>"}, {"location": "tutorial/advanced/t04_trace.html#data", "title": "Data\u00b6", "text": "<p>Through its data argument, Trace has access to the current data dictionary. You can use any keys which the Trace declared as its <code>inputs</code> to access information from the data dictionary. You can write the outputs into the <code>Data</code> dictionary with or without logging using the <code>write_with_log</code> and <code>write_without_log</code> methods respectively.</p>"}, {"location": "tutorial/advanced/t04_trace.html#system", "title": "System\u00b6", "text": "<p>Traces have access to the current <code>System</code> instance which has information about the <code>Network</code> and training process. The information contained in <code>System</code> is listed below:</p> <ul> <li>global_step</li> <li>num_devices</li> <li>log_steps</li> <li>total_epochs</li> <li>epoch_idx</li> <li>batch_idx</li> <li>stop_training</li> <li>network</li> <li>max_train_steps_per_epoch</li> <li>max_eval_steps_per_epoch</li> <li>summary</li> <li>experiment_time</li> </ul> <p>We will showcase <code>System</code> usage in the other trace usages section of this tutorial.</p>"}, {"location": "tutorial/advanced/t04_trace.html#trace-communication", "title": "Trace Communication\u00b6", "text": "<p>We can have multiple traces in a network where the output of one trace is utilized as an input for another, as depicted below:</p>"}, {"location": "tutorial/advanced/t04_trace.html#other-trace-usages", "title": "Other Trace Usages\u00b6", "text": ""}, {"location": "tutorial/advanced/t04_trace.html#debuggingmonitoring", "title": "Debugging/Monitoring\u00b6", "text": "<p>Lets implement a custom trace to monitor a model's predictions. Using this, any discrepancy from the expected behavior can be checked and the relevant corrections can be made:</p>"}, {"location": "tutorial/advanced/t04_trace.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html", "title": "Advanced Tutorial 5: Scheduler", "text": "In\u00a0[1]: Copied! <pre>from fastestimator.schedule import EpochScheduler\nbatch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64})\n</pre> from fastestimator.schedule import EpochScheduler batch_size = EpochScheduler(epoch_dict={1:16, 2:32, 4:64}) In\u00a0[2]: Copied! <pre>for epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 16\nAt epoch 2, batch size is 32\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 64\n</pre> In\u00a0[3]: Copied! <pre>from fastestimator.schedule import RepeatScheduler\nbatch_size = RepeatScheduler(repeat_list=[32, 64])\n\nfor epoch in range(1, 6):\n    print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch)))\n</pre> from fastestimator.schedule import RepeatScheduler batch_size = RepeatScheduler(repeat_list=[32, 64])  for epoch in range(1, 6):     print(\"At epoch {}, batch size is {}\".format(epoch, batch_size.get_current_value(epoch))) <pre>At epoch 1, batch size is 32\nAt epoch 2, batch size is 64\nAt epoch 3, batch size is 32\nAt epoch 4, batch size is 64\nAt epoch 5, batch size is 32\n</pre> In\u00a0[4]: Copied! <pre>from fastestimator.dataset.data import mnist, cifar10\nfrom fastestimator.schedule import EpochScheduler\n\ntrain_data1, eval_data = mnist.load_data()\ntrain_data2, _ = mnist.load_data()\ntrain_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2})\n</pre> from fastestimator.dataset.data import mnist, cifar10 from fastestimator.schedule import EpochScheduler  train_data1, eval_data = mnist.load_data() train_data2, _ = mnist.load_data() train_data = EpochScheduler(epoch_dict={1:train_data1, 3: train_data2}) In\u00a0[5]: Copied! <pre>batch_size = RepeatScheduler(repeat_list=[32,64])\n</pre> batch_size = RepeatScheduler(repeat_list=[32,64]) In\u00a0[6]: Copied! <pre>from fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.numpyop.multivariate import Rotate\nimport fastestimator as fe\n\nrotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})\n\npipeline = fe.Pipeline(train_data=train_data, \n                       eval_data=eval_data,\n                       batch_size=batch_size, \n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.numpyop.multivariate import Rotate import fastestimator as fe  rotate_op = EpochScheduler(epoch_dict={1:Rotate(image_in=\"x\", image_out=\"x\",limit=30), 3:None})  pipeline = fe.Pipeline(train_data=train_data,                         eval_data=eval_data,                        batch_size=batch_size,                         ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), rotate_op, Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[7]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n\nmodel_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\")\n</pre> from fastestimator.architecture.tensorflow import LeNet  model_1 = fe.build(model_fn=LeNet, optimizer_fn=EpochScheduler(epoch_dict={1:\"adam\", 2: \"sgd\"}), model_name=\"m1\") In\u00a0[8]: Copied! <pre>from fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.op.tensorop.loss import CrossEntropy\n\nmodel_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")\n\nmodel_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"), \n             3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}\n\nupdate_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}\n\nnetwork = fe.Network(ops=[EpochScheduler(model_map),\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n                          EpochScheduler(update_map)])\n</pre> from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.op.tensorop.loss import CrossEntropy  model_2 = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"m2\")  model_map = {1: ModelOp(model=model_1, inputs=\"x\", outputs=\"y_pred\"),               3: ModelOp(model=model_2, inputs=\"x\", outputs=\"y_pred\")}  update_map = {1: UpdateOp(model=model_1, loss_name=\"ce\"), 3: UpdateOp(model=model_2, loss_name=\"ce\")}  network = fe.Network(ops=[EpochScheduler(model_map),                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),                           EpochScheduler(update_map)]) In\u00a0[9]: Copied! <pre>from fastestimator.trace.io import ModelSaver\nimport tempfile\n\nsave_folder = tempfile.mkdtemp()\n\n#Disable model saving by setting None on 3rd epoch:\nmodelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})\n\nmodelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})\n\ntraces=[modelsaver1, modelsaver2]\n</pre> from fastestimator.trace.io import ModelSaver import tempfile  save_folder = tempfile.mkdtemp()  #Disable model saving by setting None on 3rd epoch: modelsaver1 = EpochScheduler({2:ModelSaver(model=model_1,save_dir=save_folder), 3:None})  modelsaver2 = EpochScheduler({3:ModelSaver(model=model_2,save_dir=save_folder)})  traces=[modelsaver1, modelsaver2] In\u00a0[10]: Copied! <pre>estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300)\nestimator.fit()\n</pre> estimator = fe.Estimator(pipeline=pipeline, network=network, traces=traces, epochs=3, log_steps=300) estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 300; \nFastEstimator-Train: step: 1; ce: 2.2846737; \nFastEstimator-Train: step: 300; ce: 0.15281834; steps/sec: 145.67; \nFastEstimator-Train: step: 600; ce: 0.17162594; steps/sec: 131.37; \nFastEstimator-Train: step: 900; ce: 0.21567878; steps/sec: 116.73; \nFastEstimator-Train: step: 1200; ce: 0.30176234; steps/sec: 101.18; \nFastEstimator-Train: step: 1500; ce: 0.08476916; steps/sec: 94.35; \nFastEstimator-Train: step: 1800; ce: 0.030844048; steps/sec: 94.01; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 19.23 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.09244396; \nFastEstimator-Train: step: 2100; ce: 0.05626972; steps/sec: 87.24; \nFastEstimator-Train: step: 2400; ce: 0.008934505; steps/sec: 89.98; \nFastEstimator-Train: step: 2700; ce: 0.15866429; steps/sec: 84.82; \nFastEstimator-ModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp5ofz2w4k/m1_epoch_2.h5\nFastEstimator-Train: step: 2813; epoch: 2; epoch_time: 10.92 sec; \nFastEstimator-Eval: step: 2813; epoch: 2; ce: 0.054940775; \nFastEstimator-Train: step: 3000; ce: 0.26500845; steps/sec: 107.05; \nFastEstimator-Train: step: 3300; ce: 0.031274483; steps/sec: 185.96; \nFastEstimator-Train: step: 3600; ce: 0.19780423; steps/sec: 183.25; \nFastEstimator-Train: step: 3900; ce: 0.3220946; steps/sec: 188.76; \nFastEstimator-Train: step: 4200; ce: 0.10007702; steps/sec: 186.88; \nFastEstimator-Train: step: 4500; ce: 0.27808163; steps/sec: 185.04; \nFastEstimator-ModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp5ofz2w4k/m2_epoch_3.h5\nFastEstimator-Train: step: 4688; epoch: 3; epoch_time: 10.51 sec; \nFastEstimator-Eval: step: 4688; epoch: 3; ce: 0.04512677; \nFastEstimator-Finish: step: 4688; total_time: 43.77 sec; m2_lr: 0.001; m1_lr: 0.01; \n</pre>"}, {"location": "tutorial/advanced/t05_scheduler.html#advanced-tutorial-5-scheduler", "title": "Advanced Tutorial 5: Scheduler\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Scheduler<ul> <li>Concept</li> <li>EpochScheduler</li> <li>RepeatScheduler</li> </ul> </li> <li>Things You Can Schedule<ul> <li>Datasets</li> <li>Batch Size</li> <li>NumpyOps</li> <li>Optimizers</li> <li>TensorOps</li> <li>Traces</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#scheduler", "title": "Scheduler\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#concept", "title": "Concept\u00b6", "text": "<p>Deep learning training is getting more complicated every year. One major aspect of this complexity is time-dependent training. For example:</p> <ul> <li>Using different datasets for different training epochs.</li> <li>Applying different preprocessing for different epochs.</li> <li>Training different networks on different epochs.</li> <li>...</li> </ul> <p>The list goes on and on. In order to provide an easy way for users to accomplish time-dependent training, we provide the <code>Scheduler</code> class which can help you schedule any part of the training.</p> <p>Please note that the basic time unit that <code>Scheduler</code> can handle is <code>epochs</code>. If users want arbitrary scheduling cycles, the simplest way is to customize the length of one epoch in <code>Estimator</code> using max_train_steps_per_epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#epochscheduler", "title": "EpochScheduler\u00b6", "text": "<p>The most straightforward way to schedule things is through an epoch-value mapping. For example, If users want to schedule the batch size in the following way:</p> <ul> <li>epoch 1 - batchsize 16</li> <li>epoch 2 - batchsize 32</li> <li>epoch 3 - batchsize 32</li> <li>epoch 4 - batchsize 64</li> <li>epoch 5 - batchsize 64</li> </ul> <p>You can do the following:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#repeatscheduler", "title": "RepeatScheduler\u00b6", "text": "<p>If your schedule follows a repeating pattern, then you don't want to specify that for all epochs. <code>RepeatScheduler</code> is here to help you. Let's say we want the batch size on odd epochs to be 32, and on even epochs to be 64:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#things-you-can-schedule", "title": "Things You Can Schedule:\u00b6", "text": ""}, {"location": "tutorial/advanced/t05_scheduler.html#datasets", "title": "Datasets\u00b6", "text": "<p>Scheduling training or evaluation datasets is very common in deep learning. For example, in curriculum learning people will train on an easy dataset first and then gradually move on to harder datasets. For illustration purposes, let's use two different instances of the same MNIST dataset:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#batch-size", "title": "Batch Size\u00b6", "text": "<p>We can also schedule the batch size on different epochs, which may help resolve GPU resource constraints.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#numpyops", "title": "NumpyOps\u00b6", "text": "<p>Preprocessing operators can also be scheduled. For illustration purpose, we will apply a <code>Rotation</code> for the first two epochs and then not apply it for the third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#optimizers", "title": "Optimizers\u00b6", "text": "<p>For fast convergence, some people like to use different optimizers at different training phases. In our example, we will use <code>adam</code> for the first epoch and <code>sgd</code> for the second epoch.</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#tensorops", "title": "TensorOps\u00b6", "text": "<p>We can schedule <code>TensorOps</code> just like <code>NumpyOps</code>. Let's define another model <code>model_2</code> such that:</p> <ul> <li>epoch 1-2: train <code>model_1</code></li> <li>epoch 3: train <code>model_2</code></li> </ul>"}, {"location": "tutorial/advanced/t05_scheduler.html#traces", "title": "Traces\u00b6", "text": "<p><code>Traces</code> can also be scheduled. For example, we will save <code>model_1</code> at the end of second epoch and save <code>model_3</code> at the end of third epoch:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#let-the-training-begin", "title": "Let the training begin\u00b6", "text": "<p>Nothing special in here, create the estimator then start the training:</p>"}, {"location": "tutorial/advanced/t05_scheduler.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PGGAN</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html", "title": "Advanced Tutorial 6: Summary", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import TensorBoard\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3))\n]\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import TensorBoard  train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)) ] In\u00a0[2]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120)\nest.fit()\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=120) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 120; \nFastEstimator-Train: step: 1; ce: 2.310556; model_lr: 0.001; \nFastEstimator-Train: step: 120; ce: 0.37298; steps/sec: 144.44; model_lr: 0.0009975198; \nFastEstimator-Train: step: 240; ce: 0.23124042; steps/sec: 137.05; model_lr: 0.000990021; \nFastEstimator-Train: step: 360; ce: 0.025805598; steps/sec: 126.34; model_lr: 0.0009775789; \nFastEstimator-Train: step: 480; ce: 0.053540815; steps/sec: 115.62; model_lr: 0.0009603194; \nFastEstimator-Train: step: 600; ce: 0.124904916; steps/sec: 98.48; model_lr: 0.00093841663; \nFastEstimator-Train: step: 720; ce: 0.069644645; steps/sec: 102.84; model_lr: 0.00091209175; \nFastEstimator-Train: step: 840; ce: 0.054759175; steps/sec: 100.21; model_lr: 0.0008816107; \nFastEstimator-Train: step: 960; ce: 0.014191106; steps/sec: 90.53; model_lr: 0.00084728113; \nFastEstimator-Train: step: 1080; ce: 0.15409154; steps/sec: 84.12; model_lr: 0.0008094498; \nFastEstimator-Train: step: 1200; ce: 0.021797167; steps/sec: 81.62; model_lr: 0.0007684987; \nFastEstimator-Train: step: 1320; ce: 0.018265918; steps/sec: 81.65; model_lr: 0.0007248414; \nFastEstimator-Train: step: 1440; ce: 0.0836072; steps/sec: 79.25; model_lr: 0.0006789187; \nFastEstimator-Train: step: 1560; ce: 0.0046536885; steps/sec: 66.7; model_lr: 0.00063119427; \nFastEstimator-Train: step: 1680; ce: 0.0028682733; steps/sec: 82.64; model_lr: 0.0005821501; \nFastEstimator-Train: step: 1800; ce: 0.075449295; steps/sec: 77.96; model_lr: 0.00053228147; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 22.39 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.040944144; accuracy: 0.9868; \nFastEstimator-Finish: step: 1875; total_time: 23.1 sec; model_lr: 0.0005009185; \n</pre> In\u00a0[3]: Copied! <pre>est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500)\nsummary = est.fit(\"experiment1\")\n</pre> est = fe.Estimator(pipeline=pipeline, network=network, epochs=1, traces=traces, log_steps=500) summary = est.fit(\"experiment1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 500; \nFastEstimator-Train: step: 1; ce: 0.0035908362; model_lr: 0.001; \nFastEstimator-Train: step: 500; ce: 0.2240004; steps/sec: 167.16; model_lr: 0.000956986; \nFastEstimator-Train: step: 1000; ce: 0.017725334; steps/sec: 175.28; model_lr: 0.0008350416; \nFastEstimator-Train: step: 1500; ce: 0.033957843; steps/sec: 171.15; model_lr: 0.0006552519; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 11.02 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.042691424; accuracy: 0.9878; \nFastEstimator-Finish: step: 1875; total_time: 11.62 sec; model_lr: 0.0005009185; \n</pre> <p>Lets take a look at what sort of information is contained within our <code>Summary</code> object:</p> In\u00a0[4]: Copied! <pre>summary.name\n</pre> summary.name Out[4]: <pre>'experiment1'</pre> In\u00a0[5]: Copied! <pre>summary.history\n</pre> summary.history Out[5]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'num_device': {0: array(0)},\n                          'logging_interval': {0: array(500)},\n                          'ce': {1: array(0.00359084, dtype=float32),\n                           500: array(0.2240004, dtype=float32),\n                           1000: array(0.01772533, dtype=float32),\n                           1500: array(0.03395784, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095699, dtype=float32),\n                           1000: array(0.00083504, dtype=float32),\n                           1500: array(0.00065525, dtype=float32)},\n                          'steps/sec': {500: array(167.16),\n                           1000: array(175.28),\n                           1500: array(171.15)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('11.02 sec', dtype='&lt;U9')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'ce': {1875: array(0.04269142, dtype=float32)},\n                          'accuracy': {1875: array(0.9878)},\n                          'total_time': {1875: array('11.62 sec', dtype='&lt;U9')},\n                          'model_lr': {1875: array(0.00050092, dtype=float32)}})})</pre> <p>The history field can appear a little daunting, but it is simply a dictionary laid out as follows: {mode: {key: {step: value}}}. Once you have invoked the .fit() method with an experiment name, subsequent calls to .test() will add their results into the same summary dictionary:</p> In\u00a0[6]: Copied! <pre>summary = est.test()\n</pre> summary = est.test() <pre>FastEstimator-Test: step: 1875; epoch: 1; accuracy: 0.9868; \n</pre> In\u00a0[7]: Copied! <pre>summary.history\n</pre> summary.history Out[7]: <pre>defaultdict(&lt;function fastestimator.summary.summary.Summary.__init__.&lt;locals&gt;.&lt;lambda&gt;()&gt;,\n            {'train': defaultdict(dict,\n                         {'num_device': {0: array(0)},\n                          'logging_interval': {0: array(500)},\n                          'ce': {1: array(0.00359084, dtype=float32),\n                           500: array(0.2240004, dtype=float32),\n                           1000: array(0.01772533, dtype=float32),\n                           1500: array(0.03395784, dtype=float32)},\n                          'model_lr': {1: array(0.001, dtype=float32),\n                           500: array(0.00095699, dtype=float32),\n                           1000: array(0.00083504, dtype=float32),\n                           1500: array(0.00065525, dtype=float32)},\n                          'steps/sec': {500: array(167.16),\n                           1000: array(175.28),\n                           1500: array(171.15)},\n                          'epoch': {1875: 1},\n                          'epoch_time': {1875: array('11.02 sec', dtype='&lt;U9')}}),\n             'eval': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'ce': {1875: array(0.04269142, dtype=float32)},\n                          'accuracy': {1875: array(0.9878)},\n                          'total_time': {1875: array('11.62 sec', dtype='&lt;U9')},\n                          'model_lr': {1875: array(0.00050092, dtype=float32)}}),\n             'test': defaultdict(dict,\n                         {'epoch': {1875: 1},\n                          'accuracy': {1875: array(0.9868)}})})</pre> <p>Even if an experiment name was not provided during the .fit() call, it may be provided during the .test() call. The resulting summary object will, however, only contain information from the Test mode.</p> <p></p> In\u00a0[8]: Copied! <pre>summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\")\n</pre> summary = fe.summary.logs.parse_log_file(file_path=\"../resources/t06a_exp1.txt\", file_extension=\".txt\") In\u00a0[9]: Copied! <pre>summary.name\n</pre> summary.name Out[9]: <pre>'t06a_exp1'</pre> In\u00a0[10]: Copied! <pre>summary.history['eval']\n</pre> summary.history['eval'] Out[10]: <pre>defaultdict(dict,\n            {'epoch': {1875: 1.0, 3750: 2.0, 5625: 3.0},\n             'ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02382297},\n             'min_ce': {1875: 0.03284014, 3750: 0.02343675, 5625: 0.02343675},\n             'since_best': {1875: 0.0, 3750: 0.0, 5625: 1.0},\n             'accuracy': {1875: 0.9882, 3750: 0.992, 5625: 0.9922}})</pre> <p></p> In\u00a0[11]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary])\n</pre> fe.summary.logs.visualize_logs(experiments=[summary]) <p>If you are only interested in visualizing a subset of these log values, it is also possible to whitelist or blacklist values via the 'include_metrics' and 'ignore_metrics' arguments respectively:</p> In\u00a0[12]: Copied! <pre>fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"})\n</pre> fe.summary.logs.visualize_logs(experiments=[summary], include_metrics={\"accuracy\", \"ce\"}) <p>It is also possible to compare logs from different experiments, which can be especially useful when fiddling with hyper-parameter values to determine their effects on training:</p> In\u00a0[13]: Copied! <pre>fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\")\n</pre> fe.summary.logs.parse_log_files(file_paths=[\"../resources/t06a_exp1.txt\", \"../resources/t06a_exp2.txt\"], log_extension=\".txt\") <p>All of the log files within a given directory can also be compared at the same time, either by using the parse_log_dir() method or via the command line as follows: fastestimator logs --extension .txt --smooth 0 ../resources</p> <p></p> In\u00a0[14]: Copied! <pre>import tempfile\nlog_dir = tempfile.mkdtemp()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),\n    TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\")\n]\nest = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000)\nest.fit()\n</pre> import tempfile log_dir = tempfile.mkdtemp()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")], num_process=0) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ]) traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),     TensorBoard(log_dir=log_dir, weight_histogram_freq=\"epoch\") ] est = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces, log_steps=1000) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Tensorboard: writing logs to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe/20200504-202406\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.296093; model1_lr: 0.001; \nFastEstimator-Train: step: 1000; ce: 0.18865156; steps/sec: 71.25; model1_lr: 0.0008350416; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 26.52 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.050555836; accuracy: 0.9816; \nFastEstimator-Train: step: 2000; ce: 0.052690372; steps/sec: 70.85; model1_lr: 0.00044870423; \nFastEstimator-Train: step: 3000; ce: 0.0037323756; steps/sec: 70.63; model1_lr: 9.664212e-05; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 26.56 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.030163307; accuracy: 0.99; \nFastEstimator-Train: step: 4000; ce: 0.063815504; steps/sec: 70.37; model1_lr: 0.0009891716; \nFastEstimator-Train: step: 5000; ce: 0.002615007; steps/sec: 73.63; model1_lr: 0.0007506123; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 25.93 sec; \nFastEstimator-Eval: step: 5625; epoch: 3; ce: 0.030318245; accuracy: 0.9902; \nFastEstimator-Finish: step: 5625; total_time: 81.43 sec; model1_lr: 0.0005009185; \n</pre> <p>Now let's launch TensorBoard to visualize our logs. Note that this call will prevent any subsequent Jupyter Notebook cells from running until you manually terminate it.</p> In\u00a0[15]: Copied! <pre>#!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe\n</pre> #!tensorboard --reload_multifile=true --logdir /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpb_oy2ihe <p>The TensorBoard display should look something like this:</p> <p></p> <p></p>"}, {"location": "tutorial/advanced/t06_summary.html#advanced-tutorial-6-summary", "title": "Advanced Tutorial 6: Summary\u00b6", "text": ""}, {"location": "tutorial/advanced/t06_summary.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Experiment Logging</li> <li>Experiment Summaries</li> <li>Log Parsing</li> <li>Summary Visualization</li> <li>TensorBoard Visualization</li> </ul>"}, {"location": "tutorial/advanced/t06_summary.html#preliminary-setup", "title": "Preliminary Setup\u00b6", "text": "<p>We will first set up a basic MNIST example for the rest of the demonstrations:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-logging", "title": "Experiment Logging\u00b6", "text": "<p>As you may have noticed if you have used FastEstimator, log messages are printed to the screen during training. If you want to persist these log messages for later records, you can simply pipe them into a file when launching training from the command line, or else just copy and paste the messages from the console into a persistent file on the disk. FastEstimator allows logging to be controlled via arguments passed to the <code>Estimator</code> class, as described in the tutorial 7 in the beginner section. Let's see an example logging every 120 steps:</p>"}, {"location": "tutorial/advanced/t06_summary.html#experiment-summaries", "title": "Experiment Summaries\u00b6", "text": "<p>Having log messages on the screen can be handy, but what if you want to access these messages within python? Enter the <code>Summary</code> class. <code>Summary</code> objects contain information about the training over time, and will be automatically generated when the <code>Estimator</code> fit() method is invoked with an experiment name:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-parsing", "title": "Log Parsing\u00b6", "text": "<p>Suppose that you have a log file saved to disk, and you want to create an in-memory <code>Summary</code> representation of it. This can be done through FastEstimator logging utilities:</p>"}, {"location": "tutorial/advanced/t06_summary.html#log-visualization", "title": "Log Visualization\u00b6", "text": "<p>While seeing log data as numbers can be informative, visualizations of data are often more useful. FastEstimator provides several ways to visualize log data: from python using <code>Summary</code> objects or log files, as well as through the command line.</p>"}, {"location": "tutorial/advanced/t06_summary.html#tensorboard", "title": "TensorBoard\u00b6", "text": "<p>Of course, no modern AI framework would be complete without TensorBoard integration. In FastEstimator, all that is required to achieve TensorBoard integration is to add the TensorBoard <code>Trace</code> to the list of traces passed to the <code>Estimator</code>:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html", "title": "Advanced Tutorial 7: Learning Rate Scheduling", "text": "<p>Learning rate schedules can be implemented using the <code>LRScheduler</code> <code>Trace</code>. <code>LRScheduler</code> takes the model and learning schedule through the lr_fn parameter. lr_fn should be a function/lambda function with 'step' or 'epoch' as its input parameter. This determines whether the learning schedule will be applied at a step or epoch level.</p> <p>For more details on traces, you can visit tutorial 7 in the beginner section and tutorial 4 in the advanced section.</p> <p>Let's create a function to generate the pipeline, model, and network to be used for this tutorial:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\n\ndef get_pipeline_model_network(model_name=\"LeNet\"):\n    train_data, _ = mnist.load_data()\n\n    pipeline = fe.Pipeline(train_data=train_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), \n                                Minmax(inputs=\"x\", outputs=\"x\")])\n\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)\n\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n\n    return pipeline, model, network\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp   def get_pipeline_model_network(model_name=\"LeNet\"):     train_data, _ = mnist.load_data()      pipeline = fe.Pipeline(train_data=train_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\"),                                  Minmax(inputs=\"x\", outputs=\"x\")])      model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=model_name)      network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")     ])      return pipeline, model, network <p></p> <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.summary.logs import visualize_logs\nfrom fastestimator.trace.adapt import LRScheduler\n\ndef lr_schedule(epoch):\n    lr = 0.001*(20-epoch+1)/20\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)\n\nhistory = estimator.fit(summary=\"Experiment_1\")\n</pre> from fastestimator.summary.logs import visualize_logs from fastestimator.trace.adapt import LRScheduler  def lr_schedule(epoch):     lr = 0.001*(20-epoch+1)/20     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=3, traces=traces)  history = estimator.fit(summary=\"Experiment_1\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3121834; LeNet_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.3843257; steps/sec: 139.02; LeNet_lr: 0.001; \nFastEstimator-Train: step: 200; ce: 0.117751315; steps/sec: 131.96; LeNet_lr: 0.001; \nFastEstimator-Train: step: 300; ce: 0.20433763; steps/sec: 129.58; LeNet_lr: 0.001; \nFastEstimator-Train: step: 400; ce: 0.10046323; steps/sec: 122.35; LeNet_lr: 0.001; \nFastEstimator-Train: step: 500; ce: 0.18251789; steps/sec: 122.15; LeNet_lr: 0.001; \nFastEstimator-Train: step: 600; ce: 0.027669977; steps/sec: 118.68; LeNet_lr: 0.001; \nFastEstimator-Train: step: 700; ce: 0.018796597; steps/sec: 117.34; LeNet_lr: 0.001; \nFastEstimator-Train: step: 800; ce: 0.1343742; steps/sec: 113.8; LeNet_lr: 0.001; \nFastEstimator-Train: step: 900; ce: 0.066348195; steps/sec: 101.95; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1000; ce: 0.21500783; steps/sec: 103.25; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1100; ce: 0.020392025; steps/sec: 101.94; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1200; ce: 0.06266867; steps/sec: 101.34; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1300; ce: 0.0051358286; steps/sec: 98.58; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1400; ce: 0.11623003; steps/sec: 95.29; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1500; ce: 0.2841274; steps/sec: 94.73; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1600; ce: 0.0059423; steps/sec: 91.64; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1700; ce: 0.020643737; steps/sec: 93.76; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1800; ce: 0.05520491; steps/sec: 94.92; LeNet_lr: 0.001; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 19.52 sec; \nFastEstimator-Train: step: 1900; ce: 0.011271543; steps/sec: 94.66; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2000; ce: 0.02188245; steps/sec: 89.22; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2100; ce: 0.0048960065; steps/sec: 84.91; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2200; ce: 0.009766675; steps/sec: 90.6; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2300; ce: 0.008525206; steps/sec: 88.56; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2400; ce: 0.007846909; steps/sec: 85.83; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2500; ce: 0.11211253; steps/sec: 84.11; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2600; ce: 0.013980574; steps/sec: 80.19; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2700; ce: 0.026221849; steps/sec: 79.1; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2800; ce: 0.025860263; steps/sec: 78.37; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 2900; ce: 0.008109703; steps/sec: 77.45; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3000; ce: 0.09107354; steps/sec: 75.67; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3100; ce: 0.003782929; steps/sec: 73.88; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3200; ce: 0.0090578655; steps/sec: 72.33; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3300; ce: 0.103154205; steps/sec: 83.93; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3400; ce: 0.013315021; steps/sec: 79.49; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3500; ce: 0.017441805; steps/sec: 85.13; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3600; ce: 0.017847143; steps/sec: 77.92; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3700; ce: 0.04056422; steps/sec: 82.93; LeNet_lr: 0.00095; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 23.04 sec; \nFastEstimator-Train: step: 3800; ce: 0.004228523; steps/sec: 79.59; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 3900; ce: 0.0015197117; steps/sec: 81.38; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4000; ce: 0.0004167799; steps/sec: 79.58; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4100; ce: 0.012444577; steps/sec: 79.87; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4200; ce: 0.011467964; steps/sec: 80.24; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4300; ce: 0.004452401; steps/sec: 76.3; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4400; ce: 0.0045919186; steps/sec: 77.09; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4500; ce: 0.05509679; steps/sec: 80.72; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4600; ce: 0.0028458317; steps/sec: 76.64; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4700; ce: 0.014053181; steps/sec: 78.09; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4800; ce: 0.00821719; steps/sec: 77.62; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 4900; ce: 0.05650976; steps/sec: 81.53; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5000; ce: 0.044777524; steps/sec: 75.98; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5100; ce: 0.000998376; steps/sec: 76.29; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5200; ce: 0.00041511073; steps/sec: 73.71; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5300; ce: 0.023241056; steps/sec: 86.19; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5400; ce: 0.18524896; steps/sec: 81.82; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5500; ce: 0.12962568; steps/sec: 83.83; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5600; ce: 0.0027833423; steps/sec: 83.36; LeNet_lr: 0.0009; \nFastEstimator-Train: step: 5625; epoch: 3; epoch_time: 23.63 sec; \nFastEstimator-Finish: step: 5625; total_time: 66.27 sec; LeNet_lr: 0.0009; \n</pre> <p>The learning rate is available in the training log at steps specified using the log_steps parameter in the <code>Estimator</code>. By default, training is logged every 100 steps.</p> In\u00a0[3]: Copied! <pre>visualize_logs(history, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history, include_metrics=\"LeNet_lr\") <p>As you can see, the learning rate changes only after every epoch.</p> <p></p> In\u00a0[4]: Copied! <pre>def lr_schedule(step):\n    lr = 0.001*(7500-step+1)/7500\n    return lr\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lr_schedule)\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory2 = estimator.fit(summary=\"Experiment_2\")\n</pre> def lr_schedule(step):     lr = 0.001*(7500-step+1)/7500     return lr  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lr_schedule) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history2 = estimator.fit(summary=\"Experiment_2\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3268642; LeNet_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.34729302; steps/sec: 81.02; LeNet_lr: 0.0009868; \nFastEstimator-Train: step: 200; ce: 0.16947752; steps/sec: 79.79; LeNet_lr: 0.00097346667; \nFastEstimator-Train: step: 300; ce: 0.1158019; steps/sec: 79.11; LeNet_lr: 0.00096013333; \nFastEstimator-Train: step: 400; ce: 0.08662731; steps/sec: 77.06; LeNet_lr: 0.0009468; \nFastEstimator-Train: step: 500; ce: 0.0455483; steps/sec: 75.55; LeNet_lr: 0.00093346665; \nFastEstimator-Train: step: 600; ce: 0.17311177; steps/sec: 77.16; LeNet_lr: 0.0009201333; \nFastEstimator-Train: step: 700; ce: 0.07044801; steps/sec: 77.25; LeNet_lr: 0.0009068; \nFastEstimator-Train: step: 800; ce: 0.060867704; steps/sec: 76.46; LeNet_lr: 0.00089346664; \nFastEstimator-Train: step: 900; ce: 0.14833035; steps/sec: 77.65; LeNet_lr: 0.00088013336; \nFastEstimator-Train: step: 1000; ce: 0.01502298; steps/sec: 75.22; LeNet_lr: 0.0008668; \nFastEstimator-Train: step: 1100; ce: 0.14445232; steps/sec: 75.63; LeNet_lr: 0.0008534667; \nFastEstimator-Train: step: 1200; ce: 0.027258653; steps/sec: 74.33; LeNet_lr: 0.00084013335; \nFastEstimator-Train: step: 1300; ce: 0.12475329; steps/sec: 73.37; LeNet_lr: 0.0008268; \nFastEstimator-Train: step: 1400; ce: 0.09298558; steps/sec: 76.89; LeNet_lr: 0.0008134667; \nFastEstimator-Train: step: 1500; ce: 0.048965212; steps/sec: 71.28; LeNet_lr: 0.00080013333; \nFastEstimator-Train: step: 1600; ce: 0.043585315; steps/sec: 74.56; LeNet_lr: 0.0007868; \nFastEstimator-Train: step: 1700; ce: 0.18490058; steps/sec: 72.17; LeNet_lr: 0.00077346666; \nFastEstimator-Train: step: 1800; ce: 0.08914224; steps/sec: 73.68; LeNet_lr: 0.0007601333; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 24.87 sec; \nFastEstimator-Train: step: 1900; ce: 0.06549401; steps/sec: 72.05; LeNet_lr: 0.0007468; \nFastEstimator-Train: step: 2000; ce: 0.005398229; steps/sec: 73.46; LeNet_lr: 0.00073346664; \nFastEstimator-Train: step: 2100; ce: 0.19036639; steps/sec: 73.44; LeNet_lr: 0.0007201333; \nFastEstimator-Train: step: 2200; ce: 0.0037528607; steps/sec: 74.23; LeNet_lr: 0.0007068; \nFastEstimator-Train: step: 2300; ce: 0.07193564; steps/sec: 73.72; LeNet_lr: 0.0006934667; \nFastEstimator-Train: step: 2400; ce: 0.27834976; steps/sec: 72.84; LeNet_lr: 0.00068013335; \nFastEstimator-Train: step: 2500; ce: 0.058940012; steps/sec: 74.08; LeNet_lr: 0.0006668; \nFastEstimator-Train: step: 2600; ce: 0.14127687; steps/sec: 73.21; LeNet_lr: 0.0006534667; \nFastEstimator-Train: step: 2700; ce: 0.03426341; steps/sec: 75.49; LeNet_lr: 0.00064013334; \nFastEstimator-Train: step: 2800; ce: 0.033499897; steps/sec: 71.58; LeNet_lr: 0.0006268; \nFastEstimator-Train: step: 2900; ce: 0.008997633; steps/sec: 75.63; LeNet_lr: 0.00061346666; \nFastEstimator-Train: step: 3000; ce: 0.02539826; steps/sec: 74.05; LeNet_lr: 0.0006001333; \nFastEstimator-Train: step: 3100; ce: 0.10326672; steps/sec: 72.52; LeNet_lr: 0.0005868; \nFastEstimator-Train: step: 3200; ce: 0.008950228; steps/sec: 78.7; LeNet_lr: 0.00057346665; \nFastEstimator-Train: step: 3300; ce: 0.023044724; steps/sec: 76.58; LeNet_lr: 0.0005601333; \nFastEstimator-Train: step: 3400; ce: 0.030512333; steps/sec: 72.12; LeNet_lr: 0.0005468; \nFastEstimator-Train: step: 3500; ce: 0.0038094707; steps/sec: 72.78; LeNet_lr: 0.0005334667; \nFastEstimator-Train: step: 3600; ce: 0.041347966; steps/sec: 75.93; LeNet_lr: 0.00052013336; \nFastEstimator-Train: step: 3700; ce: 0.0005119173; steps/sec: 72.98; LeNet_lr: 0.0005068; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 25.35 sec; \nFastEstimator-Finish: step: 3750; total_time: 50.26 sec; LeNet_lr: 0.0005001333; \n</pre> In\u00a0[5]: Copied! <pre>visualize_logs(history2, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history2, include_metrics=\"LeNet_lr\") <p></p> <p></p> In\u00a0[6]: Copied! <pre>from fastestimator.schedule import cosine_decay\n\npipeline, model, network = get_pipeline_model_network()\n\ntraces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3))\nestimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)\n\nhistory3 = estimator.fit(summary=\"Experiment_3\")\n</pre> from fastestimator.schedule import cosine_decay  pipeline, model, network = get_pipeline_model_network()  traces = LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=1875, init_lr=1e-3)) estimator = fe.Estimator(pipeline=pipeline, network=network, epochs=2, traces=traces)  history3 = estimator.fit(summary=\"Experiment_3\") <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3090043; LeNet_lr: 0.001; \nFastEstimator-Train: step: 100; ce: 0.34759182; steps/sec: 72.64; LeNet_lr: 0.0009931439; \nFastEstimator-Train: step: 200; ce: 0.124293044; steps/sec: 72.99; LeNet_lr: 0.0009724906; \nFastEstimator-Train: step: 300; ce: 0.11162343; steps/sec: 76.42; LeNet_lr: 0.00093861774; \nFastEstimator-Train: step: 400; ce: 0.1846501; steps/sec: 73.36; LeNet_lr: 0.0008924742; \nFastEstimator-Train: step: 500; ce: 0.09742119; steps/sec: 72.06; LeNet_lr: 0.0008353522; \nFastEstimator-Train: step: 600; ce: 0.013896314; steps/sec: 74.61; LeNet_lr: 0.0007688517; \nFastEstimator-Train: step: 700; ce: 0.10306329; steps/sec: 71.42; LeNet_lr: 0.00069483527; \nFastEstimator-Train: step: 800; ce: 0.020540427; steps/sec: 72.34; LeNet_lr: 0.00061537593; \nFastEstimator-Train: step: 900; ce: 0.33192694; steps/sec: 74.66; LeNet_lr: 0.0005326991; \nFastEstimator-Train: step: 1000; ce: 0.027314447; steps/sec: 73.71; LeNet_lr: 0.00044912045; \nFastEstimator-Train: step: 1100; ce: 0.07889113; steps/sec: 71.89; LeNet_lr: 0.00036698082; \nFastEstimator-Train: step: 1200; ce: 0.13510469; steps/sec: 71.83; LeNet_lr: 0.0002885808; \nFastEstimator-Train: step: 1300; ce: 0.026300007; steps/sec: 75.5; LeNet_lr: 0.00021611621; \nFastEstimator-Train: step: 1400; ce: 0.028623505; steps/sec: 70.93; LeNet_lr: 0.00015161661; \nFastEstimator-Train: step: 1500; ce: 0.12303919; steps/sec: 72.35; LeNet_lr: 9.688851e-05; \nFastEstimator-Train: step: 1600; ce: 0.08176289; steps/sec: 73.9; LeNet_lr: 5.3464726e-05; \nFastEstimator-Train: step: 1700; ce: 0.018114068; steps/sec: 76.93; LeNet_lr: 2.2561479e-05; \nFastEstimator-Train: step: 1800; ce: 0.02731928; steps/sec: 73.86; LeNet_lr: 5.0442964e-06; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 25.59 sec; \nFastEstimator-Train: step: 1900; ce: 0.014814988; steps/sec: 76.8; LeNet_lr: 0.0009995962; \nFastEstimator-Train: step: 2000; ce: 0.054916177; steps/sec: 76.62; LeNet_lr: 0.000989258; \nFastEstimator-Train: step: 2100; ce: 0.016524037; steps/sec: 75.3; LeNet_lr: 0.0009652308; \nFastEstimator-Train: step: 2200; ce: 0.03584575; steps/sec: 77.71; LeNet_lr: 0.0009281874; \nFastEstimator-Train: step: 2300; ce: 0.016135138; steps/sec: 73.3; LeNet_lr: 0.00087916537; \nFastEstimator-Train: step: 2400; ce: 0.017202383; steps/sec: 76.79; LeNet_lr: 0.0008195377; \nFastEstimator-Train: step: 2500; ce: 0.02264627; steps/sec: 75.4; LeNet_lr: 0.00075097446; \nFastEstimator-Train: step: 2600; ce: 0.014660467; steps/sec: 81.77; LeNet_lr: 0.00067539595; \nFastEstimator-Train: step: 2700; ce: 0.1418988; steps/sec: 76.47; LeNet_lr: 0.0005949189; \nFastEstimator-Train: step: 2800; ce: 0.033834495; steps/sec: 74.0; LeNet_lr: 0.00051179744; \nFastEstimator-Train: step: 2900; ce: 0.117458574; steps/sec: 75.8; LeNet_lr: 0.00042835958; \nFastEstimator-Train: step: 3000; ce: 0.039562702; steps/sec: 74.06; LeNet_lr: 0.0003469422; \nFastEstimator-Train: step: 3100; ce: 0.013925586; steps/sec: 77.25; LeNet_lr: 0.00026982563; \nFastEstimator-Train: step: 3200; ce: 0.010165918; steps/sec: 74.82; LeNet_lr: 0.0001991698; \nFastEstimator-Train: step: 3300; ce: 0.009503109; steps/sec: 74.25; LeNet_lr: 0.00013695359; \nFastEstimator-Train: step: 3400; ce: 0.02341089; steps/sec: 75.41; LeNet_lr: 8.491957e-05; \nFastEstimator-Train: step: 3500; ce: 0.06209151; steps/sec: 74.37; LeNet_lr: 4.452509e-05; \nFastEstimator-Train: step: 3600; ce: 0.105473876; steps/sec: 78.39; LeNet_lr: 1.6901524e-05; \nFastEstimator-Train: step: 3700; ce: 0.010678038; steps/sec: 74.96; LeNet_lr: 2.8225472e-06; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 24.75 sec; \nFastEstimator-Finish: step: 3750; total_time: 50.39 sec; LeNet_lr: 1.0007011e-06; \n</pre> In\u00a0[7]: Copied! <pre>visualize_logs(history3, include_metrics=\"LeNet_lr\")\n</pre> visualize_logs(history3, include_metrics=\"LeNet_lr\") <p></p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#advanced-tutorial-7-learning-rate-scheduling", "title": "Advanced Tutorial 7: Learning Rate Scheduling\u00b6", "text": ""}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss:</p> <ul> <li>Customizing a Learning Rate Schedule Function<ul> <li>epoch-wise</li> <li>step-wise</li> </ul> </li> <li>Using a Built-In lr_schedule Function<ul> <li>cosine decay</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#customizing-a-learning-rate-schedule-function", "title": "Customizing a Learning Rate Schedule Function\u00b6", "text": "<p>We can specify a custom learning schedule by passing a custom function to the lr_fn parameter of <code>LRScheduler</code>. We can have this learning rate schedule applied at either the epoch or step level. Epoch and step both start from 1.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#epoch-wise", "title": "Epoch-wise\u00b6", "text": "<p>To apply learning rate scheduling at an epoch level, the custom function should have 'epoch' as its parameter. Let's look at the example below which demonstrates this. We will be using the summary parameter in the fit method to be able to visualize the learning rate later. You can go through tutorial 6 in the advanced section for more details on accessing training history.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#step-wise", "title": "Step-wise\u00b6", "text": "<p>The custom function should have 'step' as its parameter for step-based learning rate schedules.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#using-built-in-lr_schedule-function", "title": "Using Built-In lr_schedule Function\u00b6", "text": "<p>Some learning rates schedules are widely popular in the deep learning community. We have implemented some of them in FastEstimator so that you don't need to write a custom schedule for them. We will be showcasing the <code>cosine decay</code> schedule below.</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#cosine_decay", "title": "cosine_decay\u00b6", "text": "<p>We can specify the length of the decay cycle and initial learning rate using cycle_length and init_lr respectively. Similar to custom learning schedule, lr_fn should have step or epoch as a parameter. The FastEstimator cosine decay can be used as follows:</p>"}, {"location": "tutorial/advanced/t07_learning_rate_scheduling.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/advanced/t08_xai.html", "title": "Advanced Tutorial 8: Explainable AI (XAI)", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.tensorflow import LeNet\nfrom fastestimator.backend import squeeze, to_number\nfrom fastestimator.dataset.data import cifar10\nfrom fastestimator.op.numpyop.univariate import Normalize\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver, ImageViewer, TensorBoard\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.xai import Saliency\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlabel_mapping = {\n    'airplane': 0,\n    'automobile': 1,\n    'bird': 2,\n    'cat': 3,\n    'deer': 4,\n    'dog': 5,\n    'frog': 6,\n    'horse': 7,\n    'ship': 8,\n    'truck': 9\n}\n\nbatch_size=32\n\ntrain_data, eval_data = cifar10.load_data()\ntest_data = eval_data.split(0.5)\npipeline = fe.Pipeline(\n    train_data=train_data,\n    eval_data=eval_data,\n    test_data=test_data,\n    batch_size=batch_size,\n    ops=[Normalize(inputs=\"x\", outputs=\"x\")],\n    num_process=0)\n\nmodel = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\")\nnetwork = fe.Network(ops=[\n    ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n    CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n    UpdateOp(model=model, loss_name=\"ce\")\n])\n\ntraces = [\n    Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n    LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),\n    Saliency(model=model,\n             model_inputs=\"x\",\n             class_key=\"y\",\n             model_outputs=\"y_pred\",\n             samples=5,\n             label_mapping=label_mapping),\n    ImageViewer(inputs=\"saliency\")\n]\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=5,\n                         traces=traces,\n                         log_steps=1000)\n</pre> import fastestimator as fe from fastestimator.architecture.tensorflow import LeNet from fastestimator.backend import squeeze, to_number from fastestimator.dataset.data import cifar10 from fastestimator.op.numpyop.univariate import Normalize from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver, ImageViewer, TensorBoard from fastestimator.trace.metric import Accuracy from fastestimator.trace.xai import Saliency  import matplotlib.pyplot as plt import numpy as np  label_mapping = {     'airplane': 0,     'automobile': 1,     'bird': 2,     'cat': 3,     'deer': 4,     'dog': 5,     'frog': 6,     'horse': 7,     'ship': 8,     'truck': 9 }  batch_size=32  train_data, eval_data = cifar10.load_data() test_data = eval_data.split(0.5) pipeline = fe.Pipeline(     train_data=train_data,     eval_data=eval_data,     test_data=test_data,     batch_size=batch_size,     ops=[Normalize(inputs=\"x\", outputs=\"x\")],     num_process=0)  model = fe.build(model_fn=lambda: LeNet(input_shape=(32, 32, 3)), optimizer_fn=\"adam\") network = fe.Network(ops=[     ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),     CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),     UpdateOp(model=model, loss_name=\"ce\") ])  traces = [     Accuracy(true_key=\"y\", pred_key=\"y_pred\"),     LRScheduler(model=model, lr_fn=lambda step: cosine_decay(step, cycle_length=3750, init_lr=1e-3)),     Saliency(model=model,              model_inputs=\"x\",              class_key=\"y\",              model_outputs=\"y_pred\",              samples=5,              label_mapping=label_mapping),     ImageViewer(inputs=\"saliency\") ] estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=5,                          traces=traces,                          log_steps=1000) <p>In this example we will be using the <code>ImageViewer</code> <code>Trace</code>, since it will allow us to visualize the outputs within this Notebook. If you wanted your images to appear in TensorBoard, simply construct a <code>TensorBoard</code> <code>Trace</code> with the \"write_images\" argument set to \"saliency\".</p> In\u00a0[2]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 1000; \nFastEstimator-Train: step: 1; ce: 2.3806455; model_lr: 0.001; \nFastEstimator-Train: step: 1000; ce: 1.4054501; steps/sec: 94.35; model_lr: 0.0008350416; \nFastEstimator-Train: step: 1563; epoch: 1; epoch_time: 19.51 sec; \n</pre> <pre>FastEstimator-Eval: step: 1563; epoch: 1; ce: 1.1105024; accuracy: 0.5994; \nFastEstimator-Train: step: 2000; ce: 0.81444156; steps/sec: 80.06; model_lr: 0.00044870423; \nFastEstimator-Train: step: 3000; ce: 0.76189625; steps/sec: 64.37; model_lr: 9.664212e-05; \nFastEstimator-Train: step: 3126; epoch: 2; epoch_time: 23.28 sec; \n</pre> <pre>FastEstimator-Eval: step: 3126; epoch: 2; ce: 0.91278446; accuracy: 0.677; \nFastEstimator-Train: step: 4000; ce: 0.94472736; steps/sec: 57.3; model_lr: 0.0009891716; \nFastEstimator-Train: step: 4689; epoch: 3; epoch_time: 27.2 sec; \n</pre> <pre>FastEstimator-Eval: step: 4689; epoch: 3; ce: 0.9597512; accuracy: 0.6574; \nFastEstimator-Train: step: 5000; ce: 1.0804241; steps/sec: 58.34; model_lr: 0.0007506123; \nFastEstimator-Train: step: 6000; ce: 0.6896681; steps/sec: 57.78; model_lr: 0.00034654405; \nFastEstimator-Train: step: 6252; epoch: 4; epoch_time: 27.12 sec; \n</pre> <pre>FastEstimator-Eval: step: 6252; epoch: 4; ce: 0.8268497; accuracy: 0.713; \nFastEstimator-Train: step: 7000; ce: 0.65570337; steps/sec: 56.06; model_lr: 4.435441e-05; \nFastEstimator-Train: step: 7815; epoch: 5; epoch_time: 28.31 sec; \n</pre> <pre>FastEstimator-Eval: step: 7815; epoch: 5; ce: 0.8746784; accuracy: 0.7012; \nFastEstimator-Finish: step: 7815; total_time: 148.33 sec; model_lr: 0.0009828171; \n</pre> In\u00a0[3]: Copied! <pre>estimator.test()\n</pre> estimator.test() <pre>FastEstimator-Test: step: 7815; epoch: 5; accuracy: 0.692; \n</pre> <p>In the images above, the 'saliency' column corresponds to a raw saliency mask generated by back-propagating a model's output prediction onto the input image. 'Smoothed saliency' combines multiple saliency masks for each image 'x', where each mask is generated by slightly perturbing the input 'x' before running the forward and backward gradient passes. The number of samples to be combined is controlled by the \"smoothing\" argument in the <code>Saliency</code> <code>Trace</code> constructor. 'Integrated saliency' is a saliency mask generated by starting from a baseline blank image and linearly interpolating the image towards 'x' over a number of steps defined by the \"integrating\" argument in the Saliency constructor. The resulting masks are then combined together. The 'SmInt Saliency' (Smoothed-Integrated) column combines smoothing and integration together. SmInt is generally considered to give the most reliable indication of the important features in an image, but it also takes the longest to compute. It is possible to disable the more complex columns by setting the 'smoothing' and 'integrating' parameters to 0. The 'x saliency' column shows the input image overlaid with whatever saliency column is furthest to the right (SmInt, unless that has been disabled).</p> <p></p> In\u00a0[4]: Copied! <pre>import tempfile\nimport os\n\npipeline.batch_size = 6\nbatch = pipeline.get_results()\nbatch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow\n\nsaliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\")\nimages = saliency_generator.get_masks(batch=batch)\n\n# Let's convert 'y' and 'y_pred' from numeric values to strings for readability:\nval_to_label = {val: key for key, val in label_mapping.items()}\ny = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))])\ny_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])\n\n# Now simply load up an ImgData object and let it handle laying out the final result for you\nsave_dir = tempfile.mkdtemp()\nimages = fe.util.ImgData(colormap=\"inferno\", y=y, y_pred=y_pred, x=batch[\"x\"], saliency=images[\"saliency\"])\nfig = images.paint_figure(save_path=os.path.join(save_dir, \"t08a_saliency.png\")) # save_path is optional, but a useful feature to know about\nplt.show()\n</pre> import tempfile import os  pipeline.batch_size = 6 batch = pipeline.get_results() batch = fe.backend.to_tensor(batch, \"tf\")  # Convert the batch to TensorFlow  saliency_generator = fe.xai.SaliencyNet(model=model, model_inputs=\"x\", model_outputs=\"y_pred\") images = saliency_generator.get_masks(batch=batch)  # Let's convert 'y' and 'y_pred' from numeric values to strings for readability: val_to_label = {val: key for key, val in label_mapping.items()} y = np.array([val_to_label[clazz] for clazz in to_number(squeeze(batch[\"y\"]))]) y_pred = np.array([val_to_label[clazz] for clazz in to_number(squeeze(images[\"y_pred\"]))])  # Now simply load up an ImgData object and let it handle laying out the final result for you save_dir = tempfile.mkdtemp() images = fe.util.ImgData(colormap=\"inferno\", y=y, y_pred=y_pred, x=batch[\"x\"], saliency=images[\"saliency\"]) fig = images.paint_figure(save_path=os.path.join(save_dir, \"t08a_saliency.png\")) # save_path is optional, but a useful feature to know about plt.show() <p>The <code>SaliencyNet</code> class also provides 'get_smoothed_masks' and 'get_integrated_masks' methods for generating the more complicated saliency maps.</p>"}, {"location": "tutorial/advanced/t08_xai.html#advanced-tutorial-8-explainable-ai-xai", "title": "Advanced Tutorial 8: Explainable AI (XAI)\u00b6", "text": ""}, {"location": "tutorial/advanced/t08_xai.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will discuss the following topics:</p> <ul> <li>Saliency Maps<ul> <li>With Traces</li> <li>Without Traces</li> </ul> </li> </ul>"}, {"location": "tutorial/advanced/t08_xai.html#saliency-maps", "title": "Saliency Maps\u00b6", "text": "<p>Suppose you have a neural network that is performing image classification. The network tells you that the image it is looking at is an airplane, but you want to know whether it is really detecting an airplane, or if it is 'cheating' by noticing the blue sky in the image background. To answer this question, all you need to do is add the <code>Saliency</code> <code>Trace</code> to your list of traces, and pass its output to one of either the <code>ImageSaver</code>, <code>ImageViewer</code>, or <code>TensorBoard</code> <code>Traces</code>.</p>"}, {"location": "tutorial/advanced/t08_xai.html#saliency-maps-without-traces", "title": "Saliency Maps without Traces\u00b6", "text": "<p>Suppose that you want to generate Saliency masks without using a <code>Trace</code>. This can be done through the fe.xai package:</p>"}, {"location": "tutorial/beginner/t01_getting_started.html", "title": "Tutorial 1: Getting Started", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\n\ntrain_data, eval_data = mnist.load_data()\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")])\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax  train_data, eval_data = mnist.load_data()  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), Minmax(inputs=\"x\", outputs=\"x\")]) In\u00a0[2]: Copied! <pre>from fastestimator.architecture.tensorflow import LeNet\n# from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model\n\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\nnetwork = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        UpdateOp(model=model, loss_name=\"ce\") \n    ])\n</pre> from fastestimator.architecture.tensorflow import LeNet # from fastestimator.architecture.pytorch import LeNet  # One can also use a pytorch model  from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         UpdateOp(model=model, loss_name=\"ce\")      ]) In\u00a0[3]: Copied! <pre>from fastestimator.trace.metric import Accuracy\nfrom fastestimator.trace.io import BestModelSaver\nimport tempfile\n\ntraces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),\n          BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=traces)\n</pre> from fastestimator.trace.metric import Accuracy from fastestimator.trace.io import BestModelSaver import tempfile  traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),           BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=traces) In\u00a0[4]: Copied! <pre>estimator.fit()\n</pre> estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.325205; \nFastEstimator-Train: step: 100; ce: 0.37162033; steps/sec: 161.0; \nFastEstimator-Train: step: 200; ce: 0.24027318; steps/sec: 166.53; \nFastEstimator-Train: step: 300; ce: 0.042502172; steps/sec: 160.22; \nFastEstimator-Train: step: 400; ce: 0.08067161; steps/sec: 160.19; \nFastEstimator-Train: step: 500; ce: 0.0573852; steps/sec: 149.4; \nFastEstimator-Train: step: 600; ce: 0.0157291; steps/sec: 146.06; \nFastEstimator-Train: step: 700; ce: 0.21018827; steps/sec: 140.01; \nFastEstimator-Train: step: 800; ce: 0.008484628; steps/sec: 135.1; \nFastEstimator-Train: step: 900; ce: 0.02928259; steps/sec: 128.3; \nFastEstimator-Train: step: 1000; ce: 0.061196238; steps/sec: 126.4; \nFastEstimator-Train: step: 1100; ce: 0.06762987; steps/sec: 120.72; \nFastEstimator-Train: step: 1200; ce: 0.0072296523; steps/sec: 118.11; \nFastEstimator-Train: step: 1300; ce: 0.08244678; steps/sec: 110.16; \nFastEstimator-Train: step: 1400; ce: 0.07375234; steps/sec: 105.76; \nFastEstimator-Train: step: 1500; ce: 0.03207487; steps/sec: 104.01; \nFastEstimator-Train: step: 1600; ce: 0.1325811; steps/sec: 104.97; \nFastEstimator-Train: step: 1700; ce: 0.2333475; steps/sec: 99.21; \nFastEstimator-Train: step: 1800; ce: 0.081265345; steps/sec: 101.39; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 17.21 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmplq_y8tyg/model_best_accuracy.h5\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.05035614; accuracy: 0.9828; since_best_accuracy: 0; max_accuracy: 0.9828; \nFastEstimator-Train: step: 1900; ce: 0.24747448; steps/sec: 100.72; \nFastEstimator-Train: step: 2000; ce: 0.056484234; steps/sec: 169.42; \nFastEstimator-Train: step: 2100; ce: 0.1583787; steps/sec: 186.35; \nFastEstimator-Train: step: 2200; ce: 0.004822081; steps/sec: 179.8; \nFastEstimator-Train: step: 2300; ce: 0.027388994; steps/sec: 180.22; \nFastEstimator-Train: step: 2400; ce: 0.017995346; steps/sec: 183.84; \nFastEstimator-Train: step: 2500; ce: 0.0071977032; steps/sec: 184.27; \nFastEstimator-Train: step: 2600; ce: 0.034278065; steps/sec: 182.51; \nFastEstimator-Train: step: 2700; ce: 0.045357186; steps/sec: 181.42; \nFastEstimator-Train: step: 2800; ce: 0.057187617; steps/sec: 182.88; \nFastEstimator-Train: step: 2900; ce: 0.04257428; steps/sec: 178.63; \nFastEstimator-Train: step: 3000; ce: 0.26984444; steps/sec: 167.96; \nFastEstimator-Train: step: 3100; ce: 0.026010124; steps/sec: 166.83; \nFastEstimator-Train: step: 3200; ce: 0.03834851; steps/sec: 161.82; \nFastEstimator-Train: step: 3300; ce: 0.01365272; steps/sec: 166.79; \nFastEstimator-Train: step: 3400; ce: 0.015053293; steps/sec: 164.75; \nFastEstimator-Train: step: 3500; ce: 0.0041770767; steps/sec: 163.45; \nFastEstimator-Train: step: 3600; ce: 0.0006832063; steps/sec: 162.57; \nFastEstimator-Train: step: 3700; ce: 0.015146113; steps/sec: 158.26; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 11.0 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmplq_y8tyg/model_best_accuracy.h5\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.0408412; accuracy: 0.9875; since_best_accuracy: 0; max_accuracy: 0.9875; \nFastEstimator-Finish: step: 3750; total_time: 30.16 sec; model_lr: 0.001; \n</pre> In\u00a0[5]: Copied! <pre>import numpy as np\n\ndata = eval_data[0]\ndata = pipeline.transform(data, mode=\"eval\")\ndata = network.transform(data, mode=\"eval\")\n\nprint(\"Ground truth class is {}\".format(data[\"y\"][0]))\nprint(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"])))\nimg = fe.util.ImgData(x=data[\"x\"])\nfig = img.paint_figure()\n</pre> import numpy as np  data = eval_data[0] data = pipeline.transform(data, mode=\"eval\") data = network.transform(data, mode=\"eval\")  print(\"Ground truth class is {}\".format(data[\"y\"][0])) print(\"Predicted class is {}\".format(np.argmax(data[\"y_pred\"]))) img = fe.util.ImgData(x=data[\"x\"]) fig = img.paint_figure() <pre>Ground truth class is 7\nPredicted class is 7\n</pre>"}, {"location": "tutorial/beginner/t01_getting_started.html#tutorial-1-getting-started", "title": "Tutorial 1: Getting Started\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#overview", "title": "Overview\u00b6", "text": "<p>Welcome to FastEstimator! In this tutorial we are going to cover:</p> <ul> <li>The three main APIs of FastEstimator: <code>Pipeline</code>, <code>Network</code>, <code>Estimator</code></li> <li>An image classification example<ul> <li>Pipeline</li> <li>Network</li> <li>Estimator</li> <li>Training</li> <li>Inferencing</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t01_getting_started.html#three-main-apis", "title": "Three main APIs\u00b6", "text": "<p>All deep learning training work\ufb02ows involve the following three essential components, each mapping to a critical API in FastEstimator.</p> <ul> <li><p>Data pipeline: extracts data from disk/RAM, performs transformations. -&gt;  <code>fe.Pipeline</code></p> </li> <li><p>Network: performs trainable and differentiable operations. -&gt;  <code>fe.Network</code></p> </li> <li><p>Training loop: combines the data pipeline and network in an iterative process. -&gt;  <code>fe.Estimator</code></p> </li> </ul>  Any deep learning task can be constructed by following the 3 main steps:"}, {"location": "tutorial/beginner/t01_getting_started.html#image-classification-example", "title": "Image Classification Example\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#step-1-pipeline", "title": "Step 1 - Pipeline\u00b6", "text": "<p>We use FastEstimator dataset API to load the MNIST dataset. Please check out tutorial 2 for more details about the dataset API. In this case our data preprocessing involves:</p> <ol> <li>Expand image dimension from (28,28) to (28, 28, 1) for convenience during convolution operations.</li> <li>Rescale pixel values from [0, 255] to [0, 1].</li> </ol> <p>Please check out tutorial 3 for details about <code>Operator</code> and tutorial 4 for <code>Pipeline</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-2-network", "title": "Step 2 - Network\u00b6", "text": "<p>The model definition can be either from <code>tf.keras.Model</code> or <code>torch.nn.Module</code>, for more info about network definitions, check out tutorial 5. The differentiable operations during training are listed as follows:</p> <ol> <li>Feed the preprocessed images to the network and get prediction scores.</li> <li>Calculate <code>CrossEntropy</code> (loss) between prediction scores and ground truth.</li> <li>Update the model by minimizing <code>CrossEntropy</code>.</li> </ol> <p>For more info about <code>Network</code> and its operators, check out tutorial 6.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#step-3-estimator", "title": "Step 3 - Estimator\u00b6", "text": "<p>We define the <code>Estimator</code> to connect the <code>Network</code> to the <code>Pipeline</code>, and compute accuracy as a validation metric. Please see tutorial 7 for more about <code>Estimator</code> and <code>Traces</code>.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#start-training", "title": "Start Training\u00b6", "text": ""}, {"location": "tutorial/beginner/t01_getting_started.html#inferencing", "title": "Inferencing\u00b6", "text": "<p>After training, we can do inferencing on new data with <code>Pipeline.transform</code> and <code>Netowork.transform</code>. Please checkout tutorial 8 for more details.</p>"}, {"location": "tutorial/beginner/t01_getting_started.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>DNN</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html", "title": "Tutorial 2: Creating a FastEstimator dataset", "text": "<p>A Dataset in FastEstimator is a class that wraps raw input data and makes it easier to ingest into your model(s). In this tutorial we will learn about the different ways we can create these Datasets.</p> <p>The FastEstimator Dataset class inherits from the PyTorch Dataset class which provides a clean and efficient interface to load raw data. Thus, any code that you have written for PyTorch will continue to work in FastEstimator too. For a refresher on PyTorch Datasets you can go here.</p> <p>In this tutorial we will focus on two key functionalities that we need to provide for the Dataset class. The first one is the ability to get an individual data entry from the Dataset and the second one is the ability to get the length of the Dataset. This is done as follows:</p> <ul> <li>len(dataset) should return the size (number of samples) of the dataset.</li> <li>dataset[i] should return the i-th sample in the dataset. The return value should be a dictionary with data values keyed by strings.</li> </ul> <p>Let's create a simple PyTorch Dataset which shows this functionality.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nfrom torch.utils.data import Dataset\n\nclass mydataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data['x'].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n\na = {'x': np.random.rand(100,5), 'y': np.random.rand(100)}\nds = mydataset(a)\nprint(ds[0])\nprint(len(ds))\n</pre> import numpy as np from torch.utils.data import Dataset  class mydataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data['x'].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data}  a = {'x': np.random.rand(100,5), 'y': np.random.rand(100)} ds = mydataset(a) print(ds[0]) print(len(ds)) <pre>{'x': array([0.11588935, 0.27958611, 0.45703942, 0.36171531, 0.66472315]), 'y': 0.5592775462425909}\n100\n</pre> <p></p> <p>In this section we will showcase how a Dataset can be created using FastEstimator. This tutorial shows three ways to create Datasets. The first uses data from disk, the second uses data already in memory, and the third uses a generator to create a Dataset.</p> <p></p> <p>In this tutorial we will showcase two ways to create a Dataset from disk:</p> <p></p> <p>To showcase this we will first have to create a dummy directory structure representing the two classes. Then we create a few files in each of the directories. The following image shows the hierarchy of our temporary data directory:</p> <p></p> <p>Let's prepare the data according to the directory structure:</p> In\u00a0[2]: Copied! <pre>import os\nimport tempfile\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\na_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\nb_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)\n\na1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\")\na2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")\n\nb1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\")\nb2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\")\n</pre> import os import tempfile  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  a_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname) b_tmpdirname = tempfile.TemporaryDirectory(dir=tmpdirname)  a1 = open(os.path.join(a_tmpdirname.name, \"a1.txt\"), \"x\") a2 = open(os.path.join(a_tmpdirname.name, \"a2.txt\"), \"x\")  b1 = open(os.path.join(b_tmpdirname.name, \"b1.txt\"), \"x\") b2 = open(os.path.join(b_tmpdirname.name, \"b2.txt\"), \"x\") <p>Once that is done, all you have to do is create a Dataset by passing the dummy directory to the <code>LabeledDirDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[3]: Copied! <pre>dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.LabeledDirDataset(root_dir=tmpdirname)  print(dataset[0]) print(len(dataset)) <pre>{'x': '/var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmp8ttxesg9/tmpcmkfpq_n/a1.txt', 'y': 0}\n4\n</pre> <p></p> <p>To showcase creating a Dataset based on a CSV file, we now create a dummy CSV file representing information for the two classes. First, let's create the data to be used as input as follows:</p> In\u00a0[4]: Copied! <pre>import os\nimport tempfile\nimport pandas as pd\n\nimport fastestimator as fe\n\ntmpdirname = tempfile.mkdtemp()\n\ndata = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]}\ndf = pd.DataFrame(data=data)\ndf.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False)\n</pre> import os import tempfile import pandas as pd  import fastestimator as fe  tmpdirname = tempfile.mkdtemp()  data = {'x': ['a1.txt', 'a2.txt', 'b1.txt', 'b2.txt'], 'y': [0, 0, 1, 1]} df = pd.DataFrame(data=data) df.to_csv(os.path.join(tmpdirname, 'data.csv'), index=False) <p>Once that is done you can create a Dataset by passing the CSV to the <code>CSVDataset</code> class constructor. The following code snippet shows how this can be done:</p> In\u00a0[5]: Copied! <pre>dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))\n\nprint(dataset[0])\nprint(len(dataset))\n</pre> dataset = fe.dataset.CSVDataset(file_path=os.path.join(tmpdirname, 'data.csv'))  print(dataset[0]) print(len(dataset)) <pre>{'x': 'a1.txt', 'y': 0}\n4\n</pre> <p></p> <p>It is also possible to create a Dataset from data stored in memory. This may be useful for smaller datasets.</p> <p></p> <p>If you already have data in memory in the form of a Numpy array, it is easy to convert this data into a FastEstimator Dataset. To accomplish this, simply pass your data dictionary into the <code>NumpyDataset</code> class constructor. The following code snippet demonstrates this:</p> In\u00a0[6]: Copied! <pre>import numpy as np\nimport tensorflow as tf\n\nimport fastestimator as fe\n\n(x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data()\ntrain_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train})\neval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})\n\nprint (train_data[0]['y'])\nprint (len(train_data))\n</pre> import numpy as np import tensorflow as tf  import fastestimator as fe  (x_train, y_train), (x_eval, y_eval) = tf.keras.datasets.mnist.load_data() train_data = fe.dataset.NumpyDataset({\"x\": x_train, \"y\": y_train}) eval_data = fe.dataset.NumpyDataset({\"x\": x_eval, \"y\": y_eval})  print (train_data[0]['y']) print (len(train_data)) <pre>5\n60000\n</pre> <p></p> <p>It is also possible to create a Dataset using generators. As an example, we will first create a generator which will generate random input data for us.</p> In\u00a0[7]: Copied! <pre>import numpy as np\n\ndef inputs():\n    while True:\n        yield {'x': np.random.rand(4), 'y':np.random.randint(2)}\n</pre> import numpy as np  def inputs():     while True:         yield {'x': np.random.rand(4), 'y':np.random.randint(2)} <p>We then pass the generator as an argument to the <code>GeneratorDataset</code> class:</p> In\u00a0[8]: Copied! <pre>from fastestimator.dataset import GeneratorDataset\n\ndataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10)\nprint(dataset[0])\nprint(len(dataset))\n</pre> from fastestimator.dataset import GeneratorDataset  dataset = GeneratorDataset(generator=inputs(), samples_per_epoch=10) print(dataset[0]) print(len(dataset)) <pre>{'x': array([0.30590938, 0.65189247, 0.37606477, 0.01100033]), 'y': 1}\n10\n</pre> <p></p>"}, {"location": "tutorial/beginner/t02_dataset.html#tutorial-2-creating-a-fastestimator-dataset", "title": "Tutorial 2: Creating a FastEstimator dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover three different ways to create a Dataset using FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Torch Dataset Recap</li> <li>FastEstimator Dataset<ul> <li>Dataset from disk<ul> <li>LabeledDirDataset</li> <li>CSVDataset</li> </ul> </li> <li>Dataset from memory<ul> <li>NumpyDataset</li> </ul> </li> <li>Dataset from generator</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t02_dataset.html#torch-dataset-recap", "title": "Torch Dataset Recap\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#fastestimator-dataset", "title": "FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#1-dataset-from-disk", "title": "1. Dataset from disk\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#11-labeleddirdataset", "title": "1.1 LabeledDirDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#12-csvdataset", "title": "1.2 CSVDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#2-dataset-from-memory", "title": "2. Dataset from memory\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#21-numpydataset", "title": "2.1 NumpyDataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#3-dataset-from-generator", "title": "3. Dataset from Generator\u00b6", "text": ""}, {"location": "tutorial/beginner/t02_dataset.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNET</li> <li>DCGAN</li> <li>Siamese Networks</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html", "title": "Tutorial 3: Operator", "text": "In\u00a0[1]: Copied! <pre>class Op:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n    \n    def forward(self, data, state):\n        return data\n</pre> class Op:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode          def forward(self, data, state):         return data"}, {"location": "tutorial/beginner/t03_operator.html#tutorial-3-operator", "title": "Tutorial 3: Operator\u00b6", "text": ""}, {"location": "tutorial/beginner/t03_operator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will introduce the <code>Operator</code> - a fundamental building block within FastEstimator. This tutorial is structured as follows:</p> <ul> <li>Operator Definition</li> <li>Operator Structure</li> <li>Operator Expression</li> <li>Deep Learning Examples using Operators</li> </ul>"}, {"location": "tutorial/beginner/t03_operator.html#operator-definition", "title": "Operator Definition\u00b6", "text": "<p>From tutorial 1, we know that the preprocessing in <code>Pipeline</code> and the training in <code>Network</code> can be divided into several sub-tasks:</p> <ul> <li>Pipeline: <code>Expand_dim</code> -&gt; <code>Minmax</code></li> <li>Network: <code>ModelOp</code> -&gt; <code>CrossEntropy</code> -&gt; <code>UpdateOp</code></li> </ul> <p>Each sub-task is a modular unit that takes inputs, performs an operation, and then produces outputs. We therefore call these sub-tasks <code>Operator</code>s, and they form the building blocks of the FastEstimator <code>Pipeline</code> and <code>Network</code> APIs.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-structure", "title": "Operator Structure\u00b6", "text": "<p>An Operator has 3 main components:</p> <ul> <li>inputs: the key(s) of input data</li> <li>outputs: the key(s) of output data</li> <li>forward function: the transformation to be applied</li> </ul> <p>The base class constructor also takes a <code>mode</code> argument, but for now we will ignore it since <code>mode</code> will be discussed extensively in tutorial 9.</p>"}, {"location": "tutorial/beginner/t03_operator.html#operator-expression", "title": "Operator Expression\u00b6", "text": "<p>In this section, we will demonstrate how different tasks can be concisely expressed in operators.</p>"}, {"location": "tutorial/beginner/t03_operator.html#single-operator", "title": "Single Operator\u00b6", "text": "<p>If the task only requires taking one feature as input and transforming it to overwrite the old feature (e.g, <code>Minmax</code>), it can be expressed as:</p> <p></p> <p>If the task involves taking multiple features and overwriting them respectively (e.g, rotation of both an image and its mask), it can be expressed as:</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#multiple-operators", "title": "Multiple Operators\u00b6", "text": "<p>If there are two <code>Operator</code>s executing in a sequential manner (e.g, <code>Minmax</code> followed by <code>Transpose</code>), it can be expressed as:</p> <p></p> <p><code>Operator</code>s can also easily handle more complicated data flows:</p> <p></p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#deep-learning-examples-using-operators", "title": "Deep Learning Examples using Operators\u00b6", "text": "<p>In this section, we will show you how deep learning tasks can be modularized into combinations of <code>Operator</code>s. Please note that the <code>Operator</code> expressions we provide in this section are essentially pseudo-code. Links to full python examples are also provided.</p>"}, {"location": "tutorial/beginner/t03_operator.html#image-classification", "title": "Image Classification:\u00b6", "text": "<p>MNIST</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#dc-gan", "title": "DC-GAN:\u00b6", "text": "<p>DC-GAN</p> <p></p>"}, {"location": "tutorial/beginner/t03_operator.html#adversarial-hardening", "title": "Adversarial Hardening:\u00b6", "text": "<p>FGSM</p> <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html", "title": "Tutorial 4: Pipeline", "text": "<p>In tutorial 2 we demonstrated different ways to construct FastEstimator datasets. Here we will see how datasets can be loaded in the <code>Pipeline</code> and how various operations can then be applied to the data. <code>fe.Pipeline</code> handles three different types of datasets:</p> <ul> <li>tf.data.Dataset</li> <li>torch.data.Dataloader</li> <li>fe.dataset</li> </ul> <p>Let's create an example <code>tf.data.Dataset</code> and <code>torch.data.Dataloader</code> from numpy arrays and we will load them into a <code>Pipeline</code>:</p> In\u00a0[1]: Copied! <pre>import numpy as np\n\n# Make some random data to serve as the source for our datasets\nx_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1)))\ntrain_data = {\"x\": x_train, \"y\": y_train}\n</pre> import numpy as np  # Make some random data to serve as the source for our datasets x_train, y_train = (np.random.sample((100, 2)), np.random.sample((100, 1))) train_data = {\"x\": x_train, \"y\": y_train} <p></p> In\u00a0[2]: Copied! <pre>import fastestimator as fe\nimport tensorflow as tf\n\n# Create a tf.data.Dataset from sample data\ndataset_tf = tf.data.Dataset.from_tensor_slices(train_data)\ndataset_tf = dataset_tf.batch(4)\n\n# Load data into the pipeline\npipeline_tf = fe.Pipeline(dataset_tf)\n</pre> import fastestimator as fe import tensorflow as tf  # Create a tf.data.Dataset from sample data dataset_tf = tf.data.Dataset.from_tensor_slices(train_data) dataset_tf = dataset_tf.batch(4)  # Load data into the pipeline pipeline_tf = fe.Pipeline(dataset_tf) <p></p> <p>We will create a custom dataset class to load our train data into a PyTorch DataLoader.</p> In\u00a0[3]: Copied! <pre>from torch.utils.data import Dataset\n\nclass TorchCustomDataset(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n    def __len__(self):\n        return self.data[\"x\"].shape[0]\n    def __getitem__(self, idx):\n        return {key: self.data[key][idx] for key in self.data}\n</pre> from torch.utils.data import Dataset  class TorchCustomDataset(Dataset):     def __init__(self, data):         super().__init__()         self.data = data     def __len__(self):         return self.data[\"x\"].shape[0]     def __getitem__(self, idx):         return {key: self.data[key][idx] for key in self.data} In\u00a0[4]: Copied! <pre>import torch\nfrom torch.utils import data\n\n# Create a torch.data.Dataloader from sample data\ndataset_torch = TorchCustomDataset(train_data)\ndataloader_torch = data.DataLoader(dataset_torch, batch_size=4)\n\n# Load data into the pipeline\npipeline_torch = fe.Pipeline(dataloader_torch)\n</pre> import torch from torch.utils import data  # Create a torch.data.Dataloader from sample data dataset_torch = TorchCustomDataset(train_data) dataloader_torch = data.DataLoader(dataset_torch, batch_size=4)  # Load data into the pipeline pipeline_torch = fe.Pipeline(dataloader_torch) <p></p> <p>Next, we will see how to use one of the Fastestimator Datasets in the <code>Pipeline</code>. We will create <code>fe.dataset.NumpyDataset</code> and load it into our pipeline. As we saw in tutorial 2, <code>NumpyDataset</code> takes a dictionary with keys for the input data and ground truth labels.</p> In\u00a0[5]: Copied! <pre>from fastestimator.dataset.numpy_dataset import NumpyDataset\n\n# Create a NumpyDataset from the sample data\ndataset_fe = NumpyDataset(train_data)\n\npipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1)\n</pre> from fastestimator.dataset.numpy_dataset import NumpyDataset  # Create a NumpyDataset from the sample data dataset_fe = NumpyDataset(train_data)  pipeline_fe = fe.Pipeline(train_data=dataset_fe, batch_size=1) <p></p> <p>After loading the data or performing preprocessing tasks, you might want to inspect the data in the <code>Pipeline</code> and ensure the output of the <code>Pipeline</code> is as you expected. <code>fe.Pipeline.get_results</code> provides this feature:</p> In\u00a0[6]: Copied! <pre>pipeline_tf.get_results(num_steps=1)\n</pre> pipeline_tf.get_results(num_steps=1) Out[6]: <pre>{'x': &lt;tf.Tensor: shape=(4, 2), dtype=float64, numpy=\n array([[0.25898835, 0.23625184],\n        [0.70300216, 0.63624074],\n        [0.79369219, 0.62362004],\n        [0.93831427, 0.87692817]])&gt;,\n 'y': &lt;tf.Tensor: shape=(4, 1), dtype=float64, numpy=\n array([[0.95529881],\n        [0.41207025],\n        [0.46023815],\n        [0.18431654]])&gt;}</pre> <p></p> <p>In tutorial 3, we learned about <code>Operators</code> and their structure. They are used in FastEstimator for constructing workflow graphs. Here we will talk specifically about Numpy Operators (<code>NumpyOp</code>s) and how to use them in <code>Pipeline</code>.</p> <p><code>NumpyOp</code>s form the foundation of FastEstimator data augmentation within the <code>Pipeline</code>, and inherit from the <code>Op</code> base class. They perform preprocessing and augmentation tasks on non-Tensor data. With a list of <code>NumpyOp</code>s, even complicated preprocessing tasks can be implemented in only a few lines of code. Many of the augmentation operations in FastEstimator leverage the image augmentation library albumentations.</p> <p><code>NumpyOp</code> can be further subdivided into three main categories:</p> <ul> <li>Univariate <code>NumpyOp</code>s</li> <li>Multivariate <code>NumpyOp</code>s</li> <li>Meta <code>NumpyOp</code>s</li> </ul> <p>In addition to the pre-built offerings, we can customize the <code>NumpyOp</code> to perform our own operations on the data. By inheriting <code>fe.op.numpyop</code> we can create custom <code>NumpyOp</code>s and use them in our <code>Pipeline</code>. In this tutorial, we will learn about Univariate, Multivariate and Custom Numpy Operators. We will discuss Meta NumpyOp's an advanced tutorial.</p> <p>To demonstrate use of operators, we will first load the Fashion MNIST dataset in our Pipeline and then will define list of Numpy Operators for preprocessing data. We will then visualize the <code>Pipeline</code>s inputs and outputs.</p> In\u00a0[7]: Copied! <pre>from fastestimator.dataset.data import mnist\n\nmnist_train, mnist_eval = mnist.load_data()\n</pre> from fastestimator.dataset.data import mnist  mnist_train, mnist_eval = mnist.load_data() <p></p> <p>Univariate Numpy Operators perform the same operation for all input features. They take one or more input(s) and return an equal number of outputs, applying the same transformation to each input/output pair. For example, <code>Minmax</code> is an univariate Numpy Operator. No matter what feature it is given, it will perform:</p> <p>data = (data - min) / (max - min)</p> In\u00a0[8]: Copied! <pre>from fastestimator.op.numpyop.univariate import Minmax\n\nminmax_op = Minmax(inputs=\"x\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop.univariate import Minmax  minmax_op = Minmax(inputs=\"x\", outputs=\"x_out\") <p></p> <p>Multivariate Numpy Operators perform different operations based on the nature of the input features. For example, if you have an image with an associated mask as well as bounding boxes, rotating all three of these objects together requires the backend code to know which of the inputs is an image and which is a bounding box. Here we will demonstrate the <code>Rotate</code> Numpy Operator which will rotate images randomly by some angle in the range (-180, 180) degrees.</p> In\u00a0[9]: Copied! <pre>from fastestimator.op.numpyop.multivariate import Rotate\n\nrotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180)\n</pre> from fastestimator.op.numpyop.multivariate import Rotate  rotation_op = Rotate(image_in=\"x_out\", image_out=\"x_out\", limit=180) <p></p> <p>Let's create custom Numpy Operator that adds random noise to the input images.</p> In\u00a0[10]: Copied! <pre>from fastestimator.op.numpyop import NumpyOp\n\nclass AddRandomNoise(NumpyOp):\n    def forward(self, data, state):\n        # generate noise array with 0 mean and 0.1 standard deviation\n        noise = np.random.normal(0, 0.1, data.shape)\n        data = data + noise\n        return data\n    \nrandom_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\")\n</pre> from fastestimator.op.numpyop import NumpyOp  class AddRandomNoise(NumpyOp):     def forward(self, data, state):         # generate noise array with 0 mean and 0.1 standard deviation         noise = np.random.normal(0, 0.1, data.shape)         data = data + noise         return data      random_noise_op = AddRandomNoise(inputs=\"x_out\", outputs=\"x_out\") <p></p> <p>Now, let's add our <code>NumpyOp</code>s into the <code>Pipeline</code> and visualize the results.</p> In\u00a0[11]: Copied! <pre>pipeline = fe.Pipeline(train_data=mnist_train,\n                       eval_data=mnist_eval,\n                       ops=[minmax_op, rotation_op, random_noise_op],\n                       batch_size=3)\n\ndata = pipeline.get_results()\nimg = fe.util.ImgData(original_image=data[\"x\"], pipeline_output=data[\"x_out\"])\nfig = img.paint_figure()\n</pre> pipeline = fe.Pipeline(train_data=mnist_train,                        eval_data=mnist_eval,                        ops=[minmax_op, rotation_op, random_noise_op],                        batch_size=3)  data = pipeline.get_results() img = fe.util.ImgData(original_image=data[\"x\"], pipeline_output=data[\"x_out\"]) fig = img.paint_figure() <p></p>"}, {"location": "tutorial/beginner/t04_pipeline.html#tutorial-4-pipeline", "title": "Tutorial 4: Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about the following:</p> <ul> <li>Loading data into a <code>Pipeline</code><ul> <li>Using tf.data.Dataset</li> <li>Using torch.Dataloader</li> <li>Using FastEstimator Datasets</li> </ul> </li> <li>Getting results from a <code>Pipeline</code></li> <li>How to use Numpy Operators in a <code>Pipeline</code><ul> <li>Univariate Numpy Operators</li> <li>Multivariate Numpy Operators</li> <li>Customized Numpy Operators</li> <li>Visualizing <code>Pipeline</code> Output</li> </ul> </li> <li>Related Apphub Examples</li> </ul> <p>In deep learning, data preprocessing is a way of converting data from its raw form to a more usable or desired representation. It is one crucial step in model training as it directly impacts the ability of model to learn. In FastEstimator, the <code>Pipeline</code> API enables such preprocessing tasks in an efficient manner. The <code>Pipeline</code> manages everything from  extracting data from the disk up until it is fed into the model. <code>Pipeline</code> operations usually happen on the CPU.</p>"}, {"location": "tutorial/beginner/t04_pipeline.html#loading-data-into-a-pipeline", "title": "Loading data into a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-tfdatadataset", "title": "Using tf.data.Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-torchdatadataloader", "title": "Using torch.data.Dataloader\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-a-fastestimator-dataset", "title": "Using a FastEstimator Dataset\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#getting-results-from-a-pipeline", "title": "Getting results from a Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#using-numpy-operators-in-pipeline", "title": "Using Numpy Operators in Pipeline\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#univariate-numpyop", "title": "Univariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#multivariate-numpyop", "title": "Multivariate NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#custom-numpyop", "title": "Custom NumpyOp\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#visualizing-pipeline-outputs", "title": "Visualizing Pipeline Outputs\u00b6", "text": ""}, {"location": "tutorial/beginner/t04_pipeline.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> <li>Bert</li> <li>FGSM</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html", "title": "Tutorial 5: Model", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef my_model_tf(input_shape=(30, ), num_classes=2):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))\n    model.add(tf.keras.layers.Dense(8, activation=\"relu\"))\n    model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n    return model\n\nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\")\n</pre> import fastestimator as fe import tensorflow as tf from tensorflow.keras import layers  def my_model_tf(input_shape=(30, ), num_classes=2):     model = tf.keras.Sequential()     model.add(tf.keras.layers.Dense(32, activation=\"relu\", input_shape=input_shape))     model.add(tf.keras.layers.Dense(8, activation=\"relu\"))     model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))     return model  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=\"adam\") In\u00a0[2]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.nn.functional as fn\n\nclass my_model_torch(nn.Module):\n    def __init__(self, num_inputs=30, num_classes=2):\n        super().__init__()\n        self.layers = nn.Sequential(nn.Linear(num_inputs, 32), \n                                    nn.ReLU(inplace=True), \n                                    nn.Linear(32, 8), \n                                    nn.ReLU(inplace=True),\n                                    nn.Linear(8, num_classes))\n\n    def forward(self, x):\n        x = self.layers(x)\n        x_label = torch.softmax(x, dim=-1)\n        return x_label\n\n    \nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\")\n</pre> import torch import torch.nn as nn import torch.nn.functional as fn  class my_model_torch(nn.Module):     def __init__(self, num_inputs=30, num_classes=2):         super().__init__()         self.layers = nn.Sequential(nn.Linear(num_inputs, 32),                                      nn.ReLU(inplace=True),                                      nn.Linear(32, 8),                                      nn.ReLU(inplace=True),                                     nn.Linear(8, num_classes))      def forward(self, x):         x = self.layers(x)         x_label = torch.softmax(x, dim=-1)         return x_label       model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=\"adam\") In\u00a0[3]: Copied! <pre>from fastestimator.architecture.pytorch import LeNet\n# from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model\n\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n</pre> from fastestimator.architecture.pytorch import LeNet # from fastestimator.architecture.tensorflow import LeNet  # One can also use a TensorFlow model  model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\") In\u00a0[4]: Copied! <pre>resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\")\n</pre> resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights='imagenet'), optimizer_fn=\"adam\") In\u00a0[5]: Copied! <pre>from torchvision import models\n\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\")\n</pre> from torchvision import models  resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=True), optimizer_fn=\"adam\") In\u00a0[6]: Copied! <pre># TensorFlow \nmodel_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))\n\n# PyTorch\nmodel_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4))\n</pre> # TensorFlow  model_tf = fe.build(model_fn=my_model_tf, optimizer_fn=lambda: tf.optimizers.Adam(1e-4))  # PyTorch model_torch = fe.build(model_fn=my_model_torch, optimizer_fn=lambda x: torch.optim.Adam(params=x, lr=1e-4)) <p>If a model function returns multiple models, a list of optimizers can be provided. See the pggan apphub for an example with multiple models and optimizers.</p> <p></p> In\u00a0[7]: Copied! <pre>import os\nimport tempfile\n\nmodel_dir = tempfile.mkdtemp()\n\n# TensorFlow\nfe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")\n\n# PyTorch\nfe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\")\n</pre> import os import tempfile  model_dir = tempfile.mkdtemp()  # TensorFlow fe.backend.save_model(resnet50_tf, save_dir=model_dir, model_name= \"resnet50_tf\")  # PyTorch fe.backend.save_model(resnet50_torch, save_dir=model_dir, model_name= \"resnet50_torch\") Out[7]: <pre>'/var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpl70q0hk6/resnet50_torch.pt'</pre> In\u00a0[8]: Copied! <pre># TensorFlow\nresnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None), \n                       optimizer_fn=\"adam\", \n                       weights_path=os.path.join(model_dir, \"resnet50_tf.h5\"))\n</pre> # TensorFlow resnet50_tf = fe.build(model_fn=lambda: tf.keras.applications.ResNet50(weights=None),                         optimizer_fn=\"adam\",                         weights_path=os.path.join(model_dir, \"resnet50_tf.h5\")) In\u00a0[9]: Copied! <pre># PyTorch\nresnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False), \n                          optimizer_fn=\"adam\", \n                          weights_path=os.path.join(model_dir, \"resnet50_torch.pt\"))\n</pre> # PyTorch resnet50_torch = fe.build(model_fn=lambda: models.resnet50(pretrained=False),                            optimizer_fn=\"adam\",                            weights_path=os.path.join(model_dir, \"resnet50_torch.pt\")) <p></p> In\u00a0[10]: Copied! <pre>model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\nprint(\"Model Name: \", model.model_name)\n</pre> model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\") print(\"Model Name: \", model.model_name) <pre>Model Name:  LeNet\n</pre> <p>If a model function returns multiple models, a list of model_names can be given. See the pggan apphub for an illustration with multiple models and model names.</p> <p></p>"}, {"location": "tutorial/beginner/t05_model.html#tutorial-5-model", "title": "Tutorial 5: Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we will cover:</p> <ul> <li>Instantiating and Compiling a Model</li> <li>The Model Function<ul> <li>Custom Models</li> <li>FastEstimator Models</li> <li>Pre-Trained Models</li> </ul> </li> <li>The Optimizer Function</li> <li>Loading Model Weights</li> <li>Specifying a Model Name</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#instantiating-and-compiling-a-model", "title": "Instantiating and Compiling a model\u00b6", "text": "<p>We need to specify two things to instantiate and compile a model:</p> <ul> <li>model_fn</li> <li>optimizer_fn</li> </ul> <p>Model definitions can be implemented in Tensorflow or Pytorch and instantiated by calling <code>fe.build</code> which constructs a model instance and associates it with the specified optimizer.</p>"}, {"location": "tutorial/beginner/t05_model.html#model-function", "title": "Model Function\u00b6", "text": "<p><code>model_fn</code> should be a function/lambda function which returns either a <code>tf.keras.Model</code> or <code>torch.nn.Module</code>. FastEstimator provides several ways to specify the model architecture:</p> <ul> <li>Custom model architecture</li> <li>Importing a pre-built model architecture from FastEstimator</li> <li>Importing pre-trained models/architectures from PyTorch or TensorFlow</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#custom-model-architecture", "title": "Custom model architecture\u00b6", "text": "<p>Let's create a custom model in TensorFlow and PyTorch for demonstration.</p>"}, {"location": "tutorial/beginner/t05_model.html#tfkerasmodel", "title": "tf.keras.Model\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#torchnnmodule", "title": "torch.nn.Module\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#importing-model-architecture-from-fastestimator", "title": "Importing model architecture from FastEstimator\u00b6", "text": "<p>Below we import a PyTorch LeNet architecture from FastEstimator. See our Architectures folder for a full list of the architectures provided by FastEstimator.</p>"}, {"location": "tutorial/beginner/t05_model.html#importing-pre-trained-modelsarchitectures-from-pytorch-or-tensorflow", "title": "Importing pre-trained models/architectures from PyTorch or TensorFlow\u00b6", "text": "<p>Below we show how to define a model function using a pre-trained resnet model provided by TensorFlow and PyTorch respectively. We load the pre-trained models using a lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-tfkerasapplications", "title": "Pre-trained model from tf.keras.applications\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#pre-trained-model-from-torchvision", "title": "Pre-trained model from torchvision\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#optimizer-function", "title": "Optimizer function\u00b6", "text": "<p><code>optimizer_fn</code> can be a string or lambda function.</p>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-string", "title": "Optimizer from String\u00b6", "text": "<p>Specifying a string for the <code>optimizer_fn</code> loads the optimizer with default parameters. The optimizer strings accepted by FastEstimator are as follows:</p> <ul> <li>Adadelta: 'adadelta'</li> <li>Adagrad: 'adagrad'</li> <li>Adam: 'adam'</li> <li>Adamax: 'adamax'</li> <li>RMSprop: 'rmsprop'</li> <li>SGD: 'sgd'</li> </ul>"}, {"location": "tutorial/beginner/t05_model.html#optimizer-from-function", "title": "Optimizer from Function\u00b6", "text": "<p>To specify specific values for the optimizer learning rate or other parameters, we need to pass a lambda function to the <code>optimizer_fn</code>.</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-model-weights", "title": "Loading model weights\u00b6", "text": "<p>We often need to load the weights of a saved model. Model weights can be loaded by specifying the path of the saved weights using the <code>weights_path</code> parameter. Let's use the resnet models created earlier to showcase this.</p>"}, {"location": "tutorial/beginner/t05_model.html#saving-model-weights", "title": "Saving model weights\u00b6", "text": "<p>Here, we create a temporary directory and use FastEstimator backend to save the weights of our previously created resnet50 models:</p>"}, {"location": "tutorial/beginner/t05_model.html#loading-weights-for-tensorflow-and-pytorch-models", "title": "Loading weights for TensorFlow and PyTorch models\u00b6", "text": ""}, {"location": "tutorial/beginner/t05_model.html#specifying-a-model-name", "title": "Specifying a Model Name\u00b6", "text": "<p>The name of a model can be specified using the <code>model_name</code> parameter. The name of the model is helpful in distinguishing models when multiple are present.</p>"}, {"location": "tutorial/beginner/t05_model.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>PG-GAN</li> <li>Uncertainty Weighted Loss</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html", "title": "Tutorial 6: Network", "text": "<p>As the figure shows, models (orange) are only piece of a <code>Network</code>. It also includes other operations such as loss computation (blue) and update rules (green) that will be used during the training process.</p> <p></p> <p>A <code>Network</code> is composed of basic units called <code>TensorOps</code>. All of the building blocks inside a <code>Network</code> should derive from the <code>TensorOp</code> base class. A <code>TensorOp</code> is a kind of <code>Op</code> and therefore follows the same rules described in tutorial 3.</p> <p></p> <p>There are some common <code>TensorOp</code> classes we would like to specially mention because of their prevalence:</p> <p></p> <p></p> In\u00a0[1]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport tensorflow as tf\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return tf.reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import tensorflow as tf  class ReduceMean(TensorOp):     def forward(self, data, state):         return tf.reduce_mean(data) <p></p> In\u00a0[2]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nimport torch\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return torch.mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp import torch  class ReduceMean(TensorOp):     def forward(self, data, state):         return torch.mean(data) <p></p> In\u00a0[3]: Copied! <pre>from fastestimator.op.tensorop import TensorOp\nfrom fastestimator.backend import reduce_mean\n\nclass ReduceMean(TensorOp):\n    def forward(self, data, state):\n        return reduce_mean(data)\n</pre> from fastestimator.op.tensorop import TensorOp from fastestimator.backend import reduce_mean  class ReduceMean(TensorOp):     def forward(self, data, state):         return reduce_mean(data) <p></p>"}, {"location": "tutorial/beginner/t06_network.html#tutorial-6-network", "title": "Tutorial 6: Network\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li><code>Network</code> Scope</li> <li><code>TensorOp</code> and its Children</li> <li>How to Customize a <code>TensorOp</code><ul> <li>TensorFlow</li> <li>PyTorch</li> <li>fe.backend</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t06_network.html#network-scope", "title": "Network Scope\u00b6", "text": "<p><code>Network</code> is one of the three main FastestEstimator APIs that defines not only a neural network model but also all of the operations to be performed on it. This can include the deep-learning model itself, loss calculations, model updating rules, and any other functionality that you wish to execute within a GPU.</p> <p>Here we show two <code>Network</code> example graphs to enhance the concept:</p>"}, {"location": "tutorial/beginner/t06_network.html#tensorop-and-its-children", "title": "TensorOp and its Children\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#modelop", "title": "ModelOp\u00b6", "text": "<p>Any model instance created from <code>fe.build</code> (see tutorial 5) needs to be packaged as a <code>ModelOp</code> such that it can interact with other components inside the <code>Network</code> API. The orange blocks in the first figure are <code>ModelOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#updateop", "title": "UpdateOp\u00b6", "text": "<p>FastEstimator use <code>UpdateOp</code> to associate the model with its loss. Unlike other <code>Ops</code> that use <code>inputs</code> and <code>outputs</code> for expressing their connections, <code>UpdateOp</code> uses the arguments <code>loss</code>, and <code>model</code> instead. The green blocks in the first figure are <code>UpdateOps</code>.</p>"}, {"location": "tutorial/beginner/t06_network.html#others-loss-gradient-etc", "title": "Others (loss, gradient, etc.)\u00b6", "text": "<p>There are many ready-to-use <code>TensorOps</code> that users can directly import from <code>fe.op.tensorop</code>. Some examples include loss and gradient computation ops. For all available Ops please check out the FastEstimator API.</p>"}, {"location": "tutorial/beginner/t06_network.html#customize-a-tensorop", "title": "Customize a TensorOp\u00b6", "text": "<p>FastEstimator provides flexibility that allows users to customize their own <code>TensorOp</code>s by wrapping TensorFlow or PyTorch library calls, or by leveraging <code>fe.backend</code> API functions. Users only need to inherit the <code>TensorOp</code> class and overwrite its <code>forward</code> function.</p> <p>If you want to customize a <code>TensorOp</code> by directly leveraging API calls from TensorFlow or PyTorch, please make sure that all of the <code>TensorOp</code>s in the <code>Network</code> are backend-consistent. In other words, you cannot have <code>TensorOp</code>s built specifically for TensorFlow and PyTorch in the same <code>Network</code>. Note that the <code>ModelOp</code> backend is determined by which library the model function uses, and so must be consistent with any custom <code>TensorOp</code> that you write.</p> <p>Here we are going to demonstrate how to build a <code>TenorOp</code> that takes high dimensional inputs and returns an average scalar value.</p>"}, {"location": "tutorial/beginner/t06_network.html#example-using-tensorflow", "title": "Example Using TensorFlow\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-pytorch", "title": "Example Using PyTorch\u00b6", "text": ""}, {"location": "tutorial/beginner/t06_network.html#example-using-febackend", "title": "Example Using <code>fe.backend</code>\u00b6", "text": "<p>You don't need to worry about backend consistency if you import a FastEstimator-provided <code>TensorOp</code>, or customize your <code>TenosorOp</code> using the <code>fe.backend</code> API. FastEstimator auto-magically handles everything for you.</p>"}, {"location": "tutorial/beginner/t06_network.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>Fast Style Transfer</li> <li>DC-GAN</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html", "title": "Tutorial 7: Estimator", "text": "<p><code>Estimator</code> is the API that manages everything related to the training loop. It combines <code>Pipeline</code> and <code>Network</code> together and provides users with fine-grain control over the training loop. Before we demonstrate different ways to control the training loop let's define a template similar to tutorial 1, but this time we will use a PyTorch model.</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.architecture.pytorch import LeNet\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nimport tempfile\n\ndef get_estimator(log_steps=100, monitor_names=None, use_trace=False, max_train_steps_per_epoch=None, epochs=2):\n    # step 1\n    train_data, eval_data = mnist.load_data()\n    test_data = eval_data.split(0.5)\n    pipeline = fe.Pipeline(train_data=train_data,\n                           eval_data=eval_data,\n                           test_data=test_data,\n                           batch_size=32,\n                           ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])\n    # step 2\n    model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")\n    network = fe.Network(ops=[\n        ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),\n        CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),\n        UpdateOp(model=model, loss_name=\"ce\")\n    ])\n    # step 3\n    traces = None\n    if use_trace:\n        traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"), \n                  BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]\n    estimator = fe.Estimator(pipeline=pipeline,\n                             network=network,\n                             epochs=epochs,\n                             traces=traces,\n                             max_train_steps_per_epoch=max_train_steps_per_epoch,\n                             log_steps=log_steps,\n                             monitor_names=monitor_names)\n    return estimator\n</pre> import fastestimator as fe from fastestimator.architecture.pytorch import LeNet from fastestimator.dataset.data import mnist from fastestimator.op.numpyop.univariate import ExpandDims, Minmax from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp import tempfile  def get_estimator(log_steps=100, monitor_names=None, use_trace=False, max_train_steps_per_epoch=None, epochs=2):     # step 1     train_data, eval_data = mnist.load_data()     test_data = eval_data.split(0.5)     pipeline = fe.Pipeline(train_data=train_data,                            eval_data=eval_data,                            test_data=test_data,                            batch_size=32,                            ops=[ExpandDims(inputs=\"x\", outputs=\"x\", axis=0), Minmax(inputs=\"x\", outputs=\"x\")])     # step 2     model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\", model_name=\"LeNet\")     network = fe.Network(ops=[         ModelOp(model=model, inputs=\"x\", outputs=\"y_pred\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\"),         CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce1\"),         UpdateOp(model=model, loss_name=\"ce\")     ])     # step 3     traces = None     if use_trace:         traces = [Accuracy(true_key=\"y\", pred_key=\"y_pred\"),                    BestModelSaver(model=model, save_dir=tempfile.mkdtemp(), metric=\"accuracy\", save_best_mode=\"max\")]     estimator = fe.Estimator(pipeline=pipeline,                              network=network,                              epochs=epochs,                              traces=traces,                              max_train_steps_per_epoch=max_train_steps_per_epoch,                              log_steps=log_steps,                              monitor_names=monitor_names)     return estimator <p>Let's train our model using the default <code>Estimator</code> arguments:</p> In\u00a0[2]: Copied! <pre>est = get_estimator()\nest.fit()\n</pre> est = get_estimator() est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.2985432; \nFastEstimator-Train: step: 100; ce: 0.33677763; steps/sec: 43.98; \nFastEstimator-Train: step: 200; ce: 0.3549296; steps/sec: 45.97; \nFastEstimator-Train: step: 300; ce: 0.17926084; steps/sec: 46.62; \nFastEstimator-Train: step: 400; ce: 0.32462734; steps/sec: 46.91; \nFastEstimator-Train: step: 500; ce: 0.05164891; steps/sec: 47.18; \nFastEstimator-Train: step: 600; ce: 0.0906372; steps/sec: 45.5; \nFastEstimator-Train: step: 700; ce: 0.46759754; steps/sec: 45.0; \nFastEstimator-Train: step: 800; ce: 0.025921348; steps/sec: 43.85; \nFastEstimator-Train: step: 900; ce: 0.21584965; steps/sec: 44.17; \nFastEstimator-Train: step: 1000; ce: 0.1303818; steps/sec: 44.68; \nFastEstimator-Train: step: 1100; ce: 0.256935; steps/sec: 43.92; \nFastEstimator-Train: step: 1200; ce: 0.052581083; steps/sec: 43.21; \nFastEstimator-Train: step: 1300; ce: 0.030862458; steps/sec: 42.97; \nFastEstimator-Train: step: 1400; ce: 0.115828656; steps/sec: 42.55; \nFastEstimator-Train: step: 1500; ce: 0.033370342; steps/sec: 43.89; \nFastEstimator-Train: step: 1600; ce: 0.0928934; steps/sec: 43.56; \nFastEstimator-Train: step: 1700; ce: 0.05145497; steps/sec: 43.06; \nFastEstimator-Train: step: 1800; ce: 0.14278823; steps/sec: 43.23; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 42.33 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.057005133; \nFastEstimator-Train: step: 1900; ce: 0.08283445; steps/sec: 39.21; \nFastEstimator-Train: step: 2000; ce: 0.031674776; steps/sec: 46.4; \nFastEstimator-Train: step: 2100; ce: 0.022434138; steps/sec: 46.2; \nFastEstimator-Train: step: 2200; ce: 0.0041575576; steps/sec: 46.57; \nFastEstimator-Train: step: 2300; ce: 0.028007038; steps/sec: 46.55; \nFastEstimator-Train: step: 2400; ce: 0.11569328; steps/sec: 46.18; \nFastEstimator-Train: step: 2500; ce: 0.1477213; steps/sec: 46.04; \nFastEstimator-Train: step: 2600; ce: 0.21895751; steps/sec: 45.41; \nFastEstimator-Train: step: 2700; ce: 0.008701714; steps/sec: 44.15; \nFastEstimator-Train: step: 2800; ce: 0.006247335; steps/sec: 42.0; \nFastEstimator-Train: step: 2900; ce: 0.0016122407; steps/sec: 42.0; \nFastEstimator-Train: step: 3000; ce: 0.005287632; steps/sec: 41.4; \nFastEstimator-Train: step: 3100; ce: 0.013425731; steps/sec: 41.41; \nFastEstimator-Train: step: 3200; ce: 0.00874802; steps/sec: 39.84; \nFastEstimator-Train: step: 3300; ce: 0.025417497; steps/sec: 40.25; \nFastEstimator-Train: step: 3400; ce: 0.08027805; steps/sec: 39.33; \nFastEstimator-Train: step: 3500; ce: 0.020149795; steps/sec: 39.69; \nFastEstimator-Train: step: 3600; ce: 0.010977306; steps/sec: 39.68; \nFastEstimator-Train: step: 3700; ce: 0.075040415; steps/sec: 39.68; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 44.2 sec; \nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.04138615; \nFastEstimator-Finish: step: 3750; total_time: 89.69 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[3]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4)\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3073506; \nFastEstimator-Train: step: 100; ce: 0.5364497; steps/sec: 38.56; \nFastEstimator-Train: step: 200; ce: 0.17832895; steps/sec: 42.4; \nFastEstimator-Train: step: 300; ce: 0.2198829; steps/sec: 41.62; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 7.42 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.15399536; \nFastEstimator-Train: step: 400; ce: 0.13039914; steps/sec: 38.44; \nFastEstimator-Train: step: 500; ce: 0.120313495; steps/sec: 42.95; \nFastEstimator-Train: step: 600; ce: 0.14686579; steps/sec: 43.12; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 7.25 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.10223439; \nFastEstimator-Train: step: 700; ce: 0.17189693; steps/sec: 37.89; \nFastEstimator-Train: step: 800; ce: 0.025620187; steps/sec: 41.49; \nFastEstimator-Train: step: 900; ce: 0.017038438; steps/sec: 41.58; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 7.46 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.06282204; \nFastEstimator-Train: step: 1000; ce: 0.038011674; steps/sec: 37.24; \nFastEstimator-Train: step: 1100; ce: 0.03683513; steps/sec: 42.89; \nFastEstimator-Train: step: 1200; ce: 0.023527239; steps/sec: 41.78; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 7.41 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.079378836; \nFastEstimator-Finish: step: 1200; total_time: 36.24 sec; LeNet_lr: 0.001; \n</pre> <p></p> <p></p> In\u00a0[4]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=0)\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=0) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 0; \nFastEstimator-Eval: step: 300; epoch: 1; ce: 0.15603326; \nFastEstimator-Eval: step: 600; epoch: 2; ce: 0.09531953; \nFastEstimator-Eval: step: 900; epoch: 3; ce: 0.06877253; \nFastEstimator-Eval: step: 1200; epoch: 4; ce: 0.05356282; \nFastEstimator-Finish: step: 1200; total_time: 36.81 sec; LeNet_lr: 0.001; \n</pre> <p></p> In\u00a0[5]: Copied! <pre>est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\")\nest.fit()\n</pre> est = get_estimator(max_train_steps_per_epoch=300, epochs=4, log_steps=150, monitor_names=\"ce1\") est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 150; \nFastEstimator-Train: step: 1; ce1: 2.30421; ce: 2.30421; \nFastEstimator-Train: step: 150; ce1: 0.35948867; ce: 0.35948867; steps/sec: 38.23; \nFastEstimator-Train: step: 300; ce1: 0.16791707; ce: 0.16791707; steps/sec: 40.98; \nFastEstimator-Train: step: 300; epoch: 1; epoch_time: 7.64 sec; \nFastEstimator-Eval: step: 300; epoch: 1; ce1: 0.2302698; ce: 0.2302698; \nFastEstimator-Train: step: 450; ce1: 0.14853987; ce: 0.14853987; steps/sec: 38.23; \nFastEstimator-Train: step: 600; ce1: 0.49784163; ce: 0.49784163; steps/sec: 40.68; \nFastEstimator-Train: step: 600; epoch: 2; epoch_time: 7.61 sec; \nFastEstimator-Eval: step: 600; epoch: 2; ce1: 0.12643811; ce: 0.12643811; \nFastEstimator-Train: step: 750; ce1: 0.18601; ce: 0.18601; steps/sec: 37.24; \nFastEstimator-Train: step: 900; ce1: 0.12327108; ce: 0.12327108; steps/sec: 40.5; \nFastEstimator-Train: step: 900; epoch: 3; epoch_time: 7.73 sec; \nFastEstimator-Eval: step: 900; epoch: 3; ce1: 0.069144465; ce: 0.069144465; \nFastEstimator-Train: step: 1050; ce1: 0.1580712; ce: 0.1580712; steps/sec: 37.91; \nFastEstimator-Train: step: 1200; ce1: 0.20800333; ce: 0.20800333; steps/sec: 40.61; \nFastEstimator-Train: step: 1200; epoch: 4; epoch_time: 7.65 sec; \nFastEstimator-Eval: step: 1200; epoch: 4; ce1: 0.06323946; ce: 0.06323946; \nFastEstimator-Finish: step: 1200; total_time: 37.49 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see, both <code>ce</code> and <code>ce1</code> showed up in the log above. Unsurprisingly, their values are identical because because they have the same inputs and forward function.</p> <p></p> <p></p> <p></p> In\u00a0[6]: Copied! <pre>class Trace:\n    def __init__(self, inputs=None, outputs=None, mode=None):\n        self.inputs = inputs\n        self.outputs = outputs\n        self.mode = mode\n\n    def on_begin(self, data):\n\"\"\"Runs once at the beginning of training\"\"\"\n\n    def on_epoch_begin(self, data):\n\"\"\"Runs at the beginning of each epoch\"\"\"\n\n    def on_batch_begin(self, data):\n\"\"\"Runs at the beginning of each batch\"\"\"\n\n    def on_batch_end(self, data):\n\"\"\"Runs at the end of each batch\"\"\"\n\n    def on_epoch_end(self, data):\n\"\"\"Runs at the end of each epoch\"\"\"\n\n    def on_end(self, data):\n\"\"\"Runs once at the end training\"\"\"\n</pre> class Trace:     def __init__(self, inputs=None, outputs=None, mode=None):         self.inputs = inputs         self.outputs = outputs         self.mode = mode      def on_begin(self, data):         \"\"\"Runs once at the beginning of training\"\"\"      def on_epoch_begin(self, data):         \"\"\"Runs at the beginning of each epoch\"\"\"      def on_batch_begin(self, data):         \"\"\"Runs at the beginning of each batch\"\"\"      def on_batch_end(self, data):         \"\"\"Runs at the end of each batch\"\"\"      def on_epoch_end(self, data):         \"\"\"Runs at the end of each epoch\"\"\"      def on_end(self, data):         \"\"\"Runs once at the end training\"\"\" <p>Given the structure, users can customize their own functions at different stages and insert them into the training loop. We will leave the customization of <code>Traces</code> to the advanced tutorial. For now, let's use some pre-built <code>Traces</code> from FastEstimator.</p> <p>During the training loop in our earlier example, we want 2 things to happen:</p> <ol> <li>Save the model weights if the evaluation loss is the best we have seen so far</li> <li>Calculate the model accuracy during evaluation</li> </ol> <p></p> In\u00a0[7]: Copied! <pre>from fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\n\nest = get_estimator(use_trace=True)\nest.fit()\n</pre> from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy  est = get_estimator(use_trace=True) est.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.317368; \nFastEstimator-Train: step: 100; ce: 0.32270017; steps/sec: 38.37; \nFastEstimator-Train: step: 200; ce: 0.4691573; steps/sec: 41.07; \nFastEstimator-Train: step: 300; ce: 0.16797979; steps/sec: 41.48; \nFastEstimator-Train: step: 400; ce: 0.22231343; steps/sec: 40.29; \nFastEstimator-Train: step: 500; ce: 0.15864769; steps/sec: 40.23; \nFastEstimator-Train: step: 600; ce: 0.21094382; steps/sec: 40.3; \nFastEstimator-Train: step: 700; ce: 0.2174505; steps/sec: 39.09; \nFastEstimator-Train: step: 800; ce: 0.1638605; steps/sec: 37.76; \nFastEstimator-Train: step: 900; ce: 0.10876638; steps/sec: 38.04; \nFastEstimator-Train: step: 1000; ce: 0.045762353; steps/sec: 37.84; \nFastEstimator-Train: step: 1100; ce: 0.1986717; steps/sec: 37.9; \nFastEstimator-Train: step: 1200; ce: 0.019097174; steps/sec: 38.52; \nFastEstimator-Train: step: 1300; ce: 0.014496669; steps/sec: 38.07; \nFastEstimator-Train: step: 1400; ce: 0.12824036; steps/sec: 37.98; \nFastEstimator-Train: step: 1500; ce: 0.12543677; steps/sec: 37.89; \nFastEstimator-Train: step: 1600; ce: 0.054099947; steps/sec: 38.18; \nFastEstimator-Train: step: 1700; ce: 0.03653385; steps/sec: 38.03; \nFastEstimator-Train: step: 1800; ce: 0.021161698; steps/sec: 38.84; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 48.39 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpe3yqgszs/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.048291177; accuracy: 0.9846; since_best_accuracy: 0; max_accuracy: 0.9846; \nFastEstimator-Train: step: 1900; ce: 0.05266206; steps/sec: 35.05; \nFastEstimator-Train: step: 2000; ce: 0.010248414; steps/sec: 39.34; \nFastEstimator-Train: step: 2100; ce: 0.100841954; steps/sec: 40.43; \nFastEstimator-Train: step: 2200; ce: 0.099233195; steps/sec: 40.17; \nFastEstimator-Train: step: 2300; ce: 0.014007135; steps/sec: 39.87; \nFastEstimator-Train: step: 2400; ce: 0.100575976; steps/sec: 40.11; \nFastEstimator-Train: step: 2500; ce: 0.014702196; steps/sec: 39.65; \nFastEstimator-Train: step: 2600; ce: 0.017802792; steps/sec: 38.99; \nFastEstimator-Train: step: 2700; ce: 0.07476275; steps/sec: 39.37; \nFastEstimator-Train: step: 2800; ce: 0.0125279; steps/sec: 39.71; \nFastEstimator-Train: step: 2900; ce: 0.02689986; steps/sec: 39.72; \nFastEstimator-Train: step: 3000; ce: 0.00028639697; steps/sec: 38.95; \nFastEstimator-Train: step: 3100; ce: 0.02897156; steps/sec: 37.94; \nFastEstimator-Train: step: 3200; ce: 0.13989474; steps/sec: 38.29; \nFastEstimator-Train: step: 3300; ce: 0.0010959036; steps/sec: 38.47; \nFastEstimator-Train: step: 3400; ce: 0.014437494; steps/sec: 38.27; \nFastEstimator-Train: step: 3500; ce: 0.13830313; steps/sec: 38.12; \nFastEstimator-Train: step: 3600; ce: 0.0012470288; steps/sec: 38.18; \nFastEstimator-Train: step: 3700; ce: 0.004030655; steps/sec: 38.39; \nFastEstimator-Train: step: 3750; epoch: 2; epoch_time: 48.25 sec; \nFastEstimator-BestModelSaver: Saved model to /var/folders/lx/drkxftt117gblvgsp1p39rlc0000gn/T/tmpe3yqgszs/LeNet_best_accuracy.pt\nFastEstimator-Eval: step: 3750; epoch: 2; ce: 0.046231214; accuracy: 0.9854; since_best_accuracy: 0; max_accuracy: 0.9854; \nFastEstimator-Finish: step: 3750; total_time: 100.12 sec; LeNet_lr: 0.001; \n</pre> <p>As we can see from the log, the model is saved in a predefined location and the accuracy is displayed during evaluation.</p> <p></p> In\u00a0[8]: Copied! <pre>est.test()\n</pre> est.test() <pre>FastEstimator-Test: step: 3750; epoch: 2; accuracy: 0.9844; \n</pre> <p>This will feed all of your test dataset through the <code>Pipeline</code> and <code>Network</code>, and finally execute the traces (in our case, compute accuracy) just like during the training.</p> <p></p>"}, {"location": "tutorial/beginner/t07_estimator.html#tutorial-7-estimator", "title": "Tutorial 7: Estimator\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial, we will talk about:</p> <ul> <li>Estimator API<ul> <li>Reducing the number of training steps per epoch</li> <li>Reducing the number of evaluation steps per epoch</li> <li>Changing logging behavior</li> <li>Monitoring intermediate results during training</li> </ul> </li> <li>Trace<ul> <li>Concept</li> <li>Structure</li> <li>Usage</li> </ul> </li> <li>Model Testing</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t07_estimator.html#estimator-api", "title": "Estimator API\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-training-steps-per-epoch", "title": "Reduce the number of training steps per epoch\u00b6", "text": "<p>In general, one epoch of training means that every element in the training dataset will be visited exactly one time. If evaluation data is available, evaluation happens after every epoch by default. Consider the following two scenarios:</p> <ul> <li>The training dataset is very large such that evaluation needs to happen multiple times during one epoch.</li> <li>Different training datasets are being used for different epochs, but the number of training steps should be consistent between each epoch.</li> </ul> <p>One easy solution to the above scenarios is to limit the number of training steps per epoch. For example, if we want to train for only 300 steps per epoch, with training lasting for 4 epochs (1200 steps total), we would do the following:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#reduce-the-number-of-evaluation-steps-per-epoch", "title": "Reduce the number of evaluation steps per epoch\u00b6", "text": "<p>One may need to reduce the number of evaluation steps for debugging purpose. This can be easily done by setting the <code>max_eval_steps_per_epoch</code> argument in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#change-logging-behavior", "title": "Change logging behavior\u00b6", "text": "<p>When the number of training epochs is large, the log can become verbose. You can change the logging behavior by choosing one of following options:</p> <ul> <li>set <code>log_steps</code> to <code>None</code> if you do not want to see any training logs printed.</li> <li>set <code>log_steps</code> to 0 if you only wish to see the evaluation logs.</li> <li>set <code>log_steps</code> to some integer 'x' if you want training logs to be printed every 'x' steps.</li> </ul> <p>Let's set the <code>log_steps</code> to 0:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#monitor-intermediate-results", "title": "Monitor intermediate results\u00b6", "text": "<p>You might have noticed that in our example <code>Network</code> there is an op: <code>CrossEntropy(inputs=(\"y_pred\", \"y\") outputs=\"ce1\")</code>. However, the <code>ce1</code> never shows up in the training log above. This is because FastEstimator identifies and filters out unused variables to reduce unnecessary communication between the GPU and CPU. On the contrary, <code>ce</code> shows up in the log because by default we log all loss values that are used to update models.</p> <p>But what if we want to see the value of <code>ce1</code> throughout training?</p> <p>Easy: just add <code>ce1</code> to <code>monitor_names</code> in <code>Estimator</code>.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#trace", "title": "Trace\u00b6", "text": ""}, {"location": "tutorial/beginner/t07_estimator.html#concept", "title": "Concept\u00b6", "text": "<p>Now you might be thinking: 'changing logging behavior and monitoring extra keys is cool, but where is the fine-grained access to the training loop?'</p> <p>The answer is <code>Trace</code>. <code>Trace</code> is a module that can offer you access to different training stages and allow you \"do stuff\" with them. Here are some examples of what a <code>Trace</code> can do:</p> <ul> <li>print any training data at any training step</li> <li>write results to a file during training</li> <li>change learning rate based on some loss conditions</li> <li>calculate any metrics</li> <li>order you a pizza after training ends</li> <li>...</li> </ul> <p>So what are the different training stages? They are:</p> <ul> <li>Beginning of training</li> <li>Beginning of epoch</li> <li>Beginning of batch</li> <li>End of batch</li> <li>End of epoch</li> <li>End of training</li> </ul> <p></p> <p>As we can see from the illustration above, the training process is essentially a nested combination of batch loops and epoch loops. Over the course of training, <code>Trace</code> places 6 different \"road blocks\" for you to leverage.</p>"}, {"location": "tutorial/beginner/t07_estimator.html#structure", "title": "Structure\u00b6", "text": "<p>If you are familiar with Keras, you will notice that the structure of <code>Trace</code> is very similar to the <code>Callback</code> in keras.  Despite the structural similarity, <code>Trace</code> gives you a lot more flexibility which we will talk about in depth in advanced tutorial 4. Implementation-wise, <code>Trace</code> is a python class with the following structure:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#model-testing", "title": "Model Testing\u00b6", "text": "<p>Sometimes you have a separate testing dataset other than training and evaluation data. If you want to evalate the model metrics on test data, you can simply call:</p>"}, {"location": "tutorial/beginner/t07_estimator.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>UNet</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html", "title": "Tutorial 8: Mode", "text": "In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=2,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=2,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]"}, {"location": "tutorial/beginner/t08_mode.html#tutorial-8-mode", "title": "Tutorial 8: Mode\u00b6", "text": ""}, {"location": "tutorial/beginner/t08_mode.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Modes</li> <li>When Modes are Activated</li> <li>How to Set Modes</li> <li>A Code Example</li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#modes", "title": "Modes\u00b6", "text": "<p>The development cycle of a deep learning application can usually be broken into 4 phases: training, evaluation, testing, and inference. FastEstimator provides 4 corresponding modes: <code>train</code>, <code>eval</code>, <code>test</code>, and <code>infer</code> that allow users to manage each phase independently. Users have the flexibility to construct the <code>Network</code> and <code>Pipeline</code> in different ways for each of those modes. Only a single mode can ever be active at a time, and for each given mode the corresponding graph topology will be computed and executed.</p>"}, {"location": "tutorial/beginner/t08_mode.html#when-modes-are-activated", "title": "When Modes are Activated\u00b6", "text": "<ul> <li>train: <code>estimator.fit()</code> being called, during training cycle</li> <li>eval: <code>estimator.fit()</code> being called, during evaluation cycle</li> <li>test: <code>estimator.test()</code> being called</li> <li>infer: <code>pipeline.transform(mode=\"infer\")</code> or <code>network.transform(mode=\"infer\")</code> being called (inference will be covered in tutorial 9)</li> </ul>"}, {"location": "tutorial/beginner/t08_mode.html#how-to-set-modes", "title": "How to Set Modes\u00b6", "text": "<p>From the previous tutorials we already know that <code>Ops</code> define the workflow of <code>Networks</code> and <code>Pipelines</code>, whereas <code>Traces</code> control the training process. All <code>Ops</code> and <code>Traces</code> can be specified to run in one or more modes. Here are all 5 ways to set the modes:</p> <ol> <li><p>Setting a single mode Specify the desired mode as string. Ex: Op(mode=\"train\")</p> </li> <li><p>Setting multiple modes Put all desired modes in a tuple or list as an argument. Ex: Trace(mode=[\"train\", \"test\"]) </p> </li> <li><p>Setting an exception mode Prefix a \"!\" on a mode, and then the object will execute during all modes that are NOT the specified one. Ex: Op(mode=\"!train\") </p> </li> <li><p>Setting all modes Set the mode argument equal to None. Ex: Trace(mode=None) </p> </li> <li><p>Using the default mode setting Don't specify anything in mode argument. Different <code>Ops</code> and <code>Traces</code> have different default mode settings. Ex: <code>UpdateOp</code> -&gt; default mode: train  Ex: <code>Accuracy</code> trace -&gt; default mode: eval, test</p> </li> </ol>"}, {"location": "tutorial/beginner/t08_mode.html#code-example", "title": "Code Example\u00b6", "text": "<p>Let's see come example code and visualize the topology of the corresponding execution graphs for each mode:</p>"}, {"location": "tutorial/beginner/t08_mode.html#train-mode", "title": "Train Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"train\" mode. It has a complete data pipeline including the <code>CoarseDropout</code> data augmentation Op. The data source of the pipeline is \"train_data\". The <code>Accuracy</code> Trace will not exist in this mode because the default mode of that trace is \"eval\" and \"test\".</p>"}, {"location": "tutorial/beginner/t08_mode.html#eval-mode", "title": "Eval Mode\u00b6", "text": "<p>The following figure shows the execution flow for the \"eval\" mode. The data augmentation block is missing and the pipeline data source is \"eval_data\". The <code>Accuracy</code> block exist in this mode because of its default trace setting.</p>"}, {"location": "tutorial/beginner/t08_mode.html#test-mode", "title": "Test Mode\u00b6", "text": "<p>Everything in the \"test\" mode is the same as the \"eval\" mode, except that the data source of pipeline has switched to \"test_data\":</p>"}, {"location": "tutorial/beginner/t08_mode.html#infer-mode", "title": "Infer Mode\u00b6", "text": "<p>\"Infer\" mode only has the minimum operations that model inference requires. The data source is not defined yet because input data will not be passed until the inference function is invoked. See tutorial 9 for more details.</p>"}, {"location": "tutorial/beginner/t08_mode.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>CIFAR10</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html", "title": "Tutorial 9: Inference", "text": "<p>Running inference means using a trained deep learning model to get a prediction from some input data. Users can use <code>pipeline.transform</code> and <code>network.transform</code> to feed the data forward and get the computed result in any operation mode. Here we are going to use an end-to-end example (the same example code from tutorial 8) on MNIST image classification to demonstrate how to run inference.</p> <p>We first train a deep leaning model with the following code:</p> In\u00a0[1]: Copied! <pre>import fastestimator as fe\nfrom fastestimator.dataset.data import mnist\nfrom fastestimator.schedule import cosine_decay\nfrom fastestimator.trace.adapt import LRScheduler\nfrom fastestimator.trace.io import BestModelSaver\nfrom fastestimator.trace.metric import Accuracy\nfrom fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout\nfrom fastestimator.op.tensorop.loss import CrossEntropy\nfrom fastestimator.op.tensorop.model import ModelOp, UpdateOp\nfrom fastestimator.architecture.tensorflow import LeNet\n\n\ntrain_data, eval_data = mnist.load_data()\ntest_data = eval_data.split(0.5)\nmodel = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")\n\npipeline = fe.Pipeline(train_data=train_data,\n                       eval_data=eval_data,\n                       test_data=test_data,\n                       batch_size=32,\n                       ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None\n                            Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),  \n                            CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])\n\nnetwork = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None\n                          CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),\n                          UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])\n\nestimator = fe.Estimator(pipeline=pipeline,\n                         network=network,\n                         epochs=1,\n                         traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test]\nestimator.fit()\n</pre> import fastestimator as fe from fastestimator.dataset.data import mnist from fastestimator.schedule import cosine_decay from fastestimator.trace.adapt import LRScheduler from fastestimator.trace.io import BestModelSaver from fastestimator.trace.metric import Accuracy from fastestimator.op.numpyop.univariate import ExpandDims, Minmax, CoarseDropout from fastestimator.op.tensorop.loss import CrossEntropy from fastestimator.op.tensorop.model import ModelOp, UpdateOp from fastestimator.architecture.tensorflow import LeNet   train_data, eval_data = mnist.load_data() test_data = eval_data.split(0.5) model = fe.build(model_fn=LeNet, optimizer_fn=\"adam\")  pipeline = fe.Pipeline(train_data=train_data,                        eval_data=eval_data,                        test_data=test_data,                        batch_size=32,                        ops=[ExpandDims(inputs=\"x\", outputs=\"x\"), #default mode=None                             Minmax(inputs=\"x\", outputs=\"x_out\", mode=None),                               CoarseDropout(inputs=\"x_out\", outputs=\"x_out\", mode=\"train\")])  network = fe.Network(ops=[ModelOp(model=model, inputs=\"x_out\", outputs=\"y_pred\"), #default mode=None                           CrossEntropy(inputs=(\"y_pred\", \"y\"), outputs=\"ce\", mode=\"!infer\"),                           UpdateOp(model=model, loss_name=\"ce\", mode=\"train\")])  estimator = fe.Estimator(pipeline=pipeline,                          network=network,                          epochs=1,                          traces=Accuracy(true_key=\"y\", pred_key=\"y_pred\")) # default mode=[eval, test] estimator.fit() <pre>    ______           __  ______     __  _                 __            \n   / ____/___ ______/ /_/ ____/____/ /_(_)___ ___  ____ _/ /_____  _____\n  / /_  / __ `/ ___/ __/ __/ / ___/ __/ / __ `__ \\/ __ `/ __/ __ \\/ ___/\n / __/ / /_/ (__  ) /_/ /___(__  ) /_/ / / / / / / /_/ / /_/ /_/ / /    \n/_/    \\__,_/____/\\__/_____/____/\\__/_/_/ /_/ /_/\\__,_/\\__/\\____/_/     \n                                                                        \n\nFastEstimator-Warn: No ModelSaver Trace detected. Models will not be saved.\nFastEstimator-Start: step: 1; num_device: 0; logging_interval: 100; \nFastEstimator-Train: step: 1; ce: 2.3035316; \nFastEstimator-Train: step: 100; ce: 1.6618028; steps/sec: 146.74; \nFastEstimator-Train: step: 200; ce: 1.4808083; steps/sec: 151.68; \nFastEstimator-Train: step: 300; ce: 1.0872928; steps/sec: 150.25; \nFastEstimator-Train: step: 400; ce: 1.1683241; steps/sec: 144.47; \nFastEstimator-Train: step: 500; ce: 0.63509166; steps/sec: 142.96; \nFastEstimator-Train: step: 600; ce: 0.84414047; steps/sec: 137.78; \nFastEstimator-Train: step: 700; ce: 0.90303344; steps/sec: 141.27; \nFastEstimator-Train: step: 800; ce: 0.6876491; steps/sec: 130.39; \nFastEstimator-Train: step: 900; ce: 0.7615918; steps/sec: 134.94; \nFastEstimator-Train: step: 1000; ce: 0.7081696; steps/sec: 134.51; \nFastEstimator-Train: step: 1100; ce: 1.054371; steps/sec: 129.01; \nFastEstimator-Train: step: 1200; ce: 0.65663564; steps/sec: 125.8; \nFastEstimator-Train: step: 1300; ce: 0.80188024; steps/sec: 119.49; \nFastEstimator-Train: step: 1400; ce: 1.1256162; steps/sec: 115.14; \nFastEstimator-Train: step: 1500; ce: 1.4453554; steps/sec: 112.19; \nFastEstimator-Train: step: 1600; ce: 0.52970994; steps/sec: 106.5; \nFastEstimator-Train: step: 1700; ce: 0.48612577; steps/sec: 103.63; \nFastEstimator-Train: step: 1800; ce: 1.1535486; steps/sec: 106.13; \nFastEstimator-Train: step: 1875; epoch: 1; epoch_time: 17.0 sec; \nFastEstimator-Eval: step: 1875; epoch: 1; ce: 0.17459252; accuracy: 0.948; \nFastEstimator-Finish: step: 1875; total_time: 17.72 sec; model_lr: 0.001; \n</pre> <p>Let's create a customized print function to showcase our inferencing easier:</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport tensorflow as tf\n\ndef print_dict_but_value(data):\n    for key, value in data.items():\n        if isinstance(value, np.ndarray):\n            print(\"{}: ndarray with shape {}\".format(key, value.shape))\n        \n        elif isinstance(value, tf.Tensor):\n            print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))\n        \n        else:\n            print(\"{}: {}\".format(key, value))\n</pre> import numpy as np import tensorflow as tf  def print_dict_but_value(data):     for key, value in data.items():         if isinstance(value, np.ndarray):             print(\"{}: ndarray with shape {}\".format(key, value.shape))                  elif isinstance(value, tf.Tensor):             print(\"{}: tf.Tensor with shape {}\".format(key, value.shape))                  else:             print(\"{}: {}\".format(key, value)) <p>The following figure shows the complete execution graph (consisting <code>Pipeline</code> and <code>Network</code>) for the \"infer\" mode:</p> <p></p> <p>Our goal is to provide an input image \"x\" and get the prediction result \"y_pred\".</p> <p></p> In\u00a0[3]: Copied! <pre>import copy \n\ninfer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])}\nprint_dict_but_value(infer_data)\n</pre> import copy   infer_data = {\"x\": copy.deepcopy(eval_data[0][\"x\"])} print_dict_but_value(infer_data) <pre>x: ndarray with shape (28, 28)\n</pre> In\u00a0[4]: Copied! <pre>infer_data = pipeline.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = pipeline.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: ndarray with shape (1, 28, 28, 1)\nx_out: ndarray with shape (1, 28, 28, 1)\n</pre> <p></p> In\u00a0[5]: Copied! <pre>infer_data = network.transform(infer_data, mode=\"infer\")\nprint_dict_but_value(infer_data)\n</pre> infer_data = network.transform(infer_data, mode=\"infer\") print_dict_but_value(infer_data) <pre>x: tf.Tensor with shape (1, 28, 28, 1)\nx_out: tf.Tensor with shape (1, 28, 28, 1)\ny_pred: tf.Tensor with shape (1, 10)\n</pre> <p>Now we can visualize the input image and compare with its prediction class.</p> In\u00a0[6]: Copied! <pre>print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"])))\nimg = fe.util.ImgData(x=infer_data[\"x\"])\nfig = img.paint_figure()\n</pre> print(\"Predicted class is {}\".format(np.argmax(infer_data[\"y_pred\"]))) img = fe.util.ImgData(x=infer_data[\"x\"]) fig = img.paint_figure() <pre>Predicted class is 7\n</pre> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#tutorial-9-inference", "title": "Tutorial 9: Inference\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#overview", "title": "Overview\u00b6", "text": "<p>In this tutorial we are going to cover:</p> <ul> <li>Running inference with the transform method<ul> <li>Pipeline.transform</li> <li>Network.transform</li> </ul> </li> <li>Related Apphub Examples</li> </ul>"}, {"location": "tutorial/beginner/t09_inference.html#running-inference-with-transform-method", "title": "Running inference with transform method\u00b6", "text": ""}, {"location": "tutorial/beginner/t09_inference.html#pipelinetransform", "title": "Pipeline.transform\u00b6", "text": "<p>The <code>Pipeline</code> object has a <code>transform</code> method that runs the pipeline graph (\"x\" to \"x_out\") when inference data (a dictionary of keys and values like {\"x\":image}), is inserted. The returned output will be the dictionary of computed results after applying all <code>Pipeline</code> Ops, where the dictionary values will be Numpy arrays.</p> <p></p> <p>Here we take eval_data's first image, package it into a dictionary, and then call <code>pipeline.transform</code>:</p>"}, {"location": "tutorial/beginner/t09_inference.html#networktransform", "title": "Network.transform\u00b6", "text": "<p>We then use the network object to call the <code>transform</code> method that runs the network graph (\"x_out\" to \"y_pred\"). Much like with <code>pipeline.transform</code>, it will return it's Op outputs, though this time in the form of a dictionary of Tensors. The data type of the returned values depends on the backend of the network. It is <code>tf.Tensor</code> when using the TensorFlow backend and <code>torch.Tensor</code> with PyTorch. Please check out tutorial 6 for more details about <code>Network</code> backends).</p> <p></p>"}, {"location": "tutorial/beginner/t09_inference.html#apphub-examples", "title": "Apphub Examples\u00b6", "text": "<p>You can find some practical examples of the concepts described here in the following FastEstimator Apphubs:</p> <ul> <li>MNIST</li> <li>IMDB</li> </ul>"}, {"location": "tutorial/beginner/t10_cli.html", "title": "Tutorial 10: How to use FastEstimator Command Line Interface (CLI)", "text": ""}, {"location": "tutorial/beginner/t10_cli.html#overview", "title": "Overview", "text": "<p>FastEstimator comes with a set of CLI commands that can help users train and test their models quickly. In this tutorial, we will go through the CLI usage and the arguments these CLI commands take. This tutorial is divided into the following sections:</p> <ul> <li>How Does the CLI Work</li> <li>CLI Usage</li> <li>Sending Input Args to <code>get_estimator</code><ul> <li>Using --arg</li> <li>Using a JSON file</li> </ul> </li> </ul> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#how_does_the_cli_work", "title": "How Does the CLI Work", "text": "<p>Given a python file, the FastEstimator CLI looks for a <code>get_estimator</code> function to get the estimator definition. It then calls either the <code>fit()</code> or <code>test()</code> functions on the returned estimator instance to train or test the model.</p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#cli_usage", "title": "CLI Usage", "text": "<p>In this section we will show the actual commands that we can use to train and test our models. We will use mnist_tf.py for illustration.</p> <p>To call <code>estimator.fit()</code> and start the training on terminal:</p> <pre><code>$ fastestimator train mnist_tf.py\n</code></pre> <p>To call <code>estimator.test()</code> and start testing on terminal:</p> <pre><code>$ fastestimator test mnist_tf.py\n</code></pre> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#sending_input_args_to_get_estimator", "title": "Sending Input Args to <code>get_estimator</code>", "text": "<p>We can also pass arguments to the <code>get_estimator</code> function call from the CLI. The following code snippet shows the <code>get_estimator</code> method for our MNIST example: <pre><code>def get_estimator(epochs=2, batch_size=32, ...):\n...\n</code></pre></p> <p>Next, we try to change these arguments in two ways:</p> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_--arg", "title": "Using --arg", "text": "<p>To pass the arguments directly from the CLI we can use the <code>--arg</code> format. The following shows an example of how we can set the number of epochs to 3 and batch_size to 64:</p> <pre><code>$ fastestimator train mnist_tf.py --epochs 3 --batch_size 64\n</code></pre> <p></p>"}, {"location": "tutorial/beginner/t10_cli.html#using_a_json_file", "title": "Using a JSON file", "text": "<p>The other way we can send arguments is by using the <code>--hyperparameters</code> argument and passing it a json file containing all the training hyperparameters like epochs, batch_size, optimizer, etc. This option is really useful when you want to repeat the training job more than once and/or the list of the hyperparameter is getting really long. The following shows an example JSON file and how it could be used for our MNIST example: <pre><code>JSON:\n{\n    \"epochs\": 1,\n    \"batch_size\": 64\n}\n</code></pre> <pre><code>$ fastestimator train mnist_tf.py --hyperparameters hp.json\n</code></pre></p>"}]}